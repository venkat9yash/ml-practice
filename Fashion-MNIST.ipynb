{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "from collections import namedtuple\n",
    "import seaborn as sns; \n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "now = datetime.utcnow(). strftime(\"% Y% m% d% H% M% S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/ run-{}/\". format( root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### IMPORTING THE FASHION TEST AND TRAINING DATA INTO DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_file = 'fashionmnist/fashion-mnist_train.csv'\n",
    "test_file = 'fashionmnist/fashion-mnist_test.csv'\n",
    "train_df = pd.read_csv(training_file)\n",
    "test_df = pd.read_csv(test_file)\n",
    "# train_df.head(10)\n",
    "train_ds = train_df.values\n",
    "test_ds = test_df.values\n",
    "train_ds_X = train_ds[:,1:]\n",
    "test_ds_X = test_ds[:,1:]\n",
    "train_ds_Y = train_ds[:,:1]\n",
    "test_ds_Y = test_ds[:,:1]\n",
    "train_ds_X_rc = train_ds_X.reshape(-1,28,28)\n",
    "test_ds_X_rc = test_ds_X.reshape(-1,28,28)\n",
    "train_ds_X_shape = train_ds_X.reshape(-1,28,28,1)\n",
    "test_ds_X_val = test_ds_X_rc.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DISPLAY IT INTO IMAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "def show_image(search_index):\n",
    "    #search_index = 12343\n",
    "    label_classes = ['top','trouser','pullover','dress','coat','sandal','shirt','sneaker','bag','boot']\n",
    "    from matplotlib import pyplot as plt\n",
    "    print(\"IMAGE TYPE:\", label_classes[train_ds_Y[search_index][0]])\n",
    "    plt.imshow(train_ds_X_rc[search_index], interpolation='gaussian', cmap=\"gray\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE TYPE: pullover\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAICCAYAAABfmb61AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQggBQFFACCGEkAX/H1yH3GhnnyTjAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x12d5eb160>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 257,
       "width": 258
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(18333)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NORMALIZE AND ONE HOT CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(x, n_classes):\n",
    "    return np.eye(n_classes)[x.reshape(-1)]\n",
    "\n",
    "def normalize(train_ds_X):\n",
    "    norm_func = lambda t: t / 256\n",
    "    vfunc = np.vectorize(norm_func)\n",
    "    return vfunc(train_ds_X)\n",
    "train_ds_X_shape = normalize(train_ds_X_shape)\n",
    "train_ds_Y = one_hot_encode(train_ds_Y,10)\n",
    "test_ds_X_shape = normalize(test_ds_X_val)\n",
    "test_ds_Y = one_hot_encode(test_ds_Y,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "\n",
    "def get_data_batches(batch_size = 128):\n",
    "    X, Y = unison_shuffled_copies(train_ds_X_shape, train_ds_Y)\n",
    "    number_batches = int(len(X)/batch_size)\n",
    "    x_batches = np.array(np.array_split(X, number_batches))\n",
    "    y_batches = np.array(np.array_split(Y, number_batches))\n",
    "    return x_batches,y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(937,)\n"
     ]
    }
   ],
   "source": [
    "xx, yy = get_data_batches(64)\n",
    "print(xx.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BUILD THE CNN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BASIC INPUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_net_image_input(image_shape):\n",
    "    return tf.placeholder(tf.float32, shape=(None,) + image_shape , name=\"x\")\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    return tf.placeholder(tf.float32, shape=(None,n_classes) , name=\"y\")\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    return tf.placeholder(tf.float32, name=\"keep_prob\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### CONV LAYER WITH MAX POOLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \n",
    "    # Output depth\n",
    "    k_output = conv_num_outputs\n",
    "    \n",
    "    input_shape = x_tensor.get_shape().as_list()\n",
    "\n",
    "    # Image Properties\n",
    "    image_width = input_shape[1]\n",
    "    image_height = input_shape[2]\n",
    "    color_channels = input_shape[3]\n",
    "\n",
    "    # Convolution filter\n",
    "    filter_size_width = conv_ksize[0]\n",
    "    filter_size_height = conv_ksize[1]\n",
    "    \n",
    "    \n",
    "    # Weight and bias\n",
    "    weight = tf.Variable(tf.truncated_normal(\n",
    "        [filter_size_height, filter_size_width, color_channels, k_output]))\n",
    "    bias = tf.Variable(tf.zeros(k_output))\n",
    "    \n",
    "    #weight should be propotional to filter size\n",
    "    conv_layer = tf.nn.conv2d(x_tensor, weight, strides=[1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    \n",
    "    # Add bias\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, bias)\n",
    "    \n",
    "    # Apply activation function\n",
    "    \n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    \n",
    "    #Adding max pooling layer to it\n",
    "    \n",
    "    return tf.nn.max_pool(\n",
    "        conv_layer,\n",
    "        ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "        strides=[1, pool_strides[0],pool_strides[1], 1],\n",
    "        padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FLATTEN LAYER TO CONVERT 4D TO 2D (NUMBER BATCHES, FLATENNED DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten(x_tensor):\n",
    "    return tf.contrib.layers.flatten(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FULLY CONNECTED DENSE LAYER WITH RELU ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    return tf.contrib.layers.fully_connected(x_tensor, num_outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### OUTPUT A SIMPLE DENSE LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    return tf.layers.dense(inputs=x_tensor, units=num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BUILDING THE CONV NET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    # Currently we have two Fully Connected and one conv,(TODO: Try atleast 2 conv layers)\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "        \n",
    "    #Parameters input, conv net dimensions, filter sizes, stride dimensions, pool size, pool stride\n",
    "    conv_output = conv2d_maxpool(x, 10, (4,4), (2,2), (2,2), (2,2))\n",
    "    \n",
    "    conv_output = conv2d_maxpool(conv_output, 30, (2,2), (1,1), (1,1), (1,1))\n",
    "\n",
    "    # Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    flattened_output = flatten(conv_output)\n",
    "    \n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    fully_con_output = fully_conn(flattened_output, 256)\n",
    "    # Adding a drop out layer\n",
    "    fully_con_output = tf.nn.dropout(fully_con_output, keep_prob)\n",
    "    # Adding another layer\n",
    "    \n",
    "    fully_con_output = fully_conn(fully_con_output, 64)\n",
    "    # Adding a drop out layer\n",
    "    fully_con_output = tf.nn.dropout(fully_con_output, keep_prob)\n",
    "    \n",
    "    fully_con_output = fully_conn(fully_con_output, 32)\n",
    "      \n",
    "    #Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    final_output = output(fully_con_output, 10)\n",
    "    \n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((28, 28, 1))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "# Save logits with a particular name to use later\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "learning_rate=0.01\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### FUNCTION TO PRINT STATS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    # Calculate batch loss and accuracy\n",
    "    loss = session.run(cost, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "        keep_prob: 1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={\n",
    "        x: test_ds_X_shape,\n",
    "        y: test_ds_Y,\n",
    "        keep_prob: 1.})\n",
    "    print(\"Loss in this Epoch is:\",loss * 100,\"%\")\n",
    "    print(\"Accuracy in this Epoch is:\",valid_acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RUN NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    session.run(optimizer, feed_dict={\n",
    "        x: feature_batch,\n",
    "        y: label_batch,\n",
    "    keep_prob: keep_probability})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### HYPER PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 256\n",
    "keep_probability = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Loss in this Epoch is: 1160.06383896 %\n",
      "Accuracy in this Epoch is: 10.000000149 %\n",
      "Epoch  1, Batch 0 Loss:9.740909576416016\n",
      "Epoch  1, Batch 1 Loss:11.004676818847656\n",
      "Epoch  1, Batch 2 Loss:8.21949577331543\n",
      "Epoch  1, Batch 3 Loss:7.067757606506348\n",
      "Epoch  1, Batch 4 Loss:6.615631580352783\n",
      "Epoch  1, Batch 5 Loss:3.918010950088501\n",
      "Epoch  1, Batch 6 Loss:2.516124963760376\n",
      "Epoch  1, Batch 7 Loss:2.4978766441345215\n",
      "Epoch  1, Batch 8 Loss:3.098522901535034\n",
      "Epoch  1, Batch 9 Loss:2.7534830570220947\n",
      "Epoch  1, Batch 10 Loss:2.250993490219116\n",
      "Epoch  1, Batch 11 Loss:2.136115550994873\n",
      "Epoch  1, Batch 12 Loss:2.1812171936035156\n",
      "Epoch  1, Batch 13 Loss:1.9972983598709106\n",
      "Epoch  1, Batch 14 Loss:1.8469089269638062\n",
      "Epoch  1, Batch 15 Loss:1.8317807912826538\n",
      "Epoch  1, Batch 16 Loss:1.897744059562683\n",
      "Epoch  1, Batch 17 Loss:1.9299979209899902\n",
      "Epoch  1, Batch 18 Loss:1.864304542541504\n",
      "Epoch  1, Batch 19 Loss:1.8448574542999268\n",
      "Epoch  1, Batch 20 Loss:1.7827763557434082\n",
      "Epoch  1, Batch 21 Loss:1.725138783454895\n",
      "Epoch  1, Batch 22 Loss:1.7068772315979004\n",
      "Epoch  1, Batch 23 Loss:1.6812900304794312\n",
      "Epoch  1, Batch 24 Loss:1.6868126392364502\n",
      "Epoch  1, Batch 25 Loss:1.693105697631836\n",
      "Epoch  1, Batch 26 Loss:1.6402835845947266\n",
      "Epoch  1, Batch 27 Loss:1.6848888397216797\n",
      "Epoch  1, Batch 28 Loss:1.6182568073272705\n",
      "Epoch  1, Batch 29 Loss:1.6739475727081299\n",
      "Epoch  1, Batch 30 Loss:1.6378751993179321\n",
      "Epoch  1, Batch 31 Loss:1.6075977087020874\n",
      "Epoch  1, Batch 32 Loss:1.6016463041305542\n",
      "Epoch  1, Batch 33 Loss:1.6585533618927002\n",
      "Epoch  1, Batch 34 Loss:1.5546525716781616\n",
      "Epoch  1, Batch 35 Loss:1.6076847314834595\n",
      "Epoch  1, Batch 36 Loss:1.5838969945907593\n",
      "Epoch  1, Batch 37 Loss:1.5007014274597168\n",
      "Epoch  1, Batch 38 Loss:1.5952068567276\n",
      "Epoch  1, Batch 39 Loss:1.4623891115188599\n",
      "Epoch  1, Batch 40 Loss:1.43804132938385\n",
      "Epoch  1, Batch 41 Loss:1.5473511219024658\n",
      "Epoch  1, Batch 42 Loss:1.4861290454864502\n",
      "Epoch  1, Batch 43 Loss:1.4840562343597412\n",
      "Epoch  1, Batch 44 Loss:1.4437350034713745\n",
      "Epoch  1, Batch 45 Loss:1.3653775453567505\n",
      "Epoch  1, Batch 46 Loss:1.3166333436965942\n",
      "Epoch  1, Batch 47 Loss:1.316556453704834\n",
      "Epoch  1, Batch 48 Loss:1.3554391860961914\n",
      "Epoch  1, Batch 49 Loss:1.2424176931381226\n",
      "Epoch  1, Batch 50 Loss:1.1941136121749878\n",
      "Epoch  1, Batch 51 Loss:1.3475494384765625\n",
      "Epoch  1, Batch 52 Loss:1.2126741409301758\n",
      "Epoch  1, Batch 53 Loss:1.3179054260253906\n",
      "Epoch  1, Batch 54 Loss:1.1910738945007324\n",
      "Epoch  1, Batch 55 Loss:1.1356651782989502\n",
      "Epoch  1, Batch 56 Loss:1.2898609638214111\n",
      "Epoch  1, Batch 57 Loss:1.2160252332687378\n",
      "Epoch  1, Batch 58 Loss:1.1218430995941162\n",
      "Epoch  1, Batch 59 Loss:1.1466683149337769\n",
      "Epoch  1, Batch 60 Loss:1.150519847869873\n",
      "Epoch  1, Batch 61 Loss:1.056564211845398\n",
      "Epoch  1, Batch 62 Loss:1.1563587188720703\n",
      "Epoch  1, Batch 63 Loss:1.0354267358779907\n",
      "Epoch  1, Batch 64 Loss:1.0042026042938232\n",
      "Epoch  1, Batch 65 Loss:1.0352123975753784\n",
      "Epoch  1, Batch 66 Loss:1.0479902029037476\n",
      "Epoch  1, Batch 67 Loss:0.9937395453453064\n",
      "Epoch  1, Batch 68 Loss:1.0265748500823975\n",
      "Epoch  1, Batch 69 Loss:0.8470748066902161\n",
      "Epoch  1, Batch 70 Loss:0.9057170152664185\n",
      "Epoch  1, Batch 71 Loss:0.8956305384635925\n",
      "Epoch  1, Batch 72 Loss:0.9660053253173828\n",
      "Epoch  1, Batch 73 Loss:0.9626450538635254\n",
      "Epoch  1, Batch 74 Loss:0.8861250281333923\n",
      "Epoch  1, Batch 75 Loss:0.9753994345664978\n",
      "Epoch  1, Batch 76 Loss:0.8940606713294983\n",
      "Epoch  1, Batch 77 Loss:0.7184219360351562\n",
      "Epoch  1, Batch 78 Loss:0.7102182507514954\n",
      "Epoch  1, Batch 79 Loss:0.9046751260757446\n",
      "Epoch  1, Batch 80 Loss:1.019923448562622\n",
      "Epoch  1, Batch 81 Loss:0.7171556949615479\n",
      "Epoch  1, Batch 82 Loss:0.6987347602844238\n",
      "Epoch  1, Batch 83 Loss:0.7995327115058899\n",
      "Epoch  1, Batch 84 Loss:0.7317500114440918\n",
      "Epoch  1, Batch 85 Loss:0.7872558236122131\n",
      "Epoch  1, Batch 86 Loss:0.6658652424812317\n",
      "Epoch  1, Batch 87 Loss:0.7371086478233337\n",
      "Epoch  1, Batch 88 Loss:0.6381834149360657\n",
      "Epoch  1, Batch 89 Loss:0.6813288927078247\n",
      "Epoch  1, Batch 90 Loss:0.7344411015510559\n",
      "Epoch  1, Batch 91 Loss:0.681930422782898\n",
      "Epoch  1, Batch 92 Loss:0.6281055212020874\n",
      "Epoch  1, Batch 93 Loss:0.6134797930717468\n",
      "Epoch  1, Batch 94 Loss:0.6230850219726562\n",
      "Epoch  1, Batch 95 Loss:0.6072370409965515\n",
      "Epoch  1, Batch 96 Loss:0.6699655055999756\n",
      "Epoch  1, Batch 97 Loss:0.6263632774353027\n",
      "Epoch  1, Batch 98 Loss:0.6722004413604736\n",
      "Epoch  1, Batch 99 Loss:0.5487433671951294\n",
      "Epoch  1, Batch 100 Loss:0.5505602359771729\n",
      "Epoch  1, Batch 101 Loss:0.5936682224273682\n",
      "Epoch  1, Batch 102 Loss:0.5731230974197388\n",
      "Epoch  1, Batch 103 Loss:0.5962653756141663\n",
      "Epoch  1, Batch 104 Loss:0.5830931663513184\n",
      "Epoch  1, Batch 105 Loss:0.6487482786178589\n",
      "Epoch  1, Batch 106 Loss:0.47649210691452026\n",
      "Epoch  1, Batch 107 Loss:0.6191962361335754\n",
      "Epoch  1, Batch 108 Loss:0.5812637805938721\n",
      "Epoch  1, Batch 109 Loss:0.5366275906562805\n",
      "Epoch  1, Batch 110 Loss:0.6077291965484619\n",
      "Epoch  1, Batch 111 Loss:0.5036291480064392\n",
      "Epoch  1, Batch 112 Loss:0.5608158111572266\n",
      "Epoch  1, Batch 113 Loss:0.5458874702453613\n",
      "Epoch  1, Batch 114 Loss:0.531291127204895\n",
      "Epoch  1, Batch 115 Loss:0.5942368507385254\n",
      "Epoch  1, Batch 116 Loss:0.6241450905799866\n",
      "Epoch  1, Batch 117 Loss:0.5944214463233948\n",
      "Epoch  1, Batch 118 Loss:0.6111412048339844\n",
      "Epoch  1, Batch 119 Loss:0.5777118802070618\n",
      "Epoch  1, Batch 120 Loss:0.5290502905845642\n",
      "Epoch  1, Batch 121 Loss:0.5121124982833862\n",
      "Epoch  1, Batch 122 Loss:0.6315702199935913\n",
      "Epoch  1, Batch 123 Loss:0.621222972869873\n",
      "Epoch  1, Batch 124 Loss:0.5432042479515076\n",
      "Epoch  1, Batch 125 Loss:0.5805996656417847\n",
      "Epoch  1, Batch 126 Loss:0.5514521598815918\n",
      "Epoch  1, Batch 127 Loss:0.444026380777359\n",
      "Epoch  1, Batch 128 Loss:0.5935808420181274\n",
      "Epoch  1, Batch 129 Loss:0.498710036277771\n",
      "Epoch  1, Batch 130 Loss:0.57530677318573\n",
      "Epoch  1, Batch 131 Loss:0.5447000861167908\n",
      "Epoch  1, Batch 132 Loss:0.5000776052474976\n",
      "Epoch  1, Batch 133 Loss:0.4655371904373169\n",
      "Epoch  1, Batch 134 Loss:0.5450114011764526\n",
      "Epoch  1, Batch 135 Loss:0.488639771938324\n",
      "Epoch  1, Batch 136 Loss:0.5056449174880981\n",
      "Epoch  1, Batch 137 Loss:0.45155084133148193\n",
      "Epoch  1, Batch 138 Loss:0.4395856261253357\n",
      "Epoch  1, Batch 139 Loss:0.46267083287239075\n",
      "Epoch  1, Batch 140 Loss:0.5505267977714539\n",
      "Epoch  1, Batch 141 Loss:0.4619966447353363\n",
      "Epoch  1, Batch 142 Loss:0.5519525408744812\n",
      "Epoch  1, Batch 143 Loss:0.43310412764549255\n",
      "Epoch  1, Batch 144 Loss:0.4704962372779846\n",
      "Epoch  1, Batch 145 Loss:0.5217228531837463\n",
      "Epoch  1, Batch 146 Loss:0.44012701511383057\n",
      "Epoch  1, Batch 147 Loss:0.5156253576278687\n",
      "Epoch  1, Batch 148 Loss:0.5427089929580688\n",
      "Epoch  1, Batch 149 Loss:0.597631573677063\n",
      "Epoch  1, Batch 150 Loss:0.5755537748336792\n",
      "Epoch  1, Batch 151 Loss:0.47524672746658325\n",
      "Epoch  1, Batch 152 Loss:0.5015730857849121\n",
      "Epoch  1, Batch 153 Loss:0.545250654220581\n",
      "Epoch  1, Batch 154 Loss:0.6518286466598511\n",
      "Epoch  1, Batch 155 Loss:0.4315448999404907\n",
      "Epoch  1, Batch 156 Loss:0.49035730957984924\n",
      "Epoch  1, Batch 157 Loss:0.5134627819061279\n",
      "Epoch  1, Batch 158 Loss:0.5053266882896423\n",
      "Epoch  1, Batch 159 Loss:0.5119630098342896\n",
      "Epoch  1, Batch 160 Loss:0.5413415431976318\n",
      "Epoch  1, Batch 161 Loss:0.4782450199127197\n",
      "Epoch  1, Batch 162 Loss:0.5347105264663696\n",
      "Epoch  1, Batch 163 Loss:0.5342587232589722\n",
      "Epoch  1, Batch 164 Loss:0.4826938509941101\n",
      "Epoch  1, Batch 165 Loss:0.6015630960464478\n",
      "Epoch  1, Batch 166 Loss:0.5060250759124756\n",
      "Epoch  1, Batch 167 Loss:0.4235745072364807\n",
      "Epoch  1, Batch 168 Loss:0.43168607354164124\n",
      "Epoch  1, Batch 169 Loss:0.5626117587089539\n",
      "Epoch  1, Batch 170 Loss:0.5291076302528381\n",
      "Epoch  1, Batch 171 Loss:0.5953828692436218\n",
      "Epoch  1, Batch 172 Loss:0.5475330948829651\n",
      "Epoch  1, Batch 173 Loss:0.44088080525398254\n",
      "Epoch  1, Batch 174 Loss:0.45746713876724243\n",
      "Epoch  1, Batch 175 Loss:0.5182012319564819\n",
      "Epoch  1, Batch 176 Loss:0.4577268362045288\n",
      "Epoch  1, Batch 177 Loss:0.4728720188140869\n",
      "Epoch  1, Batch 178 Loss:0.5441444516181946\n",
      "Epoch  1, Batch 179 Loss:0.5085888504981995\n",
      "Epoch  1, Batch 180 Loss:0.5628165006637573\n",
      "Epoch  1, Batch 181 Loss:0.44866859912872314\n",
      "Epoch  1, Batch 182 Loss:0.5651410818099976\n",
      "Epoch  1, Batch 183 Loss:0.5214364528656006\n",
      "Epoch  1, Batch 184 Loss:0.5289297103881836\n",
      "Epoch  1, Batch 185 Loss:0.5374572277069092\n",
      "Epoch  1, Batch 186 Loss:0.38287651538848877\n",
      "Epoch  1, Batch 187 Loss:0.49098414182662964\n",
      "Epoch  1, Batch 188 Loss:0.4623594880104065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1, Batch 189 Loss:0.4769638776779175\n",
      "Epoch  1, Batch 190 Loss:0.4143062233924866\n",
      "Epoch  1, Batch 191 Loss:0.37721341848373413\n",
      "Epoch  1, Batch 192 Loss:0.4711039960384369\n",
      "Epoch  1, Batch 193 Loss:0.3885910212993622\n",
      "Epoch  1, Batch 194 Loss:0.46453413367271423\n",
      "Epoch  1, Batch 195 Loss:0.5508047938346863\n",
      "Epoch  1, Batch 196 Loss:0.4441644549369812\n",
      "Epoch  1, Batch 197 Loss:0.5147930383682251\n",
      "Epoch  1, Batch 198 Loss:0.4892686605453491\n",
      "Epoch  1, Batch 199 Loss:0.3703519105911255\n",
      "Epoch  1, Batch 200 Loss:0.42388927936553955\n",
      "Epoch  1, Batch 201 Loss:0.3553289473056793\n",
      "Epoch  1, Batch 202 Loss:0.4556238651275635\n",
      "Epoch  1, Batch 203 Loss:0.3917774260044098\n",
      "Epoch  1, Batch 204 Loss:0.42340338230133057\n",
      "Epoch  1, Batch 205 Loss:0.4777824282646179\n",
      "Epoch  1, Batch 206 Loss:0.45123785734176636\n",
      "Epoch  1, Batch 207 Loss:0.4164358377456665\n",
      "Epoch  1, Batch 208 Loss:0.4627704918384552\n",
      "Epoch  1, Batch 209 Loss:0.431082546710968\n",
      "Epoch  1, Batch 210 Loss:0.4809848368167877\n",
      "Epoch  1, Batch 211 Loss:0.37728238105773926\n",
      "Epoch  1, Batch 212 Loss:0.4779479205608368\n",
      "Epoch  1, Batch 213 Loss:0.49618810415267944\n",
      "Epoch  1, Batch 214 Loss:0.3654782772064209\n",
      "Epoch  1, Batch 215 Loss:0.46701180934906006\n",
      "Epoch  1, Batch 216 Loss:0.44655001163482666\n",
      "Epoch  1, Batch 217 Loss:0.4513514041900635\n",
      "Epoch  1, Batch 218 Loss:0.3979942202568054\n",
      "Epoch  1, Batch 219 Loss:0.38056454062461853\n",
      "Epoch  1, Batch 220 Loss:0.46435487270355225\n",
      "Epoch  1, Batch 221 Loss:0.48786383867263794\n",
      "Epoch  1, Batch 222 Loss:0.3687416911125183\n",
      "Epoch  1, Batch 223 Loss:0.40977153182029724\n",
      "Epoch  1, Batch 224 Loss:0.3272596597671509\n",
      "Epoch  1, Batch 225 Loss:0.36594724655151367\n",
      "Epoch  1, Batch 226 Loss:0.43898797035217285\n",
      "Epoch  1, Batch 227 Loss:0.38283079862594604\n",
      "Epoch  1, Batch 228 Loss:0.4624122381210327\n",
      "Epoch  1, Batch 229 Loss:0.48769307136535645\n",
      "Epoch  1, Batch 230 Loss:0.5569414496421814\n",
      "Epoch  1, Batch 231 Loss:0.368745893239975\n",
      "Epoch  1, Batch 232 Loss:0.4497278034687042\n",
      "Epoch  1, Batch 233 Loss:0.3939063251018524\n",
      "Loss in this Epoch is: 39.3906325102 %\n",
      "Accuracy in this Epoch is: 82.8400015831 %\n",
      "Epoch  2, Batch 0 Loss:0.36387595534324646\n",
      "Epoch  2, Batch 1 Loss:0.29790887236595154\n",
      "Epoch  2, Batch 2 Loss:0.4732665717601776\n",
      "Epoch  2, Batch 3 Loss:0.40705493092536926\n",
      "Epoch  2, Batch 4 Loss:0.4324902892112732\n",
      "Epoch  2, Batch 5 Loss:0.4318910837173462\n",
      "Epoch  2, Batch 6 Loss:0.4385222792625427\n",
      "Epoch  2, Batch 7 Loss:0.4649341404438019\n",
      "Epoch  2, Batch 8 Loss:0.45585179328918457\n",
      "Epoch  2, Batch 9 Loss:0.35098934173583984\n",
      "Epoch  2, Batch 10 Loss:0.492880254983902\n",
      "Epoch  2, Batch 11 Loss:0.4257674813270569\n",
      "Epoch  2, Batch 12 Loss:0.4009888172149658\n",
      "Epoch  2, Batch 13 Loss:0.32084399461746216\n",
      "Epoch  2, Batch 14 Loss:0.36853140592575073\n",
      "Epoch  2, Batch 15 Loss:0.30628105998039246\n",
      "Epoch  2, Batch 16 Loss:0.37921035289764404\n",
      "Epoch  2, Batch 17 Loss:0.40939685702323914\n",
      "Epoch  2, Batch 18 Loss:0.5528646111488342\n",
      "Epoch  2, Batch 19 Loss:0.40441522002220154\n",
      "Epoch  2, Batch 20 Loss:0.4362018406391144\n",
      "Epoch  2, Batch 21 Loss:0.3011161684989929\n",
      "Epoch  2, Batch 22 Loss:0.34836292266845703\n",
      "Epoch  2, Batch 23 Loss:0.4832576513290405\n",
      "Epoch  2, Batch 24 Loss:0.4133555293083191\n",
      "Epoch  2, Batch 25 Loss:0.2988303601741791\n",
      "Epoch  2, Batch 26 Loss:0.40840283036231995\n",
      "Epoch  2, Batch 27 Loss:0.3927551805973053\n",
      "Epoch  2, Batch 28 Loss:0.39281564950942993\n",
      "Epoch  2, Batch 29 Loss:0.32929593324661255\n",
      "Epoch  2, Batch 30 Loss:0.4301130175590515\n",
      "Epoch  2, Batch 31 Loss:0.3433852195739746\n",
      "Epoch  2, Batch 32 Loss:0.4027927815914154\n",
      "Epoch  2, Batch 33 Loss:0.3457399010658264\n",
      "Epoch  2, Batch 34 Loss:0.3789077401161194\n",
      "Epoch  2, Batch 35 Loss:0.42390176653862\n",
      "Epoch  2, Batch 36 Loss:0.3482683598995209\n",
      "Epoch  2, Batch 37 Loss:0.4006494879722595\n",
      "Epoch  2, Batch 38 Loss:0.3313516676425934\n",
      "Epoch  2, Batch 39 Loss:0.3567594587802887\n",
      "Epoch  2, Batch 40 Loss:0.2915782928466797\n",
      "Epoch  2, Batch 41 Loss:0.47144150733947754\n",
      "Epoch  2, Batch 42 Loss:0.3533039093017578\n",
      "Epoch  2, Batch 43 Loss:0.38062578439712524\n",
      "Epoch  2, Batch 44 Loss:0.46197283267974854\n",
      "Epoch  2, Batch 45 Loss:0.372213214635849\n",
      "Epoch  2, Batch 46 Loss:0.46050506830215454\n",
      "Epoch  2, Batch 47 Loss:0.4525493085384369\n",
      "Epoch  2, Batch 48 Loss:0.4363960027694702\n",
      "Epoch  2, Batch 49 Loss:0.3248736262321472\n",
      "Epoch  2, Batch 50 Loss:0.3711875379085541\n",
      "Epoch  2, Batch 51 Loss:0.3389732837677002\n",
      "Epoch  2, Batch 52 Loss:0.3671674132347107\n",
      "Epoch  2, Batch 53 Loss:0.37563103437423706\n",
      "Epoch  2, Batch 54 Loss:0.31753894686698914\n",
      "Epoch  2, Batch 55 Loss:0.3927710950374603\n",
      "Epoch  2, Batch 56 Loss:0.4574996531009674\n",
      "Epoch  2, Batch 57 Loss:0.3012484908103943\n",
      "Epoch  2, Batch 58 Loss:0.37047260999679565\n",
      "Epoch  2, Batch 59 Loss:0.4617607295513153\n",
      "Epoch  2, Batch 60 Loss:0.3511585295200348\n",
      "Epoch  2, Batch 61 Loss:0.48331987857818604\n",
      "Epoch  2, Batch 62 Loss:0.37879496812820435\n",
      "Epoch  2, Batch 63 Loss:0.41401997208595276\n",
      "Epoch  2, Batch 64 Loss:0.4029313921928406\n",
      "Epoch  2, Batch 65 Loss:0.4297678768634796\n",
      "Epoch  2, Batch 66 Loss:0.30099937319755554\n",
      "Epoch  2, Batch 67 Loss:0.4461984634399414\n",
      "Epoch  2, Batch 68 Loss:0.3274277150630951\n",
      "Epoch  2, Batch 69 Loss:0.43043315410614014\n",
      "Epoch  2, Batch 70 Loss:0.3585563004016876\n",
      "Epoch  2, Batch 71 Loss:0.40276020765304565\n",
      "Epoch  2, Batch 72 Loss:0.37136563658714294\n",
      "Epoch  2, Batch 73 Loss:0.40158867835998535\n",
      "Epoch  2, Batch 74 Loss:0.4153262674808502\n",
      "Epoch  2, Batch 75 Loss:0.44325679540634155\n",
      "Epoch  2, Batch 76 Loss:0.3735238015651703\n",
      "Epoch  2, Batch 77 Loss:0.2534216642379761\n",
      "Epoch  2, Batch 78 Loss:0.42403003573417664\n",
      "Epoch  2, Batch 79 Loss:0.34755071997642517\n",
      "Epoch  2, Batch 80 Loss:0.3187020421028137\n",
      "Epoch  2, Batch 81 Loss:0.400750994682312\n",
      "Epoch  2, Batch 82 Loss:0.3061608672142029\n",
      "Epoch  2, Batch 83 Loss:0.45278027653694153\n",
      "Epoch  2, Batch 84 Loss:0.34756234288215637\n",
      "Epoch  2, Batch 85 Loss:0.4113254249095917\n",
      "Epoch  2, Batch 86 Loss:0.4444020986557007\n",
      "Epoch  2, Batch 87 Loss:0.36496207118034363\n",
      "Epoch  2, Batch 88 Loss:0.4101501703262329\n",
      "Epoch  2, Batch 89 Loss:0.46572253108024597\n",
      "Epoch  2, Batch 90 Loss:0.3478030264377594\n",
      "Epoch  2, Batch 91 Loss:0.3565148711204529\n",
      "Epoch  2, Batch 92 Loss:0.37231048941612244\n",
      "Epoch  2, Batch 93 Loss:0.41827040910720825\n",
      "Epoch  2, Batch 94 Loss:0.42873328924179077\n",
      "Epoch  2, Batch 95 Loss:0.3587101101875305\n",
      "Epoch  2, Batch 96 Loss:0.2617572247982025\n",
      "Epoch  2, Batch 97 Loss:0.3633096218109131\n",
      "Epoch  2, Batch 98 Loss:0.3418969213962555\n",
      "Epoch  2, Batch 99 Loss:0.44497373700141907\n",
      "Epoch  2, Batch 100 Loss:0.344667911529541\n",
      "Epoch  2, Batch 101 Loss:0.3791061043739319\n",
      "Epoch  2, Batch 102 Loss:0.36184754967689514\n",
      "Epoch  2, Batch 103 Loss:0.2833549678325653\n",
      "Epoch  2, Batch 104 Loss:0.3599236011505127\n",
      "Epoch  2, Batch 105 Loss:0.407968133687973\n",
      "Epoch  2, Batch 106 Loss:0.36663535237312317\n",
      "Epoch  2, Batch 107 Loss:0.31046462059020996\n",
      "Epoch  2, Batch 108 Loss:0.36235499382019043\n",
      "Epoch  2, Batch 109 Loss:0.35092398524284363\n",
      "Epoch  2, Batch 110 Loss:0.4458548128604889\n",
      "Epoch  2, Batch 111 Loss:0.3957916498184204\n",
      "Epoch  2, Batch 112 Loss:0.357133150100708\n",
      "Epoch  2, Batch 113 Loss:0.40624934434890747\n",
      "Epoch  2, Batch 114 Loss:0.2992706894874573\n",
      "Epoch  2, Batch 115 Loss:0.3478371798992157\n",
      "Epoch  2, Batch 116 Loss:0.4754794239997864\n",
      "Epoch  2, Batch 117 Loss:0.34529322385787964\n",
      "Epoch  2, Batch 118 Loss:0.3965919613838196\n",
      "Epoch  2, Batch 119 Loss:0.32099348306655884\n",
      "Epoch  2, Batch 120 Loss:0.38160720467567444\n",
      "Epoch  2, Batch 121 Loss:0.3550284802913666\n",
      "Epoch  2, Batch 122 Loss:0.39072003960609436\n",
      "Epoch  2, Batch 123 Loss:0.33784040808677673\n",
      "Epoch  2, Batch 124 Loss:0.3942520022392273\n",
      "Epoch  2, Batch 125 Loss:0.429592490196228\n",
      "Epoch  2, Batch 126 Loss:0.29327940940856934\n",
      "Epoch  2, Batch 127 Loss:0.41176483035087585\n",
      "Epoch  2, Batch 128 Loss:0.34857386350631714\n",
      "Epoch  2, Batch 129 Loss:0.3531857430934906\n",
      "Epoch  2, Batch 130 Loss:0.3081037104129791\n",
      "Epoch  2, Batch 131 Loss:0.3525295853614807\n",
      "Epoch  2, Batch 132 Loss:0.36766934394836426\n",
      "Epoch  2, Batch 133 Loss:0.4568643867969513\n",
      "Epoch  2, Batch 134 Loss:0.34807878732681274\n",
      "Epoch  2, Batch 135 Loss:0.2838614583015442\n",
      "Epoch  2, Batch 136 Loss:0.44442635774612427\n",
      "Epoch  2, Batch 137 Loss:0.343914270401001\n",
      "Epoch  2, Batch 138 Loss:0.39138758182525635\n",
      "Epoch  2, Batch 139 Loss:0.4502262473106384\n",
      "Epoch  2, Batch 140 Loss:0.31416332721710205\n",
      "Epoch  2, Batch 141 Loss:0.4131086766719818\n",
      "Epoch  2, Batch 142 Loss:0.3518129289150238\n",
      "Epoch  2, Batch 143 Loss:0.37038975954055786\n",
      "Epoch  2, Batch 144 Loss:0.31201672554016113\n",
      "Epoch  2, Batch 145 Loss:0.3432387113571167\n",
      "Epoch  2, Batch 146 Loss:0.3556474447250366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2, Batch 147 Loss:0.31553906202316284\n",
      "Epoch  2, Batch 148 Loss:0.38409334421157837\n",
      "Epoch  2, Batch 149 Loss:0.3395557999610901\n",
      "Epoch  2, Batch 150 Loss:0.42775681614875793\n",
      "Epoch  2, Batch 151 Loss:0.4644981026649475\n",
      "Epoch  2, Batch 152 Loss:0.3174310028553009\n",
      "Epoch  2, Batch 153 Loss:0.36940962076187134\n",
      "Epoch  2, Batch 154 Loss:0.3153638243675232\n",
      "Epoch  2, Batch 155 Loss:0.2783392071723938\n",
      "Epoch  2, Batch 156 Loss:0.3094555139541626\n",
      "Epoch  2, Batch 157 Loss:0.2780824303627014\n",
      "Epoch  2, Batch 158 Loss:0.3642043471336365\n",
      "Epoch  2, Batch 159 Loss:0.28169822692871094\n",
      "Epoch  2, Batch 160 Loss:0.33371222019195557\n",
      "Epoch  2, Batch 161 Loss:0.3731667399406433\n",
      "Epoch  2, Batch 162 Loss:0.3371858298778534\n",
      "Epoch  2, Batch 163 Loss:0.39834463596343994\n",
      "Epoch  2, Batch 164 Loss:0.3111205995082855\n",
      "Epoch  2, Batch 165 Loss:0.28583866357803345\n",
      "Epoch  2, Batch 166 Loss:0.35390567779541016\n",
      "Epoch  2, Batch 167 Loss:0.3338068127632141\n",
      "Epoch  2, Batch 168 Loss:0.3539015054702759\n",
      "Epoch  2, Batch 169 Loss:0.3408085107803345\n",
      "Epoch  2, Batch 170 Loss:0.33622050285339355\n",
      "Epoch  2, Batch 171 Loss:0.5164845585823059\n",
      "Epoch  2, Batch 172 Loss:0.2735294699668884\n",
      "Epoch  2, Batch 173 Loss:0.39608657360076904\n",
      "Epoch  2, Batch 174 Loss:0.3890290856361389\n",
      "Epoch  2, Batch 175 Loss:0.40003305673599243\n",
      "Epoch  2, Batch 176 Loss:0.3369579315185547\n",
      "Epoch  2, Batch 177 Loss:0.37560606002807617\n",
      "Epoch  2, Batch 178 Loss:0.3153436481952667\n",
      "Epoch  2, Batch 179 Loss:0.3405975103378296\n",
      "Epoch  2, Batch 180 Loss:0.3667841851711273\n",
      "Epoch  2, Batch 181 Loss:0.39261776208877563\n",
      "Epoch  2, Batch 182 Loss:0.2824525237083435\n",
      "Epoch  2, Batch 183 Loss:0.3668675422668457\n",
      "Epoch  2, Batch 184 Loss:0.30493462085723877\n",
      "Epoch  2, Batch 185 Loss:0.36178022623062134\n",
      "Epoch  2, Batch 186 Loss:0.37727949023246765\n",
      "Epoch  2, Batch 187 Loss:0.4180542826652527\n",
      "Epoch  2, Batch 188 Loss:0.3795969486236572\n",
      "Epoch  2, Batch 189 Loss:0.34503400325775146\n",
      "Epoch  2, Batch 190 Loss:0.3788984417915344\n",
      "Epoch  2, Batch 191 Loss:0.45217496156692505\n",
      "Epoch  2, Batch 192 Loss:0.3428021967411041\n",
      "Epoch  2, Batch 193 Loss:0.3269379138946533\n",
      "Epoch  2, Batch 194 Loss:0.38993576169013977\n",
      "Epoch  2, Batch 195 Loss:0.3800699710845947\n",
      "Epoch  2, Batch 196 Loss:0.3095426857471466\n",
      "Epoch  2, Batch 197 Loss:0.3272286057472229\n",
      "Epoch  2, Batch 198 Loss:0.351568341255188\n",
      "Epoch  2, Batch 199 Loss:0.31278008222579956\n",
      "Epoch  2, Batch 200 Loss:0.41913771629333496\n",
      "Epoch  2, Batch 201 Loss:0.343838632106781\n",
      "Epoch  2, Batch 202 Loss:0.4413425028324127\n",
      "Epoch  2, Batch 203 Loss:0.39986932277679443\n",
      "Epoch  2, Batch 204 Loss:0.36982202529907227\n",
      "Epoch  2, Batch 205 Loss:0.3548884093761444\n",
      "Epoch  2, Batch 206 Loss:0.501954972743988\n",
      "Epoch  2, Batch 207 Loss:0.32397323846817017\n",
      "Epoch  2, Batch 208 Loss:0.3684438169002533\n",
      "Epoch  2, Batch 209 Loss:0.37821462750434875\n",
      "Epoch  2, Batch 210 Loss:0.33140432834625244\n",
      "Epoch  2, Batch 211 Loss:0.36550185084342957\n",
      "Epoch  2, Batch 212 Loss:0.33387240767478943\n",
      "Epoch  2, Batch 213 Loss:0.3504796028137207\n",
      "Epoch  2, Batch 214 Loss:0.38125884532928467\n",
      "Epoch  2, Batch 215 Loss:0.30252841114997864\n",
      "Epoch  2, Batch 216 Loss:0.3494603633880615\n",
      "Epoch  2, Batch 217 Loss:0.291434645652771\n",
      "Epoch  2, Batch 218 Loss:0.3061297535896301\n",
      "Epoch  2, Batch 219 Loss:0.4331745505332947\n",
      "Epoch  2, Batch 220 Loss:0.33920687437057495\n",
      "Epoch  2, Batch 221 Loss:0.32779765129089355\n",
      "Epoch  2, Batch 222 Loss:0.41272974014282227\n",
      "Epoch  2, Batch 223 Loss:0.39552146196365356\n",
      "Epoch  2, Batch 224 Loss:0.34760889410972595\n",
      "Epoch  2, Batch 225 Loss:0.4284747242927551\n",
      "Epoch  2, Batch 226 Loss:0.3054177761077881\n",
      "Epoch  2, Batch 227 Loss:0.39650022983551025\n",
      "Epoch  2, Batch 228 Loss:0.3635069727897644\n",
      "Epoch  2, Batch 229 Loss:0.3513649106025696\n",
      "Epoch  2, Batch 230 Loss:0.4311995804309845\n",
      "Epoch  2, Batch 231 Loss:0.36305588483810425\n",
      "Epoch  2, Batch 232 Loss:0.4080262780189514\n",
      "Epoch  2, Batch 233 Loss:0.3901158571243286\n",
      "Loss in this Epoch is: 39.0115857124 %\n",
      "Accuracy in this Epoch is: 86.8799984455 %\n",
      "Epoch  3, Batch 0 Loss:0.32221654057502747\n",
      "Epoch  3, Batch 1 Loss:0.332400381565094\n",
      "Epoch  3, Batch 2 Loss:0.32115352153778076\n",
      "Epoch  3, Batch 3 Loss:0.2767092287540436\n",
      "Epoch  3, Batch 4 Loss:0.26861169934272766\n",
      "Epoch  3, Batch 5 Loss:0.2844555377960205\n",
      "Epoch  3, Batch 6 Loss:0.305270791053772\n",
      "Epoch  3, Batch 7 Loss:0.300493448972702\n",
      "Epoch  3, Batch 8 Loss:0.3783513307571411\n",
      "Epoch  3, Batch 9 Loss:0.22271059453487396\n",
      "Epoch  3, Batch 10 Loss:0.315045028924942\n",
      "Epoch  3, Batch 11 Loss:0.3157421052455902\n",
      "Epoch  3, Batch 12 Loss:0.32519134879112244\n",
      "Epoch  3, Batch 13 Loss:0.26026076078414917\n",
      "Epoch  3, Batch 14 Loss:0.24145586788654327\n",
      "Epoch  3, Batch 15 Loss:0.31569811701774597\n",
      "Epoch  3, Batch 16 Loss:0.3732490539550781\n",
      "Epoch  3, Batch 17 Loss:0.35667547583580017\n",
      "Epoch  3, Batch 18 Loss:0.28655001521110535\n",
      "Epoch  3, Batch 19 Loss:0.3278425335884094\n",
      "Epoch  3, Batch 20 Loss:0.29574981331825256\n",
      "Epoch  3, Batch 21 Loss:0.31825804710388184\n",
      "Epoch  3, Batch 22 Loss:0.3812814950942993\n",
      "Epoch  3, Batch 23 Loss:0.286173015832901\n",
      "Epoch  3, Batch 24 Loss:0.2887682020664215\n",
      "Epoch  3, Batch 25 Loss:0.2977173626422882\n",
      "Epoch  3, Batch 26 Loss:0.35217908024787903\n",
      "Epoch  3, Batch 27 Loss:0.28803014755249023\n",
      "Epoch  3, Batch 28 Loss:0.3368699550628662\n",
      "Epoch  3, Batch 29 Loss:0.21251586079597473\n",
      "Epoch  3, Batch 30 Loss:0.3562852740287781\n",
      "Epoch  3, Batch 31 Loss:0.2992563247680664\n",
      "Epoch  3, Batch 32 Loss:0.31449705362319946\n",
      "Epoch  3, Batch 33 Loss:0.28996357321739197\n",
      "Epoch  3, Batch 34 Loss:0.38657328486442566\n",
      "Epoch  3, Batch 35 Loss:0.37534478306770325\n",
      "Epoch  3, Batch 36 Loss:0.354285329580307\n",
      "Epoch  3, Batch 37 Loss:0.31588172912597656\n",
      "Epoch  3, Batch 38 Loss:0.3722881078720093\n",
      "Epoch  3, Batch 39 Loss:0.24713079631328583\n",
      "Epoch  3, Batch 40 Loss:0.28226688504219055\n",
      "Epoch  3, Batch 41 Loss:0.3449150025844574\n",
      "Epoch  3, Batch 42 Loss:0.31025058031082153\n",
      "Epoch  3, Batch 43 Loss:0.22752097249031067\n",
      "Epoch  3, Batch 44 Loss:0.2954266667366028\n",
      "Epoch  3, Batch 45 Loss:0.30148765444755554\n",
      "Epoch  3, Batch 46 Loss:0.29693228006362915\n",
      "Epoch  3, Batch 47 Loss:0.304661363363266\n",
      "Epoch  3, Batch 48 Loss:0.2687318027019501\n",
      "Epoch  3, Batch 49 Loss:0.3425491154193878\n",
      "Epoch  3, Batch 50 Loss:0.3977941572666168\n",
      "Epoch  3, Batch 51 Loss:0.3745335638523102\n",
      "Epoch  3, Batch 52 Loss:0.271637499332428\n",
      "Epoch  3, Batch 53 Loss:0.29834070801734924\n",
      "Epoch  3, Batch 54 Loss:0.29207098484039307\n",
      "Epoch  3, Batch 55 Loss:0.34992027282714844\n",
      "Epoch  3, Batch 56 Loss:0.31365498900413513\n",
      "Epoch  3, Batch 57 Loss:0.29204392433166504\n",
      "Epoch  3, Batch 58 Loss:0.23027364909648895\n",
      "Epoch  3, Batch 59 Loss:0.2584608793258667\n",
      "Epoch  3, Batch 60 Loss:0.3748141825199127\n",
      "Epoch  3, Batch 61 Loss:0.36360082030296326\n",
      "Epoch  3, Batch 62 Loss:0.2559792995452881\n",
      "Epoch  3, Batch 63 Loss:0.29030561447143555\n",
      "Epoch  3, Batch 64 Loss:0.3667416274547577\n",
      "Epoch  3, Batch 65 Loss:0.2741115391254425\n",
      "Epoch  3, Batch 66 Loss:0.28136053681373596\n",
      "Epoch  3, Batch 67 Loss:0.31118637323379517\n",
      "Epoch  3, Batch 68 Loss:0.2326292246580124\n",
      "Epoch  3, Batch 69 Loss:0.23313027620315552\n",
      "Epoch  3, Batch 70 Loss:0.26746267080307007\n",
      "Epoch  3, Batch 71 Loss:0.31601232290267944\n",
      "Epoch  3, Batch 72 Loss:0.3330438733100891\n",
      "Epoch  3, Batch 73 Loss:0.2412293553352356\n",
      "Epoch  3, Batch 74 Loss:0.2903316915035248\n",
      "Epoch  3, Batch 75 Loss:0.2987993657588959\n",
      "Epoch  3, Batch 76 Loss:0.2674393653869629\n",
      "Epoch  3, Batch 77 Loss:0.31909602880477905\n",
      "Epoch  3, Batch 78 Loss:0.24210087954998016\n",
      "Epoch  3, Batch 79 Loss:0.3090501129627228\n",
      "Epoch  3, Batch 80 Loss:0.2733283042907715\n",
      "Epoch  3, Batch 81 Loss:0.2711300253868103\n",
      "Epoch  3, Batch 82 Loss:0.2688477635383606\n",
      "Epoch  3, Batch 83 Loss:0.3604167103767395\n",
      "Epoch  3, Batch 84 Loss:0.30131110548973083\n",
      "Epoch  3, Batch 85 Loss:0.33652350306510925\n",
      "Epoch  3, Batch 86 Loss:0.3304773271083832\n",
      "Epoch  3, Batch 87 Loss:0.3104516565799713\n",
      "Epoch  3, Batch 88 Loss:0.24883008003234863\n",
      "Epoch  3, Batch 89 Loss:0.41085487604141235\n",
      "Epoch  3, Batch 90 Loss:0.33022135496139526\n",
      "Epoch  3, Batch 91 Loss:0.3172507882118225\n",
      "Epoch  3, Batch 92 Loss:0.24136635661125183\n",
      "Epoch  3, Batch 93 Loss:0.33857280015945435\n",
      "Epoch  3, Batch 94 Loss:0.277575820684433\n",
      "Epoch  3, Batch 95 Loss:0.23420977592468262\n",
      "Epoch  3, Batch 96 Loss:0.37982073426246643\n",
      "Epoch  3, Batch 97 Loss:0.301477313041687\n",
      "Epoch  3, Batch 98 Loss:0.3336959481239319\n",
      "Epoch  3, Batch 99 Loss:0.3744535744190216\n",
      "Epoch  3, Batch 100 Loss:0.3318718373775482\n",
      "Epoch  3, Batch 101 Loss:0.31643593311309814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3, Batch 102 Loss:0.39604949951171875\n",
      "Epoch  3, Batch 103 Loss:0.2794501781463623\n",
      "Epoch  3, Batch 104 Loss:0.36389121413230896\n",
      "Epoch  3, Batch 105 Loss:0.30352750420570374\n",
      "Epoch  3, Batch 106 Loss:0.35223469138145447\n",
      "Epoch  3, Batch 107 Loss:0.3499516248703003\n",
      "Epoch  3, Batch 108 Loss:0.32226768136024475\n",
      "Epoch  3, Batch 109 Loss:0.31658050417900085\n",
      "Epoch  3, Batch 110 Loss:0.27250492572784424\n",
      "Epoch  3, Batch 111 Loss:0.28725188970565796\n",
      "Epoch  3, Batch 112 Loss:0.3077239394187927\n",
      "Epoch  3, Batch 113 Loss:0.351543128490448\n",
      "Epoch  3, Batch 114 Loss:0.23987364768981934\n",
      "Epoch  3, Batch 115 Loss:0.33653318881988525\n",
      "Epoch  3, Batch 116 Loss:0.2783140242099762\n",
      "Epoch  3, Batch 117 Loss:0.2909590005874634\n",
      "Epoch  3, Batch 118 Loss:0.3024596869945526\n",
      "Epoch  3, Batch 119 Loss:0.38287365436553955\n",
      "Epoch  3, Batch 120 Loss:0.32762449979782104\n",
      "Epoch  3, Batch 121 Loss:0.30890679359436035\n",
      "Epoch  3, Batch 122 Loss:0.2886033356189728\n",
      "Epoch  3, Batch 123 Loss:0.3300083875656128\n",
      "Epoch  3, Batch 124 Loss:0.29899996519088745\n",
      "Epoch  3, Batch 125 Loss:0.34833386540412903\n",
      "Epoch  3, Batch 126 Loss:0.26832103729248047\n",
      "Epoch  3, Batch 127 Loss:0.30408596992492676\n",
      "Epoch  3, Batch 128 Loss:0.3151548504829407\n",
      "Epoch  3, Batch 129 Loss:0.2627894878387451\n",
      "Epoch  3, Batch 130 Loss:0.2903671860694885\n",
      "Epoch  3, Batch 131 Loss:0.3302017152309418\n",
      "Epoch  3, Batch 132 Loss:0.31168681383132935\n",
      "Epoch  3, Batch 133 Loss:0.3543291687965393\n",
      "Epoch  3, Batch 134 Loss:0.28486236929893494\n",
      "Epoch  3, Batch 135 Loss:0.3064905107021332\n",
      "Epoch  3, Batch 136 Loss:0.28584742546081543\n",
      "Epoch  3, Batch 137 Loss:0.34101200103759766\n",
      "Epoch  3, Batch 138 Loss:0.25403520464897156\n",
      "Epoch  3, Batch 139 Loss:0.2688513398170471\n",
      "Epoch  3, Batch 140 Loss:0.3333146870136261\n",
      "Epoch  3, Batch 141 Loss:0.3747212886810303\n",
      "Epoch  3, Batch 142 Loss:0.38650286197662354\n",
      "Epoch  3, Batch 143 Loss:0.26262906193733215\n",
      "Epoch  3, Batch 144 Loss:0.2871021032333374\n",
      "Epoch  3, Batch 145 Loss:0.31015294790267944\n",
      "Epoch  3, Batch 146 Loss:0.3542190194129944\n",
      "Epoch  3, Batch 147 Loss:0.3250921964645386\n",
      "Epoch  3, Batch 148 Loss:0.30909496545791626\n",
      "Epoch  3, Batch 149 Loss:0.29796528816223145\n",
      "Epoch  3, Batch 150 Loss:0.3481740951538086\n",
      "Epoch  3, Batch 151 Loss:0.3097705841064453\n",
      "Epoch  3, Batch 152 Loss:0.25276610255241394\n",
      "Epoch  3, Batch 153 Loss:0.2413778007030487\n",
      "Epoch  3, Batch 154 Loss:0.32644838094711304\n",
      "Epoch  3, Batch 155 Loss:0.24068094789981842\n",
      "Epoch  3, Batch 156 Loss:0.36404314637184143\n",
      "Epoch  3, Batch 157 Loss:0.2900516390800476\n",
      "Epoch  3, Batch 158 Loss:0.3392835855484009\n",
      "Epoch  3, Batch 159 Loss:0.30666136741638184\n",
      "Epoch  3, Batch 160 Loss:0.23108182847499847\n",
      "Epoch  3, Batch 161 Loss:0.3380114436149597\n",
      "Epoch  3, Batch 162 Loss:0.3400023579597473\n",
      "Epoch  3, Batch 163 Loss:0.3733345866203308\n",
      "Epoch  3, Batch 164 Loss:0.33707934617996216\n",
      "Epoch  3, Batch 165 Loss:0.21941322088241577\n",
      "Epoch  3, Batch 166 Loss:0.2641769349575043\n",
      "Epoch  3, Batch 167 Loss:0.3466019034385681\n",
      "Epoch  3, Batch 168 Loss:0.35561197996139526\n",
      "Epoch  3, Batch 169 Loss:0.3257262408733368\n",
      "Epoch  3, Batch 170 Loss:0.3753112256526947\n",
      "Epoch  3, Batch 171 Loss:0.3129549026489258\n",
      "Epoch  3, Batch 172 Loss:0.25091302394866943\n",
      "Epoch  3, Batch 173 Loss:0.3633207082748413\n",
      "Epoch  3, Batch 174 Loss:0.28150099515914917\n",
      "Epoch  3, Batch 175 Loss:0.39079993963241577\n",
      "Epoch  3, Batch 176 Loss:0.3474482595920563\n",
      "Epoch  3, Batch 177 Loss:0.2523229122161865\n",
      "Epoch  3, Batch 178 Loss:0.3126077651977539\n",
      "Epoch  3, Batch 179 Loss:0.36294257640838623\n",
      "Epoch  3, Batch 180 Loss:0.32060909271240234\n",
      "Epoch  3, Batch 181 Loss:0.3540964126586914\n",
      "Epoch  3, Batch 182 Loss:0.41565120220184326\n",
      "Epoch  3, Batch 183 Loss:0.39994198083877563\n",
      "Epoch  3, Batch 184 Loss:0.25305163860321045\n",
      "Epoch  3, Batch 185 Loss:0.3764859735965729\n",
      "Epoch  3, Batch 186 Loss:0.3273726999759674\n",
      "Epoch  3, Batch 187 Loss:0.3896978497505188\n",
      "Epoch  3, Batch 188 Loss:0.21574974060058594\n",
      "Epoch  3, Batch 189 Loss:0.307626873254776\n",
      "Epoch  3, Batch 190 Loss:0.37162455916404724\n",
      "Epoch  3, Batch 191 Loss:0.37927377223968506\n",
      "Epoch  3, Batch 192 Loss:0.32756122946739197\n",
      "Epoch  3, Batch 193 Loss:0.26767128705978394\n",
      "Epoch  3, Batch 194 Loss:0.35859954357147217\n",
      "Epoch  3, Batch 195 Loss:0.39068493247032166\n",
      "Epoch  3, Batch 196 Loss:0.3398345708847046\n",
      "Epoch  3, Batch 197 Loss:0.40050143003463745\n",
      "Epoch  3, Batch 198 Loss:0.35376182198524475\n",
      "Epoch  3, Batch 199 Loss:0.29440659284591675\n",
      "Epoch  3, Batch 200 Loss:0.3007475733757019\n",
      "Epoch  3, Batch 201 Loss:0.30100464820861816\n",
      "Epoch  3, Batch 202 Loss:0.2812096178531647\n",
      "Epoch  3, Batch 203 Loss:0.2675589323043823\n",
      "Epoch  3, Batch 204 Loss:0.2533677816390991\n",
      "Epoch  3, Batch 205 Loss:0.3835808038711548\n",
      "Epoch  3, Batch 206 Loss:0.2946436405181885\n",
      "Epoch  3, Batch 207 Loss:0.2801209092140198\n",
      "Epoch  3, Batch 208 Loss:0.33322441577911377\n",
      "Epoch  3, Batch 209 Loss:0.24821224808692932\n",
      "Epoch  3, Batch 210 Loss:0.28525251150131226\n",
      "Epoch  3, Batch 211 Loss:0.3445262908935547\n",
      "Epoch  3, Batch 212 Loss:0.33601534366607666\n",
      "Epoch  3, Batch 213 Loss:0.3015763759613037\n",
      "Epoch  3, Batch 214 Loss:0.29503893852233887\n",
      "Epoch  3, Batch 215 Loss:0.27826786041259766\n",
      "Epoch  3, Batch 216 Loss:0.26336950063705444\n",
      "Epoch  3, Batch 217 Loss:0.46819257736206055\n",
      "Epoch  3, Batch 218 Loss:0.2733948230743408\n",
      "Epoch  3, Batch 219 Loss:0.3319413363933563\n",
      "Epoch  3, Batch 220 Loss:0.37070322036743164\n",
      "Epoch  3, Batch 221 Loss:0.28497713804244995\n",
      "Epoch  3, Batch 222 Loss:0.29259759187698364\n",
      "Epoch  3, Batch 223 Loss:0.29723402857780457\n",
      "Epoch  3, Batch 224 Loss:0.26381009817123413\n",
      "Epoch  3, Batch 225 Loss:0.3438310921192169\n",
      "Epoch  3, Batch 226 Loss:0.41359084844589233\n",
      "Epoch  3, Batch 227 Loss:0.2932622730731964\n",
      "Epoch  3, Batch 228 Loss:0.2892794907093048\n",
      "Epoch  3, Batch 229 Loss:0.28159260749816895\n",
      "Epoch  3, Batch 230 Loss:0.26297488808631897\n",
      "Epoch  3, Batch 231 Loss:0.31199654936790466\n",
      "Epoch  3, Batch 232 Loss:0.27962398529052734\n",
      "Epoch  3, Batch 233 Loss:0.35009652376174927\n",
      "Loss in this Epoch is: 35.0096523762 %\n",
      "Accuracy in this Epoch is: 88.0299985409 %\n",
      "Epoch  4, Batch 0 Loss:0.29247570037841797\n",
      "Epoch  4, Batch 1 Loss:0.28452447056770325\n",
      "Epoch  4, Batch 2 Loss:0.30697759985923767\n",
      "Epoch  4, Batch 3 Loss:0.2614307999610901\n",
      "Epoch  4, Batch 4 Loss:0.3001866042613983\n",
      "Epoch  4, Batch 5 Loss:0.31180375814437866\n",
      "Epoch  4, Batch 6 Loss:0.2440170794725418\n",
      "Epoch  4, Batch 7 Loss:0.2587430775165558\n",
      "Epoch  4, Batch 8 Loss:0.27060434222221375\n",
      "Epoch  4, Batch 9 Loss:0.26039770245552063\n",
      "Epoch  4, Batch 10 Loss:0.2993485629558563\n",
      "Epoch  4, Batch 11 Loss:0.2524653673171997\n",
      "Epoch  4, Batch 12 Loss:0.26196223497390747\n",
      "Epoch  4, Batch 13 Loss:0.33734193444252014\n",
      "Epoch  4, Batch 14 Loss:0.28960007429122925\n",
      "Epoch  4, Batch 15 Loss:0.3205985426902771\n",
      "Epoch  4, Batch 16 Loss:0.2550138235092163\n",
      "Epoch  4, Batch 17 Loss:0.22325827181339264\n",
      "Epoch  4, Batch 18 Loss:0.35633859038352966\n",
      "Epoch  4, Batch 19 Loss:0.2540169358253479\n",
      "Epoch  4, Batch 20 Loss:0.32622891664505005\n",
      "Epoch  4, Batch 21 Loss:0.3868074119091034\n",
      "Epoch  4, Batch 22 Loss:0.3462989926338196\n",
      "Epoch  4, Batch 23 Loss:0.4143543839454651\n",
      "Epoch  4, Batch 24 Loss:0.3302050232887268\n",
      "Epoch  4, Batch 25 Loss:0.2602817118167877\n",
      "Epoch  4, Batch 26 Loss:0.23767228424549103\n",
      "Epoch  4, Batch 27 Loss:0.35243555903434753\n",
      "Epoch  4, Batch 28 Loss:0.37163352966308594\n",
      "Epoch  4, Batch 29 Loss:0.23556263744831085\n",
      "Epoch  4, Batch 30 Loss:0.2623394727706909\n",
      "Epoch  4, Batch 31 Loss:0.3194468915462494\n",
      "Epoch  4, Batch 32 Loss:0.3438551723957062\n",
      "Epoch  4, Batch 33 Loss:0.2584349811077118\n",
      "Epoch  4, Batch 34 Loss:0.3880815804004669\n",
      "Epoch  4, Batch 35 Loss:0.31306150555610657\n",
      "Epoch  4, Batch 36 Loss:0.25793319940567017\n",
      "Epoch  4, Batch 37 Loss:0.2772052586078644\n",
      "Epoch  4, Batch 38 Loss:0.2031717151403427\n",
      "Epoch  4, Batch 39 Loss:0.2960622310638428\n",
      "Epoch  4, Batch 40 Loss:0.3250100016593933\n",
      "Epoch  4, Batch 41 Loss:0.23567341268062592\n",
      "Epoch  4, Batch 42 Loss:0.3082245886325836\n",
      "Epoch  4, Batch 43 Loss:0.2678387761116028\n",
      "Epoch  4, Batch 44 Loss:0.3604283332824707\n",
      "Epoch  4, Batch 45 Loss:0.30002734065055847\n",
      "Epoch  4, Batch 46 Loss:0.20747846364974976\n",
      "Epoch  4, Batch 47 Loss:0.31107643246650696\n",
      "Epoch  4, Batch 48 Loss:0.25220388174057007\n",
      "Epoch  4, Batch 49 Loss:0.24970287084579468\n",
      "Epoch  4, Batch 50 Loss:0.2502051293849945\n",
      "Epoch  4, Batch 51 Loss:0.36662450432777405\n",
      "Epoch  4, Batch 52 Loss:0.27530378103256226\n",
      "Epoch  4, Batch 53 Loss:0.2772068679332733\n",
      "Epoch  4, Batch 54 Loss:0.2657020688056946\n",
      "Epoch  4, Batch 55 Loss:0.37425288558006287\n",
      "Epoch  4, Batch 56 Loss:0.26842811703681946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4, Batch 57 Loss:0.21112766861915588\n",
      "Epoch  4, Batch 58 Loss:0.2598140835762024\n",
      "Epoch  4, Batch 59 Loss:0.3956831693649292\n",
      "Epoch  4, Batch 60 Loss:0.3312511146068573\n",
      "Epoch  4, Batch 61 Loss:0.2799379527568817\n",
      "Epoch  4, Batch 62 Loss:0.282980352640152\n",
      "Epoch  4, Batch 63 Loss:0.35253477096557617\n",
      "Epoch  4, Batch 64 Loss:0.30202215909957886\n",
      "Epoch  4, Batch 65 Loss:0.28843602538108826\n",
      "Epoch  4, Batch 66 Loss:0.3070085942745209\n",
      "Epoch  4, Batch 67 Loss:0.2555856704711914\n",
      "Epoch  4, Batch 68 Loss:0.3013648986816406\n",
      "Epoch  4, Batch 69 Loss:0.2763255834579468\n",
      "Epoch  4, Batch 70 Loss:0.29839324951171875\n",
      "Epoch  4, Batch 71 Loss:0.24612261354923248\n",
      "Epoch  4, Batch 72 Loss:0.2725881040096283\n",
      "Epoch  4, Batch 73 Loss:0.2625110447406769\n",
      "Epoch  4, Batch 74 Loss:0.23415954411029816\n",
      "Epoch  4, Batch 75 Loss:0.22527390718460083\n",
      "Epoch  4, Batch 76 Loss:0.2803375720977783\n",
      "Epoch  4, Batch 77 Loss:0.2572636008262634\n",
      "Epoch  4, Batch 78 Loss:0.2460034191608429\n",
      "Epoch  4, Batch 79 Loss:0.22733919322490692\n",
      "Epoch  4, Batch 80 Loss:0.297410249710083\n",
      "Epoch  4, Batch 81 Loss:0.2618812024593353\n",
      "Epoch  4, Batch 82 Loss:0.29811859130859375\n",
      "Epoch  4, Batch 83 Loss:0.30636122822761536\n",
      "Epoch  4, Batch 84 Loss:0.2077045887708664\n",
      "Epoch  4, Batch 85 Loss:0.29717108607292175\n",
      "Epoch  4, Batch 86 Loss:0.23927487432956696\n",
      "Epoch  4, Batch 87 Loss:0.28194713592529297\n",
      "Epoch  4, Batch 88 Loss:0.28667163848876953\n",
      "Epoch  4, Batch 89 Loss:0.3556899428367615\n",
      "Epoch  4, Batch 90 Loss:0.3282858729362488\n",
      "Epoch  4, Batch 91 Loss:0.264325886964798\n",
      "Epoch  4, Batch 92 Loss:0.24205851554870605\n",
      "Epoch  4, Batch 93 Loss:0.25885558128356934\n",
      "Epoch  4, Batch 94 Loss:0.3236413300037384\n",
      "Epoch  4, Batch 95 Loss:0.28946453332901\n",
      "Epoch  4, Batch 96 Loss:0.2135128676891327\n",
      "Epoch  4, Batch 97 Loss:0.31728702783584595\n",
      "Epoch  4, Batch 98 Loss:0.3314395248889923\n",
      "Epoch  4, Batch 99 Loss:0.23269487917423248\n",
      "Epoch  4, Batch 100 Loss:0.22235429286956787\n",
      "Epoch  4, Batch 101 Loss:0.22354227304458618\n",
      "Epoch  4, Batch 102 Loss:0.32818838953971863\n",
      "Epoch  4, Batch 103 Loss:0.228321373462677\n",
      "Epoch  4, Batch 104 Loss:0.24432717263698578\n",
      "Epoch  4, Batch 105 Loss:0.22181972861289978\n",
      "Epoch  4, Batch 106 Loss:0.3233931064605713\n",
      "Epoch  4, Batch 107 Loss:0.3556130528450012\n",
      "Epoch  4, Batch 108 Loss:0.2894308269023895\n",
      "Epoch  4, Batch 109 Loss:0.3197786211967468\n",
      "Epoch  4, Batch 110 Loss:0.31041717529296875\n",
      "Epoch  4, Batch 111 Loss:0.2613193988800049\n",
      "Epoch  4, Batch 112 Loss:0.2946373224258423\n",
      "Epoch  4, Batch 113 Loss:0.34781786799430847\n",
      "Epoch  4, Batch 114 Loss:0.20811091363430023\n",
      "Epoch  4, Batch 115 Loss:0.24357126653194427\n",
      "Epoch  4, Batch 116 Loss:0.21247464418411255\n",
      "Epoch  4, Batch 117 Loss:0.24392560124397278\n",
      "Epoch  4, Batch 118 Loss:0.30127429962158203\n",
      "Epoch  4, Batch 119 Loss:0.26537320017814636\n",
      "Epoch  4, Batch 120 Loss:0.2719573974609375\n",
      "Epoch  4, Batch 121 Loss:0.2520594298839569\n",
      "Epoch  4, Batch 122 Loss:0.23562614619731903\n",
      "Epoch  4, Batch 123 Loss:0.27153199911117554\n",
      "Epoch  4, Batch 124 Loss:0.2302502542734146\n",
      "Epoch  4, Batch 125 Loss:0.3167029917240143\n",
      "Epoch  4, Batch 126 Loss:0.2923842668533325\n",
      "Epoch  4, Batch 127 Loss:0.2879413068294525\n",
      "Epoch  4, Batch 128 Loss:0.18897540867328644\n",
      "Epoch  4, Batch 129 Loss:0.3715686798095703\n",
      "Epoch  4, Batch 130 Loss:0.34122371673583984\n",
      "Epoch  4, Batch 131 Loss:0.2573271095752716\n",
      "Epoch  4, Batch 132 Loss:0.25894981622695923\n",
      "Epoch  4, Batch 133 Loss:0.21166294813156128\n",
      "Epoch  4, Batch 134 Loss:0.23010611534118652\n",
      "Epoch  4, Batch 135 Loss:0.2977549433708191\n",
      "Epoch  4, Batch 136 Loss:0.3224591612815857\n",
      "Epoch  4, Batch 137 Loss:0.2252393513917923\n",
      "Epoch  4, Batch 138 Loss:0.3463956117630005\n",
      "Epoch  4, Batch 139 Loss:0.3108408451080322\n",
      "Epoch  4, Batch 140 Loss:0.2252230942249298\n",
      "Epoch  4, Batch 141 Loss:0.2463066577911377\n",
      "Epoch  4, Batch 142 Loss:0.21976706385612488\n",
      "Epoch  4, Batch 143 Loss:0.2803276479244232\n",
      "Epoch  4, Batch 144 Loss:0.30009084939956665\n",
      "Epoch  4, Batch 145 Loss:0.2376987338066101\n",
      "Epoch  4, Batch 146 Loss:0.3127254843711853\n",
      "Epoch  4, Batch 147 Loss:0.25195008516311646\n",
      "Epoch  4, Batch 148 Loss:0.23621395230293274\n",
      "Epoch  4, Batch 149 Loss:0.22455739974975586\n",
      "Epoch  4, Batch 150 Loss:0.2832143306732178\n",
      "Epoch  4, Batch 151 Loss:0.22528116405010223\n",
      "Epoch  4, Batch 152 Loss:0.3040342926979065\n",
      "Epoch  4, Batch 153 Loss:0.2940141260623932\n",
      "Epoch  4, Batch 154 Loss:0.32650136947631836\n",
      "Epoch  4, Batch 155 Loss:0.22280621528625488\n",
      "Epoch  4, Batch 156 Loss:0.32206833362579346\n",
      "Epoch  4, Batch 157 Loss:0.2797548472881317\n",
      "Epoch  4, Batch 158 Loss:0.21205131709575653\n",
      "Epoch  4, Batch 159 Loss:0.32746371626853943\n",
      "Epoch  4, Batch 160 Loss:0.25912225246429443\n",
      "Epoch  4, Batch 161 Loss:0.32063308358192444\n",
      "Epoch  4, Batch 162 Loss:0.3217759132385254\n",
      "Epoch  4, Batch 163 Loss:0.2468537986278534\n",
      "Epoch  4, Batch 164 Loss:0.2273261696100235\n",
      "Epoch  4, Batch 165 Loss:0.3285523056983948\n",
      "Epoch  4, Batch 166 Loss:0.34989461302757263\n",
      "Epoch  4, Batch 167 Loss:0.335923969745636\n",
      "Epoch  4, Batch 168 Loss:0.23197148740291595\n",
      "Epoch  4, Batch 169 Loss:0.2616835832595825\n",
      "Epoch  4, Batch 170 Loss:0.24946685135364532\n",
      "Epoch  4, Batch 171 Loss:0.28890275955200195\n",
      "Epoch  4, Batch 172 Loss:0.34129369258880615\n",
      "Epoch  4, Batch 173 Loss:0.28417426347732544\n",
      "Epoch  4, Batch 174 Loss:0.2305748462677002\n",
      "Epoch  4, Batch 175 Loss:0.30462411046028137\n",
      "Epoch  4, Batch 176 Loss:0.2656283676624298\n",
      "Epoch  4, Batch 177 Loss:0.30557364225387573\n",
      "Epoch  4, Batch 178 Loss:0.29060041904449463\n",
      "Epoch  4, Batch 179 Loss:0.24309158325195312\n",
      "Epoch  4, Batch 180 Loss:0.267686665058136\n",
      "Epoch  4, Batch 181 Loss:0.2775040864944458\n",
      "Epoch  4, Batch 182 Loss:0.1587854027748108\n",
      "Epoch  4, Batch 183 Loss:0.24421313405036926\n",
      "Epoch  4, Batch 184 Loss:0.33632028102874756\n",
      "Epoch  4, Batch 185 Loss:0.27419838309288025\n",
      "Epoch  4, Batch 186 Loss:0.26141053438186646\n",
      "Epoch  4, Batch 187 Loss:0.32420775294303894\n",
      "Epoch  4, Batch 188 Loss:0.244068905711174\n",
      "Epoch  4, Batch 189 Loss:0.27975159883499146\n",
      "Epoch  4, Batch 190 Loss:0.25308942794799805\n",
      "Epoch  4, Batch 191 Loss:0.3129587173461914\n",
      "Epoch  4, Batch 192 Loss:0.2964869439601898\n",
      "Epoch  4, Batch 193 Loss:0.4011481702327728\n",
      "Epoch  4, Batch 194 Loss:0.30624186992645264\n",
      "Epoch  4, Batch 195 Loss:0.2872723639011383\n",
      "Epoch  4, Batch 196 Loss:0.236332505941391\n",
      "Epoch  4, Batch 197 Loss:0.28201061487197876\n",
      "Epoch  4, Batch 198 Loss:0.25687122344970703\n",
      "Epoch  4, Batch 199 Loss:0.3072258532047272\n",
      "Epoch  4, Batch 200 Loss:0.2830997705459595\n",
      "Epoch  4, Batch 201 Loss:0.21751445531845093\n",
      "Epoch  4, Batch 202 Loss:0.3709108829498291\n",
      "Epoch  4, Batch 203 Loss:0.2557198107242584\n",
      "Epoch  4, Batch 204 Loss:0.3549947738647461\n",
      "Epoch  4, Batch 205 Loss:0.3591272830963135\n",
      "Epoch  4, Batch 206 Loss:0.3357996642589569\n",
      "Epoch  4, Batch 207 Loss:0.3228132426738739\n",
      "Epoch  4, Batch 208 Loss:0.30502331256866455\n",
      "Epoch  4, Batch 209 Loss:0.3071761131286621\n",
      "Epoch  4, Batch 210 Loss:0.26131585240364075\n",
      "Epoch  4, Batch 211 Loss:0.2993689179420471\n",
      "Epoch  4, Batch 212 Loss:0.38227248191833496\n",
      "Epoch  4, Batch 213 Loss:0.3144226670265198\n",
      "Epoch  4, Batch 214 Loss:0.22444123029708862\n",
      "Epoch  4, Batch 215 Loss:0.2681088447570801\n",
      "Epoch  4, Batch 216 Loss:0.21658900380134583\n",
      "Epoch  4, Batch 217 Loss:0.2985502779483795\n",
      "Epoch  4, Batch 218 Loss:0.29145413637161255\n",
      "Epoch  4, Batch 219 Loss:0.27388501167297363\n",
      "Epoch  4, Batch 220 Loss:0.30785590410232544\n",
      "Epoch  4, Batch 221 Loss:0.273907333612442\n",
      "Epoch  4, Batch 222 Loss:0.22554892301559448\n",
      "Epoch  4, Batch 223 Loss:0.21596916019916534\n",
      "Epoch  4, Batch 224 Loss:0.25584495067596436\n",
      "Epoch  4, Batch 225 Loss:0.24317948520183563\n",
      "Epoch  4, Batch 226 Loss:0.2902912497520447\n",
      "Epoch  4, Batch 227 Loss:0.2721988260746002\n",
      "Epoch  4, Batch 228 Loss:0.33547767996788025\n",
      "Epoch  4, Batch 229 Loss:0.3081974685192108\n",
      "Epoch  4, Batch 230 Loss:0.3721316456794739\n",
      "Epoch  4, Batch 231 Loss:0.20111753046512604\n",
      "Epoch  4, Batch 232 Loss:0.2797507345676422\n",
      "Epoch  4, Batch 233 Loss:0.24270564317703247\n",
      "Loss in this Epoch is: 24.2705643177 %\n",
      "Accuracy in this Epoch is: 88.7799978256 %\n",
      "Epoch  5, Batch 0 Loss:0.1977744698524475\n",
      "Epoch  5, Batch 1 Loss:0.2567898631095886\n",
      "Epoch  5, Batch 2 Loss:0.25730422139167786\n",
      "Epoch  5, Batch 3 Loss:0.2601534426212311\n",
      "Epoch  5, Batch 4 Loss:0.23956501483917236\n",
      "Epoch  5, Batch 5 Loss:0.23871548473834991\n",
      "Epoch  5, Batch 6 Loss:0.22880619764328003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5, Batch 7 Loss:0.26088646054267883\n",
      "Epoch  5, Batch 8 Loss:0.21237458288669586\n",
      "Epoch  5, Batch 9 Loss:0.33333706855773926\n",
      "Epoch  5, Batch 10 Loss:0.19963131844997406\n",
      "Epoch  5, Batch 11 Loss:0.28195711970329285\n",
      "Epoch  5, Batch 12 Loss:0.24794292449951172\n",
      "Epoch  5, Batch 13 Loss:0.3021973967552185\n",
      "Epoch  5, Batch 14 Loss:0.25767800211906433\n",
      "Epoch  5, Batch 15 Loss:0.27561238408088684\n",
      "Epoch  5, Batch 16 Loss:0.2094351202249527\n",
      "Epoch  5, Batch 17 Loss:0.24005728960037231\n",
      "Epoch  5, Batch 18 Loss:0.2894202470779419\n",
      "Epoch  5, Batch 19 Loss:0.26934564113616943\n",
      "Epoch  5, Batch 20 Loss:0.22815249860286713\n",
      "Epoch  5, Batch 21 Loss:0.23810996115207672\n",
      "Epoch  5, Batch 22 Loss:0.2182598114013672\n",
      "Epoch  5, Batch 23 Loss:0.24605326354503632\n",
      "Epoch  5, Batch 24 Loss:0.17593280971050262\n",
      "Epoch  5, Batch 25 Loss:0.21679802238941193\n",
      "Epoch  5, Batch 26 Loss:0.2362903207540512\n",
      "Epoch  5, Batch 27 Loss:0.23974157869815826\n",
      "Epoch  5, Batch 28 Loss:0.1827058047056198\n",
      "Epoch  5, Batch 29 Loss:0.2182483822107315\n",
      "Epoch  5, Batch 30 Loss:0.2459837943315506\n",
      "Epoch  5, Batch 31 Loss:0.2722156047821045\n",
      "Epoch  5, Batch 32 Loss:0.32718023657798767\n",
      "Epoch  5, Batch 33 Loss:0.224110409617424\n",
      "Epoch  5, Batch 34 Loss:0.23403671383857727\n",
      "Epoch  5, Batch 35 Loss:0.23410362005233765\n",
      "Epoch  5, Batch 36 Loss:0.24152259528636932\n",
      "Epoch  5, Batch 37 Loss:0.15921655297279358\n",
      "Epoch  5, Batch 38 Loss:0.27037495374679565\n",
      "Epoch  5, Batch 39 Loss:0.24532556533813477\n",
      "Epoch  5, Batch 40 Loss:0.17365966737270355\n",
      "Epoch  5, Batch 41 Loss:0.22659306228160858\n",
      "Epoch  5, Batch 42 Loss:0.2211170345544815\n",
      "Epoch  5, Batch 43 Loss:0.2580813765525818\n",
      "Epoch  5, Batch 44 Loss:0.22289270162582397\n",
      "Epoch  5, Batch 45 Loss:0.22099289298057556\n",
      "Epoch  5, Batch 46 Loss:0.3316624164581299\n",
      "Epoch  5, Batch 47 Loss:0.2773837745189667\n",
      "Epoch  5, Batch 48 Loss:0.23713035881519318\n",
      "Epoch  5, Batch 49 Loss:0.27550414204597473\n",
      "Epoch  5, Batch 50 Loss:0.25535649061203003\n",
      "Epoch  5, Batch 51 Loss:0.25910237431526184\n",
      "Epoch  5, Batch 52 Loss:0.28255733847618103\n",
      "Epoch  5, Batch 53 Loss:0.27535733580589294\n",
      "Epoch  5, Batch 54 Loss:0.2388886958360672\n",
      "Epoch  5, Batch 55 Loss:0.24368590116500854\n",
      "Epoch  5, Batch 56 Loss:0.26427263021469116\n",
      "Epoch  5, Batch 57 Loss:0.2992044985294342\n",
      "Epoch  5, Batch 58 Loss:0.26249149441719055\n",
      "Epoch  5, Batch 59 Loss:0.2378789335489273\n",
      "Epoch  5, Batch 60 Loss:0.2561350166797638\n",
      "Epoch  5, Batch 61 Loss:0.21510930359363556\n",
      "Epoch  5, Batch 62 Loss:0.30983152985572815\n",
      "Epoch  5, Batch 63 Loss:0.23321081697940826\n",
      "Epoch  5, Batch 64 Loss:0.2862236499786377\n",
      "Epoch  5, Batch 65 Loss:0.23100996017456055\n",
      "Epoch  5, Batch 66 Loss:0.26105111837387085\n",
      "Epoch  5, Batch 67 Loss:0.20629724860191345\n",
      "Epoch  5, Batch 68 Loss:0.2568943202495575\n",
      "Epoch  5, Batch 69 Loss:0.23038893938064575\n",
      "Epoch  5, Batch 70 Loss:0.2550743520259857\n",
      "Epoch  5, Batch 71 Loss:0.2281782478094101\n",
      "Epoch  5, Batch 72 Loss:0.24326936900615692\n",
      "Epoch  5, Batch 73 Loss:0.3016384243965149\n",
      "Epoch  5, Batch 74 Loss:0.2650936543941498\n",
      "Epoch  5, Batch 75 Loss:0.25274965167045593\n",
      "Epoch  5, Batch 76 Loss:0.22066330909729004\n",
      "Epoch  5, Batch 77 Loss:0.2622351050376892\n",
      "Epoch  5, Batch 78 Loss:0.21242673695087433\n",
      "Epoch  5, Batch 79 Loss:0.22082751989364624\n",
      "Epoch  5, Batch 80 Loss:0.24465161561965942\n",
      "Epoch  5, Batch 81 Loss:0.23619438707828522\n",
      "Epoch  5, Batch 82 Loss:0.35186976194381714\n",
      "Epoch  5, Batch 83 Loss:0.17957091331481934\n",
      "Epoch  5, Batch 84 Loss:0.2603786289691925\n",
      "Epoch  5, Batch 85 Loss:0.2684057056903839\n",
      "Epoch  5, Batch 86 Loss:0.32663387060165405\n",
      "Epoch  5, Batch 87 Loss:0.23138920962810516\n",
      "Epoch  5, Batch 88 Loss:0.2535475492477417\n",
      "Epoch  5, Batch 89 Loss:0.2720872759819031\n",
      "Epoch  5, Batch 90 Loss:0.2274327576160431\n",
      "Epoch  5, Batch 91 Loss:0.20139813423156738\n",
      "Epoch  5, Batch 92 Loss:0.28407159447669983\n",
      "Epoch  5, Batch 93 Loss:0.28749680519104004\n",
      "Epoch  5, Batch 94 Loss:0.26840510964393616\n",
      "Epoch  5, Batch 95 Loss:0.2282976508140564\n",
      "Epoch  5, Batch 96 Loss:0.17398634552955627\n",
      "Epoch  5, Batch 97 Loss:0.273989737033844\n",
      "Epoch  5, Batch 98 Loss:0.2277710735797882\n",
      "Epoch  5, Batch 99 Loss:0.23124723136425018\n",
      "Epoch  5, Batch 100 Loss:0.27760183811187744\n",
      "Epoch  5, Batch 101 Loss:0.22400982677936554\n",
      "Epoch  5, Batch 102 Loss:0.24620534479618073\n",
      "Epoch  5, Batch 103 Loss:0.23064275085926056\n",
      "Epoch  5, Batch 104 Loss:0.1988409161567688\n",
      "Epoch  5, Batch 105 Loss:0.28969061374664307\n",
      "Epoch  5, Batch 106 Loss:0.29710090160369873\n",
      "Epoch  5, Batch 107 Loss:0.2708921730518341\n",
      "Epoch  5, Batch 108 Loss:0.18664854764938354\n",
      "Epoch  5, Batch 109 Loss:0.2522871196269989\n",
      "Epoch  5, Batch 110 Loss:0.26965487003326416\n",
      "Epoch  5, Batch 111 Loss:0.2741583287715912\n",
      "Epoch  5, Batch 112 Loss:0.3079272210597992\n",
      "Epoch  5, Batch 113 Loss:0.27789291739463806\n",
      "Epoch  5, Batch 114 Loss:0.25309813022613525\n",
      "Epoch  5, Batch 115 Loss:0.2454536259174347\n",
      "Epoch  5, Batch 116 Loss:0.2728710174560547\n",
      "Epoch  5, Batch 117 Loss:0.20986536145210266\n",
      "Epoch  5, Batch 118 Loss:0.27123314142227173\n",
      "Epoch  5, Batch 119 Loss:0.14851485192775726\n",
      "Epoch  5, Batch 120 Loss:0.2415047585964203\n",
      "Epoch  5, Batch 121 Loss:0.2723994255065918\n",
      "Epoch  5, Batch 122 Loss:0.33143359422683716\n",
      "Epoch  5, Batch 123 Loss:0.24748605489730835\n",
      "Epoch  5, Batch 124 Loss:0.22090277075767517\n",
      "Epoch  5, Batch 125 Loss:0.3003561496734619\n",
      "Epoch  5, Batch 126 Loss:0.231075718998909\n",
      "Epoch  5, Batch 127 Loss:0.18631230294704437\n",
      "Epoch  5, Batch 128 Loss:0.23745384812355042\n",
      "Epoch  5, Batch 129 Loss:0.24244371056556702\n",
      "Epoch  5, Batch 130 Loss:0.33359286189079285\n",
      "Epoch  5, Batch 131 Loss:0.28677916526794434\n",
      "Epoch  5, Batch 132 Loss:0.24335476756095886\n",
      "Epoch  5, Batch 133 Loss:0.30189812183380127\n",
      "Epoch  5, Batch 134 Loss:0.2578098177909851\n",
      "Epoch  5, Batch 135 Loss:0.28919827938079834\n",
      "Epoch  5, Batch 136 Loss:0.3014526665210724\n",
      "Epoch  5, Batch 137 Loss:0.1954880654811859\n",
      "Epoch  5, Batch 138 Loss:0.2892596125602722\n",
      "Epoch  5, Batch 139 Loss:0.2354690283536911\n",
      "Epoch  5, Batch 140 Loss:0.30142802000045776\n",
      "Epoch  5, Batch 141 Loss:0.31399989128112793\n",
      "Epoch  5, Batch 142 Loss:0.2985389828681946\n",
      "Epoch  5, Batch 143 Loss:0.21953347325325012\n",
      "Epoch  5, Batch 144 Loss:0.39912664890289307\n",
      "Epoch  5, Batch 145 Loss:0.2610485553741455\n",
      "Epoch  5, Batch 146 Loss:0.30019092559814453\n",
      "Epoch  5, Batch 147 Loss:0.23807227611541748\n",
      "Epoch  5, Batch 148 Loss:0.24840468168258667\n",
      "Epoch  5, Batch 149 Loss:0.2412862479686737\n",
      "Epoch  5, Batch 150 Loss:0.3117790222167969\n",
      "Epoch  5, Batch 151 Loss:0.317335307598114\n",
      "Epoch  5, Batch 152 Loss:0.26923877000808716\n",
      "Epoch  5, Batch 153 Loss:0.27056029438972473\n",
      "Epoch  5, Batch 154 Loss:0.35321176052093506\n",
      "Epoch  5, Batch 155 Loss:0.2675638794898987\n",
      "Epoch  5, Batch 156 Loss:0.24928641319274902\n",
      "Epoch  5, Batch 157 Loss:0.27163422107696533\n",
      "Epoch  5, Batch 158 Loss:0.28354907035827637\n",
      "Epoch  5, Batch 159 Loss:0.2253521978855133\n",
      "Epoch  5, Batch 160 Loss:0.22101274132728577\n",
      "Epoch  5, Batch 161 Loss:0.2335643470287323\n",
      "Epoch  5, Batch 162 Loss:0.23265095055103302\n",
      "Epoch  5, Batch 163 Loss:0.27808892726898193\n",
      "Epoch  5, Batch 164 Loss:0.21562115848064423\n",
      "Epoch  5, Batch 165 Loss:0.21997879445552826\n",
      "Epoch  5, Batch 166 Loss:0.24741573631763458\n",
      "Epoch  5, Batch 167 Loss:0.2888249158859253\n",
      "Epoch  5, Batch 168 Loss:0.22502461075782776\n",
      "Epoch  5, Batch 169 Loss:0.24471606314182281\n",
      "Epoch  5, Batch 170 Loss:0.21997177600860596\n",
      "Epoch  5, Batch 171 Loss:0.23622123897075653\n",
      "Epoch  5, Batch 172 Loss:0.22911983728408813\n",
      "Epoch  5, Batch 173 Loss:0.2668672204017639\n",
      "Epoch  5, Batch 174 Loss:0.30962422490119934\n",
      "Epoch  5, Batch 175 Loss:0.3455911874771118\n",
      "Epoch  5, Batch 176 Loss:0.20231038331985474\n",
      "Epoch  5, Batch 177 Loss:0.22937153279781342\n",
      "Epoch  5, Batch 178 Loss:0.25426170229911804\n",
      "Epoch  5, Batch 179 Loss:0.24873922765254974\n",
      "Epoch  5, Batch 180 Loss:0.22100324928760529\n",
      "Epoch  5, Batch 181 Loss:0.2727949321269989\n",
      "Epoch  5, Batch 182 Loss:0.24984334409236908\n",
      "Epoch  5, Batch 183 Loss:0.2576861083507538\n",
      "Epoch  5, Batch 184 Loss:0.20830658078193665\n",
      "Epoch  5, Batch 185 Loss:0.25296396017074585\n",
      "Epoch  5, Batch 186 Loss:0.2124965786933899\n",
      "Epoch  5, Batch 187 Loss:0.21770107746124268\n",
      "Epoch  5, Batch 188 Loss:0.23016861081123352\n",
      "Epoch  5, Batch 189 Loss:0.18000569939613342\n",
      "Epoch  5, Batch 190 Loss:0.1591814160346985\n",
      "Epoch  5, Batch 191 Loss:0.24998529255390167\n",
      "Epoch  5, Batch 192 Loss:0.21019980311393738\n",
      "Epoch  5, Batch 193 Loss:0.3185274004936218\n",
      "Epoch  5, Batch 194 Loss:0.26345622539520264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5, Batch 195 Loss:0.22036217153072357\n",
      "Epoch  5, Batch 196 Loss:0.27242231369018555\n",
      "Epoch  5, Batch 197 Loss:0.21913591027259827\n",
      "Epoch  5, Batch 198 Loss:0.18792496621608734\n",
      "Epoch  5, Batch 199 Loss:0.2067531943321228\n",
      "Epoch  5, Batch 200 Loss:0.2514772415161133\n",
      "Epoch  5, Batch 201 Loss:0.2477944791316986\n",
      "Epoch  5, Batch 202 Loss:0.24471203982830048\n",
      "Epoch  5, Batch 203 Loss:0.213494673371315\n",
      "Epoch  5, Batch 204 Loss:0.26360008120536804\n",
      "Epoch  5, Batch 205 Loss:0.2318938970565796\n",
      "Epoch  5, Batch 206 Loss:0.24741217494010925\n",
      "Epoch  5, Batch 207 Loss:0.2846065163612366\n",
      "Epoch  5, Batch 208 Loss:0.2560999393463135\n",
      "Epoch  5, Batch 209 Loss:0.2181226909160614\n",
      "Epoch  5, Batch 210 Loss:0.18315206468105316\n",
      "Epoch  5, Batch 211 Loss:0.24750731885433197\n",
      "Epoch  5, Batch 212 Loss:0.2979721426963806\n",
      "Epoch  5, Batch 213 Loss:0.31104183197021484\n",
      "Epoch  5, Batch 214 Loss:0.17940500378608704\n",
      "Epoch  5, Batch 215 Loss:0.33317819237709045\n",
      "Epoch  5, Batch 216 Loss:0.22644658386707306\n",
      "Epoch  5, Batch 217 Loss:0.25133922696113586\n",
      "Epoch  5, Batch 218 Loss:0.29230108857154846\n",
      "Epoch  5, Batch 219 Loss:0.2273465245962143\n",
      "Epoch  5, Batch 220 Loss:0.23555532097816467\n",
      "Epoch  5, Batch 221 Loss:0.2924134135246277\n",
      "Epoch  5, Batch 222 Loss:0.23335561156272888\n",
      "Epoch  5, Batch 223 Loss:0.27861082553863525\n",
      "Epoch  5, Batch 224 Loss:0.3171457052230835\n",
      "Epoch  5, Batch 225 Loss:0.2074744552373886\n",
      "Epoch  5, Batch 226 Loss:0.20146575570106506\n",
      "Epoch  5, Batch 227 Loss:0.2916440963745117\n",
      "Epoch  5, Batch 228 Loss:0.23302973806858063\n",
      "Epoch  5, Batch 229 Loss:0.3329325318336487\n",
      "Epoch  5, Batch 230 Loss:0.21792855858802795\n",
      "Epoch  5, Batch 231 Loss:0.2503877878189087\n",
      "Epoch  5, Batch 232 Loss:0.33076775074005127\n",
      "Epoch  5, Batch 233 Loss:0.35095322132110596\n",
      "Loss in this Epoch is: 35.0953221321 %\n",
      "Accuracy in this Epoch is: 89.0100002289 %\n",
      "Epoch  6, Batch 0 Loss:0.22621649503707886\n",
      "Epoch  6, Batch 1 Loss:0.18728898465633392\n",
      "Epoch  6, Batch 2 Loss:0.1993417739868164\n",
      "Epoch  6, Batch 3 Loss:0.29883190989494324\n",
      "Epoch  6, Batch 4 Loss:0.211269348859787\n",
      "Epoch  6, Batch 5 Loss:0.24138066172599792\n",
      "Epoch  6, Batch 6 Loss:0.19381476938724518\n",
      "Epoch  6, Batch 7 Loss:0.27341848611831665\n",
      "Epoch  6, Batch 8 Loss:0.19482511281967163\n",
      "Epoch  6, Batch 9 Loss:0.2777700126171112\n",
      "Epoch  6, Batch 10 Loss:0.18116603791713715\n",
      "Epoch  6, Batch 11 Loss:0.22486287355422974\n",
      "Epoch  6, Batch 12 Loss:0.2372138947248459\n",
      "Epoch  6, Batch 13 Loss:0.21899838745594025\n",
      "Epoch  6, Batch 14 Loss:0.2012292444705963\n",
      "Epoch  6, Batch 15 Loss:0.21680770814418793\n",
      "Epoch  6, Batch 16 Loss:0.214183047413826\n",
      "Epoch  6, Batch 17 Loss:0.24698451161384583\n",
      "Epoch  6, Batch 18 Loss:0.17092444002628326\n",
      "Epoch  6, Batch 19 Loss:0.20176461338996887\n",
      "Epoch  6, Batch 20 Loss:0.24556411802768707\n",
      "Epoch  6, Batch 21 Loss:0.24147483706474304\n",
      "Epoch  6, Batch 22 Loss:0.2667536735534668\n",
      "Epoch  6, Batch 23 Loss:0.2470313161611557\n",
      "Epoch  6, Batch 24 Loss:0.19100505113601685\n",
      "Epoch  6, Batch 25 Loss:0.19939810037612915\n",
      "Epoch  6, Batch 26 Loss:0.18471969664096832\n",
      "Epoch  6, Batch 27 Loss:0.23994649946689606\n",
      "Epoch  6, Batch 28 Loss:0.16754290461540222\n",
      "Epoch  6, Batch 29 Loss:0.17917649447917938\n",
      "Epoch  6, Batch 30 Loss:0.24498382210731506\n",
      "Epoch  6, Batch 31 Loss:0.22722148895263672\n",
      "Epoch  6, Batch 32 Loss:0.30506443977355957\n",
      "Epoch  6, Batch 33 Loss:0.24373973906040192\n",
      "Epoch  6, Batch 34 Loss:0.2009860873222351\n",
      "Epoch  6, Batch 35 Loss:0.30960628390312195\n",
      "Epoch  6, Batch 36 Loss:0.24465686082839966\n",
      "Epoch  6, Batch 37 Loss:0.20097064971923828\n",
      "Epoch  6, Batch 38 Loss:0.17133797705173492\n",
      "Epoch  6, Batch 39 Loss:0.258821576833725\n",
      "Epoch  6, Batch 40 Loss:0.2140997052192688\n",
      "Epoch  6, Batch 41 Loss:0.27335125207901\n",
      "Epoch  6, Batch 42 Loss:0.295246422290802\n",
      "Epoch  6, Batch 43 Loss:0.22433479130268097\n",
      "Epoch  6, Batch 44 Loss:0.20391708612442017\n",
      "Epoch  6, Batch 45 Loss:0.22343064844608307\n",
      "Epoch  6, Batch 46 Loss:0.2849276661872864\n",
      "Epoch  6, Batch 47 Loss:0.24539229273796082\n",
      "Epoch  6, Batch 48 Loss:0.1732577681541443\n",
      "Epoch  6, Batch 49 Loss:0.21073715388774872\n",
      "Epoch  6, Batch 50 Loss:0.17980873584747314\n",
      "Epoch  6, Batch 51 Loss:0.24886688590049744\n",
      "Epoch  6, Batch 52 Loss:0.19391340017318726\n",
      "Epoch  6, Batch 53 Loss:0.2607721984386444\n",
      "Epoch  6, Batch 54 Loss:0.27508544921875\n",
      "Epoch  6, Batch 55 Loss:0.2537923753261566\n",
      "Epoch  6, Batch 56 Loss:0.28256094455718994\n",
      "Epoch  6, Batch 57 Loss:0.19134119153022766\n",
      "Epoch  6, Batch 58 Loss:0.21434637904167175\n",
      "Epoch  6, Batch 59 Loss:0.2559439539909363\n",
      "Epoch  6, Batch 60 Loss:0.24135705828666687\n",
      "Epoch  6, Batch 61 Loss:0.2848104238510132\n",
      "Epoch  6, Batch 62 Loss:0.3088776171207428\n",
      "Epoch  6, Batch 63 Loss:0.21410372853279114\n",
      "Epoch  6, Batch 64 Loss:0.17018301784992218\n",
      "Epoch  6, Batch 65 Loss:0.20072706043720245\n",
      "Epoch  6, Batch 66 Loss:0.24893219769001007\n",
      "Epoch  6, Batch 67 Loss:0.2407960146665573\n",
      "Epoch  6, Batch 68 Loss:0.25173741579055786\n",
      "Epoch  6, Batch 69 Loss:0.26400431990623474\n",
      "Epoch  6, Batch 70 Loss:0.2478289157152176\n",
      "Epoch  6, Batch 71 Loss:0.25828003883361816\n",
      "Epoch  6, Batch 72 Loss:0.2652612626552582\n",
      "Epoch  6, Batch 73 Loss:0.19597767293453217\n",
      "Epoch  6, Batch 74 Loss:0.198391854763031\n",
      "Epoch  6, Batch 75 Loss:0.2380657196044922\n",
      "Epoch  6, Batch 76 Loss:0.23550857603549957\n",
      "Epoch  6, Batch 77 Loss:0.20483367145061493\n",
      "Epoch  6, Batch 78 Loss:0.2560087740421295\n",
      "Epoch  6, Batch 79 Loss:0.19785025715827942\n",
      "Epoch  6, Batch 80 Loss:0.17333035171031952\n",
      "Epoch  6, Batch 81 Loss:0.24263609945774078\n",
      "Epoch  6, Batch 82 Loss:0.307155579328537\n",
      "Epoch  6, Batch 83 Loss:0.23945464193820953\n",
      "Epoch  6, Batch 84 Loss:0.18031589686870575\n",
      "Epoch  6, Batch 85 Loss:0.22905252873897552\n",
      "Epoch  6, Batch 86 Loss:0.19631342589855194\n",
      "Epoch  6, Batch 87 Loss:0.19557631015777588\n",
      "Epoch  6, Batch 88 Loss:0.22280733287334442\n",
      "Epoch  6, Batch 89 Loss:0.15704132616519928\n",
      "Epoch  6, Batch 90 Loss:0.2375245839357376\n",
      "Epoch  6, Batch 91 Loss:0.18067465722560883\n",
      "Epoch  6, Batch 92 Loss:0.17473912239074707\n",
      "Epoch  6, Batch 93 Loss:0.23487740755081177\n",
      "Epoch  6, Batch 94 Loss:0.23740394413471222\n",
      "Epoch  6, Batch 95 Loss:0.18925319612026215\n",
      "Epoch  6, Batch 96 Loss:0.22713333368301392\n",
      "Epoch  6, Batch 97 Loss:0.2216993272304535\n",
      "Epoch  6, Batch 98 Loss:0.18344944715499878\n",
      "Epoch  6, Batch 99 Loss:0.2540709674358368\n",
      "Epoch  6, Batch 100 Loss:0.2695198953151703\n",
      "Epoch  6, Batch 101 Loss:0.251705139875412\n",
      "Epoch  6, Batch 102 Loss:0.18181464076042175\n",
      "Epoch  6, Batch 103 Loss:0.2625894546508789\n",
      "Epoch  6, Batch 104 Loss:0.24492794275283813\n",
      "Epoch  6, Batch 105 Loss:0.3269062340259552\n",
      "Epoch  6, Batch 106 Loss:0.25336703658103943\n",
      "Epoch  6, Batch 107 Loss:0.26230931282043457\n",
      "Epoch  6, Batch 108 Loss:0.23114782571792603\n",
      "Epoch  6, Batch 109 Loss:0.2865196764469147\n",
      "Epoch  6, Batch 110 Loss:0.17627829313278198\n",
      "Epoch  6, Batch 111 Loss:0.25226765871047974\n",
      "Epoch  6, Batch 112 Loss:0.23015284538269043\n",
      "Epoch  6, Batch 113 Loss:0.3045620918273926\n",
      "Epoch  6, Batch 114 Loss:0.22312720119953156\n",
      "Epoch  6, Batch 115 Loss:0.2622489333152771\n",
      "Epoch  6, Batch 116 Loss:0.20990146696567535\n",
      "Epoch  6, Batch 117 Loss:0.23760610818862915\n",
      "Epoch  6, Batch 118 Loss:0.2314084768295288\n",
      "Epoch  6, Batch 119 Loss:0.2488543689250946\n",
      "Epoch  6, Batch 120 Loss:0.239359050989151\n",
      "Epoch  6, Batch 121 Loss:0.20080524682998657\n",
      "Epoch  6, Batch 122 Loss:0.2325541377067566\n",
      "Epoch  6, Batch 123 Loss:0.22474892437458038\n",
      "Epoch  6, Batch 124 Loss:0.23379173874855042\n",
      "Epoch  6, Batch 125 Loss:0.17350813746452332\n",
      "Epoch  6, Batch 126 Loss:0.18730388581752777\n",
      "Epoch  6, Batch 127 Loss:0.18799692392349243\n",
      "Epoch  6, Batch 128 Loss:0.228642076253891\n",
      "Epoch  6, Batch 129 Loss:0.25757887959480286\n",
      "Epoch  6, Batch 130 Loss:0.1990198940038681\n",
      "Epoch  6, Batch 131 Loss:0.2048642337322235\n",
      "Epoch  6, Batch 132 Loss:0.2329409271478653\n",
      "Epoch  6, Batch 133 Loss:0.2411210983991623\n",
      "Epoch  6, Batch 134 Loss:0.28067588806152344\n",
      "Epoch  6, Batch 135 Loss:0.18534141778945923\n",
      "Epoch  6, Batch 136 Loss:0.21957898139953613\n",
      "Epoch  6, Batch 137 Loss:0.2626144289970398\n",
      "Epoch  6, Batch 138 Loss:0.2604396343231201\n",
      "Epoch  6, Batch 139 Loss:0.26660752296447754\n",
      "Epoch  6, Batch 140 Loss:0.3005366921424866\n",
      "Epoch  6, Batch 141 Loss:0.22146815061569214\n",
      "Epoch  6, Batch 142 Loss:0.25967341661453247\n",
      "Epoch  6, Batch 143 Loss:0.30657723546028137\n",
      "Epoch  6, Batch 144 Loss:0.2505311965942383\n",
      "Epoch  6, Batch 145 Loss:0.20798903703689575\n",
      "Epoch  6, Batch 146 Loss:0.228631854057312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6, Batch 147 Loss:0.19902291893959045\n",
      "Epoch  6, Batch 148 Loss:0.20130732655525208\n",
      "Epoch  6, Batch 149 Loss:0.22571440041065216\n",
      "Epoch  6, Batch 150 Loss:0.22778385877609253\n",
      "Epoch  6, Batch 151 Loss:0.24269527196884155\n",
      "Epoch  6, Batch 152 Loss:0.2017025351524353\n",
      "Epoch  6, Batch 153 Loss:0.21926411986351013\n",
      "Epoch  6, Batch 154 Loss:0.1914757639169693\n",
      "Epoch  6, Batch 155 Loss:0.3217325806617737\n",
      "Epoch  6, Batch 156 Loss:0.19214048981666565\n",
      "Epoch  6, Batch 157 Loss:0.2176269292831421\n",
      "Epoch  6, Batch 158 Loss:0.14224329590797424\n",
      "Epoch  6, Batch 159 Loss:0.19289939105510712\n",
      "Epoch  6, Batch 160 Loss:0.15059927105903625\n",
      "Epoch  6, Batch 161 Loss:0.18555381894111633\n",
      "Epoch  6, Batch 162 Loss:0.2620266377925873\n",
      "Epoch  6, Batch 163 Loss:0.2527405023574829\n",
      "Epoch  6, Batch 164 Loss:0.3073909878730774\n",
      "Epoch  6, Batch 165 Loss:0.19841691851615906\n",
      "Epoch  6, Batch 166 Loss:0.17786026000976562\n",
      "Epoch  6, Batch 167 Loss:0.21611902117729187\n",
      "Epoch  6, Batch 168 Loss:0.28038859367370605\n",
      "Epoch  6, Batch 169 Loss:0.3093370795249939\n",
      "Epoch  6, Batch 170 Loss:0.295027494430542\n",
      "Epoch  6, Batch 171 Loss:0.27544665336608887\n",
      "Epoch  6, Batch 172 Loss:0.2958148717880249\n",
      "Epoch  6, Batch 173 Loss:0.2632366418838501\n",
      "Epoch  6, Batch 174 Loss:0.28804564476013184\n",
      "Epoch  6, Batch 175 Loss:0.24330037832260132\n",
      "Epoch  6, Batch 176 Loss:0.2399652749300003\n",
      "Epoch  6, Batch 177 Loss:0.26345980167388916\n",
      "Epoch  6, Batch 178 Loss:0.2764919400215149\n",
      "Epoch  6, Batch 179 Loss:0.2212296426296234\n",
      "Epoch  6, Batch 180 Loss:0.2173176109790802\n",
      "Epoch  6, Batch 181 Loss:0.25623247027397156\n",
      "Epoch  6, Batch 182 Loss:0.22564487159252167\n",
      "Epoch  6, Batch 183 Loss:0.23459546267986298\n",
      "Epoch  6, Batch 184 Loss:0.19407710433006287\n",
      "Epoch  6, Batch 185 Loss:0.3019178807735443\n",
      "Epoch  6, Batch 186 Loss:0.20432473719120026\n",
      "Epoch  6, Batch 187 Loss:0.2119341641664505\n",
      "Epoch  6, Batch 188 Loss:0.25496023893356323\n",
      "Epoch  6, Batch 189 Loss:0.1733168363571167\n",
      "Epoch  6, Batch 190 Loss:0.24334681034088135\n",
      "Epoch  6, Batch 191 Loss:0.20721092820167542\n",
      "Epoch  6, Batch 192 Loss:0.2268815040588379\n",
      "Epoch  6, Batch 193 Loss:0.281843364238739\n",
      "Epoch  6, Batch 194 Loss:0.2247529923915863\n",
      "Epoch  6, Batch 195 Loss:0.22889383137226105\n",
      "Epoch  6, Batch 196 Loss:0.19419266283512115\n",
      "Epoch  6, Batch 197 Loss:0.29137706756591797\n",
      "Epoch  6, Batch 198 Loss:0.23521272838115692\n",
      "Epoch  6, Batch 199 Loss:0.3135627210140228\n",
      "Epoch  6, Batch 200 Loss:0.19699685275554657\n",
      "Epoch  6, Batch 201 Loss:0.24376268684864044\n",
      "Epoch  6, Batch 202 Loss:0.1738053560256958\n",
      "Epoch  6, Batch 203 Loss:0.256596177816391\n",
      "Epoch  6, Batch 204 Loss:0.24353551864624023\n",
      "Epoch  6, Batch 205 Loss:0.2708697021007538\n",
      "Epoch  6, Batch 206 Loss:0.21164226531982422\n",
      "Epoch  6, Batch 207 Loss:0.1888582706451416\n",
      "Epoch  6, Batch 208 Loss:0.19061362743377686\n",
      "Epoch  6, Batch 209 Loss:0.289742112159729\n",
      "Epoch  6, Batch 210 Loss:0.25760698318481445\n",
      "Epoch  6, Batch 211 Loss:0.1738612949848175\n",
      "Epoch  6, Batch 212 Loss:0.2524678111076355\n",
      "Epoch  6, Batch 213 Loss:0.26304808259010315\n",
      "Epoch  6, Batch 214 Loss:0.26178184151649475\n",
      "Epoch  6, Batch 215 Loss:0.18773813545703888\n",
      "Epoch  6, Batch 216 Loss:0.19534724950790405\n",
      "Epoch  6, Batch 217 Loss:0.19970184564590454\n",
      "Epoch  6, Batch 218 Loss:0.34976139664649963\n",
      "Epoch  6, Batch 219 Loss:0.3015984892845154\n",
      "Epoch  6, Batch 220 Loss:0.19362770020961761\n",
      "Epoch  6, Batch 221 Loss:0.28136885166168213\n",
      "Epoch  6, Batch 222 Loss:0.1965060830116272\n",
      "Epoch  6, Batch 223 Loss:0.20273005962371826\n",
      "Epoch  6, Batch 224 Loss:0.2716173529624939\n",
      "Epoch  6, Batch 225 Loss:0.21718290448188782\n",
      "Epoch  6, Batch 226 Loss:0.2189250886440277\n",
      "Epoch  6, Batch 227 Loss:0.31867048144340515\n",
      "Epoch  6, Batch 228 Loss:0.23085209727287292\n",
      "Epoch  6, Batch 229 Loss:0.2281239777803421\n",
      "Epoch  6, Batch 230 Loss:0.21998712420463562\n",
      "Epoch  6, Batch 231 Loss:0.2553144693374634\n",
      "Epoch  6, Batch 232 Loss:0.2901837229728699\n",
      "Epoch  6, Batch 233 Loss:0.23421970009803772\n",
      "Loss in this Epoch is: 23.4219700098 %\n",
      "Accuracy in this Epoch is: 88.2000029087 %\n",
      "Epoch  7, Batch 0 Loss:0.23749861121177673\n",
      "Epoch  7, Batch 1 Loss:0.18731063604354858\n",
      "Epoch  7, Batch 2 Loss:0.2030031383037567\n",
      "Epoch  7, Batch 3 Loss:0.22708195447921753\n",
      "Epoch  7, Batch 4 Loss:0.19471214711666107\n",
      "Epoch  7, Batch 5 Loss:0.1937447041273117\n",
      "Epoch  7, Batch 6 Loss:0.23018617928028107\n",
      "Epoch  7, Batch 7 Loss:0.22649680078029633\n",
      "Epoch  7, Batch 8 Loss:0.23100663721561432\n",
      "Epoch  7, Batch 9 Loss:0.21289585530757904\n",
      "Epoch  7, Batch 10 Loss:0.23972617089748383\n",
      "Epoch  7, Batch 11 Loss:0.2414361983537674\n",
      "Epoch  7, Batch 12 Loss:0.2380514144897461\n",
      "Epoch  7, Batch 13 Loss:0.24847771227359772\n",
      "Epoch  7, Batch 14 Loss:0.19435565173625946\n",
      "Epoch  7, Batch 15 Loss:0.2111959159374237\n",
      "Epoch  7, Batch 16 Loss:0.22444313764572144\n",
      "Epoch  7, Batch 17 Loss:0.2152010202407837\n",
      "Epoch  7, Batch 18 Loss:0.17725729942321777\n",
      "Epoch  7, Batch 19 Loss:0.2307443767786026\n",
      "Epoch  7, Batch 20 Loss:0.190813347697258\n",
      "Epoch  7, Batch 21 Loss:0.18847212195396423\n",
      "Epoch  7, Batch 22 Loss:0.20211707055568695\n",
      "Epoch  7, Batch 23 Loss:0.17968672513961792\n",
      "Epoch  7, Batch 24 Loss:0.2368134707212448\n",
      "Epoch  7, Batch 25 Loss:0.23070161044597626\n",
      "Epoch  7, Batch 26 Loss:0.1843777745962143\n",
      "Epoch  7, Batch 27 Loss:0.24884840846061707\n",
      "Epoch  7, Batch 28 Loss:0.2127276360988617\n",
      "Epoch  7, Batch 29 Loss:0.17574837803840637\n",
      "Epoch  7, Batch 30 Loss:0.1966233253479004\n",
      "Epoch  7, Batch 31 Loss:0.20420099794864655\n",
      "Epoch  7, Batch 32 Loss:0.1838470995426178\n",
      "Epoch  7, Batch 33 Loss:0.1789005696773529\n",
      "Epoch  7, Batch 34 Loss:0.14173081517219543\n",
      "Epoch  7, Batch 35 Loss:0.23309053480625153\n",
      "Epoch  7, Batch 36 Loss:0.1976689100265503\n",
      "Epoch  7, Batch 37 Loss:0.2110273241996765\n",
      "Epoch  7, Batch 38 Loss:0.13663993775844574\n",
      "Epoch  7, Batch 39 Loss:0.16258642077445984\n",
      "Epoch  7, Batch 40 Loss:0.24910756945610046\n",
      "Epoch  7, Batch 41 Loss:0.15527616441249847\n",
      "Epoch  7, Batch 42 Loss:0.220403254032135\n",
      "Epoch  7, Batch 43 Loss:0.21886712312698364\n",
      "Epoch  7, Batch 44 Loss:0.157723531126976\n",
      "Epoch  7, Batch 45 Loss:0.19837723672389984\n",
      "Epoch  7, Batch 46 Loss:0.17456774413585663\n",
      "Epoch  7, Batch 47 Loss:0.1857077181339264\n",
      "Epoch  7, Batch 48 Loss:0.23959961533546448\n",
      "Epoch  7, Batch 49 Loss:0.1996643990278244\n",
      "Epoch  7, Batch 50 Loss:0.20460203289985657\n",
      "Epoch  7, Batch 51 Loss:0.24580159783363342\n",
      "Epoch  7, Batch 52 Loss:0.17430734634399414\n",
      "Epoch  7, Batch 53 Loss:0.19743585586547852\n",
      "Epoch  7, Batch 54 Loss:0.25738945603370667\n",
      "Epoch  7, Batch 55 Loss:0.22572311758995056\n",
      "Epoch  7, Batch 56 Loss:0.14232094585895538\n",
      "Epoch  7, Batch 57 Loss:0.138715460896492\n",
      "Epoch  7, Batch 58 Loss:0.2268451303243637\n",
      "Epoch  7, Batch 59 Loss:0.2444310039281845\n",
      "Epoch  7, Batch 60 Loss:0.18768538534641266\n",
      "Epoch  7, Batch 61 Loss:0.16702838242053986\n",
      "Epoch  7, Batch 62 Loss:0.19825081527233124\n",
      "Epoch  7, Batch 63 Loss:0.20714765787124634\n",
      "Epoch  7, Batch 64 Loss:0.2152649164199829\n",
      "Epoch  7, Batch 65 Loss:0.283538281917572\n",
      "Epoch  7, Batch 66 Loss:0.2977641820907593\n",
      "Epoch  7, Batch 67 Loss:0.19584083557128906\n",
      "Epoch  7, Batch 68 Loss:0.24332810938358307\n",
      "Epoch  7, Batch 69 Loss:0.1504121720790863\n",
      "Epoch  7, Batch 70 Loss:0.21348287165164948\n",
      "Epoch  7, Batch 71 Loss:0.18924309313297272\n",
      "Epoch  7, Batch 72 Loss:0.18567092716693878\n",
      "Epoch  7, Batch 73 Loss:0.16791750490665436\n",
      "Epoch  7, Batch 74 Loss:0.20125828683376312\n",
      "Epoch  7, Batch 75 Loss:0.18918582797050476\n",
      "Epoch  7, Batch 76 Loss:0.2110709846019745\n",
      "Epoch  7, Batch 77 Loss:0.19273103773593903\n",
      "Epoch  7, Batch 78 Loss:0.2411932498216629\n",
      "Epoch  7, Batch 79 Loss:0.19854103028774261\n",
      "Epoch  7, Batch 80 Loss:0.1975884735584259\n",
      "Epoch  7, Batch 81 Loss:0.17105132341384888\n",
      "Epoch  7, Batch 82 Loss:0.1703145056962967\n",
      "Epoch  7, Batch 83 Loss:0.27112793922424316\n",
      "Epoch  7, Batch 84 Loss:0.15821918845176697\n",
      "Epoch  7, Batch 85 Loss:0.2674564719200134\n",
      "Epoch  7, Batch 86 Loss:0.19334757328033447\n",
      "Epoch  7, Batch 87 Loss:0.15480339527130127\n",
      "Epoch  7, Batch 88 Loss:0.2008882761001587\n",
      "Epoch  7, Batch 89 Loss:0.2223922163248062\n",
      "Epoch  7, Batch 90 Loss:0.2500258684158325\n",
      "Epoch  7, Batch 91 Loss:0.1808767318725586\n",
      "Epoch  7, Batch 92 Loss:0.23659484088420868\n",
      "Epoch  7, Batch 93 Loss:0.2633028030395508\n",
      "Epoch  7, Batch 94 Loss:0.24684767425060272\n",
      "Epoch  7, Batch 95 Loss:0.16898977756500244\n",
      "Epoch  7, Batch 96 Loss:0.18291491270065308\n",
      "Epoch  7, Batch 97 Loss:0.2305857092142105\n",
      "Epoch  7, Batch 98 Loss:0.24470563232898712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7, Batch 99 Loss:0.21558551490306854\n",
      "Epoch  7, Batch 100 Loss:0.20985732972621918\n",
      "Epoch  7, Batch 101 Loss:0.2576324939727783\n",
      "Epoch  7, Batch 102 Loss:0.16629323363304138\n",
      "Epoch  7, Batch 103 Loss:0.25307220220565796\n",
      "Epoch  7, Batch 104 Loss:0.22516997158527374\n",
      "Epoch  7, Batch 105 Loss:0.1939609944820404\n",
      "Epoch  7, Batch 106 Loss:0.15686452388763428\n",
      "Epoch  7, Batch 107 Loss:0.2038012146949768\n",
      "Epoch  7, Batch 108 Loss:0.23967687785625458\n",
      "Epoch  7, Batch 109 Loss:0.2490016520023346\n",
      "Epoch  7, Batch 110 Loss:0.17495808005332947\n",
      "Epoch  7, Batch 111 Loss:0.18406541645526886\n",
      "Epoch  7, Batch 112 Loss:0.1673567146062851\n",
      "Epoch  7, Batch 113 Loss:0.1697365641593933\n",
      "Epoch  7, Batch 114 Loss:0.1906311810016632\n",
      "Epoch  7, Batch 115 Loss:0.16864249110221863\n",
      "Epoch  7, Batch 116 Loss:0.23389190435409546\n",
      "Epoch  7, Batch 117 Loss:0.2091519832611084\n",
      "Epoch  7, Batch 118 Loss:0.2385861873626709\n",
      "Epoch  7, Batch 119 Loss:0.2959563136100769\n",
      "Epoch  7, Batch 120 Loss:0.20015034079551697\n",
      "Epoch  7, Batch 121 Loss:0.23903389275074005\n",
      "Epoch  7, Batch 122 Loss:0.2589958608150482\n",
      "Epoch  7, Batch 123 Loss:0.22182482481002808\n",
      "Epoch  7, Batch 124 Loss:0.15183278918266296\n",
      "Epoch  7, Batch 125 Loss:0.19444862008094788\n",
      "Epoch  7, Batch 126 Loss:0.2198277860879898\n",
      "Epoch  7, Batch 127 Loss:0.24868488311767578\n",
      "Epoch  7, Batch 128 Loss:0.2618425190448761\n",
      "Epoch  7, Batch 129 Loss:0.18235860764980316\n",
      "Epoch  7, Batch 130 Loss:0.24485169351100922\n",
      "Epoch  7, Batch 131 Loss:0.2409108281135559\n",
      "Epoch  7, Batch 132 Loss:0.22508332133293152\n",
      "Epoch  7, Batch 133 Loss:0.18871255218982697\n",
      "Epoch  7, Batch 134 Loss:0.23474010825157166\n",
      "Epoch  7, Batch 135 Loss:0.252552330493927\n",
      "Epoch  7, Batch 136 Loss:0.20868395268917084\n",
      "Epoch  7, Batch 137 Loss:0.18299943208694458\n",
      "Epoch  7, Batch 138 Loss:0.24133267998695374\n",
      "Epoch  7, Batch 139 Loss:0.1251925230026245\n",
      "Epoch  7, Batch 140 Loss:0.310438334941864\n",
      "Epoch  7, Batch 141 Loss:0.23562049865722656\n",
      "Epoch  7, Batch 142 Loss:0.21760216355323792\n",
      "Epoch  7, Batch 143 Loss:0.1917843073606491\n",
      "Epoch  7, Batch 144 Loss:0.18425369262695312\n",
      "Epoch  7, Batch 145 Loss:0.1853276491165161\n",
      "Epoch  7, Batch 146 Loss:0.21824319660663605\n",
      "Epoch  7, Batch 147 Loss:0.1642160415649414\n",
      "Epoch  7, Batch 148 Loss:0.23312388360500336\n",
      "Epoch  7, Batch 149 Loss:0.16927790641784668\n",
      "Epoch  7, Batch 150 Loss:0.26449054479599\n",
      "Epoch  7, Batch 151 Loss:0.22903291881084442\n",
      "Epoch  7, Batch 152 Loss:0.19124773144721985\n",
      "Epoch  7, Batch 153 Loss:0.19928856194019318\n",
      "Epoch  7, Batch 154 Loss:0.2343405783176422\n",
      "Epoch  7, Batch 155 Loss:0.2674452066421509\n",
      "Epoch  7, Batch 156 Loss:0.20403534173965454\n",
      "Epoch  7, Batch 157 Loss:0.18363934755325317\n",
      "Epoch  7, Batch 158 Loss:0.18127253651618958\n",
      "Epoch  7, Batch 159 Loss:0.16502180695533752\n",
      "Epoch  7, Batch 160 Loss:0.25779983401298523\n",
      "Epoch  7, Batch 161 Loss:0.2541739344596863\n",
      "Epoch  7, Batch 162 Loss:0.16847534477710724\n",
      "Epoch  7, Batch 163 Loss:0.22252841293811798\n",
      "Epoch  7, Batch 164 Loss:0.2147212028503418\n",
      "Epoch  7, Batch 165 Loss:0.23503175377845764\n",
      "Epoch  7, Batch 166 Loss:0.1741459220647812\n",
      "Epoch  7, Batch 167 Loss:0.22612795233726501\n",
      "Epoch  7, Batch 168 Loss:0.21589341759681702\n",
      "Epoch  7, Batch 169 Loss:0.18915894627571106\n",
      "Epoch  7, Batch 170 Loss:0.22685135900974274\n",
      "Epoch  7, Batch 171 Loss:0.23418255150318146\n",
      "Epoch  7, Batch 172 Loss:0.20866811275482178\n",
      "Epoch  7, Batch 173 Loss:0.2671774625778198\n",
      "Epoch  7, Batch 174 Loss:0.2409818470478058\n",
      "Epoch  7, Batch 175 Loss:0.25136780738830566\n",
      "Epoch  7, Batch 176 Loss:0.1700671911239624\n",
      "Epoch  7, Batch 177 Loss:0.24285182356834412\n",
      "Epoch  7, Batch 178 Loss:0.23745118081569672\n",
      "Epoch  7, Batch 179 Loss:0.22341656684875488\n",
      "Epoch  7, Batch 180 Loss:0.2109854817390442\n",
      "Epoch  7, Batch 181 Loss:0.22456227242946625\n",
      "Epoch  7, Batch 182 Loss:0.19386905431747437\n",
      "Epoch  7, Batch 183 Loss:0.23785802721977234\n",
      "Epoch  7, Batch 184 Loss:0.23339173197746277\n",
      "Epoch  7, Batch 185 Loss:0.22652313113212585\n",
      "Epoch  7, Batch 186 Loss:0.2346392422914505\n",
      "Epoch  7, Batch 187 Loss:0.22515007853507996\n",
      "Epoch  7, Batch 188 Loss:0.20800244808197021\n",
      "Epoch  7, Batch 189 Loss:0.31394898891448975\n",
      "Epoch  7, Batch 190 Loss:0.1704573780298233\n",
      "Epoch  7, Batch 191 Loss:0.2250126451253891\n",
      "Epoch  7, Batch 192 Loss:0.2394377887248993\n",
      "Epoch  7, Batch 193 Loss:0.20037582516670227\n",
      "Epoch  7, Batch 194 Loss:0.20586685836315155\n",
      "Epoch  7, Batch 195 Loss:0.19990530610084534\n",
      "Epoch  7, Batch 196 Loss:0.24742455780506134\n",
      "Epoch  7, Batch 197 Loss:0.2124880850315094\n",
      "Epoch  7, Batch 198 Loss:0.20070421695709229\n",
      "Epoch  7, Batch 199 Loss:0.18481601774692535\n",
      "Epoch  7, Batch 200 Loss:0.21034514904022217\n",
      "Epoch  7, Batch 201 Loss:0.16222119331359863\n",
      "Epoch  7, Batch 202 Loss:0.1879577934741974\n",
      "Epoch  7, Batch 203 Loss:0.2461315095424652\n",
      "Epoch  7, Batch 204 Loss:0.2422528862953186\n",
      "Epoch  7, Batch 205 Loss:0.2578597664833069\n",
      "Epoch  7, Batch 206 Loss:0.24414217472076416\n",
      "Epoch  7, Batch 207 Loss:0.2088807225227356\n",
      "Epoch  7, Batch 208 Loss:0.1837199628353119\n",
      "Epoch  7, Batch 209 Loss:0.2308352291584015\n",
      "Epoch  7, Batch 210 Loss:0.20071616768836975\n",
      "Epoch  7, Batch 211 Loss:0.26306599378585815\n",
      "Epoch  7, Batch 212 Loss:0.21253155171871185\n",
      "Epoch  7, Batch 213 Loss:0.21464957296848297\n",
      "Epoch  7, Batch 214 Loss:0.19922862946987152\n",
      "Epoch  7, Batch 215 Loss:0.14726276695728302\n",
      "Epoch  7, Batch 216 Loss:0.20667028427124023\n",
      "Epoch  7, Batch 217 Loss:0.17075231671333313\n",
      "Epoch  7, Batch 218 Loss:0.24232855439186096\n",
      "Epoch  7, Batch 219 Loss:0.218889981508255\n",
      "Epoch  7, Batch 220 Loss:0.2471892386674881\n",
      "Epoch  7, Batch 221 Loss:0.24009214341640472\n",
      "Epoch  7, Batch 222 Loss:0.23156598210334778\n",
      "Epoch  7, Batch 223 Loss:0.18252593278884888\n",
      "Epoch  7, Batch 224 Loss:0.21587297320365906\n",
      "Epoch  7, Batch 225 Loss:0.18372344970703125\n",
      "Epoch  7, Batch 226 Loss:0.206072598695755\n",
      "Epoch  7, Batch 227 Loss:0.27415066957473755\n",
      "Epoch  7, Batch 228 Loss:0.24950039386749268\n",
      "Epoch  7, Batch 229 Loss:0.2658895254135132\n",
      "Epoch  7, Batch 230 Loss:0.2701495885848999\n",
      "Epoch  7, Batch 231 Loss:0.2455473244190216\n",
      "Epoch  7, Batch 232 Loss:0.17922890186309814\n",
      "Epoch  7, Batch 233 Loss:0.16674476861953735\n",
      "Loss in this Epoch is: 16.674476862 %\n",
      "Accuracy in this Epoch is: 88.330000639 %\n",
      "Epoch  8, Batch 0 Loss:0.17508022487163544\n",
      "Epoch  8, Batch 1 Loss:0.1469113528728485\n",
      "Epoch  8, Batch 2 Loss:0.20220142602920532\n",
      "Epoch  8, Batch 3 Loss:0.238862082362175\n",
      "Epoch  8, Batch 4 Loss:0.15364331007003784\n",
      "Epoch  8, Batch 5 Loss:0.22514501214027405\n",
      "Epoch  8, Batch 6 Loss:0.21598416566848755\n",
      "Epoch  8, Batch 7 Loss:0.16745376586914062\n",
      "Epoch  8, Batch 8 Loss:0.2158629298210144\n",
      "Epoch  8, Batch 9 Loss:0.168034166097641\n",
      "Epoch  8, Batch 10 Loss:0.23418277502059937\n",
      "Epoch  8, Batch 11 Loss:0.2068769335746765\n",
      "Epoch  8, Batch 12 Loss:0.16165493428707123\n",
      "Epoch  8, Batch 13 Loss:0.16084636747837067\n",
      "Epoch  8, Batch 14 Loss:0.22615768015384674\n",
      "Epoch  8, Batch 15 Loss:0.22520758211612701\n",
      "Epoch  8, Batch 16 Loss:0.19446194171905518\n",
      "Epoch  8, Batch 17 Loss:0.193097323179245\n",
      "Epoch  8, Batch 18 Loss:0.1600421965122223\n",
      "Epoch  8, Batch 19 Loss:0.20509259402751923\n",
      "Epoch  8, Batch 20 Loss:0.14660052955150604\n",
      "Epoch  8, Batch 21 Loss:0.128776416182518\n",
      "Epoch  8, Batch 22 Loss:0.16983678936958313\n",
      "Epoch  8, Batch 23 Loss:0.1926427036523819\n",
      "Epoch  8, Batch 24 Loss:0.1912531703710556\n",
      "Epoch  8, Batch 25 Loss:0.19010061025619507\n",
      "Epoch  8, Batch 26 Loss:0.15719538927078247\n",
      "Epoch  8, Batch 27 Loss:0.2060440182685852\n",
      "Epoch  8, Batch 28 Loss:0.15944179892539978\n",
      "Epoch  8, Batch 29 Loss:0.211979478597641\n",
      "Epoch  8, Batch 30 Loss:0.20287485420703888\n",
      "Epoch  8, Batch 31 Loss:0.16824457049369812\n",
      "Epoch  8, Batch 32 Loss:0.1270744651556015\n",
      "Epoch  8, Batch 33 Loss:0.1993614286184311\n",
      "Epoch  8, Batch 34 Loss:0.2105875313282013\n",
      "Epoch  8, Batch 35 Loss:0.2210220843553543\n",
      "Epoch  8, Batch 36 Loss:0.27816715836524963\n",
      "Epoch  8, Batch 37 Loss:0.2246193140745163\n",
      "Epoch  8, Batch 38 Loss:0.2369289994239807\n",
      "Epoch  8, Batch 39 Loss:0.15694797039031982\n",
      "Epoch  8, Batch 40 Loss:0.25724858045578003\n",
      "Epoch  8, Batch 41 Loss:0.1271565854549408\n",
      "Epoch  8, Batch 42 Loss:0.2017904371023178\n",
      "Epoch  8, Batch 43 Loss:0.17803239822387695\n",
      "Epoch  8, Batch 44 Loss:0.1992684155702591\n",
      "Epoch  8, Batch 45 Loss:0.16575290262699127\n",
      "Epoch  8, Batch 46 Loss:0.1831163913011551\n",
      "Epoch  8, Batch 47 Loss:0.24550257623195648\n",
      "Epoch  8, Batch 48 Loss:0.2667904496192932\n",
      "Epoch  8, Batch 49 Loss:0.10853778570890427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8, Batch 50 Loss:0.1715089976787567\n",
      "Epoch  8, Batch 51 Loss:0.1870259940624237\n",
      "Epoch  8, Batch 52 Loss:0.21140208840370178\n",
      "Epoch  8, Batch 53 Loss:0.16572214663028717\n",
      "Epoch  8, Batch 54 Loss:0.1767403781414032\n",
      "Epoch  8, Batch 55 Loss:0.2414216697216034\n",
      "Epoch  8, Batch 56 Loss:0.24119938910007477\n",
      "Epoch  8, Batch 57 Loss:0.19423119723796844\n",
      "Epoch  8, Batch 58 Loss:0.1603667140007019\n",
      "Epoch  8, Batch 59 Loss:0.20279718935489655\n",
      "Epoch  8, Batch 60 Loss:0.22505289316177368\n",
      "Epoch  8, Batch 61 Loss:0.17729200422763824\n",
      "Epoch  8, Batch 62 Loss:0.18180575966835022\n",
      "Epoch  8, Batch 63 Loss:0.2603002190589905\n",
      "Epoch  8, Batch 64 Loss:0.27935224771499634\n",
      "Epoch  8, Batch 65 Loss:0.25377583503723145\n",
      "Epoch  8, Batch 66 Loss:0.18219266831874847\n",
      "Epoch  8, Batch 67 Loss:0.17309340834617615\n",
      "Epoch  8, Batch 68 Loss:0.15724590420722961\n",
      "Epoch  8, Batch 69 Loss:0.1823156476020813\n",
      "Epoch  8, Batch 70 Loss:0.179002046585083\n",
      "Epoch  8, Batch 71 Loss:0.1717132329940796\n",
      "Epoch  8, Batch 72 Loss:0.23097744584083557\n",
      "Epoch  8, Batch 73 Loss:0.2112874537706375\n",
      "Epoch  8, Batch 74 Loss:0.2245432734489441\n",
      "Epoch  8, Batch 75 Loss:0.1893550008535385\n",
      "Epoch  8, Batch 76 Loss:0.24544939398765564\n",
      "Epoch  8, Batch 77 Loss:0.17612117528915405\n",
      "Epoch  8, Batch 78 Loss:0.2176135778427124\n",
      "Epoch  8, Batch 79 Loss:0.2031613141298294\n",
      "Epoch  8, Batch 80 Loss:0.18256312608718872\n",
      "Epoch  8, Batch 81 Loss:0.30711260437965393\n",
      "Epoch  8, Batch 82 Loss:0.18191100656986237\n",
      "Epoch  8, Batch 83 Loss:0.24499668180942535\n",
      "Epoch  8, Batch 84 Loss:0.14151987433433533\n",
      "Epoch  8, Batch 85 Loss:0.2660316228866577\n",
      "Epoch  8, Batch 86 Loss:0.18109259009361267\n",
      "Epoch  8, Batch 87 Loss:0.16004033386707306\n",
      "Epoch  8, Batch 88 Loss:0.23003709316253662\n",
      "Epoch  8, Batch 89 Loss:0.2267664670944214\n",
      "Epoch  8, Batch 90 Loss:0.22301672399044037\n",
      "Epoch  8, Batch 91 Loss:0.14772161841392517\n",
      "Epoch  8, Batch 92 Loss:0.1786719411611557\n",
      "Epoch  8, Batch 93 Loss:0.1483595371246338\n",
      "Epoch  8, Batch 94 Loss:0.2453090250492096\n",
      "Epoch  8, Batch 95 Loss:0.14061850309371948\n",
      "Epoch  8, Batch 96 Loss:0.2892959713935852\n",
      "Epoch  8, Batch 97 Loss:0.29094862937927246\n",
      "Epoch  8, Batch 98 Loss:0.2265826314687729\n",
      "Epoch  8, Batch 99 Loss:0.2393375188112259\n",
      "Epoch  8, Batch 100 Loss:0.16065838932991028\n",
      "Epoch  8, Batch 101 Loss:0.1631852388381958\n",
      "Epoch  8, Batch 102 Loss:0.195344015955925\n",
      "Epoch  8, Batch 103 Loss:0.17997238039970398\n",
      "Epoch  8, Batch 104 Loss:0.20629066228866577\n",
      "Epoch  8, Batch 105 Loss:0.19938009977340698\n",
      "Epoch  8, Batch 106 Loss:0.19663569331169128\n",
      "Epoch  8, Batch 107 Loss:0.20090465247631073\n",
      "Epoch  8, Batch 108 Loss:0.26825690269470215\n",
      "Epoch  8, Batch 109 Loss:0.23816469311714172\n",
      "Epoch  8, Batch 110 Loss:0.15361620485782623\n",
      "Epoch  8, Batch 111 Loss:0.2138756811618805\n",
      "Epoch  8, Batch 112 Loss:0.17535346746444702\n",
      "Epoch  8, Batch 113 Loss:0.2792602777481079\n",
      "Epoch  8, Batch 114 Loss:0.3062416911125183\n",
      "Epoch  8, Batch 115 Loss:0.23041734099388123\n",
      "Epoch  8, Batch 116 Loss:0.3072589933872223\n",
      "Epoch  8, Batch 117 Loss:0.1727651059627533\n",
      "Epoch  8, Batch 118 Loss:0.19115754961967468\n",
      "Epoch  8, Batch 119 Loss:0.22892606258392334\n",
      "Epoch  8, Batch 120 Loss:0.19554460048675537\n",
      "Epoch  8, Batch 121 Loss:0.2142871916294098\n",
      "Epoch  8, Batch 122 Loss:0.18403999507427216\n",
      "Epoch  8, Batch 123 Loss:0.27306243777275085\n",
      "Epoch  8, Batch 124 Loss:0.16314660012722015\n",
      "Epoch  8, Batch 125 Loss:0.1898803412914276\n",
      "Epoch  8, Batch 126 Loss:0.27718472480773926\n",
      "Epoch  8, Batch 127 Loss:0.1886407732963562\n",
      "Epoch  8, Batch 128 Loss:0.2418895810842514\n",
      "Epoch  8, Batch 129 Loss:0.266975462436676\n",
      "Epoch  8, Batch 130 Loss:0.15783321857452393\n",
      "Epoch  8, Batch 131 Loss:0.23821058869361877\n",
      "Epoch  8, Batch 132 Loss:0.17546097934246063\n",
      "Epoch  8, Batch 133 Loss:0.18205931782722473\n",
      "Epoch  8, Batch 134 Loss:0.1532604694366455\n",
      "Epoch  8, Batch 135 Loss:0.24297837913036346\n",
      "Epoch  8, Batch 136 Loss:0.1644720882177353\n",
      "Epoch  8, Batch 137 Loss:0.23176568746566772\n",
      "Epoch  8, Batch 138 Loss:0.14578816294670105\n",
      "Epoch  8, Batch 139 Loss:0.21423649787902832\n",
      "Epoch  8, Batch 140 Loss:0.3024415671825409\n",
      "Epoch  8, Batch 141 Loss:0.19471263885498047\n",
      "Epoch  8, Batch 142 Loss:0.18836535513401031\n",
      "Epoch  8, Batch 143 Loss:0.24041131138801575\n",
      "Epoch  8, Batch 144 Loss:0.2293214499950409\n",
      "Epoch  8, Batch 145 Loss:0.20994216203689575\n",
      "Epoch  8, Batch 146 Loss:0.2052525281906128\n",
      "Epoch  8, Batch 147 Loss:0.2376859188079834\n",
      "Epoch  8, Batch 148 Loss:0.15476994216442108\n",
      "Epoch  8, Batch 149 Loss:0.21488261222839355\n",
      "Epoch  8, Batch 150 Loss:0.1754080206155777\n",
      "Epoch  8, Batch 151 Loss:0.17912253737449646\n",
      "Epoch  8, Batch 152 Loss:0.21474617719650269\n",
      "Epoch  8, Batch 153 Loss:0.181749626994133\n",
      "Epoch  8, Batch 154 Loss:0.1720551699399948\n",
      "Epoch  8, Batch 155 Loss:0.1690734475851059\n",
      "Epoch  8, Batch 156 Loss:0.14791113138198853\n",
      "Epoch  8, Batch 157 Loss:0.20120011270046234\n",
      "Epoch  8, Batch 158 Loss:0.21959318220615387\n",
      "Epoch  8, Batch 159 Loss:0.22279319167137146\n",
      "Epoch  8, Batch 160 Loss:0.21675556898117065\n",
      "Epoch  8, Batch 161 Loss:0.22433160245418549\n",
      "Epoch  8, Batch 162 Loss:0.19749347865581512\n",
      "Epoch  8, Batch 163 Loss:0.20487630367279053\n",
      "Epoch  8, Batch 164 Loss:0.2264729142189026\n",
      "Epoch  8, Batch 165 Loss:0.17538632452487946\n",
      "Epoch  8, Batch 166 Loss:0.23485517501831055\n",
      "Epoch  8, Batch 167 Loss:0.22056643664836884\n",
      "Epoch  8, Batch 168 Loss:0.250326007604599\n",
      "Epoch  8, Batch 169 Loss:0.208258256316185\n",
      "Epoch  8, Batch 170 Loss:0.17686980962753296\n",
      "Epoch  8, Batch 171 Loss:0.17572441697120667\n",
      "Epoch  8, Batch 172 Loss:0.15441332757472992\n",
      "Epoch  8, Batch 173 Loss:0.1885872483253479\n",
      "Epoch  8, Batch 174 Loss:0.2604173719882965\n",
      "Epoch  8, Batch 175 Loss:0.23390430212020874\n",
      "Epoch  8, Batch 176 Loss:0.19522380828857422\n",
      "Epoch  8, Batch 177 Loss:0.18925461173057556\n",
      "Epoch  8, Batch 178 Loss:0.2021852433681488\n",
      "Epoch  8, Batch 179 Loss:0.18264348804950714\n",
      "Epoch  8, Batch 180 Loss:0.20658299326896667\n",
      "Epoch  8, Batch 181 Loss:0.1849362552165985\n",
      "Epoch  8, Batch 182 Loss:0.17924627661705017\n",
      "Epoch  8, Batch 183 Loss:0.1507677584886551\n",
      "Epoch  8, Batch 184 Loss:0.2357921600341797\n",
      "Epoch  8, Batch 185 Loss:0.23325005173683167\n",
      "Epoch  8, Batch 186 Loss:0.13259357213974\n",
      "Epoch  8, Batch 187 Loss:0.18508362770080566\n",
      "Epoch  8, Batch 188 Loss:0.2860618531703949\n",
      "Epoch  8, Batch 189 Loss:0.18916228413581848\n",
      "Epoch  8, Batch 190 Loss:0.15236873924732208\n",
      "Epoch  8, Batch 191 Loss:0.19210083782672882\n",
      "Epoch  8, Batch 192 Loss:0.21442854404449463\n",
      "Epoch  8, Batch 193 Loss:0.25664222240448\n",
      "Epoch  8, Batch 194 Loss:0.12166110426187515\n",
      "Epoch  8, Batch 195 Loss:0.20678739249706268\n",
      "Epoch  8, Batch 196 Loss:0.21258671581745148\n",
      "Epoch  8, Batch 197 Loss:0.14076942205429077\n",
      "Epoch  8, Batch 198 Loss:0.2448728233575821\n",
      "Epoch  8, Batch 199 Loss:0.17084082961082458\n",
      "Epoch  8, Batch 200 Loss:0.2364276647567749\n",
      "Epoch  8, Batch 201 Loss:0.17019635438919067\n",
      "Epoch  8, Batch 202 Loss:0.20461446046829224\n",
      "Epoch  8, Batch 203 Loss:0.13852347433567047\n",
      "Epoch  8, Batch 204 Loss:0.267706036567688\n",
      "Epoch  8, Batch 205 Loss:0.16976691782474518\n",
      "Epoch  8, Batch 206 Loss:0.15737783908843994\n",
      "Epoch  8, Batch 207 Loss:0.191534161567688\n",
      "Epoch  8, Batch 208 Loss:0.17330099642276764\n",
      "Epoch  8, Batch 209 Loss:0.2415827065706253\n",
      "Epoch  8, Batch 210 Loss:0.18831440806388855\n",
      "Epoch  8, Batch 211 Loss:0.16653305292129517\n",
      "Epoch  8, Batch 212 Loss:0.20255425572395325\n",
      "Epoch  8, Batch 213 Loss:0.21381109952926636\n",
      "Epoch  8, Batch 214 Loss:0.16903601586818695\n",
      "Epoch  8, Batch 215 Loss:0.2024642825126648\n",
      "Epoch  8, Batch 216 Loss:0.21834713220596313\n",
      "Epoch  8, Batch 217 Loss:0.2112087905406952\n",
      "Epoch  8, Batch 218 Loss:0.20615069568157196\n",
      "Epoch  8, Batch 219 Loss:0.22169388830661774\n",
      "Epoch  8, Batch 220 Loss:0.22690856456756592\n",
      "Epoch  8, Batch 221 Loss:0.18221768736839294\n",
      "Epoch  8, Batch 222 Loss:0.20778903365135193\n",
      "Epoch  8, Batch 223 Loss:0.1961069107055664\n",
      "Epoch  8, Batch 224 Loss:0.20803293585777283\n",
      "Epoch  8, Batch 225 Loss:0.17268265783786774\n",
      "Epoch  8, Batch 226 Loss:0.19217129051685333\n",
      "Epoch  8, Batch 227 Loss:0.1521475911140442\n",
      "Epoch  8, Batch 228 Loss:0.20024144649505615\n",
      "Epoch  8, Batch 229 Loss:0.21949607133865356\n",
      "Epoch  8, Batch 230 Loss:0.20278021693229675\n",
      "Epoch  8, Batch 231 Loss:0.2257852852344513\n",
      "Epoch  8, Batch 232 Loss:0.1822662651538849\n",
      "Epoch  8, Batch 233 Loss:0.19605669379234314\n",
      "Loss in this Epoch is: 19.6056693792 %\n",
      "Accuracy in this Epoch is: 89.5900011063 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9, Batch 0 Loss:0.13394048810005188\n",
      "Epoch  9, Batch 1 Loss:0.1898641586303711\n",
      "Epoch  9, Batch 2 Loss:0.24018971621990204\n",
      "Epoch  9, Batch 3 Loss:0.16206148266792297\n",
      "Epoch  9, Batch 4 Loss:0.18024088442325592\n",
      "Epoch  9, Batch 5 Loss:0.15518058836460114\n",
      "Epoch  9, Batch 6 Loss:0.16548728942871094\n",
      "Epoch  9, Batch 7 Loss:0.18375040590763092\n",
      "Epoch  9, Batch 8 Loss:0.22223950922489166\n",
      "Epoch  9, Batch 9 Loss:0.18741267919540405\n",
      "Epoch  9, Batch 10 Loss:0.1897694319486618\n",
      "Epoch  9, Batch 11 Loss:0.16445592045783997\n",
      "Epoch  9, Batch 12 Loss:0.1516413539648056\n",
      "Epoch  9, Batch 13 Loss:0.11480652540922165\n",
      "Epoch  9, Batch 14 Loss:0.14037226140499115\n",
      "Epoch  9, Batch 15 Loss:0.18836134672164917\n",
      "Epoch  9, Batch 16 Loss:0.16063399612903595\n",
      "Epoch  9, Batch 17 Loss:0.16260109841823578\n",
      "Epoch  9, Batch 18 Loss:0.20073486864566803\n",
      "Epoch  9, Batch 19 Loss:0.15908151865005493\n",
      "Epoch  9, Batch 20 Loss:0.20622800290584564\n",
      "Epoch  9, Batch 21 Loss:0.14214274287223816\n",
      "Epoch  9, Batch 22 Loss:0.2635470926761627\n",
      "Epoch  9, Batch 23 Loss:0.1324411928653717\n",
      "Epoch  9, Batch 24 Loss:0.1441284567117691\n",
      "Epoch  9, Batch 25 Loss:0.17909014225006104\n",
      "Epoch  9, Batch 26 Loss:0.14707690477371216\n",
      "Epoch  9, Batch 27 Loss:0.20629389584064484\n",
      "Epoch  9, Batch 28 Loss:0.17787031829357147\n",
      "Epoch  9, Batch 29 Loss:0.2541560232639313\n",
      "Epoch  9, Batch 30 Loss:0.20387162268161774\n",
      "Epoch  9, Batch 31 Loss:0.16070596873760223\n",
      "Epoch  9, Batch 32 Loss:0.20733624696731567\n",
      "Epoch  9, Batch 33 Loss:0.17935942113399506\n",
      "Epoch  9, Batch 34 Loss:0.25809890031814575\n",
      "Epoch  9, Batch 35 Loss:0.1685442179441452\n",
      "Epoch  9, Batch 36 Loss:0.18321986496448517\n",
      "Epoch  9, Batch 37 Loss:0.19298768043518066\n",
      "Epoch  9, Batch 38 Loss:0.16084498167037964\n",
      "Epoch  9, Batch 39 Loss:0.2262488603591919\n",
      "Epoch  9, Batch 40 Loss:0.13349181413650513\n",
      "Epoch  9, Batch 41 Loss:0.20377160608768463\n",
      "Epoch  9, Batch 42 Loss:0.1832326352596283\n",
      "Epoch  9, Batch 43 Loss:0.15705128014087677\n",
      "Epoch  9, Batch 44 Loss:0.17993389070034027\n",
      "Epoch  9, Batch 45 Loss:0.11483423411846161\n",
      "Epoch  9, Batch 46 Loss:0.16907574236392975\n",
      "Epoch  9, Batch 47 Loss:0.14679747819900513\n",
      "Epoch  9, Batch 48 Loss:0.24735242128372192\n",
      "Epoch  9, Batch 49 Loss:0.24488192796707153\n",
      "Epoch  9, Batch 50 Loss:0.18243010342121124\n",
      "Epoch  9, Batch 51 Loss:0.19028523564338684\n",
      "Epoch  9, Batch 52 Loss:0.18536333739757538\n",
      "Epoch  9, Batch 53 Loss:0.1610761433839798\n",
      "Epoch  9, Batch 54 Loss:0.17409281432628632\n",
      "Epoch  9, Batch 55 Loss:0.18498307466506958\n",
      "Epoch  9, Batch 56 Loss:0.1407017707824707\n",
      "Epoch  9, Batch 57 Loss:0.2578699290752411\n",
      "Epoch  9, Batch 58 Loss:0.1639227569103241\n",
      "Epoch  9, Batch 59 Loss:0.1789606809616089\n",
      "Epoch  9, Batch 60 Loss:0.21293959021568298\n",
      "Epoch  9, Batch 61 Loss:0.22522099316120148\n",
      "Epoch  9, Batch 62 Loss:0.14342960715293884\n",
      "Epoch  9, Batch 63 Loss:0.1801372617483139\n",
      "Epoch  9, Batch 64 Loss:0.22047443687915802\n",
      "Epoch  9, Batch 65 Loss:0.2087211012840271\n",
      "Epoch  9, Batch 66 Loss:0.23739612102508545\n",
      "Epoch  9, Batch 67 Loss:0.18425530195236206\n",
      "Epoch  9, Batch 68 Loss:0.18902082741260529\n",
      "Epoch  9, Batch 69 Loss:0.23579441010951996\n",
      "Epoch  9, Batch 70 Loss:0.21170435845851898\n",
      "Epoch  9, Batch 71 Loss:0.1997169703245163\n",
      "Epoch  9, Batch 72 Loss:0.1886884719133377\n",
      "Epoch  9, Batch 73 Loss:0.14685486257076263\n",
      "Epoch  9, Batch 74 Loss:0.19827306270599365\n",
      "Epoch  9, Batch 75 Loss:0.1535494476556778\n",
      "Epoch  9, Batch 76 Loss:0.22073426842689514\n",
      "Epoch  9, Batch 77 Loss:0.1498715579509735\n",
      "Epoch  9, Batch 78 Loss:0.24494963884353638\n",
      "Epoch  9, Batch 79 Loss:0.10722342133522034\n",
      "Epoch  9, Batch 80 Loss:0.17353211343288422\n",
      "Epoch  9, Batch 81 Loss:0.13384005427360535\n",
      "Epoch  9, Batch 82 Loss:0.16422004997730255\n",
      "Epoch  9, Batch 83 Loss:0.2360081523656845\n",
      "Epoch  9, Batch 84 Loss:0.16413626074790955\n",
      "Epoch  9, Batch 85 Loss:0.14107367396354675\n",
      "Epoch  9, Batch 86 Loss:0.19672343134880066\n",
      "Epoch  9, Batch 87 Loss:0.16294151544570923\n",
      "Epoch  9, Batch 88 Loss:0.1659165918827057\n",
      "Epoch  9, Batch 89 Loss:0.1706489622592926\n",
      "Epoch  9, Batch 90 Loss:0.09749435633420944\n",
      "Epoch  9, Batch 91 Loss:0.15582557022571564\n",
      "Epoch  9, Batch 92 Loss:0.18211638927459717\n",
      "Epoch  9, Batch 93 Loss:0.18387676775455475\n",
      "Epoch  9, Batch 94 Loss:0.1523493528366089\n",
      "Epoch  9, Batch 95 Loss:0.18900419771671295\n",
      "Epoch  9, Batch 96 Loss:0.189703568816185\n",
      "Epoch  9, Batch 97 Loss:0.18924814462661743\n",
      "Epoch  9, Batch 98 Loss:0.18955248594284058\n",
      "Epoch  9, Batch 99 Loss:0.1775854229927063\n",
      "Epoch  9, Batch 100 Loss:0.1786988377571106\n",
      "Epoch  9, Batch 101 Loss:0.16095295548439026\n",
      "Epoch  9, Batch 102 Loss:0.14546534419059753\n",
      "Epoch  9, Batch 103 Loss:0.22649908065795898\n",
      "Epoch  9, Batch 104 Loss:0.10843509435653687\n",
      "Epoch  9, Batch 105 Loss:0.2508571743965149\n",
      "Epoch  9, Batch 106 Loss:0.18067994713783264\n",
      "Epoch  9, Batch 107 Loss:0.13283920288085938\n",
      "Epoch  9, Batch 108 Loss:0.21708574891090393\n",
      "Epoch  9, Batch 109 Loss:0.15367697179317474\n",
      "Epoch  9, Batch 110 Loss:0.1748824119567871\n",
      "Epoch  9, Batch 111 Loss:0.183327779173851\n",
      "Epoch  9, Batch 112 Loss:0.18993619084358215\n",
      "Epoch  9, Batch 113 Loss:0.19450139999389648\n",
      "Epoch  9, Batch 114 Loss:0.13543686270713806\n",
      "Epoch  9, Batch 115 Loss:0.14662835001945496\n",
      "Epoch  9, Batch 116 Loss:0.19322672486305237\n",
      "Epoch  9, Batch 117 Loss:0.15663038194179535\n",
      "Epoch  9, Batch 118 Loss:0.14414125680923462\n",
      "Epoch  9, Batch 119 Loss:0.23351265490055084\n",
      "Epoch  9, Batch 120 Loss:0.20240288972854614\n",
      "Epoch  9, Batch 121 Loss:0.2618469297885895\n",
      "Epoch  9, Batch 122 Loss:0.21114732325077057\n",
      "Epoch  9, Batch 123 Loss:0.2001953125\n",
      "Epoch  9, Batch 124 Loss:0.18715853989124298\n",
      "Epoch  9, Batch 125 Loss:0.16286616027355194\n",
      "Epoch  9, Batch 126 Loss:0.16584083437919617\n",
      "Epoch  9, Batch 127 Loss:0.17004117369651794\n",
      "Epoch  9, Batch 128 Loss:0.23941850662231445\n",
      "Epoch  9, Batch 129 Loss:0.16885589063167572\n",
      "Epoch  9, Batch 130 Loss:0.1590448021888733\n",
      "Epoch  9, Batch 131 Loss:0.17440584301948547\n",
      "Epoch  9, Batch 132 Loss:0.1264801025390625\n",
      "Epoch  9, Batch 133 Loss:0.14430001378059387\n",
      "Epoch  9, Batch 134 Loss:0.09753978252410889\n",
      "Epoch  9, Batch 135 Loss:0.150928795337677\n",
      "Epoch  9, Batch 136 Loss:0.12807103991508484\n",
      "Epoch  9, Batch 137 Loss:0.21245023608207703\n",
      "Epoch  9, Batch 138 Loss:0.14517070353031158\n",
      "Epoch  9, Batch 139 Loss:0.20971788465976715\n",
      "Epoch  9, Batch 140 Loss:0.12889112532138824\n",
      "Epoch  9, Batch 141 Loss:0.17989236116409302\n",
      "Epoch  9, Batch 142 Loss:0.1595279574394226\n",
      "Epoch  9, Batch 143 Loss:0.16887740790843964\n",
      "Epoch  9, Batch 144 Loss:0.14007142186164856\n",
      "Epoch  9, Batch 145 Loss:0.15474574267864227\n",
      "Epoch  9, Batch 146 Loss:0.17628028988838196\n",
      "Epoch  9, Batch 147 Loss:0.20186248421669006\n",
      "Epoch  9, Batch 148 Loss:0.19646970927715302\n",
      "Epoch  9, Batch 149 Loss:0.17066311836242676\n",
      "Epoch  9, Batch 150 Loss:0.17882835865020752\n",
      "Epoch  9, Batch 151 Loss:0.17067065834999084\n",
      "Epoch  9, Batch 152 Loss:0.26642683148384094\n",
      "Epoch  9, Batch 153 Loss:0.20998461544513702\n",
      "Epoch  9, Batch 154 Loss:0.1591072380542755\n",
      "Epoch  9, Batch 155 Loss:0.19154009222984314\n",
      "Epoch  9, Batch 156 Loss:0.1984638273715973\n",
      "Epoch  9, Batch 157 Loss:0.12285307794809341\n",
      "Epoch  9, Batch 158 Loss:0.1615758240222931\n",
      "Epoch  9, Batch 159 Loss:0.23683331906795502\n",
      "Epoch  9, Batch 160 Loss:0.18282583355903625\n",
      "Epoch  9, Batch 161 Loss:0.1761876344680786\n",
      "Epoch  9, Batch 162 Loss:0.2227097451686859\n",
      "Epoch  9, Batch 163 Loss:0.21265794336795807\n",
      "Epoch  9, Batch 164 Loss:0.26616108417510986\n",
      "Epoch  9, Batch 165 Loss:0.20366156101226807\n",
      "Epoch  9, Batch 166 Loss:0.25713974237442017\n",
      "Epoch  9, Batch 167 Loss:0.22785012423992157\n",
      "Epoch  9, Batch 168 Loss:0.16360589861869812\n",
      "Epoch  9, Batch 169 Loss:0.16352519392967224\n",
      "Epoch  9, Batch 170 Loss:0.19427654147148132\n",
      "Epoch  9, Batch 171 Loss:0.19110454618930817\n",
      "Epoch  9, Batch 172 Loss:0.15142636001110077\n",
      "Epoch  9, Batch 173 Loss:0.19052034616470337\n",
      "Epoch  9, Batch 174 Loss:0.1876705139875412\n",
      "Epoch  9, Batch 175 Loss:0.13295307755470276\n",
      "Epoch  9, Batch 176 Loss:0.19463501870632172\n",
      "Epoch  9, Batch 177 Loss:0.2027549147605896\n",
      "Epoch  9, Batch 178 Loss:0.2054927945137024\n",
      "Epoch  9, Batch 179 Loss:0.20313730835914612\n",
      "Epoch  9, Batch 180 Loss:0.20611101388931274\n",
      "Epoch  9, Batch 181 Loss:0.1825377345085144\n",
      "Epoch  9, Batch 182 Loss:0.20082007348537445\n",
      "Epoch  9, Batch 183 Loss:0.1295962631702423\n",
      "Epoch  9, Batch 184 Loss:0.21146340668201447\n",
      "Epoch  9, Batch 185 Loss:0.20393551886081696\n",
      "Epoch  9, Batch 186 Loss:0.22159382700920105\n",
      "Epoch  9, Batch 187 Loss:0.13515204191207886\n",
      "Epoch  9, Batch 188 Loss:0.19065627455711365\n",
      "Epoch  9, Batch 189 Loss:0.19639120995998383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9, Batch 190 Loss:0.16907495260238647\n",
      "Epoch  9, Batch 191 Loss:0.22780856490135193\n",
      "Epoch  9, Batch 192 Loss:0.1731712818145752\n",
      "Epoch  9, Batch 193 Loss:0.15512628853321075\n",
      "Epoch  9, Batch 194 Loss:0.1532515287399292\n",
      "Epoch  9, Batch 195 Loss:0.18472544848918915\n",
      "Epoch  9, Batch 196 Loss:0.26383015513420105\n",
      "Epoch  9, Batch 197 Loss:0.16388370096683502\n",
      "Epoch  9, Batch 198 Loss:0.1995052993297577\n",
      "Epoch  9, Batch 199 Loss:0.19012492895126343\n",
      "Epoch  9, Batch 200 Loss:0.17304019629955292\n",
      "Epoch  9, Batch 201 Loss:0.20432308316230774\n",
      "Epoch  9, Batch 202 Loss:0.2043580710887909\n",
      "Epoch  9, Batch 203 Loss:0.14530140161514282\n",
      "Epoch  9, Batch 204 Loss:0.2222045361995697\n",
      "Epoch  9, Batch 205 Loss:0.2411046028137207\n",
      "Epoch  9, Batch 206 Loss:0.17438599467277527\n",
      "Epoch  9, Batch 207 Loss:0.23940816521644592\n",
      "Epoch  9, Batch 208 Loss:0.24313382804393768\n",
      "Epoch  9, Batch 209 Loss:0.17633819580078125\n",
      "Epoch  9, Batch 210 Loss:0.1863245964050293\n",
      "Epoch  9, Batch 211 Loss:0.20894755423069\n",
      "Epoch  9, Batch 212 Loss:0.19447049498558044\n",
      "Epoch  9, Batch 213 Loss:0.1596071720123291\n",
      "Epoch  9, Batch 214 Loss:0.18918341398239136\n",
      "Epoch  9, Batch 215 Loss:0.22247722744941711\n",
      "Epoch  9, Batch 216 Loss:0.15811513364315033\n",
      "Epoch  9, Batch 217 Loss:0.17471826076507568\n",
      "Epoch  9, Batch 218 Loss:0.21375331282615662\n",
      "Epoch  9, Batch 219 Loss:0.2057318538427353\n",
      "Epoch  9, Batch 220 Loss:0.1589057892560959\n",
      "Epoch  9, Batch 221 Loss:0.17193275690078735\n",
      "Epoch  9, Batch 222 Loss:0.23775595426559448\n",
      "Epoch  9, Batch 223 Loss:0.1670626401901245\n",
      "Epoch  9, Batch 224 Loss:0.14887180924415588\n",
      "Epoch  9, Batch 225 Loss:0.21437399089336395\n",
      "Epoch  9, Batch 226 Loss:0.10633666813373566\n",
      "Epoch  9, Batch 227 Loss:0.161035418510437\n",
      "Epoch  9, Batch 228 Loss:0.2055886685848236\n",
      "Epoch  9, Batch 229 Loss:0.24570025503635406\n",
      "Epoch  9, Batch 230 Loss:0.15475863218307495\n",
      "Epoch  9, Batch 231 Loss:0.23503297567367554\n",
      "Epoch  9, Batch 232 Loss:0.14557459950447083\n",
      "Epoch  9, Batch 233 Loss:0.1386842429637909\n",
      "Loss in this Epoch is: 13.8684242964 %\n",
      "Accuracy in this Epoch is: 89.3999993801 %\n",
      "Epoch 10, Batch 0 Loss:0.1391545981168747\n",
      "Epoch 10, Batch 1 Loss:0.17584046721458435\n",
      "Epoch 10, Batch 2 Loss:0.16012869775295258\n",
      "Epoch 10, Batch 3 Loss:0.2023969292640686\n",
      "Epoch 10, Batch 4 Loss:0.13893529772758484\n",
      "Epoch 10, Batch 5 Loss:0.1895357072353363\n",
      "Epoch 10, Batch 6 Loss:0.20726647973060608\n",
      "Epoch 10, Batch 7 Loss:0.1685609370470047\n",
      "Epoch 10, Batch 8 Loss:0.15333499014377594\n",
      "Epoch 10, Batch 9 Loss:0.17673195898532867\n",
      "Epoch 10, Batch 10 Loss:0.12680624425411224\n",
      "Epoch 10, Batch 11 Loss:0.1470593810081482\n",
      "Epoch 10, Batch 12 Loss:0.160572811961174\n",
      "Epoch 10, Batch 13 Loss:0.11770230531692505\n",
      "Epoch 10, Batch 14 Loss:0.19107981026172638\n",
      "Epoch 10, Batch 15 Loss:0.17149004340171814\n",
      "Epoch 10, Batch 16 Loss:0.20205803215503693\n",
      "Epoch 10, Batch 17 Loss:0.15279215574264526\n",
      "Epoch 10, Batch 18 Loss:0.18145786225795746\n",
      "Epoch 10, Batch 19 Loss:0.19087111949920654\n",
      "Epoch 10, Batch 20 Loss:0.15204279124736786\n",
      "Epoch 10, Batch 21 Loss:0.15949927270412445\n",
      "Epoch 10, Batch 22 Loss:0.1509130746126175\n",
      "Epoch 10, Batch 23 Loss:0.18715541064739227\n",
      "Epoch 10, Batch 24 Loss:0.12946367263793945\n",
      "Epoch 10, Batch 25 Loss:0.10723259299993515\n",
      "Epoch 10, Batch 26 Loss:0.19584156572818756\n",
      "Epoch 10, Batch 27 Loss:0.13295945525169373\n",
      "Epoch 10, Batch 28 Loss:0.0959489643573761\n",
      "Epoch 10, Batch 29 Loss:0.17989398539066315\n",
      "Epoch 10, Batch 30 Loss:0.16934284567832947\n",
      "Epoch 10, Batch 31 Loss:0.16047613322734833\n",
      "Epoch 10, Batch 32 Loss:0.18178749084472656\n",
      "Epoch 10, Batch 33 Loss:0.14577294886112213\n",
      "Epoch 10, Batch 34 Loss:0.1901344656944275\n",
      "Epoch 10, Batch 35 Loss:0.24798555672168732\n",
      "Epoch 10, Batch 36 Loss:0.18253004550933838\n",
      "Epoch 10, Batch 37 Loss:0.16879397630691528\n",
      "Epoch 10, Batch 38 Loss:0.19594348967075348\n",
      "Epoch 10, Batch 39 Loss:0.15199723839759827\n",
      "Epoch 10, Batch 40 Loss:0.1708727926015854\n",
      "Epoch 10, Batch 41 Loss:0.12116683274507523\n",
      "Epoch 10, Batch 42 Loss:0.147969588637352\n",
      "Epoch 10, Batch 43 Loss:0.12244132906198502\n",
      "Epoch 10, Batch 44 Loss:0.18208399415016174\n",
      "Epoch 10, Batch 45 Loss:0.10757249593734741\n",
      "Epoch 10, Batch 46 Loss:0.1597181260585785\n",
      "Epoch 10, Batch 47 Loss:0.09481323510408401\n",
      "Epoch 10, Batch 48 Loss:0.15819241106510162\n",
      "Epoch 10, Batch 49 Loss:0.17086146771907806\n",
      "Epoch 10, Batch 50 Loss:0.19337302446365356\n",
      "Epoch 10, Batch 51 Loss:0.17178921401500702\n",
      "Epoch 10, Batch 52 Loss:0.18968577682971954\n",
      "Epoch 10, Batch 53 Loss:0.11167436838150024\n",
      "Epoch 10, Batch 54 Loss:0.19263817369937897\n",
      "Epoch 10, Batch 55 Loss:0.14961999654769897\n",
      "Epoch 10, Batch 56 Loss:0.13420476019382477\n",
      "Epoch 10, Batch 57 Loss:0.17399962246418\n",
      "Epoch 10, Batch 58 Loss:0.20271018147468567\n",
      "Epoch 10, Batch 59 Loss:0.17458303272724152\n",
      "Epoch 10, Batch 60 Loss:0.1312042772769928\n",
      "Epoch 10, Batch 61 Loss:0.19362878799438477\n",
      "Epoch 10, Batch 62 Loss:0.18836966156959534\n",
      "Epoch 10, Batch 63 Loss:0.1437120884656906\n",
      "Epoch 10, Batch 64 Loss:0.13009507954120636\n",
      "Epoch 10, Batch 65 Loss:0.16330638527870178\n",
      "Epoch 10, Batch 66 Loss:0.2769113779067993\n",
      "Epoch 10, Batch 67 Loss:0.15919949114322662\n",
      "Epoch 10, Batch 68 Loss:0.10895926505327225\n",
      "Epoch 10, Batch 69 Loss:0.1253553181886673\n",
      "Epoch 10, Batch 70 Loss:0.14665450155735016\n",
      "Epoch 10, Batch 71 Loss:0.1700904816389084\n",
      "Epoch 10, Batch 72 Loss:0.2773616909980774\n",
      "Epoch 10, Batch 73 Loss:0.13288074731826782\n",
      "Epoch 10, Batch 74 Loss:0.11183781921863556\n",
      "Epoch 10, Batch 75 Loss:0.15619578957557678\n",
      "Epoch 10, Batch 76 Loss:0.1870061308145523\n",
      "Epoch 10, Batch 77 Loss:0.18956364691257477\n",
      "Epoch 10, Batch 78 Loss:0.16210129857063293\n",
      "Epoch 10, Batch 79 Loss:0.1331842541694641\n",
      "Epoch 10, Batch 80 Loss:0.15288382768630981\n",
      "Epoch 10, Batch 81 Loss:0.20694299042224884\n",
      "Epoch 10, Batch 82 Loss:0.16317114233970642\n",
      "Epoch 10, Batch 83 Loss:0.19228963553905487\n",
      "Epoch 10, Batch 84 Loss:0.15922147035598755\n",
      "Epoch 10, Batch 85 Loss:0.2640267312526703\n",
      "Epoch 10, Batch 86 Loss:0.1942683458328247\n",
      "Epoch 10, Batch 87 Loss:0.14494574069976807\n",
      "Epoch 10, Batch 88 Loss:0.1389373242855072\n",
      "Epoch 10, Batch 89 Loss:0.22394321858882904\n",
      "Epoch 10, Batch 90 Loss:0.2066870480775833\n",
      "Epoch 10, Batch 91 Loss:0.18850070238113403\n",
      "Epoch 10, Batch 92 Loss:0.12858666479587555\n",
      "Epoch 10, Batch 93 Loss:0.15760764479637146\n",
      "Epoch 10, Batch 94 Loss:0.18167921900749207\n",
      "Epoch 10, Batch 95 Loss:0.18144817650318146\n",
      "Epoch 10, Batch 96 Loss:0.17354634404182434\n",
      "Epoch 10, Batch 97 Loss:0.1780119240283966\n",
      "Epoch 10, Batch 98 Loss:0.16493363678455353\n",
      "Epoch 10, Batch 99 Loss:0.21271483600139618\n",
      "Epoch 10, Batch 100 Loss:0.11079786717891693\n",
      "Epoch 10, Batch 101 Loss:0.1339297592639923\n",
      "Epoch 10, Batch 102 Loss:0.17669777572155\n",
      "Epoch 10, Batch 103 Loss:0.16120676696300507\n",
      "Epoch 10, Batch 104 Loss:0.18313521146774292\n",
      "Epoch 10, Batch 105 Loss:0.1622033417224884\n",
      "Epoch 10, Batch 106 Loss:0.16435784101486206\n",
      "Epoch 10, Batch 107 Loss:0.13296301662921906\n",
      "Epoch 10, Batch 108 Loss:0.17511099576950073\n",
      "Epoch 10, Batch 109 Loss:0.13103163242340088\n",
      "Epoch 10, Batch 110 Loss:0.19810107350349426\n",
      "Epoch 10, Batch 111 Loss:0.12736313045024872\n",
      "Epoch 10, Batch 112 Loss:0.17829662561416626\n",
      "Epoch 10, Batch 113 Loss:0.12169080972671509\n",
      "Epoch 10, Batch 114 Loss:0.16673152148723602\n",
      "Epoch 10, Batch 115 Loss:0.1560276746749878\n",
      "Epoch 10, Batch 116 Loss:0.11641457676887512\n",
      "Epoch 10, Batch 117 Loss:0.12470436096191406\n",
      "Epoch 10, Batch 118 Loss:0.1749383807182312\n",
      "Epoch 10, Batch 119 Loss:0.16659408807754517\n",
      "Epoch 10, Batch 120 Loss:0.2287595123052597\n",
      "Epoch 10, Batch 121 Loss:0.10449144244194031\n",
      "Epoch 10, Batch 122 Loss:0.15099963545799255\n",
      "Epoch 10, Batch 123 Loss:0.21438860893249512\n",
      "Epoch 10, Batch 124 Loss:0.08928052335977554\n",
      "Epoch 10, Batch 125 Loss:0.1339433193206787\n",
      "Epoch 10, Batch 126 Loss:0.16558369994163513\n",
      "Epoch 10, Batch 127 Loss:0.20557203888893127\n",
      "Epoch 10, Batch 128 Loss:0.22584080696105957\n",
      "Epoch 10, Batch 129 Loss:0.1639942228794098\n",
      "Epoch 10, Batch 130 Loss:0.20555421710014343\n",
      "Epoch 10, Batch 131 Loss:0.17396365106105804\n",
      "Epoch 10, Batch 132 Loss:0.17367196083068848\n",
      "Epoch 10, Batch 133 Loss:0.21114298701286316\n",
      "Epoch 10, Batch 134 Loss:0.1540209800004959\n",
      "Epoch 10, Batch 135 Loss:0.26034486293792725\n",
      "Epoch 10, Batch 136 Loss:0.17158015072345734\n",
      "Epoch 10, Batch 137 Loss:0.1981821060180664\n",
      "Epoch 10, Batch 138 Loss:0.19360113143920898\n",
      "Epoch 10, Batch 139 Loss:0.2234252691268921\n",
      "Epoch 10, Batch 140 Loss:0.15219667553901672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Batch 141 Loss:0.13772764801979065\n",
      "Epoch 10, Batch 142 Loss:0.1319008618593216\n",
      "Epoch 10, Batch 143 Loss:0.18785546720027924\n",
      "Epoch 10, Batch 144 Loss:0.216242253780365\n",
      "Epoch 10, Batch 145 Loss:0.13954372704029083\n",
      "Epoch 10, Batch 146 Loss:0.16043275594711304\n",
      "Epoch 10, Batch 147 Loss:0.14747405052185059\n",
      "Epoch 10, Batch 148 Loss:0.15681752562522888\n",
      "Epoch 10, Batch 149 Loss:0.2002537101507187\n",
      "Epoch 10, Batch 150 Loss:0.17303985357284546\n",
      "Epoch 10, Batch 151 Loss:0.2008465826511383\n",
      "Epoch 10, Batch 152 Loss:0.12242749333381653\n",
      "Epoch 10, Batch 153 Loss:0.18406441807746887\n",
      "Epoch 10, Batch 154 Loss:0.13283251225948334\n",
      "Epoch 10, Batch 155 Loss:0.15808264911174774\n",
      "Epoch 10, Batch 156 Loss:0.15834087133407593\n",
      "Epoch 10, Batch 157 Loss:0.23489224910736084\n",
      "Epoch 10, Batch 158 Loss:0.14081260561943054\n",
      "Epoch 10, Batch 159 Loss:0.20294862985610962\n",
      "Epoch 10, Batch 160 Loss:0.2183920443058014\n",
      "Epoch 10, Batch 161 Loss:0.14384515583515167\n",
      "Epoch 10, Batch 162 Loss:0.17634353041648865\n",
      "Epoch 10, Batch 163 Loss:0.2074207365512848\n",
      "Epoch 10, Batch 164 Loss:0.1973879039287567\n",
      "Epoch 10, Batch 165 Loss:0.16948477923870087\n",
      "Epoch 10, Batch 166 Loss:0.16551385819911957\n",
      "Epoch 10, Batch 167 Loss:0.18342450261116028\n",
      "Epoch 10, Batch 168 Loss:0.22031033039093018\n",
      "Epoch 10, Batch 169 Loss:0.19387492537498474\n",
      "Epoch 10, Batch 170 Loss:0.14478826522827148\n",
      "Epoch 10, Batch 171 Loss:0.21703073382377625\n",
      "Epoch 10, Batch 172 Loss:0.1595676988363266\n",
      "Epoch 10, Batch 173 Loss:0.17147403955459595\n",
      "Epoch 10, Batch 174 Loss:0.21945613622665405\n",
      "Epoch 10, Batch 175 Loss:0.2026386559009552\n",
      "Epoch 10, Batch 176 Loss:0.1753111481666565\n",
      "Epoch 10, Batch 177 Loss:0.23688700795173645\n",
      "Epoch 10, Batch 178 Loss:0.14768795669078827\n",
      "Epoch 10, Batch 179 Loss:0.22086700797080994\n",
      "Epoch 10, Batch 180 Loss:0.20250293612480164\n",
      "Epoch 10, Batch 181 Loss:0.2367163598537445\n",
      "Epoch 10, Batch 182 Loss:0.20681723952293396\n",
      "Epoch 10, Batch 183 Loss:0.16959723830223083\n",
      "Epoch 10, Batch 184 Loss:0.15346509218215942\n",
      "Epoch 10, Batch 185 Loss:0.14673717319965363\n",
      "Epoch 10, Batch 186 Loss:0.20169982314109802\n",
      "Epoch 10, Batch 187 Loss:0.17434759438037872\n",
      "Epoch 10, Batch 188 Loss:0.19584006071090698\n",
      "Epoch 10, Batch 189 Loss:0.15284258127212524\n",
      "Epoch 10, Batch 190 Loss:0.24494317173957825\n",
      "Epoch 10, Batch 191 Loss:0.13885101675987244\n",
      "Epoch 10, Batch 192 Loss:0.21328526735305786\n",
      "Epoch 10, Batch 193 Loss:0.18029634654521942\n",
      "Epoch 10, Batch 194 Loss:0.13711631298065186\n",
      "Epoch 10, Batch 195 Loss:0.19222158193588257\n",
      "Epoch 10, Batch 196 Loss:0.1694529950618744\n",
      "Epoch 10, Batch 197 Loss:0.17396283149719238\n",
      "Epoch 10, Batch 198 Loss:0.19402743875980377\n",
      "Epoch 10, Batch 199 Loss:0.1566077619791031\n",
      "Epoch 10, Batch 200 Loss:0.15348288416862488\n",
      "Epoch 10, Batch 201 Loss:0.19819414615631104\n",
      "Epoch 10, Batch 202 Loss:0.182882159948349\n",
      "Epoch 10, Batch 203 Loss:0.17102520167827606\n",
      "Epoch 10, Batch 204 Loss:0.16705504059791565\n",
      "Epoch 10, Batch 205 Loss:0.16942451894283295\n",
      "Epoch 10, Batch 206 Loss:0.19828079640865326\n",
      "Epoch 10, Batch 207 Loss:0.1668590009212494\n",
      "Epoch 10, Batch 208 Loss:0.22971169650554657\n",
      "Epoch 10, Batch 209 Loss:0.1358477771282196\n",
      "Epoch 10, Batch 210 Loss:0.1625986099243164\n",
      "Epoch 10, Batch 211 Loss:0.196369469165802\n",
      "Epoch 10, Batch 212 Loss:0.17270362377166748\n",
      "Epoch 10, Batch 213 Loss:0.19702741503715515\n",
      "Epoch 10, Batch 214 Loss:0.2069437950849533\n",
      "Epoch 10, Batch 215 Loss:0.17901554703712463\n",
      "Epoch 10, Batch 216 Loss:0.1918812096118927\n",
      "Epoch 10, Batch 217 Loss:0.15077440440654755\n",
      "Epoch 10, Batch 218 Loss:0.16545812785625458\n",
      "Epoch 10, Batch 219 Loss:0.16649533808231354\n",
      "Epoch 10, Batch 220 Loss:0.17363879084587097\n",
      "Epoch 10, Batch 221 Loss:0.2151515632867813\n",
      "Epoch 10, Batch 222 Loss:0.1399155855178833\n",
      "Epoch 10, Batch 223 Loss:0.17510974407196045\n",
      "Epoch 10, Batch 224 Loss:0.15566386282444\n",
      "Epoch 10, Batch 225 Loss:0.13034287095069885\n",
      "Epoch 10, Batch 226 Loss:0.18715566396713257\n",
      "Epoch 10, Batch 227 Loss:0.1776629537343979\n",
      "Epoch 10, Batch 228 Loss:0.12324245274066925\n",
      "Epoch 10, Batch 229 Loss:0.14728927612304688\n",
      "Epoch 10, Batch 230 Loss:0.13136190176010132\n",
      "Epoch 10, Batch 231 Loss:0.1526639759540558\n",
      "Epoch 10, Batch 232 Loss:0.2341289520263672\n",
      "Epoch 10, Batch 233 Loss:0.1623365432024002\n",
      "Loss in this Epoch is: 16.2336543202 %\n",
      "Accuracy in this Epoch is: 89.2300009727 %\n",
      "Epoch 11, Batch 0 Loss:0.11626274138689041\n",
      "Epoch 11, Batch 1 Loss:0.2129160612821579\n",
      "Epoch 11, Batch 2 Loss:0.1303025782108307\n",
      "Epoch 11, Batch 3 Loss:0.14717146754264832\n",
      "Epoch 11, Batch 4 Loss:0.16934813559055328\n",
      "Epoch 11, Batch 5 Loss:0.1327327936887741\n",
      "Epoch 11, Batch 6 Loss:0.10721191763877869\n",
      "Epoch 11, Batch 7 Loss:0.16702578961849213\n",
      "Epoch 11, Batch 8 Loss:0.20085418224334717\n",
      "Epoch 11, Batch 9 Loss:0.1471676528453827\n",
      "Epoch 11, Batch 10 Loss:0.19682972133159637\n",
      "Epoch 11, Batch 11 Loss:0.17882680892944336\n",
      "Epoch 11, Batch 12 Loss:0.1760377734899521\n",
      "Epoch 11, Batch 13 Loss:0.1396504044532776\n",
      "Epoch 11, Batch 14 Loss:0.16940930485725403\n",
      "Epoch 11, Batch 15 Loss:0.18783064186573029\n",
      "Epoch 11, Batch 16 Loss:0.09572812169790268\n",
      "Epoch 11, Batch 17 Loss:0.165674090385437\n",
      "Epoch 11, Batch 18 Loss:0.17011460661888123\n",
      "Epoch 11, Batch 19 Loss:0.13966691493988037\n",
      "Epoch 11, Batch 20 Loss:0.16429239511489868\n",
      "Epoch 11, Batch 21 Loss:0.12905433773994446\n",
      "Epoch 11, Batch 22 Loss:0.16475600004196167\n",
      "Epoch 11, Batch 23 Loss:0.16456422209739685\n",
      "Epoch 11, Batch 24 Loss:0.09500567615032196\n",
      "Epoch 11, Batch 25 Loss:0.17506296932697296\n",
      "Epoch 11, Batch 26 Loss:0.21574535965919495\n",
      "Epoch 11, Batch 27 Loss:0.1725873500108719\n",
      "Epoch 11, Batch 28 Loss:0.1628425270318985\n",
      "Epoch 11, Batch 29 Loss:0.18176168203353882\n",
      "Epoch 11, Batch 30 Loss:0.1942441463470459\n",
      "Epoch 11, Batch 31 Loss:0.16816569864749908\n",
      "Epoch 11, Batch 32 Loss:0.1324385404586792\n",
      "Epoch 11, Batch 33 Loss:0.1513446420431137\n",
      "Epoch 11, Batch 34 Loss:0.14894787967205048\n",
      "Epoch 11, Batch 35 Loss:0.16624285280704498\n",
      "Epoch 11, Batch 36 Loss:0.21243655681610107\n",
      "Epoch 11, Batch 37 Loss:0.1323765218257904\n",
      "Epoch 11, Batch 38 Loss:0.18308216333389282\n",
      "Epoch 11, Batch 39 Loss:0.1496914029121399\n",
      "Epoch 11, Batch 40 Loss:0.13074709475040436\n",
      "Epoch 11, Batch 41 Loss:0.15100589394569397\n",
      "Epoch 11, Batch 42 Loss:0.15708789229393005\n",
      "Epoch 11, Batch 43 Loss:0.12521912157535553\n",
      "Epoch 11, Batch 44 Loss:0.12918473780155182\n",
      "Epoch 11, Batch 45 Loss:0.12928199768066406\n",
      "Epoch 11, Batch 46 Loss:0.16726553440093994\n",
      "Epoch 11, Batch 47 Loss:0.15399295091629028\n",
      "Epoch 11, Batch 48 Loss:0.16518519818782806\n",
      "Epoch 11, Batch 49 Loss:0.16934390366077423\n",
      "Epoch 11, Batch 50 Loss:0.142049640417099\n",
      "Epoch 11, Batch 51 Loss:0.19477345049381256\n",
      "Epoch 11, Batch 52 Loss:0.16930517554283142\n",
      "Epoch 11, Batch 53 Loss:0.17388653755187988\n",
      "Epoch 11, Batch 54 Loss:0.10588853806257248\n",
      "Epoch 11, Batch 55 Loss:0.13587574660778046\n",
      "Epoch 11, Batch 56 Loss:0.1294391006231308\n",
      "Epoch 11, Batch 57 Loss:0.17687323689460754\n",
      "Epoch 11, Batch 58 Loss:0.23131170868873596\n",
      "Epoch 11, Batch 59 Loss:0.15146613121032715\n",
      "Epoch 11, Batch 60 Loss:0.18090814352035522\n",
      "Epoch 11, Batch 61 Loss:0.19537314772605896\n",
      "Epoch 11, Batch 62 Loss:0.11960092931985855\n",
      "Epoch 11, Batch 63 Loss:0.16068921983242035\n",
      "Epoch 11, Batch 64 Loss:0.1622776836156845\n",
      "Epoch 11, Batch 65 Loss:0.13984444737434387\n",
      "Epoch 11, Batch 66 Loss:0.1657838225364685\n",
      "Epoch 11, Batch 67 Loss:0.1471177339553833\n",
      "Epoch 11, Batch 68 Loss:0.14973841607570648\n",
      "Epoch 11, Batch 69 Loss:0.12742389738559723\n",
      "Epoch 11, Batch 70 Loss:0.17772908508777618\n",
      "Epoch 11, Batch 71 Loss:0.169099360704422\n",
      "Epoch 11, Batch 72 Loss:0.17929691076278687\n",
      "Epoch 11, Batch 73 Loss:0.16541917622089386\n",
      "Epoch 11, Batch 74 Loss:0.13364827632904053\n",
      "Epoch 11, Batch 75 Loss:0.1578613966703415\n",
      "Epoch 11, Batch 76 Loss:0.1732771247625351\n",
      "Epoch 11, Batch 77 Loss:0.12091245502233505\n",
      "Epoch 11, Batch 78 Loss:0.13243883848190308\n",
      "Epoch 11, Batch 79 Loss:0.15001529455184937\n",
      "Epoch 11, Batch 80 Loss:0.16329099237918854\n",
      "Epoch 11, Batch 81 Loss:0.14360374212265015\n",
      "Epoch 11, Batch 82 Loss:0.15383341908454895\n",
      "Epoch 11, Batch 83 Loss:0.15003181993961334\n",
      "Epoch 11, Batch 84 Loss:0.2109917253255844\n",
      "Epoch 11, Batch 85 Loss:0.1650373488664627\n",
      "Epoch 11, Batch 86 Loss:0.15334075689315796\n",
      "Epoch 11, Batch 87 Loss:0.17597292363643646\n",
      "Epoch 11, Batch 88 Loss:0.17777031660079956\n",
      "Epoch 11, Batch 89 Loss:0.13526694476604462\n",
      "Epoch 11, Batch 90 Loss:0.22575874626636505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Batch 91 Loss:0.16575488448143005\n",
      "Epoch 11, Batch 92 Loss:0.16950766742229462\n",
      "Epoch 11, Batch 93 Loss:0.1965802013874054\n",
      "Epoch 11, Batch 94 Loss:0.20035675168037415\n",
      "Epoch 11, Batch 95 Loss:0.1905566155910492\n",
      "Epoch 11, Batch 96 Loss:0.14017435908317566\n",
      "Epoch 11, Batch 97 Loss:0.14395186305046082\n",
      "Epoch 11, Batch 98 Loss:0.18953490257263184\n",
      "Epoch 11, Batch 99 Loss:0.1968347132205963\n",
      "Epoch 11, Batch 100 Loss:0.14596101641654968\n",
      "Epoch 11, Batch 101 Loss:0.1878143846988678\n",
      "Epoch 11, Batch 102 Loss:0.1606786549091339\n",
      "Epoch 11, Batch 103 Loss:0.16418185830116272\n",
      "Epoch 11, Batch 104 Loss:0.16304132342338562\n",
      "Epoch 11, Batch 105 Loss:0.15727418661117554\n",
      "Epoch 11, Batch 106 Loss:0.1579308956861496\n",
      "Epoch 11, Batch 107 Loss:0.10055795311927795\n",
      "Epoch 11, Batch 108 Loss:0.12483467161655426\n",
      "Epoch 11, Batch 109 Loss:0.1434042751789093\n",
      "Epoch 11, Batch 110 Loss:0.1776849627494812\n",
      "Epoch 11, Batch 111 Loss:0.1426941454410553\n",
      "Epoch 11, Batch 112 Loss:0.1727675497531891\n",
      "Epoch 11, Batch 113 Loss:0.14870837330818176\n",
      "Epoch 11, Batch 114 Loss:0.26107096672058105\n",
      "Epoch 11, Batch 115 Loss:0.13892114162445068\n",
      "Epoch 11, Batch 116 Loss:0.1425599753856659\n",
      "Epoch 11, Batch 117 Loss:0.17386910319328308\n",
      "Epoch 11, Batch 118 Loss:0.14643332362174988\n",
      "Epoch 11, Batch 119 Loss:0.19588813185691833\n",
      "Epoch 11, Batch 120 Loss:0.15944893658161163\n",
      "Epoch 11, Batch 121 Loss:0.11677002161741257\n",
      "Epoch 11, Batch 122 Loss:0.14089328050613403\n",
      "Epoch 11, Batch 123 Loss:0.14484119415283203\n",
      "Epoch 11, Batch 124 Loss:0.13071639835834503\n",
      "Epoch 11, Batch 125 Loss:0.19203689694404602\n",
      "Epoch 11, Batch 126 Loss:0.16025134921073914\n",
      "Epoch 11, Batch 127 Loss:0.12586289644241333\n",
      "Epoch 11, Batch 128 Loss:0.15708193182945251\n",
      "Epoch 11, Batch 129 Loss:0.12830263376235962\n",
      "Epoch 11, Batch 130 Loss:0.10748860239982605\n",
      "Epoch 11, Batch 131 Loss:0.17800727486610413\n",
      "Epoch 11, Batch 132 Loss:0.1927039623260498\n",
      "Epoch 11, Batch 133 Loss:0.17460373044013977\n",
      "Epoch 11, Batch 134 Loss:0.14168798923492432\n",
      "Epoch 11, Batch 135 Loss:0.15474167466163635\n",
      "Epoch 11, Batch 136 Loss:0.12055160850286484\n",
      "Epoch 11, Batch 137 Loss:0.19204361736774445\n",
      "Epoch 11, Batch 138 Loss:0.20810040831565857\n",
      "Epoch 11, Batch 139 Loss:0.1730237603187561\n",
      "Epoch 11, Batch 140 Loss:0.15451596677303314\n",
      "Epoch 11, Batch 141 Loss:0.1555832326412201\n",
      "Epoch 11, Batch 142 Loss:0.11420153826475143\n",
      "Epoch 11, Batch 143 Loss:0.24283835291862488\n",
      "Epoch 11, Batch 144 Loss:0.2381267100572586\n",
      "Epoch 11, Batch 145 Loss:0.1448064148426056\n",
      "Epoch 11, Batch 146 Loss:0.15975478291511536\n",
      "Epoch 11, Batch 147 Loss:0.18025802075862885\n",
      "Epoch 11, Batch 148 Loss:0.15655751526355743\n",
      "Epoch 11, Batch 149 Loss:0.18142761290073395\n",
      "Epoch 11, Batch 150 Loss:0.16126863658428192\n",
      "Epoch 11, Batch 151 Loss:0.1698509007692337\n",
      "Epoch 11, Batch 152 Loss:0.15826316177845\n",
      "Epoch 11, Batch 153 Loss:0.17913375794887543\n",
      "Epoch 11, Batch 154 Loss:0.16705942153930664\n",
      "Epoch 11, Batch 155 Loss:0.10506713390350342\n",
      "Epoch 11, Batch 156 Loss:0.14412081241607666\n",
      "Epoch 11, Batch 157 Loss:0.1545075923204422\n",
      "Epoch 11, Batch 158 Loss:0.12862209975719452\n",
      "Epoch 11, Batch 159 Loss:0.16914024949073792\n",
      "Epoch 11, Batch 160 Loss:0.12065131962299347\n",
      "Epoch 11, Batch 161 Loss:0.16965807974338531\n",
      "Epoch 11, Batch 162 Loss:0.19219285249710083\n",
      "Epoch 11, Batch 163 Loss:0.22415632009506226\n",
      "Epoch 11, Batch 164 Loss:0.17675644159317017\n",
      "Epoch 11, Batch 165 Loss:0.2043895721435547\n",
      "Epoch 11, Batch 166 Loss:0.13337790966033936\n",
      "Epoch 11, Batch 167 Loss:0.16304567456245422\n",
      "Epoch 11, Batch 168 Loss:0.14915892481803894\n",
      "Epoch 11, Batch 169 Loss:0.15266555547714233\n",
      "Epoch 11, Batch 170 Loss:0.12488610297441483\n",
      "Epoch 11, Batch 171 Loss:0.14058274030685425\n",
      "Epoch 11, Batch 172 Loss:0.1673133671283722\n",
      "Epoch 11, Batch 173 Loss:0.1763678640127182\n",
      "Epoch 11, Batch 174 Loss:0.16980180144309998\n",
      "Epoch 11, Batch 175 Loss:0.16891711950302124\n",
      "Epoch 11, Batch 176 Loss:0.17184646427631378\n",
      "Epoch 11, Batch 177 Loss:0.13558344542980194\n",
      "Epoch 11, Batch 178 Loss:0.17075279355049133\n",
      "Epoch 11, Batch 179 Loss:0.22560276091098785\n",
      "Epoch 11, Batch 180 Loss:0.2120363861322403\n",
      "Epoch 11, Batch 181 Loss:0.12931975722312927\n",
      "Epoch 11, Batch 182 Loss:0.13552802801132202\n",
      "Epoch 11, Batch 183 Loss:0.18463373184204102\n",
      "Epoch 11, Batch 184 Loss:0.18187907338142395\n",
      "Epoch 11, Batch 185 Loss:0.1415119171142578\n",
      "Epoch 11, Batch 186 Loss:0.14715471863746643\n",
      "Epoch 11, Batch 187 Loss:0.118008553981781\n",
      "Epoch 11, Batch 188 Loss:0.11600546538829803\n",
      "Epoch 11, Batch 189 Loss:0.1505536437034607\n",
      "Epoch 11, Batch 190 Loss:0.198062002658844\n",
      "Epoch 11, Batch 191 Loss:0.09970241039991379\n",
      "Epoch 11, Batch 192 Loss:0.12913748621940613\n",
      "Epoch 11, Batch 193 Loss:0.19116252660751343\n",
      "Epoch 11, Batch 194 Loss:0.2206439971923828\n",
      "Epoch 11, Batch 195 Loss:0.203598290681839\n",
      "Epoch 11, Batch 196 Loss:0.13979721069335938\n",
      "Epoch 11, Batch 197 Loss:0.13555678725242615\n",
      "Epoch 11, Batch 198 Loss:0.1068086177110672\n",
      "Epoch 11, Batch 199 Loss:0.14990252256393433\n",
      "Epoch 11, Batch 200 Loss:0.1596590131521225\n",
      "Epoch 11, Batch 201 Loss:0.20355507731437683\n",
      "Epoch 11, Batch 202 Loss:0.14046397805213928\n",
      "Epoch 11, Batch 203 Loss:0.16958892345428467\n",
      "Epoch 11, Batch 204 Loss:0.13354438543319702\n",
      "Epoch 11, Batch 205 Loss:0.14141738414764404\n",
      "Epoch 11, Batch 206 Loss:0.13911393284797668\n",
      "Epoch 11, Batch 207 Loss:0.18330934643745422\n",
      "Epoch 11, Batch 208 Loss:0.12557893991470337\n",
      "Epoch 11, Batch 209 Loss:0.19524520635604858\n",
      "Epoch 11, Batch 210 Loss:0.165985107421875\n",
      "Epoch 11, Batch 211 Loss:0.16471542418003082\n",
      "Epoch 11, Batch 212 Loss:0.13850566744804382\n",
      "Epoch 11, Batch 213 Loss:0.1348990797996521\n",
      "Epoch 11, Batch 214 Loss:0.17191722989082336\n",
      "Epoch 11, Batch 215 Loss:0.17521601915359497\n",
      "Epoch 11, Batch 216 Loss:0.08332469314336777\n",
      "Epoch 11, Batch 217 Loss:0.12427681684494019\n",
      "Epoch 11, Batch 218 Loss:0.20003977417945862\n",
      "Epoch 11, Batch 219 Loss:0.163199320435524\n",
      "Epoch 11, Batch 220 Loss:0.19413214921951294\n",
      "Epoch 11, Batch 221 Loss:0.12141560018062592\n",
      "Epoch 11, Batch 222 Loss:0.17548882961273193\n",
      "Epoch 11, Batch 223 Loss:0.11359342932701111\n",
      "Epoch 11, Batch 224 Loss:0.15544074773788452\n",
      "Epoch 11, Batch 225 Loss:0.1632191240787506\n",
      "Epoch 11, Batch 226 Loss:0.16075921058654785\n",
      "Epoch 11, Batch 227 Loss:0.19004403054714203\n",
      "Epoch 11, Batch 228 Loss:0.14576023817062378\n",
      "Epoch 11, Batch 229 Loss:0.153923898935318\n",
      "Epoch 11, Batch 230 Loss:0.16090035438537598\n",
      "Epoch 11, Batch 231 Loss:0.1816040575504303\n",
      "Epoch 11, Batch 232 Loss:0.13507767021656036\n",
      "Epoch 11, Batch 233 Loss:0.17159533500671387\n",
      "Loss in this Epoch is: 17.1595335007 %\n",
      "Accuracy in this Epoch is: 89.5600020885 %\n",
      "Epoch 12, Batch 0 Loss:0.13585911691188812\n",
      "Epoch 12, Batch 1 Loss:0.14559262990951538\n",
      "Epoch 12, Batch 2 Loss:0.08134783804416656\n",
      "Epoch 12, Batch 3 Loss:0.14549344778060913\n",
      "Epoch 12, Batch 4 Loss:0.11682602018117905\n",
      "Epoch 12, Batch 5 Loss:0.1596648246049881\n",
      "Epoch 12, Batch 6 Loss:0.15667572617530823\n",
      "Epoch 12, Batch 7 Loss:0.15369464457035065\n",
      "Epoch 12, Batch 8 Loss:0.13938303291797638\n",
      "Epoch 12, Batch 9 Loss:0.14545635879039764\n",
      "Epoch 12, Batch 10 Loss:0.12043995410203934\n",
      "Epoch 12, Batch 11 Loss:0.1380685716867447\n",
      "Epoch 12, Batch 12 Loss:0.15939080715179443\n",
      "Epoch 12, Batch 13 Loss:0.0917334109544754\n",
      "Epoch 12, Batch 14 Loss:0.12361804395914078\n",
      "Epoch 12, Batch 15 Loss:0.1467686891555786\n",
      "Epoch 12, Batch 16 Loss:0.1801013946533203\n",
      "Epoch 12, Batch 17 Loss:0.15553544461727142\n",
      "Epoch 12, Batch 18 Loss:0.13099436461925507\n",
      "Epoch 12, Batch 19 Loss:0.20597298443317413\n",
      "Epoch 12, Batch 20 Loss:0.14176683127880096\n",
      "Epoch 12, Batch 21 Loss:0.12275233119726181\n",
      "Epoch 12, Batch 22 Loss:0.1594957709312439\n",
      "Epoch 12, Batch 23 Loss:0.14760345220565796\n",
      "Epoch 12, Batch 24 Loss:0.15336135029792786\n",
      "Epoch 12, Batch 25 Loss:0.1605696976184845\n",
      "Epoch 12, Batch 26 Loss:0.10820329934358597\n",
      "Epoch 12, Batch 27 Loss:0.1362263262271881\n",
      "Epoch 12, Batch 28 Loss:0.1827283501625061\n",
      "Epoch 12, Batch 29 Loss:0.2166159451007843\n",
      "Epoch 12, Batch 30 Loss:0.14535896480083466\n",
      "Epoch 12, Batch 31 Loss:0.17339853942394257\n",
      "Epoch 12, Batch 32 Loss:0.1715431660413742\n",
      "Epoch 12, Batch 33 Loss:0.0958545133471489\n",
      "Epoch 12, Batch 34 Loss:0.08432508260011673\n",
      "Epoch 12, Batch 35 Loss:0.1318000704050064\n",
      "Epoch 12, Batch 36 Loss:0.12172453105449677\n",
      "Epoch 12, Batch 37 Loss:0.12129537761211395\n",
      "Epoch 12, Batch 38 Loss:0.1309947967529297\n",
      "Epoch 12, Batch 39 Loss:0.1389024257659912\n",
      "Epoch 12, Batch 40 Loss:0.17572899162769318\n",
      "Epoch 12, Batch 41 Loss:0.11062362045049667\n",
      "Epoch 12, Batch 42 Loss:0.18061643838882446\n",
      "Epoch 12, Batch 43 Loss:0.1326913684606552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 44 Loss:0.1573905497789383\n",
      "Epoch 12, Batch 45 Loss:0.1452730894088745\n",
      "Epoch 12, Batch 46 Loss:0.18783894181251526\n",
      "Epoch 12, Batch 47 Loss:0.09524306654930115\n",
      "Epoch 12, Batch 48 Loss:0.13023269176483154\n",
      "Epoch 12, Batch 49 Loss:0.10746461898088455\n",
      "Epoch 12, Batch 50 Loss:0.13371184468269348\n",
      "Epoch 12, Batch 51 Loss:0.1085304245352745\n",
      "Epoch 12, Batch 52 Loss:0.1109519973397255\n",
      "Epoch 12, Batch 53 Loss:0.15149685740470886\n",
      "Epoch 12, Batch 54 Loss:0.13768668472766876\n",
      "Epoch 12, Batch 55 Loss:0.10469971597194672\n",
      "Epoch 12, Batch 56 Loss:0.1384708434343338\n",
      "Epoch 12, Batch 57 Loss:0.11710857599973679\n",
      "Epoch 12, Batch 58 Loss:0.1543508917093277\n",
      "Epoch 12, Batch 59 Loss:0.12886781990528107\n",
      "Epoch 12, Batch 60 Loss:0.15247119963169098\n",
      "Epoch 12, Batch 61 Loss:0.14022989571094513\n",
      "Epoch 12, Batch 62 Loss:0.1556851714849472\n",
      "Epoch 12, Batch 63 Loss:0.1616550236940384\n",
      "Epoch 12, Batch 64 Loss:0.18741059303283691\n",
      "Epoch 12, Batch 65 Loss:0.10859251022338867\n",
      "Epoch 12, Batch 66 Loss:0.1346750408411026\n",
      "Epoch 12, Batch 67 Loss:0.15699221193790436\n",
      "Epoch 12, Batch 68 Loss:0.12698635458946228\n",
      "Epoch 12, Batch 69 Loss:0.18291731178760529\n",
      "Epoch 12, Batch 70 Loss:0.1019478514790535\n",
      "Epoch 12, Batch 71 Loss:0.2450254112482071\n",
      "Epoch 12, Batch 72 Loss:0.1630035787820816\n",
      "Epoch 12, Batch 73 Loss:0.1483178734779358\n",
      "Epoch 12, Batch 74 Loss:0.13870106637477875\n",
      "Epoch 12, Batch 75 Loss:0.1981382966041565\n",
      "Epoch 12, Batch 76 Loss:0.17917004227638245\n",
      "Epoch 12, Batch 77 Loss:0.13624148070812225\n",
      "Epoch 12, Batch 78 Loss:0.21109014749526978\n",
      "Epoch 12, Batch 79 Loss:0.1451355665922165\n",
      "Epoch 12, Batch 80 Loss:0.10555718839168549\n",
      "Epoch 12, Batch 81 Loss:0.17949554324150085\n",
      "Epoch 12, Batch 82 Loss:0.14023229479789734\n",
      "Epoch 12, Batch 83 Loss:0.1364974081516266\n",
      "Epoch 12, Batch 84 Loss:0.10845772176980972\n",
      "Epoch 12, Batch 85 Loss:0.10534342378377914\n",
      "Epoch 12, Batch 86 Loss:0.11381147801876068\n",
      "Epoch 12, Batch 87 Loss:0.11604370921850204\n",
      "Epoch 12, Batch 88 Loss:0.12963151931762695\n",
      "Epoch 12, Batch 89 Loss:0.142778679728508\n",
      "Epoch 12, Batch 90 Loss:0.1521005779504776\n",
      "Epoch 12, Batch 91 Loss:0.15371295809745789\n",
      "Epoch 12, Batch 92 Loss:0.17705103754997253\n",
      "Epoch 12, Batch 93 Loss:0.13307413458824158\n",
      "Epoch 12, Batch 94 Loss:0.1565142273902893\n",
      "Epoch 12, Batch 95 Loss:0.13170656561851501\n",
      "Epoch 12, Batch 96 Loss:0.10062886774539948\n",
      "Epoch 12, Batch 97 Loss:0.1803164780139923\n",
      "Epoch 12, Batch 98 Loss:0.14203877747058868\n",
      "Epoch 12, Batch 99 Loss:0.1911870539188385\n",
      "Epoch 12, Batch 100 Loss:0.16154512763023376\n",
      "Epoch 12, Batch 101 Loss:0.11846434324979782\n",
      "Epoch 12, Batch 102 Loss:0.13916946947574615\n",
      "Epoch 12, Batch 103 Loss:0.15370556712150574\n",
      "Epoch 12, Batch 104 Loss:0.11580918729305267\n",
      "Epoch 12, Batch 105 Loss:0.15945103764533997\n",
      "Epoch 12, Batch 106 Loss:0.11438658833503723\n",
      "Epoch 12, Batch 107 Loss:0.1650780290365219\n",
      "Epoch 12, Batch 108 Loss:0.17238783836364746\n",
      "Epoch 12, Batch 109 Loss:0.11745968461036682\n",
      "Epoch 12, Batch 110 Loss:0.12860901653766632\n",
      "Epoch 12, Batch 111 Loss:0.14649775624275208\n",
      "Epoch 12, Batch 112 Loss:0.1503530889749527\n",
      "Epoch 12, Batch 113 Loss:0.1358063966035843\n",
      "Epoch 12, Batch 114 Loss:0.12379441410303116\n",
      "Epoch 12, Batch 115 Loss:0.19482502341270447\n",
      "Epoch 12, Batch 116 Loss:0.129737988114357\n",
      "Epoch 12, Batch 117 Loss:0.09659712016582489\n",
      "Epoch 12, Batch 118 Loss:0.1301712989807129\n",
      "Epoch 12, Batch 119 Loss:0.111996591091156\n",
      "Epoch 12, Batch 120 Loss:0.06193062290549278\n",
      "Epoch 12, Batch 121 Loss:0.15300798416137695\n",
      "Epoch 12, Batch 122 Loss:0.1896597146987915\n",
      "Epoch 12, Batch 123 Loss:0.10808510333299637\n",
      "Epoch 12, Batch 124 Loss:0.24387359619140625\n",
      "Epoch 12, Batch 125 Loss:0.12135432660579681\n",
      "Epoch 12, Batch 126 Loss:0.1536232829093933\n",
      "Epoch 12, Batch 127 Loss:0.1285618543624878\n",
      "Epoch 12, Batch 128 Loss:0.158376544713974\n",
      "Epoch 12, Batch 129 Loss:0.20321470499038696\n",
      "Epoch 12, Batch 130 Loss:0.17473728954792023\n",
      "Epoch 12, Batch 131 Loss:0.18997380137443542\n",
      "Epoch 12, Batch 132 Loss:0.14929725229740143\n",
      "Epoch 12, Batch 133 Loss:0.11919871717691422\n",
      "Epoch 12, Batch 134 Loss:0.14148321747779846\n",
      "Epoch 12, Batch 135 Loss:0.13084150850772858\n",
      "Epoch 12, Batch 136 Loss:0.12711122632026672\n",
      "Epoch 12, Batch 137 Loss:0.1368691474199295\n",
      "Epoch 12, Batch 138 Loss:0.16178229451179504\n",
      "Epoch 12, Batch 139 Loss:0.16316992044448853\n",
      "Epoch 12, Batch 140 Loss:0.15670263767242432\n",
      "Epoch 12, Batch 141 Loss:0.13960671424865723\n",
      "Epoch 12, Batch 142 Loss:0.13062205910682678\n",
      "Epoch 12, Batch 143 Loss:0.13327285647392273\n",
      "Epoch 12, Batch 144 Loss:0.11862526834011078\n",
      "Epoch 12, Batch 145 Loss:0.22298605740070343\n",
      "Epoch 12, Batch 146 Loss:0.226027712225914\n",
      "Epoch 12, Batch 147 Loss:0.15059366822242737\n",
      "Epoch 12, Batch 148 Loss:0.14559832215309143\n",
      "Epoch 12, Batch 149 Loss:0.1723467856645584\n",
      "Epoch 12, Batch 150 Loss:0.14249390363693237\n",
      "Epoch 12, Batch 151 Loss:0.1559378206729889\n",
      "Epoch 12, Batch 152 Loss:0.19556325674057007\n",
      "Epoch 12, Batch 153 Loss:0.14439237117767334\n",
      "Epoch 12, Batch 154 Loss:0.13387031853199005\n",
      "Epoch 12, Batch 155 Loss:0.20664048194885254\n",
      "Epoch 12, Batch 156 Loss:0.15922263264656067\n",
      "Epoch 12, Batch 157 Loss:0.1853002905845642\n",
      "Epoch 12, Batch 158 Loss:0.17539913952350616\n",
      "Epoch 12, Batch 159 Loss:0.16105647385120392\n",
      "Epoch 12, Batch 160 Loss:0.1779753863811493\n",
      "Epoch 12, Batch 161 Loss:0.14049336314201355\n",
      "Epoch 12, Batch 162 Loss:0.23466357588768005\n",
      "Epoch 12, Batch 163 Loss:0.21778061985969543\n",
      "Epoch 12, Batch 164 Loss:0.19143232703208923\n",
      "Epoch 12, Batch 165 Loss:0.0962008461356163\n",
      "Epoch 12, Batch 166 Loss:0.1733734905719757\n",
      "Epoch 12, Batch 167 Loss:0.15719524025917053\n",
      "Epoch 12, Batch 168 Loss:0.1667737364768982\n",
      "Epoch 12, Batch 169 Loss:0.13497498631477356\n",
      "Epoch 12, Batch 170 Loss:0.1584872454404831\n",
      "Epoch 12, Batch 171 Loss:0.13915453851222992\n",
      "Epoch 12, Batch 172 Loss:0.21686697006225586\n",
      "Epoch 12, Batch 173 Loss:0.16112641990184784\n",
      "Epoch 12, Batch 174 Loss:0.14435778558254242\n",
      "Epoch 12, Batch 175 Loss:0.12377304583787918\n",
      "Epoch 12, Batch 176 Loss:0.13977311551570892\n",
      "Epoch 12, Batch 177 Loss:0.11836269497871399\n",
      "Epoch 12, Batch 178 Loss:0.16174694895744324\n",
      "Epoch 12, Batch 179 Loss:0.17152780294418335\n",
      "Epoch 12, Batch 180 Loss:0.13972118496894836\n",
      "Epoch 12, Batch 181 Loss:0.22131022810935974\n",
      "Epoch 12, Batch 182 Loss:0.15070223808288574\n",
      "Epoch 12, Batch 183 Loss:0.1706574261188507\n",
      "Epoch 12, Batch 184 Loss:0.14176280796527863\n",
      "Epoch 12, Batch 185 Loss:0.08000417053699493\n",
      "Epoch 12, Batch 186 Loss:0.20960378646850586\n",
      "Epoch 12, Batch 187 Loss:0.14443093538284302\n",
      "Epoch 12, Batch 188 Loss:0.194315105676651\n",
      "Epoch 12, Batch 189 Loss:0.1386723816394806\n",
      "Epoch 12, Batch 190 Loss:0.12077068537473679\n",
      "Epoch 12, Batch 191 Loss:0.14309556782245636\n",
      "Epoch 12, Batch 192 Loss:0.19347217679023743\n",
      "Epoch 12, Batch 193 Loss:0.17739516496658325\n",
      "Epoch 12, Batch 194 Loss:0.14412905275821686\n",
      "Epoch 12, Batch 195 Loss:0.16261276602745056\n",
      "Epoch 12, Batch 196 Loss:0.13819926977157593\n",
      "Epoch 12, Batch 197 Loss:0.12649136781692505\n",
      "Epoch 12, Batch 198 Loss:0.23041865229606628\n",
      "Epoch 12, Batch 199 Loss:0.22055187821388245\n",
      "Epoch 12, Batch 200 Loss:0.1428956687450409\n",
      "Epoch 12, Batch 201 Loss:0.18571996688842773\n",
      "Epoch 12, Batch 202 Loss:0.1484474092721939\n",
      "Epoch 12, Batch 203 Loss:0.14943622052669525\n",
      "Epoch 12, Batch 204 Loss:0.14131537079811096\n",
      "Epoch 12, Batch 205 Loss:0.11739437282085419\n",
      "Epoch 12, Batch 206 Loss:0.1338428109884262\n",
      "Epoch 12, Batch 207 Loss:0.20507821440696716\n",
      "Epoch 12, Batch 208 Loss:0.1500895470380783\n",
      "Epoch 12, Batch 209 Loss:0.16317278146743774\n",
      "Epoch 12, Batch 210 Loss:0.1262628734111786\n",
      "Epoch 12, Batch 211 Loss:0.12638768553733826\n",
      "Epoch 12, Batch 212 Loss:0.15221360325813293\n",
      "Epoch 12, Batch 213 Loss:0.159274160861969\n",
      "Epoch 12, Batch 214 Loss:0.1655273139476776\n",
      "Epoch 12, Batch 215 Loss:0.18035101890563965\n",
      "Epoch 12, Batch 216 Loss:0.1949409544467926\n",
      "Epoch 12, Batch 217 Loss:0.1686566174030304\n",
      "Epoch 12, Batch 218 Loss:0.1322977989912033\n",
      "Epoch 12, Batch 219 Loss:0.1727120578289032\n",
      "Epoch 12, Batch 220 Loss:0.1377716362476349\n",
      "Epoch 12, Batch 221 Loss:0.15005435049533844\n",
      "Epoch 12, Batch 222 Loss:0.10826475918292999\n",
      "Epoch 12, Batch 223 Loss:0.16913791000843048\n",
      "Epoch 12, Batch 224 Loss:0.1535925269126892\n",
      "Epoch 12, Batch 225 Loss:0.1544291228055954\n",
      "Epoch 12, Batch 226 Loss:0.15000391006469727\n",
      "Epoch 12, Batch 227 Loss:0.13280194997787476\n",
      "Epoch 12, Batch 228 Loss:0.16163939237594604\n",
      "Epoch 12, Batch 229 Loss:0.17917314171791077\n",
      "Epoch 12, Batch 230 Loss:0.09879647195339203\n",
      "Epoch 12, Batch 231 Loss:0.20114125311374664\n",
      "Epoch 12, Batch 232 Loss:0.16201050579547882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Batch 233 Loss:0.13057947158813477\n",
      "Loss in this Epoch is: 13.0579471588 %\n",
      "Accuracy in this Epoch is: 89.2899990082 %\n",
      "Epoch 13, Batch 0 Loss:0.18950669467449188\n",
      "Epoch 13, Batch 1 Loss:0.120541051030159\n",
      "Epoch 13, Batch 2 Loss:0.13248352706432343\n",
      "Epoch 13, Batch 3 Loss:0.145319402217865\n",
      "Epoch 13, Batch 4 Loss:0.12377364933490753\n",
      "Epoch 13, Batch 5 Loss:0.14763902127742767\n",
      "Epoch 13, Batch 6 Loss:0.12148900330066681\n",
      "Epoch 13, Batch 7 Loss:0.10110874474048615\n",
      "Epoch 13, Batch 8 Loss:0.13826200366020203\n",
      "Epoch 13, Batch 9 Loss:0.1022733747959137\n",
      "Epoch 13, Batch 10 Loss:0.15126368403434753\n",
      "Epoch 13, Batch 11 Loss:0.11180129647254944\n",
      "Epoch 13, Batch 12 Loss:0.16877174377441406\n",
      "Epoch 13, Batch 13 Loss:0.11440708488225937\n",
      "Epoch 13, Batch 14 Loss:0.12551984190940857\n",
      "Epoch 13, Batch 15 Loss:0.08560917526483536\n",
      "Epoch 13, Batch 16 Loss:0.16343103349208832\n",
      "Epoch 13, Batch 17 Loss:0.19041693210601807\n",
      "Epoch 13, Batch 18 Loss:0.1052139475941658\n",
      "Epoch 13, Batch 19 Loss:0.12642185389995575\n",
      "Epoch 13, Batch 20 Loss:0.14589092135429382\n",
      "Epoch 13, Batch 21 Loss:0.13841412961483002\n",
      "Epoch 13, Batch 22 Loss:0.1122351810336113\n",
      "Epoch 13, Batch 23 Loss:0.172896608710289\n",
      "Epoch 13, Batch 24 Loss:0.10138019174337387\n",
      "Epoch 13, Batch 25 Loss:0.1222146674990654\n",
      "Epoch 13, Batch 26 Loss:0.15881644189357758\n",
      "Epoch 13, Batch 27 Loss:0.11352530121803284\n",
      "Epoch 13, Batch 28 Loss:0.14684802293777466\n",
      "Epoch 13, Batch 29 Loss:0.11261441558599472\n",
      "Epoch 13, Batch 30 Loss:0.13235776126384735\n",
      "Epoch 13, Batch 31 Loss:0.1174476370215416\n",
      "Epoch 13, Batch 32 Loss:0.09051650762557983\n",
      "Epoch 13, Batch 33 Loss:0.18270985782146454\n",
      "Epoch 13, Batch 34 Loss:0.14447440207004547\n",
      "Epoch 13, Batch 35 Loss:0.11591139435768127\n",
      "Epoch 13, Batch 36 Loss:0.1603531539440155\n",
      "Epoch 13, Batch 37 Loss:0.06711369007825851\n",
      "Epoch 13, Batch 38 Loss:0.08682507276535034\n",
      "Epoch 13, Batch 39 Loss:0.16880306601524353\n",
      "Epoch 13, Batch 40 Loss:0.14686672389507294\n",
      "Epoch 13, Batch 41 Loss:0.10593987256288528\n",
      "Epoch 13, Batch 42 Loss:0.1535763442516327\n",
      "Epoch 13, Batch 43 Loss:0.11848245561122894\n",
      "Epoch 13, Batch 44 Loss:0.10701362788677216\n",
      "Epoch 13, Batch 45 Loss:0.1700790822505951\n",
      "Epoch 13, Batch 46 Loss:0.1373000144958496\n",
      "Epoch 13, Batch 47 Loss:0.16891594231128693\n",
      "Epoch 13, Batch 48 Loss:0.11660799384117126\n",
      "Epoch 13, Batch 49 Loss:0.12739863991737366\n",
      "Epoch 13, Batch 50 Loss:0.120844766497612\n",
      "Epoch 13, Batch 51 Loss:0.10484353452920914\n",
      "Epoch 13, Batch 52 Loss:0.1533799171447754\n",
      "Epoch 13, Batch 53 Loss:0.10340555757284164\n",
      "Epoch 13, Batch 54 Loss:0.10109484940767288\n",
      "Epoch 13, Batch 55 Loss:0.12226986140012741\n",
      "Epoch 13, Batch 56 Loss:0.15535056591033936\n",
      "Epoch 13, Batch 57 Loss:0.10796409845352173\n",
      "Epoch 13, Batch 58 Loss:0.16251400113105774\n",
      "Epoch 13, Batch 59 Loss:0.18839433789253235\n",
      "Epoch 13, Batch 60 Loss:0.1517370343208313\n",
      "Epoch 13, Batch 61 Loss:0.10576178133487701\n",
      "Epoch 13, Batch 62 Loss:0.12395629286766052\n",
      "Epoch 13, Batch 63 Loss:0.14452742040157318\n",
      "Epoch 13, Batch 64 Loss:0.1303076148033142\n",
      "Epoch 13, Batch 65 Loss:0.11157479137182236\n",
      "Epoch 13, Batch 66 Loss:0.13793852925300598\n",
      "Epoch 13, Batch 67 Loss:0.17551568150520325\n",
      "Epoch 13, Batch 68 Loss:0.11759357154369354\n",
      "Epoch 13, Batch 69 Loss:0.10566698759794235\n",
      "Epoch 13, Batch 70 Loss:0.1421491801738739\n",
      "Epoch 13, Batch 71 Loss:0.1359199583530426\n",
      "Epoch 13, Batch 72 Loss:0.12739430367946625\n",
      "Epoch 13, Batch 73 Loss:0.11945436149835587\n",
      "Epoch 13, Batch 74 Loss:0.14356665313243866\n",
      "Epoch 13, Batch 75 Loss:0.12770146131515503\n",
      "Epoch 13, Batch 76 Loss:0.16071482002735138\n",
      "Epoch 13, Batch 77 Loss:0.162164106965065\n",
      "Epoch 13, Batch 78 Loss:0.08991379290819168\n",
      "Epoch 13, Batch 79 Loss:0.10878817737102509\n",
      "Epoch 13, Batch 80 Loss:0.07366403937339783\n",
      "Epoch 13, Batch 81 Loss:0.09386499971151352\n",
      "Epoch 13, Batch 82 Loss:0.12325412780046463\n",
      "Epoch 13, Batch 83 Loss:0.10237499326467514\n",
      "Epoch 13, Batch 84 Loss:0.11918345838785172\n",
      "Epoch 13, Batch 85 Loss:0.12361665815114975\n",
      "Epoch 13, Batch 86 Loss:0.12014444917440414\n",
      "Epoch 13, Batch 87 Loss:0.09989137947559357\n",
      "Epoch 13, Batch 88 Loss:0.1483919769525528\n",
      "Epoch 13, Batch 89 Loss:0.15097197890281677\n",
      "Epoch 13, Batch 90 Loss:0.11406292021274567\n",
      "Epoch 13, Batch 91 Loss:0.13865283131599426\n",
      "Epoch 13, Batch 92 Loss:0.1395227611064911\n",
      "Epoch 13, Batch 93 Loss:0.18058063089847565\n",
      "Epoch 13, Batch 94 Loss:0.1399664729833603\n",
      "Epoch 13, Batch 95 Loss:0.14252696931362152\n",
      "Epoch 13, Batch 96 Loss:0.12091073393821716\n",
      "Epoch 13, Batch 97 Loss:0.15010561048984528\n",
      "Epoch 13, Batch 98 Loss:0.12349813431501389\n",
      "Epoch 13, Batch 99 Loss:0.16074016690254211\n",
      "Epoch 13, Batch 100 Loss:0.16726040840148926\n",
      "Epoch 13, Batch 101 Loss:0.13152697682380676\n",
      "Epoch 13, Batch 102 Loss:0.08283434808254242\n",
      "Epoch 13, Batch 103 Loss:0.16709904372692108\n",
      "Epoch 13, Batch 104 Loss:0.11422300338745117\n",
      "Epoch 13, Batch 105 Loss:0.12852105498313904\n",
      "Epoch 13, Batch 106 Loss:0.12559159100055695\n",
      "Epoch 13, Batch 107 Loss:0.12047505378723145\n",
      "Epoch 13, Batch 108 Loss:0.15027570724487305\n",
      "Epoch 13, Batch 109 Loss:0.14870783686637878\n",
      "Epoch 13, Batch 110 Loss:0.1827845573425293\n",
      "Epoch 13, Batch 111 Loss:0.2143290489912033\n",
      "Epoch 13, Batch 112 Loss:0.15835681557655334\n",
      "Epoch 13, Batch 113 Loss:0.12218736857175827\n",
      "Epoch 13, Batch 114 Loss:0.14459581673145294\n",
      "Epoch 13, Batch 115 Loss:0.14166203141212463\n",
      "Epoch 13, Batch 116 Loss:0.17387135326862335\n",
      "Epoch 13, Batch 117 Loss:0.1232459768652916\n",
      "Epoch 13, Batch 118 Loss:0.2033676952123642\n",
      "Epoch 13, Batch 119 Loss:0.20181222259998322\n",
      "Epoch 13, Batch 120 Loss:0.15698491036891937\n",
      "Epoch 13, Batch 121 Loss:0.11490461975336075\n",
      "Epoch 13, Batch 122 Loss:0.15148481726646423\n",
      "Epoch 13, Batch 123 Loss:0.16342408955097198\n",
      "Epoch 13, Batch 124 Loss:0.16936320066452026\n",
      "Epoch 13, Batch 125 Loss:0.15790590643882751\n",
      "Epoch 13, Batch 126 Loss:0.1638786345720291\n",
      "Epoch 13, Batch 127 Loss:0.1577780395746231\n",
      "Epoch 13, Batch 128 Loss:0.14664170145988464\n",
      "Epoch 13, Batch 129 Loss:0.1470796912908554\n",
      "Epoch 13, Batch 130 Loss:0.14181020855903625\n",
      "Epoch 13, Batch 131 Loss:0.15075618028640747\n",
      "Epoch 13, Batch 132 Loss:0.11460447311401367\n",
      "Epoch 13, Batch 133 Loss:0.1937427669763565\n",
      "Epoch 13, Batch 134 Loss:0.14160659909248352\n",
      "Epoch 13, Batch 135 Loss:0.10954885184764862\n",
      "Epoch 13, Batch 136 Loss:0.15245503187179565\n",
      "Epoch 13, Batch 137 Loss:0.1352069079875946\n",
      "Epoch 13, Batch 138 Loss:0.14262712001800537\n",
      "Epoch 13, Batch 139 Loss:0.11504663527011871\n",
      "Epoch 13, Batch 140 Loss:0.15253201127052307\n",
      "Epoch 13, Batch 141 Loss:0.12410737574100494\n",
      "Epoch 13, Batch 142 Loss:0.06790044158697128\n",
      "Epoch 13, Batch 143 Loss:0.16268153488636017\n",
      "Epoch 13, Batch 144 Loss:0.15725725889205933\n",
      "Epoch 13, Batch 145 Loss:0.13692636787891388\n",
      "Epoch 13, Batch 146 Loss:0.13160911202430725\n",
      "Epoch 13, Batch 147 Loss:0.10481791198253632\n",
      "Epoch 13, Batch 148 Loss:0.09211640059947968\n",
      "Epoch 13, Batch 149 Loss:0.18499577045440674\n",
      "Epoch 13, Batch 150 Loss:0.13436733186244965\n",
      "Epoch 13, Batch 151 Loss:0.13380461931228638\n",
      "Epoch 13, Batch 152 Loss:0.1669672429561615\n",
      "Epoch 13, Batch 153 Loss:0.15669742226600647\n",
      "Epoch 13, Batch 154 Loss:0.13045114278793335\n",
      "Epoch 13, Batch 155 Loss:0.09418871998786926\n",
      "Epoch 13, Batch 156 Loss:0.161202073097229\n",
      "Epoch 13, Batch 157 Loss:0.1191963329911232\n",
      "Epoch 13, Batch 158 Loss:0.107335664331913\n",
      "Epoch 13, Batch 159 Loss:0.11487600207328796\n",
      "Epoch 13, Batch 160 Loss:0.1248965933918953\n",
      "Epoch 13, Batch 161 Loss:0.12055051326751709\n",
      "Epoch 13, Batch 162 Loss:0.11435182392597198\n",
      "Epoch 13, Batch 163 Loss:0.10273247212171555\n",
      "Epoch 13, Batch 164 Loss:0.1471811830997467\n",
      "Epoch 13, Batch 165 Loss:0.17498591542243958\n",
      "Epoch 13, Batch 166 Loss:0.16555455327033997\n",
      "Epoch 13, Batch 167 Loss:0.15910014510154724\n",
      "Epoch 13, Batch 168 Loss:0.14253771305084229\n",
      "Epoch 13, Batch 169 Loss:0.10511529445648193\n",
      "Epoch 13, Batch 170 Loss:0.15128588676452637\n",
      "Epoch 13, Batch 171 Loss:0.18545982241630554\n",
      "Epoch 13, Batch 172 Loss:0.1668824553489685\n",
      "Epoch 13, Batch 173 Loss:0.1636059284210205\n",
      "Epoch 13, Batch 174 Loss:0.13734471797943115\n",
      "Epoch 13, Batch 175 Loss:0.17132005095481873\n",
      "Epoch 13, Batch 176 Loss:0.17815910279750824\n",
      "Epoch 13, Batch 177 Loss:0.1417376548051834\n",
      "Epoch 13, Batch 178 Loss:0.19239220023155212\n",
      "Epoch 13, Batch 179 Loss:0.17950847744941711\n",
      "Epoch 13, Batch 180 Loss:0.17230460047721863\n",
      "Epoch 13, Batch 181 Loss:0.11488215625286102\n",
      "Epoch 13, Batch 182 Loss:0.1374758780002594\n",
      "Epoch 13, Batch 183 Loss:0.0925593376159668\n",
      "Epoch 13, Batch 184 Loss:0.15819430351257324\n",
      "Epoch 13, Batch 185 Loss:0.12122553586959839\n",
      "Epoch 13, Batch 186 Loss:0.2014523446559906\n",
      "Epoch 13, Batch 187 Loss:0.12725424766540527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Batch 188 Loss:0.10979239642620087\n",
      "Epoch 13, Batch 189 Loss:0.103966623544693\n",
      "Epoch 13, Batch 190 Loss:0.18331418931484222\n",
      "Epoch 13, Batch 191 Loss:0.11300832033157349\n",
      "Epoch 13, Batch 192 Loss:0.0761406421661377\n",
      "Epoch 13, Batch 193 Loss:0.1971658170223236\n",
      "Epoch 13, Batch 194 Loss:0.10946457833051682\n",
      "Epoch 13, Batch 195 Loss:0.19151566922664642\n",
      "Epoch 13, Batch 196 Loss:0.15742462873458862\n",
      "Epoch 13, Batch 197 Loss:0.16206297278404236\n",
      "Epoch 13, Batch 198 Loss:0.1521129012107849\n",
      "Epoch 13, Batch 199 Loss:0.14047658443450928\n",
      "Epoch 13, Batch 200 Loss:0.13303199410438538\n",
      "Epoch 13, Batch 201 Loss:0.11915871500968933\n",
      "Epoch 13, Batch 202 Loss:0.15977799892425537\n",
      "Epoch 13, Batch 203 Loss:0.154794842004776\n",
      "Epoch 13, Batch 204 Loss:0.1432054340839386\n",
      "Epoch 13, Batch 205 Loss:0.14512020349502563\n",
      "Epoch 13, Batch 206 Loss:0.11931324005126953\n",
      "Epoch 13, Batch 207 Loss:0.11389044672250748\n",
      "Epoch 13, Batch 208 Loss:0.14520969986915588\n",
      "Epoch 13, Batch 209 Loss:0.17532894015312195\n",
      "Epoch 13, Batch 210 Loss:0.18175598978996277\n",
      "Epoch 13, Batch 211 Loss:0.11176258325576782\n",
      "Epoch 13, Batch 212 Loss:0.13404159247875214\n",
      "Epoch 13, Batch 213 Loss:0.1565936803817749\n",
      "Epoch 13, Batch 214 Loss:0.15831148624420166\n",
      "Epoch 13, Batch 215 Loss:0.1842169165611267\n",
      "Epoch 13, Batch 216 Loss:0.1275719553232193\n",
      "Epoch 13, Batch 217 Loss:0.1383402943611145\n",
      "Epoch 13, Batch 218 Loss:0.13592666387557983\n",
      "Epoch 13, Batch 219 Loss:0.13785499334335327\n",
      "Epoch 13, Batch 220 Loss:0.15753784775733948\n",
      "Epoch 13, Batch 221 Loss:0.1639622300863266\n",
      "Epoch 13, Batch 222 Loss:0.13641664385795593\n",
      "Epoch 13, Batch 223 Loss:0.1448753923177719\n",
      "Epoch 13, Batch 224 Loss:0.06937845051288605\n",
      "Epoch 13, Batch 225 Loss:0.17932529747486115\n",
      "Epoch 13, Batch 226 Loss:0.15105345845222473\n",
      "Epoch 13, Batch 227 Loss:0.16434058547019958\n",
      "Epoch 13, Batch 228 Loss:0.1002868041396141\n",
      "Epoch 13, Batch 229 Loss:0.09701377898454666\n",
      "Epoch 13, Batch 230 Loss:0.145246684551239\n",
      "Epoch 13, Batch 231 Loss:0.14512838423252106\n",
      "Epoch 13, Batch 232 Loss:0.21921420097351074\n",
      "Epoch 13, Batch 233 Loss:0.20465964078903198\n",
      "Loss in this Epoch is: 20.4659640789 %\n",
      "Accuracy in this Epoch is: 88.7600004673 %\n",
      "Epoch 14, Batch 0 Loss:0.16816562414169312\n",
      "Epoch 14, Batch 1 Loss:0.18918165564537048\n",
      "Epoch 14, Batch 2 Loss:0.16654136776924133\n",
      "Epoch 14, Batch 3 Loss:0.12849707901477814\n",
      "Epoch 14, Batch 4 Loss:0.14309591054916382\n",
      "Epoch 14, Batch 5 Loss:0.15145114064216614\n",
      "Epoch 14, Batch 6 Loss:0.13237537443637848\n",
      "Epoch 14, Batch 7 Loss:0.16554810106754303\n",
      "Epoch 14, Batch 8 Loss:0.13488350808620453\n",
      "Epoch 14, Batch 9 Loss:0.13306108117103577\n",
      "Epoch 14, Batch 10 Loss:0.11464332789182663\n",
      "Epoch 14, Batch 11 Loss:0.1268657147884369\n",
      "Epoch 14, Batch 12 Loss:0.1238475888967514\n",
      "Epoch 14, Batch 13 Loss:0.13036666810512543\n",
      "Epoch 14, Batch 14 Loss:0.13289842009544373\n",
      "Epoch 14, Batch 15 Loss:0.13082608580589294\n",
      "Epoch 14, Batch 16 Loss:0.11075231432914734\n",
      "Epoch 14, Batch 17 Loss:0.14178992807865143\n",
      "Epoch 14, Batch 18 Loss:0.12808850407600403\n",
      "Epoch 14, Batch 19 Loss:0.14677265286445618\n",
      "Epoch 14, Batch 20 Loss:0.09201721101999283\n",
      "Epoch 14, Batch 21 Loss:0.15288448333740234\n",
      "Epoch 14, Batch 22 Loss:0.1147705540060997\n",
      "Epoch 14, Batch 23 Loss:0.12639155983924866\n",
      "Epoch 14, Batch 24 Loss:0.138688862323761\n",
      "Epoch 14, Batch 25 Loss:0.09010381251573563\n",
      "Epoch 14, Batch 26 Loss:0.10706428438425064\n",
      "Epoch 14, Batch 27 Loss:0.09228914976119995\n",
      "Epoch 14, Batch 28 Loss:0.15280619263648987\n",
      "Epoch 14, Batch 29 Loss:0.10003682971000671\n",
      "Epoch 14, Batch 30 Loss:0.15904292464256287\n",
      "Epoch 14, Batch 31 Loss:0.11837251484394073\n",
      "Epoch 14, Batch 32 Loss:0.1403975635766983\n",
      "Epoch 14, Batch 33 Loss:0.10140927135944366\n",
      "Epoch 14, Batch 34 Loss:0.1333278864622116\n",
      "Epoch 14, Batch 35 Loss:0.13208219408988953\n",
      "Epoch 14, Batch 36 Loss:0.09317465126514435\n",
      "Epoch 14, Batch 37 Loss:0.13846559822559357\n",
      "Epoch 14, Batch 38 Loss:0.12966221570968628\n",
      "Epoch 14, Batch 39 Loss:0.11975494772195816\n",
      "Epoch 14, Batch 40 Loss:0.09191247075796127\n",
      "Epoch 14, Batch 41 Loss:0.11913657188415527\n",
      "Epoch 14, Batch 42 Loss:0.12893816828727722\n",
      "Epoch 14, Batch 43 Loss:0.08534864336252213\n",
      "Epoch 14, Batch 44 Loss:0.13423903286457062\n",
      "Epoch 14, Batch 45 Loss:0.1209874153137207\n",
      "Epoch 14, Batch 46 Loss:0.12677066028118134\n",
      "Epoch 14, Batch 47 Loss:0.15085439383983612\n",
      "Epoch 14, Batch 48 Loss:0.10946076363325119\n",
      "Epoch 14, Batch 49 Loss:0.12706880271434784\n",
      "Epoch 14, Batch 50 Loss:0.14042595028877258\n",
      "Epoch 14, Batch 51 Loss:0.09098738431930542\n",
      "Epoch 14, Batch 52 Loss:0.10513892769813538\n",
      "Epoch 14, Batch 53 Loss:0.11221081018447876\n",
      "Epoch 14, Batch 54 Loss:0.11717532575130463\n",
      "Epoch 14, Batch 55 Loss:0.16120153665542603\n",
      "Epoch 14, Batch 56 Loss:0.10501371324062347\n",
      "Epoch 14, Batch 57 Loss:0.09515871107578278\n",
      "Epoch 14, Batch 58 Loss:0.1325373649597168\n",
      "Epoch 14, Batch 59 Loss:0.14567190408706665\n",
      "Epoch 14, Batch 60 Loss:0.07551039755344391\n",
      "Epoch 14, Batch 61 Loss:0.15283134579658508\n",
      "Epoch 14, Batch 62 Loss:0.08740981668233871\n",
      "Epoch 14, Batch 63 Loss:0.11973067373037338\n",
      "Epoch 14, Batch 64 Loss:0.11082800477743149\n",
      "Epoch 14, Batch 65 Loss:0.11204925924539566\n",
      "Epoch 14, Batch 66 Loss:0.17737862467765808\n",
      "Epoch 14, Batch 67 Loss:0.06710925698280334\n",
      "Epoch 14, Batch 68 Loss:0.13257481157779694\n",
      "Epoch 14, Batch 69 Loss:0.14419282972812653\n",
      "Epoch 14, Batch 70 Loss:0.12441454082727432\n",
      "Epoch 14, Batch 71 Loss:0.16151639819145203\n",
      "Epoch 14, Batch 72 Loss:0.08640218526124954\n",
      "Epoch 14, Batch 73 Loss:0.2257850021123886\n",
      "Epoch 14, Batch 74 Loss:0.13430023193359375\n",
      "Epoch 14, Batch 75 Loss:0.07589137554168701\n",
      "Epoch 14, Batch 76 Loss:0.12530174851417542\n",
      "Epoch 14, Batch 77 Loss:0.09044806659221649\n",
      "Epoch 14, Batch 78 Loss:0.13202516734600067\n",
      "Epoch 14, Batch 79 Loss:0.12568755447864532\n",
      "Epoch 14, Batch 80 Loss:0.2040923684835434\n",
      "Epoch 14, Batch 81 Loss:0.10773056745529175\n",
      "Epoch 14, Batch 82 Loss:0.1503944993019104\n",
      "Epoch 14, Batch 83 Loss:0.17171964049339294\n",
      "Epoch 14, Batch 84 Loss:0.14489497244358063\n",
      "Epoch 14, Batch 85 Loss:0.10125615447759628\n",
      "Epoch 14, Batch 86 Loss:0.07136589288711548\n",
      "Epoch 14, Batch 87 Loss:0.14554309844970703\n",
      "Epoch 14, Batch 88 Loss:0.15956547856330872\n",
      "Epoch 14, Batch 89 Loss:0.1068839281797409\n",
      "Epoch 14, Batch 90 Loss:0.08970954269170761\n",
      "Epoch 14, Batch 91 Loss:0.12949322164058685\n",
      "Epoch 14, Batch 92 Loss:0.12801122665405273\n",
      "Epoch 14, Batch 93 Loss:0.11452456563711166\n",
      "Epoch 14, Batch 94 Loss:0.11626990884542465\n",
      "Epoch 14, Batch 95 Loss:0.07250940799713135\n",
      "Epoch 14, Batch 96 Loss:0.08316265046596527\n",
      "Epoch 14, Batch 97 Loss:0.1287694275379181\n",
      "Epoch 14, Batch 98 Loss:0.08614683151245117\n",
      "Epoch 14, Batch 99 Loss:0.14618593454360962\n",
      "Epoch 14, Batch 100 Loss:0.1005704402923584\n",
      "Epoch 14, Batch 101 Loss:0.13137173652648926\n",
      "Epoch 14, Batch 102 Loss:0.09588630497455597\n",
      "Epoch 14, Batch 103 Loss:0.09493333846330643\n",
      "Epoch 14, Batch 104 Loss:0.13623002171516418\n",
      "Epoch 14, Batch 105 Loss:0.11450234055519104\n",
      "Epoch 14, Batch 106 Loss:0.13600146770477295\n",
      "Epoch 14, Batch 107 Loss:0.06783583760261536\n",
      "Epoch 14, Batch 108 Loss:0.12408778071403503\n",
      "Epoch 14, Batch 109 Loss:0.15137766301631927\n",
      "Epoch 14, Batch 110 Loss:0.07250598818063736\n",
      "Epoch 14, Batch 111 Loss:0.1462855041027069\n",
      "Epoch 14, Batch 112 Loss:0.16132065653800964\n",
      "Epoch 14, Batch 113 Loss:0.13157697021961212\n",
      "Epoch 14, Batch 114 Loss:0.09344618022441864\n",
      "Epoch 14, Batch 115 Loss:0.10421194136142731\n",
      "Epoch 14, Batch 116 Loss:0.11771227419376373\n",
      "Epoch 14, Batch 117 Loss:0.12392331659793854\n",
      "Epoch 14, Batch 118 Loss:0.1494690179824829\n",
      "Epoch 14, Batch 119 Loss:0.08592411875724792\n",
      "Epoch 14, Batch 120 Loss:0.2162400782108307\n",
      "Epoch 14, Batch 121 Loss:0.12519869208335876\n",
      "Epoch 14, Batch 122 Loss:0.1762121319770813\n",
      "Epoch 14, Batch 123 Loss:0.15910948812961578\n",
      "Epoch 14, Batch 124 Loss:0.11885780841112137\n",
      "Epoch 14, Batch 125 Loss:0.12716913223266602\n",
      "Epoch 14, Batch 126 Loss:0.13181822001934052\n",
      "Epoch 14, Batch 127 Loss:0.11693724244832993\n",
      "Epoch 14, Batch 128 Loss:0.09150715917348862\n",
      "Epoch 14, Batch 129 Loss:0.11380761116743088\n",
      "Epoch 14, Batch 130 Loss:0.13298526406288147\n",
      "Epoch 14, Batch 131 Loss:0.10706628859043121\n",
      "Epoch 14, Batch 132 Loss:0.15850329399108887\n",
      "Epoch 14, Batch 133 Loss:0.12106890976428986\n",
      "Epoch 14, Batch 134 Loss:0.15239611268043518\n",
      "Epoch 14, Batch 135 Loss:0.11084505915641785\n",
      "Epoch 14, Batch 136 Loss:0.13615816831588745\n",
      "Epoch 14, Batch 137 Loss:0.15748433768749237\n",
      "Epoch 14, Batch 138 Loss:0.12405537813901901\n",
      "Epoch 14, Batch 139 Loss:0.13106054067611694\n",
      "Epoch 14, Batch 140 Loss:0.1750558316707611\n",
      "Epoch 14, Batch 141 Loss:0.1325901746749878\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Batch 142 Loss:0.16600248217582703\n",
      "Epoch 14, Batch 143 Loss:0.16577467322349548\n",
      "Epoch 14, Batch 144 Loss:0.12897136807441711\n",
      "Epoch 14, Batch 145 Loss:0.11399399489164352\n",
      "Epoch 14, Batch 146 Loss:0.14061176776885986\n",
      "Epoch 14, Batch 147 Loss:0.15003450214862823\n",
      "Epoch 14, Batch 148 Loss:0.10163973271846771\n",
      "Epoch 14, Batch 149 Loss:0.11848842352628708\n",
      "Epoch 14, Batch 150 Loss:0.09887804090976715\n",
      "Epoch 14, Batch 151 Loss:0.21595637500286102\n",
      "Epoch 14, Batch 152 Loss:0.10304410755634308\n",
      "Epoch 14, Batch 153 Loss:0.12777720391750336\n",
      "Epoch 14, Batch 154 Loss:0.1257518082857132\n",
      "Epoch 14, Batch 155 Loss:0.11616633832454681\n",
      "Epoch 14, Batch 156 Loss:0.15309549868106842\n",
      "Epoch 14, Batch 157 Loss:0.140191450715065\n",
      "Epoch 14, Batch 158 Loss:0.14512979984283447\n",
      "Epoch 14, Batch 159 Loss:0.11070793867111206\n",
      "Epoch 14, Batch 160 Loss:0.11702217161655426\n",
      "Epoch 14, Batch 161 Loss:0.14846712350845337\n",
      "Epoch 14, Batch 162 Loss:0.14194637537002563\n",
      "Epoch 14, Batch 163 Loss:0.10129032284021378\n",
      "Epoch 14, Batch 164 Loss:0.15897458791732788\n",
      "Epoch 14, Batch 165 Loss:0.14026735723018646\n",
      "Epoch 14, Batch 166 Loss:0.11120858043432236\n",
      "Epoch 14, Batch 167 Loss:0.13866427540779114\n",
      "Epoch 14, Batch 168 Loss:0.1502358764410019\n",
      "Epoch 14, Batch 169 Loss:0.09333984553813934\n",
      "Epoch 14, Batch 170 Loss:0.1404782235622406\n",
      "Epoch 14, Batch 171 Loss:0.13478657603263855\n",
      "Epoch 14, Batch 172 Loss:0.12462297827005386\n",
      "Epoch 14, Batch 173 Loss:0.12370932102203369\n",
      "Epoch 14, Batch 174 Loss:0.17085646092891693\n",
      "Epoch 14, Batch 175 Loss:0.16755908727645874\n",
      "Epoch 14, Batch 176 Loss:0.16274023056030273\n",
      "Epoch 14, Batch 177 Loss:0.18650898337364197\n",
      "Epoch 14, Batch 178 Loss:0.1401488184928894\n",
      "Epoch 14, Batch 179 Loss:0.11573128402233124\n",
      "Epoch 14, Batch 180 Loss:0.13801908493041992\n",
      "Epoch 14, Batch 181 Loss:0.1598803997039795\n",
      "Epoch 14, Batch 182 Loss:0.15212896466255188\n",
      "Epoch 14, Batch 183 Loss:0.12373056262731552\n",
      "Epoch 14, Batch 184 Loss:0.17961248755455017\n",
      "Epoch 14, Batch 185 Loss:0.12902052700519562\n",
      "Epoch 14, Batch 186 Loss:0.11146831512451172\n",
      "Epoch 14, Batch 187 Loss:0.1302509307861328\n",
      "Epoch 14, Batch 188 Loss:0.12774920463562012\n",
      "Epoch 14, Batch 189 Loss:0.11909773200750351\n",
      "Epoch 14, Batch 190 Loss:0.17751412093639374\n",
      "Epoch 14, Batch 191 Loss:0.15776807069778442\n",
      "Epoch 14, Batch 192 Loss:0.1206529438495636\n",
      "Epoch 14, Batch 193 Loss:0.16215039789676666\n",
      "Epoch 14, Batch 194 Loss:0.13307790458202362\n",
      "Epoch 14, Batch 195 Loss:0.13704632222652435\n",
      "Epoch 14, Batch 196 Loss:0.09599228203296661\n",
      "Epoch 14, Batch 197 Loss:0.12481003999710083\n",
      "Epoch 14, Batch 198 Loss:0.13693293929100037\n",
      "Epoch 14, Batch 199 Loss:0.1738012433052063\n",
      "Epoch 14, Batch 200 Loss:0.12538379430770874\n",
      "Epoch 14, Batch 201 Loss:0.20569856464862823\n",
      "Epoch 14, Batch 202 Loss:0.08750663697719574\n",
      "Epoch 14, Batch 203 Loss:0.13418126106262207\n",
      "Epoch 14, Batch 204 Loss:0.15692850947380066\n",
      "Epoch 14, Batch 205 Loss:0.11965633928775787\n",
      "Epoch 14, Batch 206 Loss:0.1531381905078888\n",
      "Epoch 14, Batch 207 Loss:0.14768654108047485\n",
      "Epoch 14, Batch 208 Loss:0.18359598517417908\n",
      "Epoch 14, Batch 209 Loss:0.11920326948165894\n",
      "Epoch 14, Batch 210 Loss:0.1545051634311676\n",
      "Epoch 14, Batch 211 Loss:0.19855229556560516\n",
      "Epoch 14, Batch 212 Loss:0.14063408970832825\n",
      "Epoch 14, Batch 213 Loss:0.1279815137386322\n",
      "Epoch 14, Batch 214 Loss:0.09850194305181503\n",
      "Epoch 14, Batch 215 Loss:0.1946181058883667\n",
      "Epoch 14, Batch 216 Loss:0.1536046266555786\n",
      "Epoch 14, Batch 217 Loss:0.20207428932189941\n",
      "Epoch 14, Batch 218 Loss:0.17913144826889038\n",
      "Epoch 14, Batch 219 Loss:0.16578294336795807\n",
      "Epoch 14, Batch 220 Loss:0.1312553435564041\n",
      "Epoch 14, Batch 221 Loss:0.18988008797168732\n",
      "Epoch 14, Batch 222 Loss:0.10476947575807571\n",
      "Epoch 14, Batch 223 Loss:0.2124161422252655\n",
      "Epoch 14, Batch 224 Loss:0.11903046071529388\n",
      "Epoch 14, Batch 225 Loss:0.12154919654130936\n",
      "Epoch 14, Batch 226 Loss:0.16848121583461761\n",
      "Epoch 14, Batch 227 Loss:0.10656513273715973\n",
      "Epoch 14, Batch 228 Loss:0.15328140556812286\n",
      "Epoch 14, Batch 229 Loss:0.15417316555976868\n",
      "Epoch 14, Batch 230 Loss:0.16295281052589417\n",
      "Epoch 14, Batch 231 Loss:0.20752087235450745\n",
      "Epoch 14, Batch 232 Loss:0.15564504265785217\n",
      "Epoch 14, Batch 233 Loss:0.1765076071023941\n",
      "Loss in this Epoch is: 17.6507607102 %\n",
      "Accuracy in this Epoch is: 89.1600012779 %\n",
      "Epoch 15, Batch 0 Loss:0.0935792475938797\n",
      "Epoch 15, Batch 1 Loss:0.14235608279705048\n",
      "Epoch 15, Batch 2 Loss:0.08252935111522675\n",
      "Epoch 15, Batch 3 Loss:0.09963306784629822\n",
      "Epoch 15, Batch 4 Loss:0.09935394674539566\n",
      "Epoch 15, Batch 5 Loss:0.09579849988222122\n",
      "Epoch 15, Batch 6 Loss:0.09265247732400894\n",
      "Epoch 15, Batch 7 Loss:0.11291927099227905\n",
      "Epoch 15, Batch 8 Loss:0.07177885621786118\n",
      "Epoch 15, Batch 9 Loss:0.10354549437761307\n",
      "Epoch 15, Batch 10 Loss:0.0975404903292656\n",
      "Epoch 15, Batch 11 Loss:0.10222305357456207\n",
      "Epoch 15, Batch 12 Loss:0.098330058157444\n",
      "Epoch 15, Batch 13 Loss:0.11826414614915848\n",
      "Epoch 15, Batch 14 Loss:0.13422946631908417\n",
      "Epoch 15, Batch 15 Loss:0.12843629717826843\n",
      "Epoch 15, Batch 16 Loss:0.1158914566040039\n",
      "Epoch 15, Batch 17 Loss:0.10518351197242737\n",
      "Epoch 15, Batch 18 Loss:0.1241518184542656\n",
      "Epoch 15, Batch 19 Loss:0.11436942964792252\n",
      "Epoch 15, Batch 20 Loss:0.10322249680757523\n",
      "Epoch 15, Batch 21 Loss:0.11064912378787994\n",
      "Epoch 15, Batch 22 Loss:0.11308909207582474\n",
      "Epoch 15, Batch 23 Loss:0.11613839864730835\n",
      "Epoch 15, Batch 24 Loss:0.13290727138519287\n",
      "Epoch 15, Batch 25 Loss:0.09961863607168198\n",
      "Epoch 15, Batch 26 Loss:0.11522442102432251\n",
      "Epoch 15, Batch 27 Loss:0.07521071285009384\n",
      "Epoch 15, Batch 28 Loss:0.08987655490636826\n",
      "Epoch 15, Batch 29 Loss:0.15791592001914978\n",
      "Epoch 15, Batch 30 Loss:0.09540819376707077\n",
      "Epoch 15, Batch 31 Loss:0.11858807504177094\n",
      "Epoch 15, Batch 32 Loss:0.14775438606739044\n",
      "Epoch 15, Batch 33 Loss:0.11975306272506714\n",
      "Epoch 15, Batch 34 Loss:0.11279699206352234\n",
      "Epoch 15, Batch 35 Loss:0.09071016311645508\n",
      "Epoch 15, Batch 36 Loss:0.13464783132076263\n",
      "Epoch 15, Batch 37 Loss:0.10099940001964569\n",
      "Epoch 15, Batch 38 Loss:0.07889419794082642\n",
      "Epoch 15, Batch 39 Loss:0.1800321340560913\n",
      "Epoch 15, Batch 40 Loss:0.13489177823066711\n",
      "Epoch 15, Batch 41 Loss:0.11095557361841202\n",
      "Epoch 15, Batch 42 Loss:0.08335862308740616\n",
      "Epoch 15, Batch 43 Loss:0.08836590498685837\n",
      "Epoch 15, Batch 44 Loss:0.10647500306367874\n",
      "Epoch 15, Batch 45 Loss:0.1225530356168747\n",
      "Epoch 15, Batch 46 Loss:0.08142063021659851\n",
      "Epoch 15, Batch 47 Loss:0.07905858010053635\n",
      "Epoch 15, Batch 48 Loss:0.08590204268693924\n",
      "Epoch 15, Batch 49 Loss:0.15364642441272736\n",
      "Epoch 15, Batch 50 Loss:0.09955012053251266\n",
      "Epoch 15, Batch 51 Loss:0.1027330681681633\n",
      "Epoch 15, Batch 52 Loss:0.08906232565641403\n",
      "Epoch 15, Batch 53 Loss:0.07655005902051926\n",
      "Epoch 15, Batch 54 Loss:0.10729166865348816\n",
      "Epoch 15, Batch 55 Loss:0.1234300434589386\n",
      "Epoch 15, Batch 56 Loss:0.08320888131856918\n",
      "Epoch 15, Batch 57 Loss:0.13238979876041412\n",
      "Epoch 15, Batch 58 Loss:0.12747682631015778\n",
      "Epoch 15, Batch 59 Loss:0.1382397562265396\n",
      "Epoch 15, Batch 60 Loss:0.1333238184452057\n",
      "Epoch 15, Batch 61 Loss:0.10673460364341736\n",
      "Epoch 15, Batch 62 Loss:0.11720965802669525\n",
      "Epoch 15, Batch 63 Loss:0.07231663912534714\n",
      "Epoch 15, Batch 64 Loss:0.06481332331895828\n",
      "Epoch 15, Batch 65 Loss:0.10228598117828369\n",
      "Epoch 15, Batch 66 Loss:0.09914078563451767\n",
      "Epoch 15, Batch 67 Loss:0.1223437488079071\n",
      "Epoch 15, Batch 68 Loss:0.142046257853508\n",
      "Epoch 15, Batch 69 Loss:0.09128321707248688\n",
      "Epoch 15, Batch 70 Loss:0.1241869255900383\n",
      "Epoch 15, Batch 71 Loss:0.13062967360019684\n",
      "Epoch 15, Batch 72 Loss:0.14287663996219635\n",
      "Epoch 15, Batch 73 Loss:0.10460112988948822\n",
      "Epoch 15, Batch 74 Loss:0.11846229434013367\n",
      "Epoch 15, Batch 75 Loss:0.12969645857810974\n",
      "Epoch 15, Batch 76 Loss:0.14142464101314545\n",
      "Epoch 15, Batch 77 Loss:0.151112899184227\n",
      "Epoch 15, Batch 78 Loss:0.16152305901050568\n",
      "Epoch 15, Batch 79 Loss:0.1067899540066719\n",
      "Epoch 15, Batch 80 Loss:0.124391108751297\n",
      "Epoch 15, Batch 81 Loss:0.133678138256073\n",
      "Epoch 15, Batch 82 Loss:0.12840025126934052\n",
      "Epoch 15, Batch 83 Loss:0.11818176507949829\n",
      "Epoch 15, Batch 84 Loss:0.12348375469446182\n",
      "Epoch 15, Batch 85 Loss:0.09680414199829102\n",
      "Epoch 15, Batch 86 Loss:0.1490805745124817\n",
      "Epoch 15, Batch 87 Loss:0.10021692514419556\n",
      "Epoch 15, Batch 88 Loss:0.12532967329025269\n",
      "Epoch 15, Batch 89 Loss:0.10027843713760376\n",
      "Epoch 15, Batch 90 Loss:0.10747162997722626\n",
      "Epoch 15, Batch 91 Loss:0.12298701703548431\n",
      "Epoch 15, Batch 92 Loss:0.1514655202627182\n",
      "Epoch 15, Batch 93 Loss:0.1352057158946991\n",
      "Epoch 15, Batch 94 Loss:0.11911730468273163\n",
      "Epoch 15, Batch 95 Loss:0.11027799546718597\n",
      "Epoch 15, Batch 96 Loss:0.14268246293067932\n",
      "Epoch 15, Batch 97 Loss:0.09278857707977295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Batch 98 Loss:0.12288045883178711\n",
      "Epoch 15, Batch 99 Loss:0.1868121325969696\n",
      "Epoch 15, Batch 100 Loss:0.1656554937362671\n",
      "Epoch 15, Batch 101 Loss:0.1352643519639969\n",
      "Epoch 15, Batch 102 Loss:0.12601955235004425\n",
      "Epoch 15, Batch 103 Loss:0.11340738087892532\n",
      "Epoch 15, Batch 104 Loss:0.1516345590353012\n",
      "Epoch 15, Batch 105 Loss:0.09493761509656906\n",
      "Epoch 15, Batch 106 Loss:0.10543651878833771\n",
      "Epoch 15, Batch 107 Loss:0.11502266675233841\n",
      "Epoch 15, Batch 108 Loss:0.14793913066387177\n",
      "Epoch 15, Batch 109 Loss:0.19527125358581543\n",
      "Epoch 15, Batch 110 Loss:0.16065643727779388\n",
      "Epoch 15, Batch 111 Loss:0.07565639913082123\n",
      "Epoch 15, Batch 112 Loss:0.15851417183876038\n",
      "Epoch 15, Batch 113 Loss:0.09651070833206177\n",
      "Epoch 15, Batch 114 Loss:0.17552253603935242\n",
      "Epoch 15, Batch 115 Loss:0.10728830099105835\n",
      "Epoch 15, Batch 116 Loss:0.10640406608581543\n",
      "Epoch 15, Batch 117 Loss:0.1269543170928955\n",
      "Epoch 15, Batch 118 Loss:0.13866478204727173\n",
      "Epoch 15, Batch 119 Loss:0.07344876974821091\n",
      "Epoch 15, Batch 120 Loss:0.13937099277973175\n",
      "Epoch 15, Batch 121 Loss:0.1185496523976326\n",
      "Epoch 15, Batch 122 Loss:0.14113429188728333\n",
      "Epoch 15, Batch 123 Loss:0.11019991338253021\n",
      "Epoch 15, Batch 124 Loss:0.11897316575050354\n",
      "Epoch 15, Batch 125 Loss:0.11822037398815155\n",
      "Epoch 15, Batch 126 Loss:0.17265412211418152\n",
      "Epoch 15, Batch 127 Loss:0.1565694510936737\n",
      "Epoch 15, Batch 128 Loss:0.07396328449249268\n",
      "Epoch 15, Batch 129 Loss:0.12763577699661255\n",
      "Epoch 15, Batch 130 Loss:0.19916631281375885\n",
      "Epoch 15, Batch 131 Loss:0.11838781833648682\n",
      "Epoch 15, Batch 132 Loss:0.11668749898672104\n",
      "Epoch 15, Batch 133 Loss:0.1342965066432953\n",
      "Epoch 15, Batch 134 Loss:0.09250491857528687\n",
      "Epoch 15, Batch 135 Loss:0.10616405308246613\n",
      "Epoch 15, Batch 136 Loss:0.12839552760124207\n",
      "Epoch 15, Batch 137 Loss:0.1596900373697281\n",
      "Epoch 15, Batch 138 Loss:0.11176971346139908\n",
      "Epoch 15, Batch 139 Loss:0.12388099730014801\n",
      "Epoch 15, Batch 140 Loss:0.1245437040925026\n",
      "Epoch 15, Batch 141 Loss:0.12528404593467712\n",
      "Epoch 15, Batch 142 Loss:0.09486278891563416\n",
      "Epoch 15, Batch 143 Loss:0.06836438924074173\n",
      "Epoch 15, Batch 144 Loss:0.1208900436758995\n",
      "Epoch 15, Batch 145 Loss:0.12201082706451416\n",
      "Epoch 15, Batch 146 Loss:0.13988597691059113\n",
      "Epoch 15, Batch 147 Loss:0.11041951179504395\n",
      "Epoch 15, Batch 148 Loss:0.13875027000904083\n",
      "Epoch 15, Batch 149 Loss:0.08090948313474655\n",
      "Epoch 15, Batch 150 Loss:0.06408865749835968\n",
      "Epoch 15, Batch 151 Loss:0.14109760522842407\n",
      "Epoch 15, Batch 152 Loss:0.05663997679948807\n",
      "Epoch 15, Batch 153 Loss:0.09392290562391281\n",
      "Epoch 15, Batch 154 Loss:0.19552716612815857\n",
      "Epoch 15, Batch 155 Loss:0.09841122478246689\n",
      "Epoch 15, Batch 156 Loss:0.13610070943832397\n",
      "Epoch 15, Batch 157 Loss:0.07379475980997086\n",
      "Epoch 15, Batch 158 Loss:0.07164281606674194\n",
      "Epoch 15, Batch 159 Loss:0.08587729930877686\n",
      "Epoch 15, Batch 160 Loss:0.11607824265956879\n",
      "Epoch 15, Batch 161 Loss:0.18099629878997803\n",
      "Epoch 15, Batch 162 Loss:0.15917350351810455\n",
      "Epoch 15, Batch 163 Loss:0.13337616622447968\n",
      "Epoch 15, Batch 164 Loss:0.1033296287059784\n",
      "Epoch 15, Batch 165 Loss:0.10015431046485901\n",
      "Epoch 15, Batch 166 Loss:0.07927756756544113\n",
      "Epoch 15, Batch 167 Loss:0.12055103480815887\n",
      "Epoch 15, Batch 168 Loss:0.09734541177749634\n",
      "Epoch 15, Batch 169 Loss:0.20727191865444183\n",
      "Epoch 15, Batch 170 Loss:0.0822373479604721\n",
      "Epoch 15, Batch 171 Loss:0.14928710460662842\n",
      "Epoch 15, Batch 172 Loss:0.1656274050474167\n",
      "Epoch 15, Batch 173 Loss:0.16064707934856415\n",
      "Epoch 15, Batch 174 Loss:0.09054473042488098\n",
      "Epoch 15, Batch 175 Loss:0.08657453954219818\n",
      "Epoch 15, Batch 176 Loss:0.08967218548059464\n",
      "Epoch 15, Batch 177 Loss:0.0857766792178154\n",
      "Epoch 15, Batch 178 Loss:0.07707583904266357\n",
      "Epoch 15, Batch 179 Loss:0.13257409632205963\n",
      "Epoch 15, Batch 180 Loss:0.14366692304611206\n",
      "Epoch 15, Batch 181 Loss:0.10738509893417358\n",
      "Epoch 15, Batch 182 Loss:0.11151735484600067\n",
      "Epoch 15, Batch 183 Loss:0.1217946782708168\n",
      "Epoch 15, Batch 184 Loss:0.17282229661941528\n",
      "Epoch 15, Batch 185 Loss:0.08376023173332214\n",
      "Epoch 15, Batch 186 Loss:0.10414796322584152\n",
      "Epoch 15, Batch 187 Loss:0.12304967641830444\n",
      "Epoch 15, Batch 188 Loss:0.12510737776756287\n",
      "Epoch 15, Batch 189 Loss:0.16383618116378784\n",
      "Epoch 15, Batch 190 Loss:0.12097586691379547\n",
      "Epoch 15, Batch 191 Loss:0.14103710651397705\n",
      "Epoch 15, Batch 192 Loss:0.16321754455566406\n",
      "Epoch 15, Batch 193 Loss:0.12876732647418976\n",
      "Epoch 15, Batch 194 Loss:0.16559383273124695\n",
      "Epoch 15, Batch 195 Loss:0.16668212413787842\n",
      "Epoch 15, Batch 196 Loss:0.11386748403310776\n",
      "Epoch 15, Batch 197 Loss:0.12220224738121033\n",
      "Epoch 15, Batch 198 Loss:0.12895514070987701\n",
      "Epoch 15, Batch 199 Loss:0.1057782769203186\n",
      "Epoch 15, Batch 200 Loss:0.13494865596294403\n",
      "Epoch 15, Batch 201 Loss:0.1748121976852417\n",
      "Epoch 15, Batch 202 Loss:0.1295468807220459\n",
      "Epoch 15, Batch 203 Loss:0.18796350061893463\n",
      "Epoch 15, Batch 204 Loss:0.19338831305503845\n",
      "Epoch 15, Batch 205 Loss:0.12178897857666016\n",
      "Epoch 15, Batch 206 Loss:0.12607206404209137\n",
      "Epoch 15, Batch 207 Loss:0.19403186440467834\n",
      "Epoch 15, Batch 208 Loss:0.17372947931289673\n",
      "Epoch 15, Batch 209 Loss:0.12003210932016373\n",
      "Epoch 15, Batch 210 Loss:0.14003285765647888\n",
      "Epoch 15, Batch 211 Loss:0.11042671650648117\n",
      "Epoch 15, Batch 212 Loss:0.11998900026082993\n",
      "Epoch 15, Batch 213 Loss:0.20131592452526093\n",
      "Epoch 15, Batch 214 Loss:0.11237281560897827\n",
      "Epoch 15, Batch 215 Loss:0.13568182289600372\n",
      "Epoch 15, Batch 216 Loss:0.13516533374786377\n",
      "Epoch 15, Batch 217 Loss:0.12786543369293213\n",
      "Epoch 15, Batch 218 Loss:0.1699802428483963\n",
      "Epoch 15, Batch 219 Loss:0.1731352061033249\n",
      "Epoch 15, Batch 220 Loss:0.16748914122581482\n",
      "Epoch 15, Batch 221 Loss:0.15205395221710205\n",
      "Epoch 15, Batch 222 Loss:0.17536002397537231\n",
      "Epoch 15, Batch 223 Loss:0.16026055812835693\n",
      "Epoch 15, Batch 224 Loss:0.12476518750190735\n",
      "Epoch 15, Batch 225 Loss:0.1418234407901764\n",
      "Epoch 15, Batch 226 Loss:0.10572987794876099\n",
      "Epoch 15, Batch 227 Loss:0.1763690561056137\n",
      "Epoch 15, Batch 228 Loss:0.1556929498910904\n",
      "Epoch 15, Batch 229 Loss:0.09867732226848602\n",
      "Epoch 15, Batch 230 Loss:0.1506045013666153\n",
      "Epoch 15, Batch 231 Loss:0.1329410821199417\n",
      "Epoch 15, Batch 232 Loss:0.12425602227449417\n",
      "Epoch 15, Batch 233 Loss:0.11306583881378174\n",
      "Loss in this Epoch is: 11.3065838814 %\n",
      "Accuracy in this Epoch is: 88.3899986744 %\n",
      "Epoch 16, Batch 0 Loss:0.125604510307312\n",
      "Epoch 16, Batch 1 Loss:0.11859378218650818\n",
      "Epoch 16, Batch 2 Loss:0.11378315091133118\n",
      "Epoch 16, Batch 3 Loss:0.15062490105628967\n",
      "Epoch 16, Batch 4 Loss:0.09582351893186569\n",
      "Epoch 16, Batch 5 Loss:0.14478495717048645\n",
      "Epoch 16, Batch 6 Loss:0.20088614523410797\n",
      "Epoch 16, Batch 7 Loss:0.10536766797304153\n",
      "Epoch 16, Batch 8 Loss:0.12047208100557327\n",
      "Epoch 16, Batch 9 Loss:0.11974488943815231\n",
      "Epoch 16, Batch 10 Loss:0.07317964732646942\n",
      "Epoch 16, Batch 11 Loss:0.10559078305959702\n",
      "Epoch 16, Batch 12 Loss:0.08519548177719116\n",
      "Epoch 16, Batch 13 Loss:0.1022319421172142\n",
      "Epoch 16, Batch 14 Loss:0.11325377225875854\n",
      "Epoch 16, Batch 15 Loss:0.1171998605132103\n",
      "Epoch 16, Batch 16 Loss:0.08744630217552185\n",
      "Epoch 16, Batch 17 Loss:0.08928505331277847\n",
      "Epoch 16, Batch 18 Loss:0.10157317668199539\n",
      "Epoch 16, Batch 19 Loss:0.1297599822282791\n",
      "Epoch 16, Batch 20 Loss:0.11642207205295563\n",
      "Epoch 16, Batch 21 Loss:0.1284118890762329\n",
      "Epoch 16, Batch 22 Loss:0.09185443073511124\n",
      "Epoch 16, Batch 23 Loss:0.11153243482112885\n",
      "Epoch 16, Batch 24 Loss:0.07416145503520966\n",
      "Epoch 16, Batch 25 Loss:0.10934796929359436\n",
      "Epoch 16, Batch 26 Loss:0.12259633094072342\n",
      "Epoch 16, Batch 27 Loss:0.0903521254658699\n",
      "Epoch 16, Batch 28 Loss:0.100340336561203\n",
      "Epoch 16, Batch 29 Loss:0.13744638860225677\n",
      "Epoch 16, Batch 30 Loss:0.11489899456501007\n",
      "Epoch 16, Batch 31 Loss:0.0546819232404232\n",
      "Epoch 16, Batch 32 Loss:0.1370105892419815\n",
      "Epoch 16, Batch 33 Loss:0.08757942169904709\n",
      "Epoch 16, Batch 34 Loss:0.09654267132282257\n",
      "Epoch 16, Batch 35 Loss:0.12453821301460266\n",
      "Epoch 16, Batch 36 Loss:0.12609873712062836\n",
      "Epoch 16, Batch 37 Loss:0.07579947263002396\n",
      "Epoch 16, Batch 38 Loss:0.09043292701244354\n",
      "Epoch 16, Batch 39 Loss:0.09796682000160217\n",
      "Epoch 16, Batch 40 Loss:0.09076650440692902\n",
      "Epoch 16, Batch 41 Loss:0.12905888259410858\n",
      "Epoch 16, Batch 42 Loss:0.11075758188962936\n",
      "Epoch 16, Batch 43 Loss:0.08011381328105927\n",
      "Epoch 16, Batch 44 Loss:0.09625665098428726\n",
      "Epoch 16, Batch 45 Loss:0.09629755467176437\n",
      "Epoch 16, Batch 46 Loss:0.1132766455411911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 47 Loss:0.11198309063911438\n",
      "Epoch 16, Batch 48 Loss:0.08730502426624298\n",
      "Epoch 16, Batch 49 Loss:0.15626700222492218\n",
      "Epoch 16, Batch 50 Loss:0.12132292240858078\n",
      "Epoch 16, Batch 51 Loss:0.15834076702594757\n",
      "Epoch 16, Batch 52 Loss:0.10658428817987442\n",
      "Epoch 16, Batch 53 Loss:0.12399458885192871\n",
      "Epoch 16, Batch 54 Loss:0.10521215945482254\n",
      "Epoch 16, Batch 55 Loss:0.11927616596221924\n",
      "Epoch 16, Batch 56 Loss:0.08869858086109161\n",
      "Epoch 16, Batch 57 Loss:0.09456570446491241\n",
      "Epoch 16, Batch 58 Loss:0.09688248485326767\n",
      "Epoch 16, Batch 59 Loss:0.09884759783744812\n",
      "Epoch 16, Batch 60 Loss:0.13061583042144775\n",
      "Epoch 16, Batch 61 Loss:0.1222940981388092\n",
      "Epoch 16, Batch 62 Loss:0.12588004767894745\n",
      "Epoch 16, Batch 63 Loss:0.1022811010479927\n",
      "Epoch 16, Batch 64 Loss:0.08810868859291077\n",
      "Epoch 16, Batch 65 Loss:0.12148723006248474\n",
      "Epoch 16, Batch 66 Loss:0.1768755465745926\n",
      "Epoch 16, Batch 67 Loss:0.09944142401218414\n",
      "Epoch 16, Batch 68 Loss:0.1242840513586998\n",
      "Epoch 16, Batch 69 Loss:0.08258184045553207\n",
      "Epoch 16, Batch 70 Loss:0.13545793294906616\n",
      "Epoch 16, Batch 71 Loss:0.14371226727962494\n",
      "Epoch 16, Batch 72 Loss:0.09570939093828201\n",
      "Epoch 16, Batch 73 Loss:0.1152142584323883\n",
      "Epoch 16, Batch 74 Loss:0.1530398577451706\n",
      "Epoch 16, Batch 75 Loss:0.12457462400197983\n",
      "Epoch 16, Batch 76 Loss:0.07436610013246536\n",
      "Epoch 16, Batch 77 Loss:0.08228114247322083\n",
      "Epoch 16, Batch 78 Loss:0.06982453912496567\n",
      "Epoch 16, Batch 79 Loss:0.08509425818920135\n",
      "Epoch 16, Batch 80 Loss:0.05341820791363716\n",
      "Epoch 16, Batch 81 Loss:0.10639519989490509\n",
      "Epoch 16, Batch 82 Loss:0.09976682811975479\n",
      "Epoch 16, Batch 83 Loss:0.08394141495227814\n",
      "Epoch 16, Batch 84 Loss:0.10300188511610031\n",
      "Epoch 16, Batch 85 Loss:0.09369843453168869\n",
      "Epoch 16, Batch 86 Loss:0.07176002860069275\n",
      "Epoch 16, Batch 87 Loss:0.10253648459911346\n",
      "Epoch 16, Batch 88 Loss:0.08746282011270523\n",
      "Epoch 16, Batch 89 Loss:0.12411869317293167\n",
      "Epoch 16, Batch 90 Loss:0.1462366282939911\n",
      "Epoch 16, Batch 91 Loss:0.1074964702129364\n",
      "Epoch 16, Batch 92 Loss:0.07141200453042984\n",
      "Epoch 16, Batch 93 Loss:0.09883540123701096\n",
      "Epoch 16, Batch 94 Loss:0.1135793998837471\n",
      "Epoch 16, Batch 95 Loss:0.15916886925697327\n",
      "Epoch 16, Batch 96 Loss:0.12175969779491425\n",
      "Epoch 16, Batch 97 Loss:0.14671167731285095\n",
      "Epoch 16, Batch 98 Loss:0.11182241141796112\n",
      "Epoch 16, Batch 99 Loss:0.1169629618525505\n",
      "Epoch 16, Batch 100 Loss:0.13082122802734375\n",
      "Epoch 16, Batch 101 Loss:0.17908263206481934\n",
      "Epoch 16, Batch 102 Loss:0.14288580417633057\n",
      "Epoch 16, Batch 103 Loss:0.09174343943595886\n",
      "Epoch 16, Batch 104 Loss:0.10127608478069305\n",
      "Epoch 16, Batch 105 Loss:0.08115598559379578\n",
      "Epoch 16, Batch 106 Loss:0.09111848473548889\n",
      "Epoch 16, Batch 107 Loss:0.12454712390899658\n",
      "Epoch 16, Batch 108 Loss:0.1299949288368225\n",
      "Epoch 16, Batch 109 Loss:0.10234995931386948\n",
      "Epoch 16, Batch 110 Loss:0.12130323052406311\n",
      "Epoch 16, Batch 111 Loss:0.12832258641719818\n",
      "Epoch 16, Batch 112 Loss:0.09826541692018509\n",
      "Epoch 16, Batch 113 Loss:0.09168097376823425\n",
      "Epoch 16, Batch 114 Loss:0.06368670612573624\n",
      "Epoch 16, Batch 115 Loss:0.14454475045204163\n",
      "Epoch 16, Batch 116 Loss:0.06893700361251831\n",
      "Epoch 16, Batch 117 Loss:0.12391185015439987\n",
      "Epoch 16, Batch 118 Loss:0.10253016650676727\n",
      "Epoch 16, Batch 119 Loss:0.1228630393743515\n",
      "Epoch 16, Batch 120 Loss:0.1289156675338745\n",
      "Epoch 16, Batch 121 Loss:0.10462813079357147\n",
      "Epoch 16, Batch 122 Loss:0.11927953362464905\n",
      "Epoch 16, Batch 123 Loss:0.12984982132911682\n",
      "Epoch 16, Batch 124 Loss:0.11353184282779694\n",
      "Epoch 16, Batch 125 Loss:0.15069782733917236\n",
      "Epoch 16, Batch 126 Loss:0.1339128464460373\n",
      "Epoch 16, Batch 127 Loss:0.1476747840642929\n",
      "Epoch 16, Batch 128 Loss:0.1042083352804184\n",
      "Epoch 16, Batch 129 Loss:0.12116483598947525\n",
      "Epoch 16, Batch 130 Loss:0.15302065014839172\n",
      "Epoch 16, Batch 131 Loss:0.13051503896713257\n",
      "Epoch 16, Batch 132 Loss:0.12781867384910583\n",
      "Epoch 16, Batch 133 Loss:0.1026410311460495\n",
      "Epoch 16, Batch 134 Loss:0.13109713792800903\n",
      "Epoch 16, Batch 135 Loss:0.14210136234760284\n",
      "Epoch 16, Batch 136 Loss:0.13255751132965088\n",
      "Epoch 16, Batch 137 Loss:0.1223951056599617\n",
      "Epoch 16, Batch 138 Loss:0.1254885494709015\n",
      "Epoch 16, Batch 139 Loss:0.07499317824840546\n",
      "Epoch 16, Batch 140 Loss:0.09742403030395508\n",
      "Epoch 16, Batch 141 Loss:0.13067801296710968\n",
      "Epoch 16, Batch 142 Loss:0.14521187543869019\n",
      "Epoch 16, Batch 143 Loss:0.15169799327850342\n",
      "Epoch 16, Batch 144 Loss:0.10510184615850449\n",
      "Epoch 16, Batch 145 Loss:0.10928235948085785\n",
      "Epoch 16, Batch 146 Loss:0.12875527143478394\n",
      "Epoch 16, Batch 147 Loss:0.1532261222600937\n",
      "Epoch 16, Batch 148 Loss:0.09797251224517822\n",
      "Epoch 16, Batch 149 Loss:0.15094725787639618\n",
      "Epoch 16, Batch 150 Loss:0.11957743018865585\n",
      "Epoch 16, Batch 151 Loss:0.11356164515018463\n",
      "Epoch 16, Batch 152 Loss:0.2053137719631195\n",
      "Epoch 16, Batch 153 Loss:0.12873287498950958\n",
      "Epoch 16, Batch 154 Loss:0.10612465441226959\n",
      "Epoch 16, Batch 155 Loss:0.09291477501392365\n",
      "Epoch 16, Batch 156 Loss:0.1080656498670578\n",
      "Epoch 16, Batch 157 Loss:0.12534105777740479\n",
      "Epoch 16, Batch 158 Loss:0.09382031857967377\n",
      "Epoch 16, Batch 159 Loss:0.09021428972482681\n",
      "Epoch 16, Batch 160 Loss:0.0894944816827774\n",
      "Epoch 16, Batch 161 Loss:0.12159211933612823\n",
      "Epoch 16, Batch 162 Loss:0.1164061650633812\n",
      "Epoch 16, Batch 163 Loss:0.12778595089912415\n",
      "Epoch 16, Batch 164 Loss:0.09181547164916992\n",
      "Epoch 16, Batch 165 Loss:0.11560637503862381\n",
      "Epoch 16, Batch 166 Loss:0.10653968900442123\n",
      "Epoch 16, Batch 167 Loss:0.09862031042575836\n",
      "Epoch 16, Batch 168 Loss:0.11922755837440491\n",
      "Epoch 16, Batch 169 Loss:0.09079097956418991\n",
      "Epoch 16, Batch 170 Loss:0.10345917195081711\n",
      "Epoch 16, Batch 171 Loss:0.1831359565258026\n",
      "Epoch 16, Batch 172 Loss:0.12380965054035187\n",
      "Epoch 16, Batch 173 Loss:0.14524246752262115\n",
      "Epoch 16, Batch 174 Loss:0.10444026440382004\n",
      "Epoch 16, Batch 175 Loss:0.12961100041866302\n",
      "Epoch 16, Batch 176 Loss:0.12122723460197449\n",
      "Epoch 16, Batch 177 Loss:0.09764239192008972\n",
      "Epoch 16, Batch 178 Loss:0.10205193608999252\n",
      "Epoch 16, Batch 179 Loss:0.16659079492092133\n",
      "Epoch 16, Batch 180 Loss:0.05737221986055374\n",
      "Epoch 16, Batch 181 Loss:0.09297758340835571\n",
      "Epoch 16, Batch 182 Loss:0.18666118383407593\n",
      "Epoch 16, Batch 183 Loss:0.14346444606781006\n",
      "Epoch 16, Batch 184 Loss:0.16328036785125732\n",
      "Epoch 16, Batch 185 Loss:0.11354735493659973\n",
      "Epoch 16, Batch 186 Loss:0.09390635788440704\n",
      "Epoch 16, Batch 187 Loss:0.12927165627479553\n",
      "Epoch 16, Batch 188 Loss:0.12152159959077835\n",
      "Epoch 16, Batch 189 Loss:0.09604845196008682\n",
      "Epoch 16, Batch 190 Loss:0.13660705089569092\n",
      "Epoch 16, Batch 191 Loss:0.0805414617061615\n",
      "Epoch 16, Batch 192 Loss:0.15282294154167175\n",
      "Epoch 16, Batch 193 Loss:0.15521888434886932\n",
      "Epoch 16, Batch 194 Loss:0.054338544607162476\n",
      "Epoch 16, Batch 195 Loss:0.13788887858390808\n",
      "Epoch 16, Batch 196 Loss:0.12150943279266357\n",
      "Epoch 16, Batch 197 Loss:0.091697096824646\n",
      "Epoch 16, Batch 198 Loss:0.08519631624221802\n",
      "Epoch 16, Batch 199 Loss:0.0977199375629425\n",
      "Epoch 16, Batch 200 Loss:0.15912358462810516\n",
      "Epoch 16, Batch 201 Loss:0.1110328808426857\n",
      "Epoch 16, Batch 202 Loss:0.14984172582626343\n",
      "Epoch 16, Batch 203 Loss:0.1661449670791626\n",
      "Epoch 16, Batch 204 Loss:0.13263335824012756\n",
      "Epoch 16, Batch 205 Loss:0.11249430477619171\n",
      "Epoch 16, Batch 206 Loss:0.1559392213821411\n",
      "Epoch 16, Batch 207 Loss:0.11656741052865982\n",
      "Epoch 16, Batch 208 Loss:0.10994673520326614\n",
      "Epoch 16, Batch 209 Loss:0.1554175168275833\n",
      "Epoch 16, Batch 210 Loss:0.2057737112045288\n",
      "Epoch 16, Batch 211 Loss:0.14423967897891998\n",
      "Epoch 16, Batch 212 Loss:0.108804851770401\n",
      "Epoch 16, Batch 213 Loss:0.15750274062156677\n",
      "Epoch 16, Batch 214 Loss:0.17045654356479645\n",
      "Epoch 16, Batch 215 Loss:0.10171405971050262\n",
      "Epoch 16, Batch 216 Loss:0.12490557134151459\n",
      "Epoch 16, Batch 217 Loss:0.12719318270683289\n",
      "Epoch 16, Batch 218 Loss:0.12218642234802246\n",
      "Epoch 16, Batch 219 Loss:0.12593117356300354\n",
      "Epoch 16, Batch 220 Loss:0.12786003947257996\n",
      "Epoch 16, Batch 221 Loss:0.1370714008808136\n",
      "Epoch 16, Batch 222 Loss:0.07616379857063293\n",
      "Epoch 16, Batch 223 Loss:0.11977473646402359\n",
      "Epoch 16, Batch 224 Loss:0.12850189208984375\n",
      "Epoch 16, Batch 225 Loss:0.11828634142875671\n",
      "Epoch 16, Batch 226 Loss:0.15627054870128632\n",
      "Epoch 16, Batch 227 Loss:0.1091327890753746\n",
      "Epoch 16, Batch 228 Loss:0.15577182173728943\n",
      "Epoch 16, Batch 229 Loss:0.17601042985916138\n",
      "Epoch 16, Batch 230 Loss:0.16403008997440338\n",
      "Epoch 16, Batch 231 Loss:0.13991385698318481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Batch 232 Loss:0.21657873690128326\n",
      "Epoch 16, Batch 233 Loss:0.15714803338050842\n",
      "Loss in this Epoch is: 15.7148033381 %\n",
      "Accuracy in this Epoch is: 89.1300022602 %\n",
      "Epoch 17, Batch 0 Loss:0.11364594101905823\n",
      "Epoch 17, Batch 1 Loss:0.09885232895612717\n",
      "Epoch 17, Batch 2 Loss:0.08758140355348587\n",
      "Epoch 17, Batch 3 Loss:0.07310417294502258\n",
      "Epoch 17, Batch 4 Loss:0.07400258630514145\n",
      "Epoch 17, Batch 5 Loss:0.13470329344272614\n",
      "Epoch 17, Batch 6 Loss:0.11504311859607697\n",
      "Epoch 17, Batch 7 Loss:0.09709098935127258\n",
      "Epoch 17, Batch 8 Loss:0.14286628365516663\n",
      "Epoch 17, Batch 9 Loss:0.10448125004768372\n",
      "Epoch 17, Batch 10 Loss:0.08988628536462784\n",
      "Epoch 17, Batch 11 Loss:0.08788097649812698\n",
      "Epoch 17, Batch 12 Loss:0.072267547249794\n",
      "Epoch 17, Batch 13 Loss:0.10221092402935028\n",
      "Epoch 17, Batch 14 Loss:0.13112790882587433\n",
      "Epoch 17, Batch 15 Loss:0.07448525726795197\n",
      "Epoch 17, Batch 16 Loss:0.14215032756328583\n",
      "Epoch 17, Batch 17 Loss:0.09167574346065521\n",
      "Epoch 17, Batch 18 Loss:0.0901830866932869\n",
      "Epoch 17, Batch 19 Loss:0.06420470029115677\n",
      "Epoch 17, Batch 20 Loss:0.1084379330277443\n",
      "Epoch 17, Batch 21 Loss:0.09316207468509674\n",
      "Epoch 17, Batch 22 Loss:0.08892437070608139\n",
      "Epoch 17, Batch 23 Loss:0.15672200918197632\n",
      "Epoch 17, Batch 24 Loss:0.12203076481819153\n",
      "Epoch 17, Batch 25 Loss:0.15592357516288757\n",
      "Epoch 17, Batch 26 Loss:0.11383987218141556\n",
      "Epoch 17, Batch 27 Loss:0.09420615434646606\n",
      "Epoch 17, Batch 28 Loss:0.10755988210439682\n",
      "Epoch 17, Batch 29 Loss:0.13739854097366333\n",
      "Epoch 17, Batch 30 Loss:0.09058267623186111\n",
      "Epoch 17, Batch 31 Loss:0.07819834351539612\n",
      "Epoch 17, Batch 32 Loss:0.08395519852638245\n",
      "Epoch 17, Batch 33 Loss:0.16053280234336853\n",
      "Epoch 17, Batch 34 Loss:0.09219489991664886\n",
      "Epoch 17, Batch 35 Loss:0.0919623076915741\n",
      "Epoch 17, Batch 36 Loss:0.12003675103187561\n",
      "Epoch 17, Batch 37 Loss:0.08797474205493927\n",
      "Epoch 17, Batch 38 Loss:0.11869645863771439\n",
      "Epoch 17, Batch 39 Loss:0.0859198048710823\n",
      "Epoch 17, Batch 40 Loss:0.08705674111843109\n",
      "Epoch 17, Batch 41 Loss:0.09421835094690323\n",
      "Epoch 17, Batch 42 Loss:0.059486374258995056\n",
      "Epoch 17, Batch 43 Loss:0.09376992285251617\n",
      "Epoch 17, Batch 44 Loss:0.06702864170074463\n",
      "Epoch 17, Batch 45 Loss:0.07362879067659378\n",
      "Epoch 17, Batch 46 Loss:0.07574473321437836\n",
      "Epoch 17, Batch 47 Loss:0.11097881197929382\n",
      "Epoch 17, Batch 48 Loss:0.07925637066364288\n",
      "Epoch 17, Batch 49 Loss:0.05416639894247055\n",
      "Epoch 17, Batch 50 Loss:0.12244817614555359\n",
      "Epoch 17, Batch 51 Loss:0.08855942636728287\n",
      "Epoch 17, Batch 52 Loss:0.09398944675922394\n",
      "Epoch 17, Batch 53 Loss:0.10478340834379196\n",
      "Epoch 17, Batch 54 Loss:0.0863664299249649\n",
      "Epoch 17, Batch 55 Loss:0.11567190289497375\n",
      "Epoch 17, Batch 56 Loss:0.06532624363899231\n",
      "Epoch 17, Batch 57 Loss:0.06410536170005798\n",
      "Epoch 17, Batch 58 Loss:0.0801161378622055\n",
      "Epoch 17, Batch 59 Loss:0.08423756062984467\n",
      "Epoch 17, Batch 60 Loss:0.09221519529819489\n",
      "Epoch 17, Batch 61 Loss:0.07758044451475143\n",
      "Epoch 17, Batch 62 Loss:0.1139225885272026\n",
      "Epoch 17, Batch 63 Loss:0.08158174902200699\n",
      "Epoch 17, Batch 64 Loss:0.11476912349462509\n",
      "Epoch 17, Batch 65 Loss:0.13539446890354156\n",
      "Epoch 17, Batch 66 Loss:0.14898882806301117\n",
      "Epoch 17, Batch 67 Loss:0.11946431547403336\n",
      "Epoch 17, Batch 68 Loss:0.0775037482380867\n",
      "Epoch 17, Batch 69 Loss:0.0923495888710022\n",
      "Epoch 17, Batch 70 Loss:0.09944562613964081\n",
      "Epoch 17, Batch 71 Loss:0.0781702846288681\n",
      "Epoch 17, Batch 72 Loss:0.10865336656570435\n",
      "Epoch 17, Batch 73 Loss:0.1068631038069725\n",
      "Epoch 17, Batch 74 Loss:0.10878615081310272\n",
      "Epoch 17, Batch 75 Loss:0.10039697587490082\n",
      "Epoch 17, Batch 76 Loss:0.07673048228025436\n",
      "Epoch 17, Batch 77 Loss:0.14221139252185822\n",
      "Epoch 17, Batch 78 Loss:0.13595210015773773\n",
      "Epoch 17, Batch 79 Loss:0.06535547971725464\n",
      "Epoch 17, Batch 80 Loss:0.0853038802742958\n",
      "Epoch 17, Batch 81 Loss:0.10507241636514664\n",
      "Epoch 17, Batch 82 Loss:0.10875637084245682\n",
      "Epoch 17, Batch 83 Loss:0.0882851853966713\n",
      "Epoch 17, Batch 84 Loss:0.07187359780073166\n",
      "Epoch 17, Batch 85 Loss:0.13366685807704926\n",
      "Epoch 17, Batch 86 Loss:0.12512031197547913\n",
      "Epoch 17, Batch 87 Loss:0.14093366265296936\n",
      "Epoch 17, Batch 88 Loss:0.07268370687961578\n",
      "Epoch 17, Batch 89 Loss:0.09551844745874405\n",
      "Epoch 17, Batch 90 Loss:0.09322497993707657\n",
      "Epoch 17, Batch 91 Loss:0.12315378338098526\n",
      "Epoch 17, Batch 92 Loss:0.07314956933259964\n",
      "Epoch 17, Batch 93 Loss:0.12858349084854126\n",
      "Epoch 17, Batch 94 Loss:0.12288950383663177\n",
      "Epoch 17, Batch 95 Loss:0.13605128228664398\n",
      "Epoch 17, Batch 96 Loss:0.06459293514490128\n",
      "Epoch 17, Batch 97 Loss:0.11497105658054352\n",
      "Epoch 17, Batch 98 Loss:0.09206073731184006\n",
      "Epoch 17, Batch 99 Loss:0.10064221173524857\n",
      "Epoch 17, Batch 100 Loss:0.09236106276512146\n",
      "Epoch 17, Batch 101 Loss:0.13585925102233887\n",
      "Epoch 17, Batch 102 Loss:0.09847073256969452\n",
      "Epoch 17, Batch 103 Loss:0.0968589335680008\n",
      "Epoch 17, Batch 104 Loss:0.13733941316604614\n",
      "Epoch 17, Batch 105 Loss:0.1624307632446289\n",
      "Epoch 17, Batch 106 Loss:0.05976531654596329\n",
      "Epoch 17, Batch 107 Loss:0.08203013241291046\n",
      "Epoch 17, Batch 108 Loss:0.17660683393478394\n",
      "Epoch 17, Batch 109 Loss:0.10490703582763672\n",
      "Epoch 17, Batch 110 Loss:0.08457161486148834\n",
      "Epoch 17, Batch 111 Loss:0.1253543496131897\n",
      "Epoch 17, Batch 112 Loss:0.07717414945363998\n",
      "Epoch 17, Batch 113 Loss:0.13703879714012146\n",
      "Epoch 17, Batch 114 Loss:0.08268557488918304\n",
      "Epoch 17, Batch 115 Loss:0.08342821896076202\n",
      "Epoch 17, Batch 116 Loss:0.14576640725135803\n",
      "Epoch 17, Batch 117 Loss:0.18990546464920044\n",
      "Epoch 17, Batch 118 Loss:0.07792536914348602\n",
      "Epoch 17, Batch 119 Loss:0.09194985032081604\n",
      "Epoch 17, Batch 120 Loss:0.09549663215875626\n",
      "Epoch 17, Batch 121 Loss:0.13655555248260498\n",
      "Epoch 17, Batch 122 Loss:0.13793431222438812\n",
      "Epoch 17, Batch 123 Loss:0.1416289061307907\n",
      "Epoch 17, Batch 124 Loss:0.07631716132164001\n",
      "Epoch 17, Batch 125 Loss:0.07906410098075867\n",
      "Epoch 17, Batch 126 Loss:0.14027050137519836\n",
      "Epoch 17, Batch 127 Loss:0.087565578520298\n",
      "Epoch 17, Batch 128 Loss:0.09410880506038666\n",
      "Epoch 17, Batch 129 Loss:0.09999215602874756\n",
      "Epoch 17, Batch 130 Loss:0.1092669740319252\n",
      "Epoch 17, Batch 131 Loss:0.08839669823646545\n",
      "Epoch 17, Batch 132 Loss:0.10193116962909698\n",
      "Epoch 17, Batch 133 Loss:0.08334537595510483\n",
      "Epoch 17, Batch 134 Loss:0.1729772388935089\n",
      "Epoch 17, Batch 135 Loss:0.1462439000606537\n",
      "Epoch 17, Batch 136 Loss:0.07220423221588135\n",
      "Epoch 17, Batch 137 Loss:0.09495106339454651\n",
      "Epoch 17, Batch 138 Loss:0.12377120554447174\n",
      "Epoch 17, Batch 139 Loss:0.10765565931797028\n",
      "Epoch 17, Batch 140 Loss:0.07679744809865952\n",
      "Epoch 17, Batch 141 Loss:0.09586963057518005\n",
      "Epoch 17, Batch 142 Loss:0.11660537123680115\n",
      "Epoch 17, Batch 143 Loss:0.16031470894813538\n",
      "Epoch 17, Batch 144 Loss:0.11982376873493195\n",
      "Epoch 17, Batch 145 Loss:0.10751934349536896\n",
      "Epoch 17, Batch 146 Loss:0.09828715771436691\n",
      "Epoch 17, Batch 147 Loss:0.12883280217647552\n",
      "Epoch 17, Batch 148 Loss:0.0652322769165039\n",
      "Epoch 17, Batch 149 Loss:0.12647846341133118\n",
      "Epoch 17, Batch 150 Loss:0.10299406945705414\n",
      "Epoch 17, Batch 151 Loss:0.10897637158632278\n",
      "Epoch 17, Batch 152 Loss:0.11022801697254181\n",
      "Epoch 17, Batch 153 Loss:0.12507259845733643\n",
      "Epoch 17, Batch 154 Loss:0.0714070200920105\n",
      "Epoch 17, Batch 155 Loss:0.08646170049905777\n",
      "Epoch 17, Batch 156 Loss:0.09838882833719254\n",
      "Epoch 17, Batch 157 Loss:0.12752537429332733\n",
      "Epoch 17, Batch 158 Loss:0.11210297048091888\n",
      "Epoch 17, Batch 159 Loss:0.11005868017673492\n",
      "Epoch 17, Batch 160 Loss:0.10879400372505188\n",
      "Epoch 17, Batch 161 Loss:0.09403733909130096\n",
      "Epoch 17, Batch 162 Loss:0.10213720798492432\n",
      "Epoch 17, Batch 163 Loss:0.10662661492824554\n",
      "Epoch 17, Batch 164 Loss:0.08151371777057648\n",
      "Epoch 17, Batch 165 Loss:0.08136716485023499\n",
      "Epoch 17, Batch 166 Loss:0.11968038976192474\n",
      "Epoch 17, Batch 167 Loss:0.0980614423751831\n",
      "Epoch 17, Batch 168 Loss:0.1323767602443695\n",
      "Epoch 17, Batch 169 Loss:0.12228474766016006\n",
      "Epoch 17, Batch 170 Loss:0.11830063909292221\n",
      "Epoch 17, Batch 171 Loss:0.11500359326601028\n",
      "Epoch 17, Batch 172 Loss:0.11813855916261673\n",
      "Epoch 17, Batch 173 Loss:0.13419631123542786\n",
      "Epoch 17, Batch 174 Loss:0.10110269486904144\n",
      "Epoch 17, Batch 175 Loss:0.07783502340316772\n",
      "Epoch 17, Batch 176 Loss:0.15635718405246735\n",
      "Epoch 17, Batch 177 Loss:0.1452256739139557\n",
      "Epoch 17, Batch 178 Loss:0.20734910666942596\n",
      "Epoch 17, Batch 179 Loss:0.05665826424956322\n",
      "Epoch 17, Batch 180 Loss:0.10725364089012146\n",
      "Epoch 17, Batch 181 Loss:0.14379362761974335\n",
      "Epoch 17, Batch 182 Loss:0.13497909903526306\n",
      "Epoch 17, Batch 183 Loss:0.11467081308364868\n",
      "Epoch 17, Batch 184 Loss:0.11958064138889313\n",
      "Epoch 17, Batch 185 Loss:0.13685016334056854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Batch 186 Loss:0.10231872648000717\n",
      "Epoch 17, Batch 187 Loss:0.13011419773101807\n",
      "Epoch 17, Batch 188 Loss:0.11269700527191162\n",
      "Epoch 17, Batch 189 Loss:0.17071038484573364\n",
      "Epoch 17, Batch 190 Loss:0.14914268255233765\n",
      "Epoch 17, Batch 191 Loss:0.10661043226718903\n",
      "Epoch 17, Batch 192 Loss:0.11925382167100906\n",
      "Epoch 17, Batch 193 Loss:0.11333005130290985\n",
      "Epoch 17, Batch 194 Loss:0.109053835272789\n",
      "Epoch 17, Batch 195 Loss:0.12814787030220032\n",
      "Epoch 17, Batch 196 Loss:0.1695108562707901\n",
      "Epoch 17, Batch 197 Loss:0.13798069953918457\n",
      "Epoch 17, Batch 198 Loss:0.10562126338481903\n",
      "Epoch 17, Batch 199 Loss:0.0721384659409523\n",
      "Epoch 17, Batch 200 Loss:0.15583284199237823\n",
      "Epoch 17, Batch 201 Loss:0.16391883790493011\n",
      "Epoch 17, Batch 202 Loss:0.14518162608146667\n",
      "Epoch 17, Batch 203 Loss:0.1603577733039856\n",
      "Epoch 17, Batch 204 Loss:0.14852280914783478\n",
      "Epoch 17, Batch 205 Loss:0.12219491600990295\n",
      "Epoch 17, Batch 206 Loss:0.09386327117681503\n",
      "Epoch 17, Batch 207 Loss:0.17776086926460266\n",
      "Epoch 17, Batch 208 Loss:0.12651415169239044\n",
      "Epoch 17, Batch 209 Loss:0.08582011610269547\n",
      "Epoch 17, Batch 210 Loss:0.14168089628219604\n",
      "Epoch 17, Batch 211 Loss:0.13963767886161804\n",
      "Epoch 17, Batch 212 Loss:0.10588488727807999\n",
      "Epoch 17, Batch 213 Loss:0.13824796676635742\n",
      "Epoch 17, Batch 214 Loss:0.13703429698944092\n",
      "Epoch 17, Batch 215 Loss:0.08930496871471405\n",
      "Epoch 17, Batch 216 Loss:0.1584509313106537\n",
      "Epoch 17, Batch 217 Loss:0.12672188878059387\n",
      "Epoch 17, Batch 218 Loss:0.12645484507083893\n",
      "Epoch 17, Batch 219 Loss:0.10679803788661957\n",
      "Epoch 17, Batch 220 Loss:0.12827962636947632\n",
      "Epoch 17, Batch 221 Loss:0.0815965011715889\n",
      "Epoch 17, Batch 222 Loss:0.10747620463371277\n",
      "Epoch 17, Batch 223 Loss:0.12938307225704193\n",
      "Epoch 17, Batch 224 Loss:0.1435643881559372\n",
      "Epoch 17, Batch 225 Loss:0.10833286494016647\n",
      "Epoch 17, Batch 226 Loss:0.09065830707550049\n",
      "Epoch 17, Batch 227 Loss:0.0967978984117508\n",
      "Epoch 17, Batch 228 Loss:0.12456473708152771\n",
      "Epoch 17, Batch 229 Loss:0.1164189949631691\n",
      "Epoch 17, Batch 230 Loss:0.08149223774671555\n",
      "Epoch 17, Batch 231 Loss:0.10376330465078354\n",
      "Epoch 17, Batch 232 Loss:0.09922617673873901\n",
      "Epoch 17, Batch 233 Loss:0.12437194585800171\n",
      "Loss in this Epoch is: 12.4371945858 %\n",
      "Accuracy in this Epoch is: 88.7399971485 %\n",
      "Epoch 18, Batch 0 Loss:0.12160249799489975\n",
      "Epoch 18, Batch 1 Loss:0.10719698667526245\n",
      "Epoch 18, Batch 2 Loss:0.14998246729373932\n",
      "Epoch 18, Batch 3 Loss:0.10415975749492645\n",
      "Epoch 18, Batch 4 Loss:0.08914739638566971\n",
      "Epoch 18, Batch 5 Loss:0.12662702798843384\n",
      "Epoch 18, Batch 6 Loss:0.07795359939336777\n",
      "Epoch 18, Batch 7 Loss:0.12226027250289917\n",
      "Epoch 18, Batch 8 Loss:0.08703314512968063\n",
      "Epoch 18, Batch 9 Loss:0.1021013855934143\n",
      "Epoch 18, Batch 10 Loss:0.09790871292352676\n",
      "Epoch 18, Batch 11 Loss:0.08511103689670563\n",
      "Epoch 18, Batch 12 Loss:0.07387915253639221\n",
      "Epoch 18, Batch 13 Loss:0.07003357261419296\n",
      "Epoch 18, Batch 14 Loss:0.07263123989105225\n",
      "Epoch 18, Batch 15 Loss:0.10682704299688339\n",
      "Epoch 18, Batch 16 Loss:0.09661324322223663\n",
      "Epoch 18, Batch 17 Loss:0.08281387388706207\n",
      "Epoch 18, Batch 18 Loss:0.09643596410751343\n",
      "Epoch 18, Batch 19 Loss:0.10919841378927231\n",
      "Epoch 18, Batch 20 Loss:0.11052944511175156\n",
      "Epoch 18, Batch 21 Loss:0.07699285447597504\n",
      "Epoch 18, Batch 22 Loss:0.1019129529595375\n",
      "Epoch 18, Batch 23 Loss:0.08916378021240234\n",
      "Epoch 18, Batch 24 Loss:0.06614051014184952\n",
      "Epoch 18, Batch 25 Loss:0.07856334745883942\n",
      "Epoch 18, Batch 26 Loss:0.09416208416223526\n",
      "Epoch 18, Batch 27 Loss:0.09257857501506805\n",
      "Epoch 18, Batch 28 Loss:0.1301223784685135\n",
      "Epoch 18, Batch 29 Loss:0.06266939640045166\n",
      "Epoch 18, Batch 30 Loss:0.07722751796245575\n",
      "Epoch 18, Batch 31 Loss:0.08248243480920792\n",
      "Epoch 18, Batch 32 Loss:0.0441456064581871\n",
      "Epoch 18, Batch 33 Loss:0.13859792053699493\n",
      "Epoch 18, Batch 34 Loss:0.08828984200954437\n",
      "Epoch 18, Batch 35 Loss:0.11970126628875732\n",
      "Epoch 18, Batch 36 Loss:0.13378357887268066\n",
      "Epoch 18, Batch 37 Loss:0.11505497246980667\n",
      "Epoch 18, Batch 38 Loss:0.08234956860542297\n",
      "Epoch 18, Batch 39 Loss:0.09062593430280685\n",
      "Epoch 18, Batch 40 Loss:0.07823506742715836\n",
      "Epoch 18, Batch 41 Loss:0.09741555899381638\n",
      "Epoch 18, Batch 42 Loss:0.09616884589195251\n",
      "Epoch 18, Batch 43 Loss:0.10225711017847061\n",
      "Epoch 18, Batch 44 Loss:0.09082283079624176\n",
      "Epoch 18, Batch 45 Loss:0.07096852362155914\n",
      "Epoch 18, Batch 46 Loss:0.10348889976739883\n",
      "Epoch 18, Batch 47 Loss:0.12111177295446396\n",
      "Epoch 18, Batch 48 Loss:0.11736228317022324\n",
      "Epoch 18, Batch 49 Loss:0.0936824232339859\n",
      "Epoch 18, Batch 50 Loss:0.0827338919043541\n",
      "Epoch 18, Batch 51 Loss:0.09664054214954376\n",
      "Epoch 18, Batch 52 Loss:0.09889253228902817\n",
      "Epoch 18, Batch 53 Loss:0.0999394953250885\n",
      "Epoch 18, Batch 54 Loss:0.073623426258564\n",
      "Epoch 18, Batch 55 Loss:0.1285487562417984\n",
      "Epoch 18, Batch 56 Loss:0.10594534128904343\n",
      "Epoch 18, Batch 57 Loss:0.06739768385887146\n",
      "Epoch 18, Batch 58 Loss:0.07145173102617264\n",
      "Epoch 18, Batch 59 Loss:0.08262863755226135\n",
      "Epoch 18, Batch 60 Loss:0.0686291977763176\n",
      "Epoch 18, Batch 61 Loss:0.05944560468196869\n",
      "Epoch 18, Batch 62 Loss:0.12019834667444229\n",
      "Epoch 18, Batch 63 Loss:0.09610022604465485\n",
      "Epoch 18, Batch 64 Loss:0.04594097658991814\n",
      "Epoch 18, Batch 65 Loss:0.04701067507266998\n",
      "Epoch 18, Batch 66 Loss:0.1045699268579483\n",
      "Epoch 18, Batch 67 Loss:0.0990564301609993\n",
      "Epoch 18, Batch 68 Loss:0.051148004829883575\n",
      "Epoch 18, Batch 69 Loss:0.10635414719581604\n",
      "Epoch 18, Batch 70 Loss:0.08518056571483612\n",
      "Epoch 18, Batch 71 Loss:0.08897227048873901\n",
      "Epoch 18, Batch 72 Loss:0.13958173990249634\n",
      "Epoch 18, Batch 73 Loss:0.11680635809898376\n",
      "Epoch 18, Batch 74 Loss:0.09645124524831772\n",
      "Epoch 18, Batch 75 Loss:0.09791705012321472\n",
      "Epoch 18, Batch 76 Loss:0.12325728684663773\n",
      "Epoch 18, Batch 77 Loss:0.09726893156766891\n",
      "Epoch 18, Batch 78 Loss:0.080956369638443\n",
      "Epoch 18, Batch 79 Loss:0.08850797265768051\n",
      "Epoch 18, Batch 80 Loss:0.0730348601937294\n",
      "Epoch 18, Batch 81 Loss:0.09458614140748978\n",
      "Epoch 18, Batch 82 Loss:0.09261809289455414\n",
      "Epoch 18, Batch 83 Loss:0.11732330918312073\n",
      "Epoch 18, Batch 84 Loss:0.09679945558309555\n",
      "Epoch 18, Batch 85 Loss:0.10263574123382568\n",
      "Epoch 18, Batch 86 Loss:0.12308362126350403\n",
      "Epoch 18, Batch 87 Loss:0.10072806477546692\n",
      "Epoch 18, Batch 88 Loss:0.043007321655750275\n",
      "Epoch 18, Batch 89 Loss:0.07386915385723114\n",
      "Epoch 18, Batch 90 Loss:0.07335463166236877\n",
      "Epoch 18, Batch 91 Loss:0.08414413034915924\n",
      "Epoch 18, Batch 92 Loss:0.06390099972486496\n",
      "Epoch 18, Batch 93 Loss:0.0818740725517273\n",
      "Epoch 18, Batch 94 Loss:0.09638766199350357\n",
      "Epoch 18, Batch 95 Loss:0.08789055794477463\n",
      "Epoch 18, Batch 96 Loss:0.1855105608701706\n",
      "Epoch 18, Batch 97 Loss:0.10425133258104324\n",
      "Epoch 18, Batch 98 Loss:0.09744582325220108\n",
      "Epoch 18, Batch 99 Loss:0.10907664895057678\n",
      "Epoch 18, Batch 100 Loss:0.09688646346330643\n",
      "Epoch 18, Batch 101 Loss:0.13480988144874573\n",
      "Epoch 18, Batch 102 Loss:0.12006191909313202\n",
      "Epoch 18, Batch 103 Loss:0.08499670028686523\n",
      "Epoch 18, Batch 104 Loss:0.06415409594774246\n",
      "Epoch 18, Batch 105 Loss:0.08758005499839783\n",
      "Epoch 18, Batch 106 Loss:0.09354911744594574\n",
      "Epoch 18, Batch 107 Loss:0.12650789320468903\n",
      "Epoch 18, Batch 108 Loss:0.1161404699087143\n",
      "Epoch 18, Batch 109 Loss:0.0669739693403244\n",
      "Epoch 18, Batch 110 Loss:0.07495005428791046\n",
      "Epoch 18, Batch 111 Loss:0.10895206034183502\n",
      "Epoch 18, Batch 112 Loss:0.1730656623840332\n",
      "Epoch 18, Batch 113 Loss:0.11527824401855469\n",
      "Epoch 18, Batch 114 Loss:0.08688976615667343\n",
      "Epoch 18, Batch 115 Loss:0.09859803318977356\n",
      "Epoch 18, Batch 116 Loss:0.12410137057304382\n",
      "Epoch 18, Batch 117 Loss:0.18446259200572968\n",
      "Epoch 18, Batch 118 Loss:0.09581814706325531\n",
      "Epoch 18, Batch 119 Loss:0.12794554233551025\n",
      "Epoch 18, Batch 120 Loss:0.13808691501617432\n",
      "Epoch 18, Batch 121 Loss:0.12839612364768982\n",
      "Epoch 18, Batch 122 Loss:0.14423340559005737\n",
      "Epoch 18, Batch 123 Loss:0.14509516954421997\n",
      "Epoch 18, Batch 124 Loss:0.15939313173294067\n",
      "Epoch 18, Batch 125 Loss:0.21114856004714966\n",
      "Epoch 18, Batch 126 Loss:0.19973254203796387\n",
      "Epoch 18, Batch 127 Loss:0.12951454520225525\n",
      "Epoch 18, Batch 128 Loss:0.13543224334716797\n",
      "Epoch 18, Batch 129 Loss:0.11343380808830261\n",
      "Epoch 18, Batch 130 Loss:0.12710043787956238\n",
      "Epoch 18, Batch 131 Loss:0.15421530604362488\n",
      "Epoch 18, Batch 132 Loss:0.15436023473739624\n",
      "Epoch 18, Batch 133 Loss:0.1198900118470192\n",
      "Epoch 18, Batch 134 Loss:0.12013203650712967\n",
      "Epoch 18, Batch 135 Loss:0.13871490955352783\n",
      "Epoch 18, Batch 136 Loss:0.12676365673542023\n",
      "Epoch 18, Batch 137 Loss:0.11287488043308258\n",
      "Epoch 18, Batch 138 Loss:0.14652667939662933\n",
      "Epoch 18, Batch 139 Loss:0.11744733899831772\n",
      "Epoch 18, Batch 140 Loss:0.10901845991611481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Batch 141 Loss:0.10343678295612335\n",
      "Epoch 18, Batch 142 Loss:0.07651218771934509\n",
      "Epoch 18, Batch 143 Loss:0.0989944264292717\n",
      "Epoch 18, Batch 144 Loss:0.1769339144229889\n",
      "Epoch 18, Batch 145 Loss:0.11262340843677521\n",
      "Epoch 18, Batch 146 Loss:0.1413855254650116\n",
      "Epoch 18, Batch 147 Loss:0.08888310194015503\n",
      "Epoch 18, Batch 148 Loss:0.11622318625450134\n",
      "Epoch 18, Batch 149 Loss:0.12158358097076416\n",
      "Epoch 18, Batch 150 Loss:0.09024535119533539\n",
      "Epoch 18, Batch 151 Loss:0.10820215940475464\n",
      "Epoch 18, Batch 152 Loss:0.14787480235099792\n",
      "Epoch 18, Batch 153 Loss:0.09465190768241882\n",
      "Epoch 18, Batch 154 Loss:0.09104476869106293\n",
      "Epoch 18, Batch 155 Loss:0.10137192159891129\n",
      "Epoch 18, Batch 156 Loss:0.09760744869709015\n",
      "Epoch 18, Batch 157 Loss:0.08129198849201202\n",
      "Epoch 18, Batch 158 Loss:0.14562274515628815\n",
      "Epoch 18, Batch 159 Loss:0.15268346667289734\n",
      "Epoch 18, Batch 160 Loss:0.14167116582393646\n",
      "Epoch 18, Batch 161 Loss:0.07242903113365173\n",
      "Epoch 18, Batch 162 Loss:0.18303710222244263\n",
      "Epoch 18, Batch 163 Loss:0.04422248527407646\n",
      "Epoch 18, Batch 164 Loss:0.13573536276817322\n",
      "Epoch 18, Batch 165 Loss:0.13674914836883545\n",
      "Epoch 18, Batch 166 Loss:0.08113088458776474\n",
      "Epoch 18, Batch 167 Loss:0.11465366929769516\n",
      "Epoch 18, Batch 168 Loss:0.12464573979377747\n",
      "Epoch 18, Batch 169 Loss:0.10692091286182404\n",
      "Epoch 18, Batch 170 Loss:0.13002511858940125\n",
      "Epoch 18, Batch 171 Loss:0.10861801356077194\n",
      "Epoch 18, Batch 172 Loss:0.08658576011657715\n",
      "Epoch 18, Batch 173 Loss:0.07266825437545776\n",
      "Epoch 18, Batch 174 Loss:0.09031938761472702\n",
      "Epoch 18, Batch 175 Loss:0.052497148513793945\n",
      "Epoch 18, Batch 176 Loss:0.1484661102294922\n",
      "Epoch 18, Batch 177 Loss:0.08701757341623306\n",
      "Epoch 18, Batch 178 Loss:0.13285502791404724\n",
      "Epoch 18, Batch 179 Loss:0.11150090396404266\n",
      "Epoch 18, Batch 180 Loss:0.17412543296813965\n",
      "Epoch 18, Batch 181 Loss:0.08062854409217834\n",
      "Epoch 18, Batch 182 Loss:0.10900860279798508\n",
      "Epoch 18, Batch 183 Loss:0.14045009016990662\n",
      "Epoch 18, Batch 184 Loss:0.17200110852718353\n",
      "Epoch 18, Batch 185 Loss:0.14153894782066345\n",
      "Epoch 18, Batch 186 Loss:0.10866008698940277\n",
      "Epoch 18, Batch 187 Loss:0.11951027065515518\n",
      "Epoch 18, Batch 188 Loss:0.10725147277116776\n",
      "Epoch 18, Batch 189 Loss:0.09724465012550354\n",
      "Epoch 18, Batch 190 Loss:0.14694276452064514\n",
      "Epoch 18, Batch 191 Loss:0.13044720888137817\n",
      "Epoch 18, Batch 192 Loss:0.09895053505897522\n",
      "Epoch 18, Batch 193 Loss:0.10777993500232697\n",
      "Epoch 18, Batch 194 Loss:0.11361797899007797\n",
      "Epoch 18, Batch 195 Loss:0.11762771010398865\n",
      "Epoch 18, Batch 196 Loss:0.08942699432373047\n",
      "Epoch 18, Batch 197 Loss:0.1028401106595993\n",
      "Epoch 18, Batch 198 Loss:0.10665334761142731\n",
      "Epoch 18, Batch 199 Loss:0.10588474571704865\n",
      "Epoch 18, Batch 200 Loss:0.10829514265060425\n",
      "Epoch 18, Batch 201 Loss:0.0651145726442337\n",
      "Epoch 18, Batch 202 Loss:0.09728199988603592\n",
      "Epoch 18, Batch 203 Loss:0.11808325350284576\n",
      "Epoch 18, Batch 204 Loss:0.12746860086917877\n",
      "Epoch 18, Batch 205 Loss:0.11886973679065704\n",
      "Epoch 18, Batch 206 Loss:0.13732004165649414\n",
      "Epoch 18, Batch 207 Loss:0.11264364421367645\n",
      "Epoch 18, Batch 208 Loss:0.13155047595500946\n",
      "Epoch 18, Batch 209 Loss:0.12151497602462769\n",
      "Epoch 18, Batch 210 Loss:0.11700908839702606\n",
      "Epoch 18, Batch 211 Loss:0.12958142161369324\n",
      "Epoch 18, Batch 212 Loss:0.16483156383037567\n",
      "Epoch 18, Batch 213 Loss:0.08522848039865494\n",
      "Epoch 18, Batch 214 Loss:0.09026739001274109\n",
      "Epoch 18, Batch 215 Loss:0.12625208497047424\n",
      "Epoch 18, Batch 216 Loss:0.1565788984298706\n",
      "Epoch 18, Batch 217 Loss:0.0837758332490921\n",
      "Epoch 18, Batch 218 Loss:0.12011299282312393\n",
      "Epoch 18, Batch 219 Loss:0.1363930106163025\n",
      "Epoch 18, Batch 220 Loss:0.10118864476680756\n",
      "Epoch 18, Batch 221 Loss:0.10099230706691742\n",
      "Epoch 18, Batch 222 Loss:0.13365165889263153\n",
      "Epoch 18, Batch 223 Loss:0.12504492700099945\n",
      "Epoch 18, Batch 224 Loss:0.1537793129682541\n",
      "Epoch 18, Batch 225 Loss:0.08886151760816574\n",
      "Epoch 18, Batch 226 Loss:0.0905592143535614\n",
      "Epoch 18, Batch 227 Loss:0.0847979485988617\n",
      "Epoch 18, Batch 228 Loss:0.11403810232877731\n",
      "Epoch 18, Batch 229 Loss:0.1290530562400818\n",
      "Epoch 18, Batch 230 Loss:0.10524523258209229\n",
      "Epoch 18, Batch 231 Loss:0.11470234394073486\n",
      "Epoch 18, Batch 232 Loss:0.08566752076148987\n",
      "Epoch 18, Batch 233 Loss:0.07416269183158875\n",
      "Loss in this Epoch is: 7.41626918316 %\n",
      "Accuracy in this Epoch is: 89.200001955 %\n",
      "Epoch 19, Batch 0 Loss:0.11525005102157593\n",
      "Epoch 19, Batch 1 Loss:0.10707562416791916\n",
      "Epoch 19, Batch 2 Loss:0.11729778349399567\n",
      "Epoch 19, Batch 3 Loss:0.07011468708515167\n",
      "Epoch 19, Batch 4 Loss:0.06729979813098907\n",
      "Epoch 19, Batch 5 Loss:0.058535750955343246\n",
      "Epoch 19, Batch 6 Loss:0.13004836440086365\n",
      "Epoch 19, Batch 7 Loss:0.07578563690185547\n",
      "Epoch 19, Batch 8 Loss:0.1228458434343338\n",
      "Epoch 19, Batch 9 Loss:0.12922462821006775\n",
      "Epoch 19, Batch 10 Loss:0.11299850046634674\n",
      "Epoch 19, Batch 11 Loss:0.07901645451784134\n",
      "Epoch 19, Batch 12 Loss:0.08793961256742477\n",
      "Epoch 19, Batch 13 Loss:0.10241545736789703\n",
      "Epoch 19, Batch 14 Loss:0.10985144972801208\n",
      "Epoch 19, Batch 15 Loss:0.07299429178237915\n",
      "Epoch 19, Batch 16 Loss:0.08597508817911148\n",
      "Epoch 19, Batch 17 Loss:0.09211686253547668\n",
      "Epoch 19, Batch 18 Loss:0.11239746958017349\n",
      "Epoch 19, Batch 19 Loss:0.09750939160585403\n",
      "Epoch 19, Batch 20 Loss:0.06016427278518677\n",
      "Epoch 19, Batch 21 Loss:0.07881652563810349\n",
      "Epoch 19, Batch 22 Loss:0.13457699120044708\n",
      "Epoch 19, Batch 23 Loss:0.06417491286993027\n",
      "Epoch 19, Batch 24 Loss:0.054786037653684616\n",
      "Epoch 19, Batch 25 Loss:0.08537351340055466\n",
      "Epoch 19, Batch 26 Loss:0.04159463942050934\n",
      "Epoch 19, Batch 27 Loss:0.08479757606983185\n",
      "Epoch 19, Batch 28 Loss:0.0560348816215992\n",
      "Epoch 19, Batch 29 Loss:0.10453005880117416\n",
      "Epoch 19, Batch 30 Loss:0.08177562057971954\n",
      "Epoch 19, Batch 31 Loss:0.0890059694647789\n",
      "Epoch 19, Batch 32 Loss:0.08133779466152191\n",
      "Epoch 19, Batch 33 Loss:0.11467980593442917\n",
      "Epoch 19, Batch 34 Loss:0.12236045300960541\n",
      "Epoch 19, Batch 35 Loss:0.08215178549289703\n",
      "Epoch 19, Batch 36 Loss:0.11598152667284012\n",
      "Epoch 19, Batch 37 Loss:0.12273241579532623\n",
      "Epoch 19, Batch 38 Loss:0.10235624015331268\n",
      "Epoch 19, Batch 39 Loss:0.06468314677476883\n",
      "Epoch 19, Batch 40 Loss:0.08700896054506302\n",
      "Epoch 19, Batch 41 Loss:0.07485079765319824\n",
      "Epoch 19, Batch 42 Loss:0.08385072648525238\n",
      "Epoch 19, Batch 43 Loss:0.09752168506383896\n",
      "Epoch 19, Batch 44 Loss:0.1055685356259346\n",
      "Epoch 19, Batch 45 Loss:0.0659722164273262\n",
      "Epoch 19, Batch 46 Loss:0.08891016989946365\n",
      "Epoch 19, Batch 47 Loss:0.06054135411977768\n",
      "Epoch 19, Batch 48 Loss:0.07341066002845764\n",
      "Epoch 19, Batch 49 Loss:0.09882333874702454\n",
      "Epoch 19, Batch 50 Loss:0.12011942267417908\n",
      "Epoch 19, Batch 51 Loss:0.09825371950864792\n",
      "Epoch 19, Batch 52 Loss:0.05271479859948158\n",
      "Epoch 19, Batch 53 Loss:0.14455179870128632\n",
      "Epoch 19, Batch 54 Loss:0.07139753550291061\n",
      "Epoch 19, Batch 55 Loss:0.0780886560678482\n",
      "Epoch 19, Batch 56 Loss:0.10984884947538376\n",
      "Epoch 19, Batch 57 Loss:0.0866057351231575\n",
      "Epoch 19, Batch 58 Loss:0.06193903461098671\n",
      "Epoch 19, Batch 59 Loss:0.05272616818547249\n",
      "Epoch 19, Batch 60 Loss:0.08514288067817688\n",
      "Epoch 19, Batch 61 Loss:0.0865851640701294\n",
      "Epoch 19, Batch 62 Loss:0.12424525618553162\n",
      "Epoch 19, Batch 63 Loss:0.03959030285477638\n",
      "Epoch 19, Batch 64 Loss:0.07825589179992676\n",
      "Epoch 19, Batch 65 Loss:0.07439195364713669\n",
      "Epoch 19, Batch 66 Loss:0.09032262116670609\n",
      "Epoch 19, Batch 67 Loss:0.0986209288239479\n",
      "Epoch 19, Batch 68 Loss:0.036008499562740326\n",
      "Epoch 19, Batch 69 Loss:0.04918789118528366\n",
      "Epoch 19, Batch 70 Loss:0.10784079134464264\n",
      "Epoch 19, Batch 71 Loss:0.0769619345664978\n",
      "Epoch 19, Batch 72 Loss:0.12105099111795425\n",
      "Epoch 19, Batch 73 Loss:0.11525656282901764\n",
      "Epoch 19, Batch 74 Loss:0.0817643329501152\n",
      "Epoch 19, Batch 75 Loss:0.05422360822558403\n",
      "Epoch 19, Batch 76 Loss:0.07232461124658585\n",
      "Epoch 19, Batch 77 Loss:0.07454738020896912\n",
      "Epoch 19, Batch 78 Loss:0.12260442227125168\n",
      "Epoch 19, Batch 79 Loss:0.09761099517345428\n",
      "Epoch 19, Batch 80 Loss:0.11611008644104004\n",
      "Epoch 19, Batch 81 Loss:0.07838665693998337\n",
      "Epoch 19, Batch 82 Loss:0.09578637033700943\n",
      "Epoch 19, Batch 83 Loss:0.07377398759126663\n",
      "Epoch 19, Batch 84 Loss:0.06659291684627533\n",
      "Epoch 19, Batch 85 Loss:0.060927558690309525\n",
      "Epoch 19, Batch 86 Loss:0.05011608451604843\n",
      "Epoch 19, Batch 87 Loss:0.08218738436698914\n",
      "Epoch 19, Batch 88 Loss:0.10415396094322205\n",
      "Epoch 19, Batch 89 Loss:0.09194409102201462\n",
      "Epoch 19, Batch 90 Loss:0.08110196143388748\n",
      "Epoch 19, Batch 91 Loss:0.07232262194156647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Batch 92 Loss:0.07127229869365692\n",
      "Epoch 19, Batch 93 Loss:0.13026584684848785\n",
      "Epoch 19, Batch 94 Loss:0.07306399196386337\n",
      "Epoch 19, Batch 95 Loss:0.060737770050764084\n",
      "Epoch 19, Batch 96 Loss:0.12165363132953644\n",
      "Epoch 19, Batch 97 Loss:0.0629550963640213\n",
      "Epoch 19, Batch 98 Loss:0.11325374245643616\n",
      "Epoch 19, Batch 99 Loss:0.08791205286979675\n",
      "Epoch 19, Batch 100 Loss:0.07088504731655121\n",
      "Epoch 19, Batch 101 Loss:0.09021574258804321\n",
      "Epoch 19, Batch 102 Loss:0.05069064721465111\n",
      "Epoch 19, Batch 103 Loss:0.17267116904258728\n",
      "Epoch 19, Batch 104 Loss:0.09138955175876617\n",
      "Epoch 19, Batch 105 Loss:0.08061918616294861\n",
      "Epoch 19, Batch 106 Loss:0.1287747621536255\n",
      "Epoch 19, Batch 107 Loss:0.10748876631259918\n",
      "Epoch 19, Batch 108 Loss:0.06775802373886108\n",
      "Epoch 19, Batch 109 Loss:0.08927331864833832\n",
      "Epoch 19, Batch 110 Loss:0.0626431256532669\n",
      "Epoch 19, Batch 111 Loss:0.07048724591732025\n",
      "Epoch 19, Batch 112 Loss:0.10360553115606308\n",
      "Epoch 19, Batch 113 Loss:0.0852445587515831\n",
      "Epoch 19, Batch 114 Loss:0.08971058577299118\n",
      "Epoch 19, Batch 115 Loss:0.12257258594036102\n",
      "Epoch 19, Batch 116 Loss:0.08410899341106415\n",
      "Epoch 19, Batch 117 Loss:0.06574416905641556\n",
      "Epoch 19, Batch 118 Loss:0.12544657289981842\n",
      "Epoch 19, Batch 119 Loss:0.14605320990085602\n",
      "Epoch 19, Batch 120 Loss:0.0922316163778305\n",
      "Epoch 19, Batch 121 Loss:0.09595045447349548\n",
      "Epoch 19, Batch 122 Loss:0.16034308075904846\n",
      "Epoch 19, Batch 123 Loss:0.08634752780199051\n",
      "Epoch 19, Batch 124 Loss:0.10893964022397995\n",
      "Epoch 19, Batch 125 Loss:0.08345332741737366\n",
      "Epoch 19, Batch 126 Loss:0.08674792945384979\n",
      "Epoch 19, Batch 127 Loss:0.1813940405845642\n",
      "Epoch 19, Batch 128 Loss:0.14071832597255707\n",
      "Epoch 19, Batch 129 Loss:0.1101071685552597\n",
      "Epoch 19, Batch 130 Loss:0.17533138394355774\n",
      "Epoch 19, Batch 131 Loss:0.1200229674577713\n",
      "Epoch 19, Batch 132 Loss:0.11680962145328522\n",
      "Epoch 19, Batch 133 Loss:0.12901854515075684\n",
      "Epoch 19, Batch 134 Loss:0.18236881494522095\n",
      "Epoch 19, Batch 135 Loss:0.14158809185028076\n",
      "Epoch 19, Batch 136 Loss:0.12941382825374603\n",
      "Epoch 19, Batch 137 Loss:0.11930263042449951\n",
      "Epoch 19, Batch 138 Loss:0.14106829464435577\n",
      "Epoch 19, Batch 139 Loss:0.0874645859003067\n",
      "Epoch 19, Batch 140 Loss:0.11291282624006271\n",
      "Epoch 19, Batch 141 Loss:0.11935843527317047\n",
      "Epoch 19, Batch 142 Loss:0.1349242925643921\n",
      "Epoch 19, Batch 143 Loss:0.1531001776456833\n",
      "Epoch 19, Batch 144 Loss:0.11041111499071121\n",
      "Epoch 19, Batch 145 Loss:0.10520143806934357\n",
      "Epoch 19, Batch 146 Loss:0.11426134407520294\n",
      "Epoch 19, Batch 147 Loss:0.09702687710523605\n",
      "Epoch 19, Batch 148 Loss:0.06761429458856583\n",
      "Epoch 19, Batch 149 Loss:0.0826537162065506\n",
      "Epoch 19, Batch 150 Loss:0.12509599328041077\n",
      "Epoch 19, Batch 151 Loss:0.11633015424013138\n",
      "Epoch 19, Batch 152 Loss:0.19009414315223694\n",
      "Epoch 19, Batch 153 Loss:0.07807134836912155\n",
      "Epoch 19, Batch 154 Loss:0.07500047981739044\n",
      "Epoch 19, Batch 155 Loss:0.11009064316749573\n",
      "Epoch 19, Batch 156 Loss:0.13217929005622864\n",
      "Epoch 19, Batch 157 Loss:0.15134258568286896\n",
      "Epoch 19, Batch 158 Loss:0.08649954944849014\n",
      "Epoch 19, Batch 159 Loss:0.08722890168428421\n",
      "Epoch 19, Batch 160 Loss:0.0883512794971466\n",
      "Epoch 19, Batch 161 Loss:0.16883307695388794\n",
      "Epoch 19, Batch 162 Loss:0.1348625123500824\n",
      "Epoch 19, Batch 163 Loss:0.14192470908164978\n",
      "Epoch 19, Batch 164 Loss:0.12336420267820358\n",
      "Epoch 19, Batch 165 Loss:0.12025900185108185\n",
      "Epoch 19, Batch 166 Loss:0.13784946501255035\n",
      "Epoch 19, Batch 167 Loss:0.12161565572023392\n",
      "Epoch 19, Batch 168 Loss:0.0750957652926445\n",
      "Epoch 19, Batch 169 Loss:0.1059378981590271\n",
      "Epoch 19, Batch 170 Loss:0.1061193197965622\n",
      "Epoch 19, Batch 171 Loss:0.12460164725780487\n",
      "Epoch 19, Batch 172 Loss:0.1260688155889511\n",
      "Epoch 19, Batch 173 Loss:0.09348072111606598\n",
      "Epoch 19, Batch 174 Loss:0.1061224490404129\n",
      "Epoch 19, Batch 175 Loss:0.08970332145690918\n",
      "Epoch 19, Batch 176 Loss:0.08283495903015137\n",
      "Epoch 19, Batch 177 Loss:0.13620513677597046\n",
      "Epoch 19, Batch 178 Loss:0.0956077054142952\n",
      "Epoch 19, Batch 179 Loss:0.13446983695030212\n",
      "Epoch 19, Batch 180 Loss:0.10953265428543091\n",
      "Epoch 19, Batch 181 Loss:0.13205815851688385\n",
      "Epoch 19, Batch 182 Loss:0.09297200292348862\n",
      "Epoch 19, Batch 183 Loss:0.12780439853668213\n",
      "Epoch 19, Batch 184 Loss:0.11345509439706802\n",
      "Epoch 19, Batch 185 Loss:0.09490838646888733\n",
      "Epoch 19, Batch 186 Loss:0.17565320432186127\n",
      "Epoch 19, Batch 187 Loss:0.16940027475357056\n",
      "Epoch 19, Batch 188 Loss:0.13331235945224762\n",
      "Epoch 19, Batch 189 Loss:0.10638220608234406\n",
      "Epoch 19, Batch 190 Loss:0.07327079772949219\n",
      "Epoch 19, Batch 191 Loss:0.1435091495513916\n",
      "Epoch 19, Batch 192 Loss:0.10146185755729675\n",
      "Epoch 19, Batch 193 Loss:0.08037833869457245\n",
      "Epoch 19, Batch 194 Loss:0.13664433360099792\n",
      "Epoch 19, Batch 195 Loss:0.11673429608345032\n",
      "Epoch 19, Batch 196 Loss:0.08954127132892609\n",
      "Epoch 19, Batch 197 Loss:0.13180002570152283\n",
      "Epoch 19, Batch 198 Loss:0.08866208791732788\n",
      "Epoch 19, Batch 199 Loss:0.11667674034833908\n",
      "Epoch 19, Batch 200 Loss:0.11215001344680786\n",
      "Epoch 19, Batch 201 Loss:0.08933047950267792\n",
      "Epoch 19, Batch 202 Loss:0.09576895833015442\n",
      "Epoch 19, Batch 203 Loss:0.1673823893070221\n",
      "Epoch 19, Batch 204 Loss:0.11263960599899292\n",
      "Epoch 19, Batch 205 Loss:0.11110271513462067\n",
      "Epoch 19, Batch 206 Loss:0.09991568326950073\n",
      "Epoch 19, Batch 207 Loss:0.11109738796949387\n",
      "Epoch 19, Batch 208 Loss:0.09734337776899338\n",
      "Epoch 19, Batch 209 Loss:0.0828922688961029\n",
      "Epoch 19, Batch 210 Loss:0.08925675600767136\n",
      "Epoch 19, Batch 211 Loss:0.09432338178157806\n",
      "Epoch 19, Batch 212 Loss:0.09787510335445404\n",
      "Epoch 19, Batch 213 Loss:0.11853170394897461\n",
      "Epoch 19, Batch 214 Loss:0.10285062342882156\n",
      "Epoch 19, Batch 215 Loss:0.12953516840934753\n",
      "Epoch 19, Batch 216 Loss:0.111772820353508\n",
      "Epoch 19, Batch 217 Loss:0.08687064051628113\n",
      "Epoch 19, Batch 218 Loss:0.11823850870132446\n",
      "Epoch 19, Batch 219 Loss:0.09207235276699066\n",
      "Epoch 19, Batch 220 Loss:0.10298105329275131\n",
      "Epoch 19, Batch 221 Loss:0.1155070811510086\n",
      "Epoch 19, Batch 222 Loss:0.13437440991401672\n",
      "Epoch 19, Batch 223 Loss:0.10250163078308105\n",
      "Epoch 19, Batch 224 Loss:0.11540853977203369\n",
      "Epoch 19, Batch 225 Loss:0.10802370309829712\n",
      "Epoch 19, Batch 226 Loss:0.09803221374750137\n",
      "Epoch 19, Batch 227 Loss:0.13947060704231262\n",
      "Epoch 19, Batch 228 Loss:0.08896838128566742\n",
      "Epoch 19, Batch 229 Loss:0.0990205630660057\n",
      "Epoch 19, Batch 230 Loss:0.13073453307151794\n",
      "Epoch 19, Batch 231 Loss:0.15658792853355408\n",
      "Epoch 19, Batch 232 Loss:0.10356759279966354\n",
      "Epoch 19, Batch 233 Loss:0.10033848881721497\n",
      "Loss in this Epoch is: 10.0338488817 %\n",
      "Accuracy in this Epoch is: 89.0600025654 %\n",
      "Epoch 20, Batch 0 Loss:0.07467995584011078\n",
      "Epoch 20, Batch 1 Loss:0.05773879960179329\n",
      "Epoch 20, Batch 2 Loss:0.0757909044623375\n",
      "Epoch 20, Batch 3 Loss:0.08969087153673172\n",
      "Epoch 20, Batch 4 Loss:0.12647773325443268\n",
      "Epoch 20, Batch 5 Loss:0.07744259387254715\n",
      "Epoch 20, Batch 6 Loss:0.12074435502290726\n",
      "Epoch 20, Batch 7 Loss:0.10626079142093658\n",
      "Epoch 20, Batch 8 Loss:0.0698096752166748\n",
      "Epoch 20, Batch 9 Loss:0.0625116229057312\n",
      "Epoch 20, Batch 10 Loss:0.13344702124595642\n",
      "Epoch 20, Batch 11 Loss:0.05307827889919281\n",
      "Epoch 20, Batch 12 Loss:0.0752870962023735\n",
      "Epoch 20, Batch 13 Loss:0.08473246544599533\n",
      "Epoch 20, Batch 14 Loss:0.07682237774133682\n",
      "Epoch 20, Batch 15 Loss:0.07025011628866196\n",
      "Epoch 20, Batch 16 Loss:0.1444433331489563\n",
      "Epoch 20, Batch 17 Loss:0.097592294216156\n",
      "Epoch 20, Batch 18 Loss:0.1409243494272232\n",
      "Epoch 20, Batch 19 Loss:0.07953539490699768\n",
      "Epoch 20, Batch 20 Loss:0.07320724427700043\n",
      "Epoch 20, Batch 21 Loss:0.16638387739658356\n",
      "Epoch 20, Batch 22 Loss:0.11528292298316956\n",
      "Epoch 20, Batch 23 Loss:0.07987788319587708\n",
      "Epoch 20, Batch 24 Loss:0.09886907041072845\n",
      "Epoch 20, Batch 25 Loss:0.09110183268785477\n",
      "Epoch 20, Batch 26 Loss:0.10280090570449829\n",
      "Epoch 20, Batch 27 Loss:0.1043432280421257\n",
      "Epoch 20, Batch 28 Loss:0.1293773204088211\n",
      "Epoch 20, Batch 29 Loss:0.06615859270095825\n",
      "Epoch 20, Batch 30 Loss:0.09076473116874695\n",
      "Epoch 20, Batch 31 Loss:0.11580438911914825\n",
      "Epoch 20, Batch 32 Loss:0.1019536554813385\n",
      "Epoch 20, Batch 33 Loss:0.061935167759656906\n",
      "Epoch 20, Batch 34 Loss:0.10614988207817078\n",
      "Epoch 20, Batch 35 Loss:0.10831081122159958\n",
      "Epoch 20, Batch 36 Loss:0.09794264286756516\n",
      "Epoch 20, Batch 37 Loss:0.11055828630924225\n",
      "Epoch 20, Batch 38 Loss:0.06636444479227066\n",
      "Epoch 20, Batch 39 Loss:0.0851157084107399\n",
      "Epoch 20, Batch 40 Loss:0.08853406459093094\n",
      "Epoch 20, Batch 41 Loss:0.052989497780799866\n",
      "Epoch 20, Batch 42 Loss:0.09953784942626953\n",
      "Epoch 20, Batch 43 Loss:0.13406026363372803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 44 Loss:0.04981471970677376\n",
      "Epoch 20, Batch 45 Loss:0.09337997436523438\n",
      "Epoch 20, Batch 46 Loss:0.04790322110056877\n",
      "Epoch 20, Batch 47 Loss:0.07880514860153198\n",
      "Epoch 20, Batch 48 Loss:0.0755174458026886\n",
      "Epoch 20, Batch 49 Loss:0.04474658519029617\n",
      "Epoch 20, Batch 50 Loss:0.10991094261407852\n",
      "Epoch 20, Batch 51 Loss:0.07783287018537521\n",
      "Epoch 20, Batch 52 Loss:0.08782743662595749\n",
      "Epoch 20, Batch 53 Loss:0.07868164777755737\n",
      "Epoch 20, Batch 54 Loss:0.0594296008348465\n",
      "Epoch 20, Batch 55 Loss:0.10141760110855103\n",
      "Epoch 20, Batch 56 Loss:0.03728179261088371\n",
      "Epoch 20, Batch 57 Loss:0.07465758919715881\n",
      "Epoch 20, Batch 58 Loss:0.10922106355428696\n",
      "Epoch 20, Batch 59 Loss:0.07462053000926971\n",
      "Epoch 20, Batch 60 Loss:0.11303501576185226\n",
      "Epoch 20, Batch 61 Loss:0.08360648155212402\n",
      "Epoch 20, Batch 62 Loss:0.10936783254146576\n",
      "Epoch 20, Batch 63 Loss:0.07791933417320251\n",
      "Epoch 20, Batch 64 Loss:0.10770722478628159\n",
      "Epoch 20, Batch 65 Loss:0.054923076182603836\n",
      "Epoch 20, Batch 66 Loss:0.07206828147172928\n",
      "Epoch 20, Batch 67 Loss:0.08381447196006775\n",
      "Epoch 20, Batch 68 Loss:0.06978927552700043\n",
      "Epoch 20, Batch 69 Loss:0.05141844600439072\n",
      "Epoch 20, Batch 70 Loss:0.08868492394685745\n",
      "Epoch 20, Batch 71 Loss:0.09963688254356384\n",
      "Epoch 20, Batch 72 Loss:0.06570669263601303\n",
      "Epoch 20, Batch 73 Loss:0.08336931467056274\n",
      "Epoch 20, Batch 74 Loss:0.11625785380601883\n",
      "Epoch 20, Batch 75 Loss:0.06271741539239883\n",
      "Epoch 20, Batch 76 Loss:0.09628608822822571\n",
      "Epoch 20, Batch 77 Loss:0.08614426851272583\n",
      "Epoch 20, Batch 78 Loss:0.08487791568040848\n",
      "Epoch 20, Batch 79 Loss:0.12261879444122314\n",
      "Epoch 20, Batch 80 Loss:0.08893540501594543\n",
      "Epoch 20, Batch 81 Loss:0.14070731401443481\n",
      "Epoch 20, Batch 82 Loss:0.10096718370914459\n",
      "Epoch 20, Batch 83 Loss:0.06596333533525467\n",
      "Epoch 20, Batch 84 Loss:0.093600332736969\n",
      "Epoch 20, Batch 85 Loss:0.08467361330986023\n",
      "Epoch 20, Batch 86 Loss:0.07430107891559601\n",
      "Epoch 20, Batch 87 Loss:0.08451100438833237\n",
      "Epoch 20, Batch 88 Loss:0.07376660406589508\n",
      "Epoch 20, Batch 89 Loss:0.04642157629132271\n",
      "Epoch 20, Batch 90 Loss:0.09921800345182419\n",
      "Epoch 20, Batch 91 Loss:0.09748759120702744\n",
      "Epoch 20, Batch 92 Loss:0.08107563108205795\n",
      "Epoch 20, Batch 93 Loss:0.09093982726335526\n",
      "Epoch 20, Batch 94 Loss:0.11520981043577194\n",
      "Epoch 20, Batch 95 Loss:0.1436346024274826\n",
      "Epoch 20, Batch 96 Loss:0.06816442310810089\n",
      "Epoch 20, Batch 97 Loss:0.14004382491111755\n",
      "Epoch 20, Batch 98 Loss:0.12422396242618561\n",
      "Epoch 20, Batch 99 Loss:0.11945150792598724\n",
      "Epoch 20, Batch 100 Loss:0.11019347608089447\n",
      "Epoch 20, Batch 101 Loss:0.09042363613843918\n",
      "Epoch 20, Batch 102 Loss:0.09609626233577728\n",
      "Epoch 20, Batch 103 Loss:0.08719979226589203\n",
      "Epoch 20, Batch 104 Loss:0.09630590677261353\n",
      "Epoch 20, Batch 105 Loss:0.09455543756484985\n",
      "Epoch 20, Batch 106 Loss:0.10390734672546387\n",
      "Epoch 20, Batch 107 Loss:0.07572609186172485\n",
      "Epoch 20, Batch 108 Loss:0.05766616389155388\n",
      "Epoch 20, Batch 109 Loss:0.0965166687965393\n",
      "Epoch 20, Batch 110 Loss:0.07885533571243286\n",
      "Epoch 20, Batch 111 Loss:0.11837708950042725\n",
      "Epoch 20, Batch 112 Loss:0.07587771117687225\n",
      "Epoch 20, Batch 113 Loss:0.12829740345478058\n",
      "Epoch 20, Batch 114 Loss:0.06937891989946365\n",
      "Epoch 20, Batch 115 Loss:0.11112436652183533\n",
      "Epoch 20, Batch 116 Loss:0.06117531657218933\n",
      "Epoch 20, Batch 117 Loss:0.12494523078203201\n",
      "Epoch 20, Batch 118 Loss:0.08890165388584137\n",
      "Epoch 20, Batch 119 Loss:0.08600490540266037\n",
      "Epoch 20, Batch 120 Loss:0.05991624295711517\n",
      "Epoch 20, Batch 121 Loss:0.1106676310300827\n",
      "Epoch 20, Batch 122 Loss:0.10452905297279358\n",
      "Epoch 20, Batch 123 Loss:0.15284386277198792\n",
      "Epoch 20, Batch 124 Loss:0.07873015105724335\n",
      "Epoch 20, Batch 125 Loss:0.1266900897026062\n",
      "Epoch 20, Batch 126 Loss:0.14495351910591125\n",
      "Epoch 20, Batch 127 Loss:0.10143759846687317\n",
      "Epoch 20, Batch 128 Loss:0.14638279378414154\n",
      "Epoch 20, Batch 129 Loss:0.1104208454489708\n",
      "Epoch 20, Batch 130 Loss:0.09071706235408783\n",
      "Epoch 20, Batch 131 Loss:0.09801506996154785\n",
      "Epoch 20, Batch 132 Loss:0.09966150671243668\n",
      "Epoch 20, Batch 133 Loss:0.11048313230276108\n",
      "Epoch 20, Batch 134 Loss:0.11138436198234558\n",
      "Epoch 20, Batch 135 Loss:0.08049227297306061\n",
      "Epoch 20, Batch 136 Loss:0.13654711842536926\n",
      "Epoch 20, Batch 137 Loss:0.08167001605033875\n",
      "Epoch 20, Batch 138 Loss:0.14808085560798645\n",
      "Epoch 20, Batch 139 Loss:0.11287148296833038\n",
      "Epoch 20, Batch 140 Loss:0.11309155076742172\n",
      "Epoch 20, Batch 141 Loss:0.07630433887243271\n",
      "Epoch 20, Batch 142 Loss:0.11570131033658981\n",
      "Epoch 20, Batch 143 Loss:0.09032773226499557\n",
      "Epoch 20, Batch 144 Loss:0.08833330869674683\n",
      "Epoch 20, Batch 145 Loss:0.0980437844991684\n",
      "Epoch 20, Batch 146 Loss:0.08641202747821808\n",
      "Epoch 20, Batch 147 Loss:0.06337200105190277\n",
      "Epoch 20, Batch 148 Loss:0.07742016017436981\n",
      "Epoch 20, Batch 149 Loss:0.11658500134944916\n",
      "Epoch 20, Batch 150 Loss:0.10046594589948654\n",
      "Epoch 20, Batch 151 Loss:0.09195879846811295\n",
      "Epoch 20, Batch 152 Loss:0.11144845932722092\n",
      "Epoch 20, Batch 153 Loss:0.07387636601924896\n",
      "Epoch 20, Batch 154 Loss:0.10830800235271454\n",
      "Epoch 20, Batch 155 Loss:0.14001870155334473\n",
      "Epoch 20, Batch 156 Loss:0.10801641643047333\n",
      "Epoch 20, Batch 157 Loss:0.08807171881198883\n",
      "Epoch 20, Batch 158 Loss:0.07289135456085205\n",
      "Epoch 20, Batch 159 Loss:0.06928360462188721\n",
      "Epoch 20, Batch 160 Loss:0.0925401896238327\n",
      "Epoch 20, Batch 161 Loss:0.11792558431625366\n",
      "Epoch 20, Batch 162 Loss:0.11654610186815262\n",
      "Epoch 20, Batch 163 Loss:0.1097923293709755\n",
      "Epoch 20, Batch 164 Loss:0.08752816170454025\n",
      "Epoch 20, Batch 165 Loss:0.1978762000799179\n",
      "Epoch 20, Batch 166 Loss:0.08964300900697708\n",
      "Epoch 20, Batch 167 Loss:0.07279713451862335\n",
      "Epoch 20, Batch 168 Loss:0.09181462228298187\n",
      "Epoch 20, Batch 169 Loss:0.11607865244150162\n",
      "Epoch 20, Batch 170 Loss:0.08283215761184692\n",
      "Epoch 20, Batch 171 Loss:0.12914609909057617\n",
      "Epoch 20, Batch 172 Loss:0.10671500116586685\n",
      "Epoch 20, Batch 173 Loss:0.09691973775625229\n",
      "Epoch 20, Batch 174 Loss:0.09430330246686935\n",
      "Epoch 20, Batch 175 Loss:0.07398176938295364\n",
      "Epoch 20, Batch 176 Loss:0.11149672418832779\n",
      "Epoch 20, Batch 177 Loss:0.08049408346414566\n",
      "Epoch 20, Batch 178 Loss:0.10895411670207977\n",
      "Epoch 20, Batch 179 Loss:0.07888462394475937\n",
      "Epoch 20, Batch 180 Loss:0.0755326896905899\n",
      "Epoch 20, Batch 181 Loss:0.060637377202510834\n",
      "Epoch 20, Batch 182 Loss:0.06088037043809891\n",
      "Epoch 20, Batch 183 Loss:0.12830770015716553\n",
      "Epoch 20, Batch 184 Loss:0.12619520723819733\n",
      "Epoch 20, Batch 185 Loss:0.11631462723016739\n",
      "Epoch 20, Batch 186 Loss:0.1127820760011673\n",
      "Epoch 20, Batch 187 Loss:0.10746937990188599\n",
      "Epoch 20, Batch 188 Loss:0.08274117112159729\n",
      "Epoch 20, Batch 189 Loss:0.17496296763420105\n",
      "Epoch 20, Batch 190 Loss:0.1310233175754547\n",
      "Epoch 20, Batch 191 Loss:0.0838281512260437\n",
      "Epoch 20, Batch 192 Loss:0.11071231961250305\n",
      "Epoch 20, Batch 193 Loss:0.09139947593212128\n",
      "Epoch 20, Batch 194 Loss:0.10666652023792267\n",
      "Epoch 20, Batch 195 Loss:0.06916105002164841\n",
      "Epoch 20, Batch 196 Loss:0.12437258660793304\n",
      "Epoch 20, Batch 197 Loss:0.12470690906047821\n",
      "Epoch 20, Batch 198 Loss:0.09212405979633331\n",
      "Epoch 20, Batch 199 Loss:0.11599114537239075\n",
      "Epoch 20, Batch 200 Loss:0.06089073047041893\n",
      "Epoch 20, Batch 201 Loss:0.10219818353652954\n",
      "Epoch 20, Batch 202 Loss:0.08117271959781647\n",
      "Epoch 20, Batch 203 Loss:0.09201915562152863\n",
      "Epoch 20, Batch 204 Loss:0.0968492403626442\n",
      "Epoch 20, Batch 205 Loss:0.07877109199762344\n",
      "Epoch 20, Batch 206 Loss:0.10548047721385956\n",
      "Epoch 20, Batch 207 Loss:0.0982898622751236\n",
      "Epoch 20, Batch 208 Loss:0.0743529349565506\n",
      "Epoch 20, Batch 209 Loss:0.07346418499946594\n",
      "Epoch 20, Batch 210 Loss:0.09941551834344864\n",
      "Epoch 20, Batch 211 Loss:0.09035047143697739\n",
      "Epoch 20, Batch 212 Loss:0.09506067633628845\n",
      "Epoch 20, Batch 213 Loss:0.08799491077661514\n",
      "Epoch 20, Batch 214 Loss:0.13193458318710327\n",
      "Epoch 20, Batch 215 Loss:0.13167302310466766\n",
      "Epoch 20, Batch 216 Loss:0.1341918408870697\n",
      "Epoch 20, Batch 217 Loss:0.07269851118326187\n",
      "Epoch 20, Batch 218 Loss:0.1372263878583908\n",
      "Epoch 20, Batch 219 Loss:0.09467761218547821\n",
      "Epoch 20, Batch 220 Loss:0.0924718976020813\n",
      "Epoch 20, Batch 221 Loss:0.12031125277280807\n",
      "Epoch 20, Batch 222 Loss:0.070408895611763\n",
      "Epoch 20, Batch 223 Loss:0.10480760037899017\n",
      "Epoch 20, Batch 224 Loss:0.11297333240509033\n",
      "Epoch 20, Batch 225 Loss:0.10979544371366501\n",
      "Epoch 20, Batch 226 Loss:0.07673348486423492\n",
      "Epoch 20, Batch 227 Loss:0.05903206393122673\n",
      "Epoch 20, Batch 228 Loss:0.08475908637046814\n",
      "Epoch 20, Batch 229 Loss:0.11943794041872025\n",
      "Epoch 20, Batch 230 Loss:0.06777559965848923\n",
      "Epoch 20, Batch 231 Loss:0.07049451023340225\n",
      "Epoch 20, Batch 232 Loss:0.0820046067237854\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Batch 233 Loss:0.08716141432523727\n",
      "Loss in this Epoch is: 8.71614143252 %\n",
      "Accuracy in this Epoch is: 89.1600012779 %\n",
      "Epoch 21, Batch 0 Loss:0.05949677154421806\n",
      "Epoch 21, Batch 1 Loss:0.07544057071208954\n",
      "Epoch 21, Batch 2 Loss:0.07975732535123825\n",
      "Epoch 21, Batch 3 Loss:0.053402505815029144\n",
      "Epoch 21, Batch 4 Loss:0.09133203327655792\n",
      "Epoch 21, Batch 5 Loss:0.10438020527362823\n",
      "Epoch 21, Batch 6 Loss:0.06798526644706726\n",
      "Epoch 21, Batch 7 Loss:0.12626494467258453\n",
      "Epoch 21, Batch 8 Loss:0.09430301189422607\n",
      "Epoch 21, Batch 9 Loss:0.06637957692146301\n",
      "Epoch 21, Batch 10 Loss:0.07559096813201904\n",
      "Epoch 21, Batch 11 Loss:0.06320125609636307\n",
      "Epoch 21, Batch 12 Loss:0.07663720101118088\n",
      "Epoch 21, Batch 13 Loss:0.0860838070511818\n",
      "Epoch 21, Batch 14 Loss:0.08864176273345947\n",
      "Epoch 21, Batch 15 Loss:0.05699402093887329\n",
      "Epoch 21, Batch 16 Loss:0.06212799623608589\n",
      "Epoch 21, Batch 17 Loss:0.08377446234226227\n",
      "Epoch 21, Batch 18 Loss:0.058978259563446045\n",
      "Epoch 21, Batch 19 Loss:0.07823284715414047\n",
      "Epoch 21, Batch 20 Loss:0.056004926562309265\n",
      "Epoch 21, Batch 21 Loss:0.07423517107963562\n",
      "Epoch 21, Batch 22 Loss:0.09288767725229263\n",
      "Epoch 21, Batch 23 Loss:0.05513794347643852\n",
      "Epoch 21, Batch 24 Loss:0.058309342712163925\n",
      "Epoch 21, Batch 25 Loss:0.04719046503305435\n",
      "Epoch 21, Batch 26 Loss:0.06296589225530624\n",
      "Epoch 21, Batch 27 Loss:0.06215817481279373\n",
      "Epoch 21, Batch 28 Loss:0.0558692030608654\n",
      "Epoch 21, Batch 29 Loss:0.06623474508523941\n",
      "Epoch 21, Batch 30 Loss:0.05425073951482773\n",
      "Epoch 21, Batch 31 Loss:0.050156161189079285\n",
      "Epoch 21, Batch 32 Loss:0.049901243299245834\n",
      "Epoch 21, Batch 33 Loss:0.07936374098062515\n",
      "Epoch 21, Batch 34 Loss:0.095047727227211\n",
      "Epoch 21, Batch 35 Loss:0.08416938781738281\n",
      "Epoch 21, Batch 36 Loss:0.07893551141023636\n",
      "Epoch 21, Batch 37 Loss:0.08770857006311417\n",
      "Epoch 21, Batch 38 Loss:0.08628033101558685\n",
      "Epoch 21, Batch 39 Loss:0.05797651782631874\n",
      "Epoch 21, Batch 40 Loss:0.06906826794147491\n",
      "Epoch 21, Batch 41 Loss:0.09266466647386551\n",
      "Epoch 21, Batch 42 Loss:0.07661106437444687\n",
      "Epoch 21, Batch 43 Loss:0.0749475508928299\n",
      "Epoch 21, Batch 44 Loss:0.08183718472719193\n",
      "Epoch 21, Batch 45 Loss:0.06184453144669533\n",
      "Epoch 21, Batch 46 Loss:0.059145327657461166\n",
      "Epoch 21, Batch 47 Loss:0.10322373360395432\n",
      "Epoch 21, Batch 48 Loss:0.075688436627388\n",
      "Epoch 21, Batch 49 Loss:0.0915091261267662\n",
      "Epoch 21, Batch 50 Loss:0.1035182923078537\n",
      "Epoch 21, Batch 51 Loss:0.06471040844917297\n",
      "Epoch 21, Batch 52 Loss:0.0854581817984581\n",
      "Epoch 21, Batch 53 Loss:0.08496180921792984\n",
      "Epoch 21, Batch 54 Loss:0.1003188043832779\n",
      "Epoch 21, Batch 55 Loss:0.07452287524938583\n",
      "Epoch 21, Batch 56 Loss:0.03453538194298744\n",
      "Epoch 21, Batch 57 Loss:0.11886250227689743\n",
      "Epoch 21, Batch 58 Loss:0.08287585526704788\n",
      "Epoch 21, Batch 59 Loss:0.08235512673854828\n",
      "Epoch 21, Batch 60 Loss:0.07419033348560333\n",
      "Epoch 21, Batch 61 Loss:0.04084207862615585\n",
      "Epoch 21, Batch 62 Loss:0.06669171154499054\n",
      "Epoch 21, Batch 63 Loss:0.07443758100271225\n",
      "Epoch 21, Batch 64 Loss:0.10543972253799438\n",
      "Epoch 21, Batch 65 Loss:0.07401280105113983\n",
      "Epoch 21, Batch 66 Loss:0.11590197682380676\n",
      "Epoch 21, Batch 67 Loss:0.07090383023023605\n",
      "Epoch 21, Batch 68 Loss:0.06827691942453384\n",
      "Epoch 21, Batch 69 Loss:0.06916983425617218\n",
      "Epoch 21, Batch 70 Loss:0.10440536588430405\n",
      "Epoch 21, Batch 71 Loss:0.0852196216583252\n",
      "Epoch 21, Batch 72 Loss:0.04952346533536911\n",
      "Epoch 21, Batch 73 Loss:0.08125076442956924\n",
      "Epoch 21, Batch 74 Loss:0.0883956179022789\n",
      "Epoch 21, Batch 75 Loss:0.07565485686063766\n",
      "Epoch 21, Batch 76 Loss:0.06367120146751404\n",
      "Epoch 21, Batch 77 Loss:0.07141602784395218\n",
      "Epoch 21, Batch 78 Loss:0.09192982316017151\n",
      "Epoch 21, Batch 79 Loss:0.0554468035697937\n",
      "Epoch 21, Batch 80 Loss:0.048871252685785294\n",
      "Epoch 21, Batch 81 Loss:0.04463343694806099\n",
      "Epoch 21, Batch 82 Loss:0.06205887347459793\n",
      "Epoch 21, Batch 83 Loss:0.0828649029135704\n",
      "Epoch 21, Batch 84 Loss:0.1316175013780594\n",
      "Epoch 21, Batch 85 Loss:0.09106148779392242\n",
      "Epoch 21, Batch 86 Loss:0.09586098045110703\n",
      "Epoch 21, Batch 87 Loss:0.11126691848039627\n",
      "Epoch 21, Batch 88 Loss:0.08786234259605408\n",
      "Epoch 21, Batch 89 Loss:0.054744068533182144\n",
      "Epoch 21, Batch 90 Loss:0.10395939648151398\n",
      "Epoch 21, Batch 91 Loss:0.06457774341106415\n",
      "Epoch 21, Batch 92 Loss:0.09170924127101898\n",
      "Epoch 21, Batch 93 Loss:0.07093062996864319\n",
      "Epoch 21, Batch 94 Loss:0.08498553931713104\n",
      "Epoch 21, Batch 95 Loss:0.08831970393657684\n",
      "Epoch 21, Batch 96 Loss:0.1338503360748291\n",
      "Epoch 21, Batch 97 Loss:0.08274883031845093\n",
      "Epoch 21, Batch 98 Loss:0.0703885555267334\n",
      "Epoch 21, Batch 99 Loss:0.08737298101186752\n",
      "Epoch 21, Batch 100 Loss:0.0841381698846817\n",
      "Epoch 21, Batch 101 Loss:0.09445899724960327\n",
      "Epoch 21, Batch 102 Loss:0.07012276351451874\n",
      "Epoch 21, Batch 103 Loss:0.0650562196969986\n",
      "Epoch 21, Batch 104 Loss:0.06577301025390625\n",
      "Epoch 21, Batch 105 Loss:0.09677289426326752\n",
      "Epoch 21, Batch 106 Loss:0.07213079929351807\n",
      "Epoch 21, Batch 107 Loss:0.10274176299571991\n",
      "Epoch 21, Batch 108 Loss:0.09636843949556351\n",
      "Epoch 21, Batch 109 Loss:0.07232318073511124\n",
      "Epoch 21, Batch 110 Loss:0.09048224985599518\n",
      "Epoch 21, Batch 111 Loss:0.08657397329807281\n",
      "Epoch 21, Batch 112 Loss:0.08960521966218948\n",
      "Epoch 21, Batch 113 Loss:0.07132577896118164\n",
      "Epoch 21, Batch 114 Loss:0.0784234032034874\n",
      "Epoch 21, Batch 115 Loss:0.12475453317165375\n",
      "Epoch 21, Batch 116 Loss:0.09993953257799149\n",
      "Epoch 21, Batch 117 Loss:0.0995994284749031\n",
      "Epoch 21, Batch 118 Loss:0.10697206854820251\n",
      "Epoch 21, Batch 119 Loss:0.08397302031517029\n",
      "Epoch 21, Batch 120 Loss:0.06521500647068024\n",
      "Epoch 21, Batch 121 Loss:0.10213235020637512\n",
      "Epoch 21, Batch 122 Loss:0.05473235249519348\n",
      "Epoch 21, Batch 123 Loss:0.09804375469684601\n",
      "Epoch 21, Batch 124 Loss:0.0768817812204361\n",
      "Epoch 21, Batch 125 Loss:0.10759469866752625\n",
      "Epoch 21, Batch 126 Loss:0.09915244579315186\n",
      "Epoch 21, Batch 127 Loss:0.06067713350057602\n",
      "Epoch 21, Batch 128 Loss:0.0970938429236412\n",
      "Epoch 21, Batch 129 Loss:0.09634613990783691\n",
      "Epoch 21, Batch 130 Loss:0.08272236585617065\n",
      "Epoch 21, Batch 131 Loss:0.07601034641265869\n",
      "Epoch 21, Batch 132 Loss:0.08239535987377167\n",
      "Epoch 21, Batch 133 Loss:0.08020368218421936\n",
      "Epoch 21, Batch 134 Loss:0.08679957687854767\n",
      "Epoch 21, Batch 135 Loss:0.0590335875749588\n",
      "Epoch 21, Batch 136 Loss:0.044682737439870834\n",
      "Epoch 21, Batch 137 Loss:0.0946822538971901\n",
      "Epoch 21, Batch 138 Loss:0.05175955593585968\n",
      "Epoch 21, Batch 139 Loss:0.0701943188905716\n",
      "Epoch 21, Batch 140 Loss:0.11458857357501984\n",
      "Epoch 21, Batch 141 Loss:0.09764610230922699\n",
      "Epoch 21, Batch 142 Loss:0.09523560851812363\n",
      "Epoch 21, Batch 143 Loss:0.034835219383239746\n",
      "Epoch 21, Batch 144 Loss:0.08607684075832367\n",
      "Epoch 21, Batch 145 Loss:0.11022677272558212\n",
      "Epoch 21, Batch 146 Loss:0.03977173566818237\n",
      "Epoch 21, Batch 147 Loss:0.09460870921611786\n",
      "Epoch 21, Batch 148 Loss:0.07985113561153412\n",
      "Epoch 21, Batch 149 Loss:0.08837901800870895\n",
      "Epoch 21, Batch 150 Loss:0.09458627551794052\n",
      "Epoch 21, Batch 151 Loss:0.05853176489472389\n",
      "Epoch 21, Batch 152 Loss:0.0935121476650238\n",
      "Epoch 21, Batch 153 Loss:0.08918055891990662\n",
      "Epoch 21, Batch 154 Loss:0.08312731981277466\n",
      "Epoch 21, Batch 155 Loss:0.05763886868953705\n",
      "Epoch 21, Batch 156 Loss:0.0967860221862793\n",
      "Epoch 21, Batch 157 Loss:0.08686897158622742\n",
      "Epoch 21, Batch 158 Loss:0.12098526954650879\n",
      "Epoch 21, Batch 159 Loss:0.1163695901632309\n",
      "Epoch 21, Batch 160 Loss:0.12990787625312805\n",
      "Epoch 21, Batch 161 Loss:0.11309833079576492\n",
      "Epoch 21, Batch 162 Loss:0.08674304187297821\n",
      "Epoch 21, Batch 163 Loss:0.09280803054571152\n",
      "Epoch 21, Batch 164 Loss:0.07942532002925873\n",
      "Epoch 21, Batch 165 Loss:0.07833121716976166\n",
      "Epoch 21, Batch 166 Loss:0.12763085961341858\n",
      "Epoch 21, Batch 167 Loss:0.08584222197532654\n",
      "Epoch 21, Batch 168 Loss:0.08006395399570465\n",
      "Epoch 21, Batch 169 Loss:0.13416306674480438\n",
      "Epoch 21, Batch 170 Loss:0.0989069864153862\n",
      "Epoch 21, Batch 171 Loss:0.120260089635849\n",
      "Epoch 21, Batch 172 Loss:0.09661775827407837\n",
      "Epoch 21, Batch 173 Loss:0.08187129348516464\n",
      "Epoch 21, Batch 174 Loss:0.09182046353816986\n",
      "Epoch 21, Batch 175 Loss:0.1250843107700348\n",
      "Epoch 21, Batch 176 Loss:0.07501812279224396\n",
      "Epoch 21, Batch 177 Loss:0.07764715701341629\n",
      "Epoch 21, Batch 178 Loss:0.05663685500621796\n",
      "Epoch 21, Batch 179 Loss:0.10403832793235779\n",
      "Epoch 21, Batch 180 Loss:0.10618610680103302\n",
      "Epoch 21, Batch 181 Loss:0.07157085835933685\n",
      "Epoch 21, Batch 182 Loss:0.09871017187833786\n",
      "Epoch 21, Batch 183 Loss:0.10088913142681122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Batch 184 Loss:0.12588751316070557\n",
      "Epoch 21, Batch 185 Loss:0.09547081589698792\n",
      "Epoch 21, Batch 186 Loss:0.08859171718358994\n",
      "Epoch 21, Batch 187 Loss:0.08081429451704025\n",
      "Epoch 21, Batch 188 Loss:0.07054245471954346\n",
      "Epoch 21, Batch 189 Loss:0.07296907901763916\n",
      "Epoch 21, Batch 190 Loss:0.172235369682312\n",
      "Epoch 21, Batch 191 Loss:0.11452329158782959\n",
      "Epoch 21, Batch 192 Loss:0.07610547542572021\n",
      "Epoch 21, Batch 193 Loss:0.09692171961069107\n",
      "Epoch 21, Batch 194 Loss:0.08356544375419617\n",
      "Epoch 21, Batch 195 Loss:0.11230827867984772\n",
      "Epoch 21, Batch 196 Loss:0.08832651376724243\n",
      "Epoch 21, Batch 197 Loss:0.10251586139202118\n",
      "Epoch 21, Batch 198 Loss:0.0948081761598587\n",
      "Epoch 21, Batch 199 Loss:0.07713325321674347\n",
      "Epoch 21, Batch 200 Loss:0.062335304915905\n",
      "Epoch 21, Batch 201 Loss:0.11004460602998734\n",
      "Epoch 21, Batch 202 Loss:0.11295264959335327\n",
      "Epoch 21, Batch 203 Loss:0.09970459342002869\n",
      "Epoch 21, Batch 204 Loss:0.10603083670139313\n",
      "Epoch 21, Batch 205 Loss:0.04647016525268555\n",
      "Epoch 21, Batch 206 Loss:0.14259503781795502\n",
      "Epoch 21, Batch 207 Loss:0.12827467918395996\n",
      "Epoch 21, Batch 208 Loss:0.09502384066581726\n",
      "Epoch 21, Batch 209 Loss:0.08663041889667511\n",
      "Epoch 21, Batch 210 Loss:0.11499899625778198\n",
      "Epoch 21, Batch 211 Loss:0.20058076083660126\n",
      "Epoch 21, Batch 212 Loss:0.09938791394233704\n",
      "Epoch 21, Batch 213 Loss:0.06168857589364052\n",
      "Epoch 21, Batch 214 Loss:0.050003912299871445\n",
      "Epoch 21, Batch 215 Loss:0.0993795320391655\n",
      "Epoch 21, Batch 216 Loss:0.08690586686134338\n",
      "Epoch 21, Batch 217 Loss:0.1279463917016983\n",
      "Epoch 21, Batch 218 Loss:0.08562727272510529\n",
      "Epoch 21, Batch 219 Loss:0.07429707050323486\n",
      "Epoch 21, Batch 220 Loss:0.0922858864068985\n",
      "Epoch 21, Batch 221 Loss:0.11534715443849564\n",
      "Epoch 21, Batch 222 Loss:0.12450060993432999\n",
      "Epoch 21, Batch 223 Loss:0.13902653753757477\n",
      "Epoch 21, Batch 224 Loss:0.08260047435760498\n",
      "Epoch 21, Batch 225 Loss:0.044873498380184174\n",
      "Epoch 21, Batch 226 Loss:0.09504739195108414\n",
      "Epoch 21, Batch 227 Loss:0.10312159359455109\n",
      "Epoch 21, Batch 228 Loss:0.10981050133705139\n",
      "Epoch 21, Batch 229 Loss:0.08840204030275345\n",
      "Epoch 21, Batch 230 Loss:0.08575185388326645\n",
      "Epoch 21, Batch 231 Loss:0.09865137934684753\n",
      "Epoch 21, Batch 232 Loss:0.10380107164382935\n",
      "Epoch 21, Batch 233 Loss:0.11390844732522964\n",
      "Loss in this Epoch is: 11.3908447325 %\n",
      "Accuracy in this Epoch is: 88.2000029087 %\n",
      "Epoch 22, Batch 0 Loss:0.09394937753677368\n",
      "Epoch 22, Batch 1 Loss:0.09247791022062302\n",
      "Epoch 22, Batch 2 Loss:0.04832497239112854\n",
      "Epoch 22, Batch 3 Loss:0.07948983460664749\n",
      "Epoch 22, Batch 4 Loss:0.07284440845251083\n",
      "Epoch 22, Batch 5 Loss:0.08139664679765701\n",
      "Epoch 22, Batch 6 Loss:0.08032768964767456\n",
      "Epoch 22, Batch 7 Loss:0.048585496842861176\n",
      "Epoch 22, Batch 8 Loss:0.0772845521569252\n",
      "Epoch 22, Batch 9 Loss:0.07796545326709747\n",
      "Epoch 22, Batch 10 Loss:0.06301078200340271\n",
      "Epoch 22, Batch 11 Loss:0.041330065578222275\n",
      "Epoch 22, Batch 12 Loss:0.04932478070259094\n",
      "Epoch 22, Batch 13 Loss:0.05952414870262146\n",
      "Epoch 22, Batch 14 Loss:0.09567983448505402\n",
      "Epoch 22, Batch 15 Loss:0.04709557816386223\n",
      "Epoch 22, Batch 16 Loss:0.05967576429247856\n",
      "Epoch 22, Batch 17 Loss:0.12434570491313934\n",
      "Epoch 22, Batch 18 Loss:0.1054440438747406\n",
      "Epoch 22, Batch 19 Loss:0.05954630672931671\n",
      "Epoch 22, Batch 20 Loss:0.09326610714197159\n",
      "Epoch 22, Batch 21 Loss:0.0943499282002449\n",
      "Epoch 22, Batch 22 Loss:0.0455632209777832\n",
      "Epoch 22, Batch 23 Loss:0.06500355899333954\n",
      "Epoch 22, Batch 24 Loss:0.037719547748565674\n",
      "Epoch 22, Batch 25 Loss:0.11198512464761734\n",
      "Epoch 22, Batch 26 Loss:0.07306341081857681\n",
      "Epoch 22, Batch 27 Loss:0.09401024878025055\n",
      "Epoch 22, Batch 28 Loss:0.07570440322160721\n",
      "Epoch 22, Batch 29 Loss:0.09307882934808731\n",
      "Epoch 22, Batch 30 Loss:0.07769778370857239\n",
      "Epoch 22, Batch 31 Loss:0.07536343485116959\n",
      "Epoch 22, Batch 32 Loss:0.08826266974210739\n",
      "Epoch 22, Batch 33 Loss:0.06738041341304779\n",
      "Epoch 22, Batch 34 Loss:0.029707010835409164\n",
      "Epoch 22, Batch 35 Loss:0.13852454721927643\n",
      "Epoch 22, Batch 36 Loss:0.061593104153871536\n",
      "Epoch 22, Batch 37 Loss:0.09722882509231567\n",
      "Epoch 22, Batch 38 Loss:0.08287940174341202\n",
      "Epoch 22, Batch 39 Loss:0.0519893541932106\n",
      "Epoch 22, Batch 40 Loss:0.06326181441545486\n",
      "Epoch 22, Batch 41 Loss:0.09434261173009872\n",
      "Epoch 22, Batch 42 Loss:0.059784092009067535\n",
      "Epoch 22, Batch 43 Loss:0.08087722957134247\n",
      "Epoch 22, Batch 44 Loss:0.12607038021087646\n",
      "Epoch 22, Batch 45 Loss:0.07101662456989288\n",
      "Epoch 22, Batch 46 Loss:0.10483675450086594\n",
      "Epoch 22, Batch 47 Loss:0.09568122774362564\n",
      "Epoch 22, Batch 48 Loss:0.1025078296661377\n",
      "Epoch 22, Batch 49 Loss:0.07686321437358856\n",
      "Epoch 22, Batch 50 Loss:0.04358764365315437\n",
      "Epoch 22, Batch 51 Loss:0.08020571619272232\n",
      "Epoch 22, Batch 52 Loss:0.06959661841392517\n",
      "Epoch 22, Batch 53 Loss:0.07560867071151733\n",
      "Epoch 22, Batch 54 Loss:0.08817967772483826\n",
      "Epoch 22, Batch 55 Loss:0.0520167350769043\n",
      "Epoch 22, Batch 56 Loss:0.0765194445848465\n",
      "Epoch 22, Batch 57 Loss:0.08530881255865097\n",
      "Epoch 22, Batch 58 Loss:0.07965975254774094\n",
      "Epoch 22, Batch 59 Loss:0.09182281047105789\n",
      "Epoch 22, Batch 60 Loss:0.05754263699054718\n",
      "Epoch 22, Batch 61 Loss:0.062446922063827515\n",
      "Epoch 22, Batch 62 Loss:0.10078723728656769\n",
      "Epoch 22, Batch 63 Loss:0.12429811805486679\n",
      "Epoch 22, Batch 64 Loss:0.04952104017138481\n",
      "Epoch 22, Batch 65 Loss:0.060393448919057846\n",
      "Epoch 22, Batch 66 Loss:0.06194846332073212\n",
      "Epoch 22, Batch 67 Loss:0.03518989682197571\n",
      "Epoch 22, Batch 68 Loss:0.07751292735338211\n",
      "Epoch 22, Batch 69 Loss:0.10381511598825455\n",
      "Epoch 22, Batch 70 Loss:0.05350763723254204\n",
      "Epoch 22, Batch 71 Loss:0.055612750351428986\n",
      "Epoch 22, Batch 72 Loss:0.1081254854798317\n",
      "Epoch 22, Batch 73 Loss:0.04621390625834465\n",
      "Epoch 22, Batch 74 Loss:0.0492614284157753\n",
      "Epoch 22, Batch 75 Loss:0.05449838191270828\n",
      "Epoch 22, Batch 76 Loss:0.05539146810770035\n",
      "Epoch 22, Batch 77 Loss:0.09234991669654846\n",
      "Epoch 22, Batch 78 Loss:0.08515962958335876\n",
      "Epoch 22, Batch 79 Loss:0.09959811717271805\n",
      "Epoch 22, Batch 80 Loss:0.09436216205358505\n",
      "Epoch 22, Batch 81 Loss:0.07777082920074463\n",
      "Epoch 22, Batch 82 Loss:0.11877903342247009\n",
      "Epoch 22, Batch 83 Loss:0.14515180885791779\n",
      "Epoch 22, Batch 84 Loss:0.09945788234472275\n",
      "Epoch 22, Batch 85 Loss:0.08307459205389023\n",
      "Epoch 22, Batch 86 Loss:0.08574672788381577\n",
      "Epoch 22, Batch 87 Loss:0.06131470575928688\n",
      "Epoch 22, Batch 88 Loss:0.08586610853672028\n",
      "Epoch 22, Batch 89 Loss:0.05392923578619957\n",
      "Epoch 22, Batch 90 Loss:0.05791780725121498\n",
      "Epoch 22, Batch 91 Loss:0.11084548383951187\n",
      "Epoch 22, Batch 92 Loss:0.09598732739686966\n",
      "Epoch 22, Batch 93 Loss:0.14198853075504303\n",
      "Epoch 22, Batch 94 Loss:0.08560781925916672\n",
      "Epoch 22, Batch 95 Loss:0.06972641497850418\n",
      "Epoch 22, Batch 96 Loss:0.07394537329673767\n",
      "Epoch 22, Batch 97 Loss:0.0921970009803772\n",
      "Epoch 22, Batch 98 Loss:0.06422965973615646\n",
      "Epoch 22, Batch 99 Loss:0.11403150111436844\n",
      "Epoch 22, Batch 100 Loss:0.09071776270866394\n",
      "Epoch 22, Batch 101 Loss:0.05340908467769623\n",
      "Epoch 22, Batch 102 Loss:0.09265842288732529\n",
      "Epoch 22, Batch 103 Loss:0.04810777306556702\n",
      "Epoch 22, Batch 104 Loss:0.06095682457089424\n",
      "Epoch 22, Batch 105 Loss:0.09784922003746033\n",
      "Epoch 22, Batch 106 Loss:0.04770854860544205\n",
      "Epoch 22, Batch 107 Loss:0.10658646374940872\n",
      "Epoch 22, Batch 108 Loss:0.14161917567253113\n",
      "Epoch 22, Batch 109 Loss:0.13678640127182007\n",
      "Epoch 22, Batch 110 Loss:0.07082214206457138\n",
      "Epoch 22, Batch 111 Loss:0.153604656457901\n",
      "Epoch 22, Batch 112 Loss:0.15027648210525513\n",
      "Epoch 22, Batch 113 Loss:0.11537912487983704\n",
      "Epoch 22, Batch 114 Loss:0.09462282061576843\n",
      "Epoch 22, Batch 115 Loss:0.08016718924045563\n",
      "Epoch 22, Batch 116 Loss:0.08743613958358765\n",
      "Epoch 22, Batch 117 Loss:0.1254425048828125\n",
      "Epoch 22, Batch 118 Loss:0.07588177919387817\n",
      "Epoch 22, Batch 119 Loss:0.07562543451786041\n",
      "Epoch 22, Batch 120 Loss:0.07702900469303131\n",
      "Epoch 22, Batch 121 Loss:0.10952425003051758\n",
      "Epoch 22, Batch 122 Loss:0.07209394127130508\n",
      "Epoch 22, Batch 123 Loss:0.14374050498008728\n",
      "Epoch 22, Batch 124 Loss:0.09266924858093262\n",
      "Epoch 22, Batch 125 Loss:0.07491715252399445\n",
      "Epoch 22, Batch 126 Loss:0.10394269973039627\n",
      "Epoch 22, Batch 127 Loss:0.08626790344715118\n",
      "Epoch 22, Batch 128 Loss:0.05775631219148636\n",
      "Epoch 22, Batch 129 Loss:0.08549067378044128\n",
      "Epoch 22, Batch 130 Loss:0.04732276126742363\n",
      "Epoch 22, Batch 131 Loss:0.1100759357213974\n",
      "Epoch 22, Batch 132 Loss:0.10059240460395813\n",
      "Epoch 22, Batch 133 Loss:0.06088988482952118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Batch 134 Loss:0.040661320090293884\n",
      "Epoch 22, Batch 135 Loss:0.13065358996391296\n",
      "Epoch 22, Batch 136 Loss:0.07511484622955322\n",
      "Epoch 22, Batch 137 Loss:0.1415538787841797\n",
      "Epoch 22, Batch 138 Loss:0.056079182773828506\n",
      "Epoch 22, Batch 139 Loss:0.08926186710596085\n",
      "Epoch 22, Batch 140 Loss:0.0813545286655426\n",
      "Epoch 22, Batch 141 Loss:0.09009634703397751\n",
      "Epoch 22, Batch 142 Loss:0.07201360166072845\n",
      "Epoch 22, Batch 143 Loss:0.08857481181621552\n",
      "Epoch 22, Batch 144 Loss:0.08705756068229675\n",
      "Epoch 22, Batch 145 Loss:0.05716405808925629\n",
      "Epoch 22, Batch 146 Loss:0.07935571670532227\n",
      "Epoch 22, Batch 147 Loss:0.07488635927438736\n",
      "Epoch 22, Batch 148 Loss:0.10741613805294037\n",
      "Epoch 22, Batch 149 Loss:0.07116582989692688\n",
      "Epoch 22, Batch 150 Loss:0.038420405238866806\n",
      "Epoch 22, Batch 151 Loss:0.1181454136967659\n",
      "Epoch 22, Batch 152 Loss:0.06965875625610352\n",
      "Epoch 22, Batch 153 Loss:0.06200743466615677\n",
      "Epoch 22, Batch 154 Loss:0.06840410828590393\n",
      "Epoch 22, Batch 155 Loss:0.04304032027721405\n",
      "Epoch 22, Batch 156 Loss:0.04865970462560654\n",
      "Epoch 22, Batch 157 Loss:0.06569646298885345\n",
      "Epoch 22, Batch 158 Loss:0.07652401179075241\n",
      "Epoch 22, Batch 159 Loss:0.12193110585212708\n",
      "Epoch 22, Batch 160 Loss:0.07284437119960785\n",
      "Epoch 22, Batch 161 Loss:0.10392814874649048\n",
      "Epoch 22, Batch 162 Loss:0.07980073988437653\n",
      "Epoch 22, Batch 163 Loss:0.08013804256916046\n",
      "Epoch 22, Batch 164 Loss:0.03466630354523659\n",
      "Epoch 22, Batch 165 Loss:0.09487751126289368\n",
      "Epoch 22, Batch 166 Loss:0.09630228579044342\n",
      "Epoch 22, Batch 167 Loss:0.07819558680057526\n",
      "Epoch 22, Batch 168 Loss:0.0789608359336853\n",
      "Epoch 22, Batch 169 Loss:0.09398345649242401\n",
      "Epoch 22, Batch 170 Loss:0.09765928238630295\n",
      "Epoch 22, Batch 171 Loss:0.0886114165186882\n",
      "Epoch 22, Batch 172 Loss:0.05585252866148949\n",
      "Epoch 22, Batch 173 Loss:0.08043915778398514\n",
      "Epoch 22, Batch 174 Loss:0.07003751397132874\n",
      "Epoch 22, Batch 175 Loss:0.06612870842218399\n",
      "Epoch 22, Batch 176 Loss:0.0840630978345871\n",
      "Epoch 22, Batch 177 Loss:0.13847798109054565\n",
      "Epoch 22, Batch 178 Loss:0.05549057573080063\n",
      "Epoch 22, Batch 179 Loss:0.09788957238197327\n",
      "Epoch 22, Batch 180 Loss:0.04581356793642044\n",
      "Epoch 22, Batch 181 Loss:0.06930868327617645\n",
      "Epoch 22, Batch 182 Loss:0.05552898347377777\n",
      "Epoch 22, Batch 183 Loss:0.08282028883695602\n",
      "Epoch 22, Batch 184 Loss:0.07205314189195633\n",
      "Epoch 22, Batch 185 Loss:0.06642992049455643\n",
      "Epoch 22, Batch 186 Loss:0.08026257902383804\n",
      "Epoch 22, Batch 187 Loss:0.0940992683172226\n",
      "Epoch 22, Batch 188 Loss:0.08692357689142227\n",
      "Epoch 22, Batch 189 Loss:0.07671137899160385\n",
      "Epoch 22, Batch 190 Loss:0.08068783581256866\n",
      "Epoch 22, Batch 191 Loss:0.08582916855812073\n",
      "Epoch 22, Batch 192 Loss:0.06273981928825378\n",
      "Epoch 22, Batch 193 Loss:0.11005696654319763\n",
      "Epoch 22, Batch 194 Loss:0.12098199129104614\n",
      "Epoch 22, Batch 195 Loss:0.08046092838048935\n",
      "Epoch 22, Batch 196 Loss:0.12635236978530884\n",
      "Epoch 22, Batch 197 Loss:0.06284190714359283\n",
      "Epoch 22, Batch 198 Loss:0.0849812924861908\n",
      "Epoch 22, Batch 199 Loss:0.06364953517913818\n",
      "Epoch 22, Batch 200 Loss:0.08274388313293457\n",
      "Epoch 22, Batch 201 Loss:0.0708497166633606\n",
      "Epoch 22, Batch 202 Loss:0.0849694162607193\n",
      "Epoch 22, Batch 203 Loss:0.09347918629646301\n",
      "Epoch 22, Batch 204 Loss:0.1637166291475296\n",
      "Epoch 22, Batch 205 Loss:0.0838126391172409\n",
      "Epoch 22, Batch 206 Loss:0.09020209312438965\n",
      "Epoch 22, Batch 207 Loss:0.09356346726417542\n",
      "Epoch 22, Batch 208 Loss:0.13529710471630096\n",
      "Epoch 22, Batch 209 Loss:0.10925613343715668\n",
      "Epoch 22, Batch 210 Loss:0.0854911059141159\n",
      "Epoch 22, Batch 211 Loss:0.08150886744260788\n",
      "Epoch 22, Batch 212 Loss:0.09600662440061569\n",
      "Epoch 22, Batch 213 Loss:0.0695016086101532\n",
      "Epoch 22, Batch 214 Loss:0.04403000697493553\n",
      "Epoch 22, Batch 215 Loss:0.09672432392835617\n",
      "Epoch 22, Batch 216 Loss:0.09878838807344437\n",
      "Epoch 22, Batch 217 Loss:0.12423329055309296\n",
      "Epoch 22, Batch 218 Loss:0.08114232122898102\n",
      "Epoch 22, Batch 219 Loss:0.09186388552188873\n",
      "Epoch 22, Batch 220 Loss:0.1250407099723816\n",
      "Epoch 22, Batch 221 Loss:0.05915356054902077\n",
      "Epoch 22, Batch 222 Loss:0.08675532042980194\n",
      "Epoch 22, Batch 223 Loss:0.07171247899532318\n",
      "Epoch 22, Batch 224 Loss:0.07678526639938354\n",
      "Epoch 22, Batch 225 Loss:0.10309012979269028\n",
      "Epoch 22, Batch 226 Loss:0.118080273270607\n",
      "Epoch 22, Batch 227 Loss:0.10898391902446747\n",
      "Epoch 22, Batch 228 Loss:0.09541679173707962\n",
      "Epoch 22, Batch 229 Loss:0.12847808003425598\n",
      "Epoch 22, Batch 230 Loss:0.08582204580307007\n",
      "Epoch 22, Batch 231 Loss:0.07058601081371307\n",
      "Epoch 22, Batch 232 Loss:0.08840731531381607\n",
      "Epoch 22, Batch 233 Loss:0.06847177445888519\n",
      "Loss in this Epoch is: 6.84717744589 %\n",
      "Accuracy in this Epoch is: 88.8400018215 %\n",
      "Epoch 23, Batch 0 Loss:0.0615382082760334\n",
      "Epoch 23, Batch 1 Loss:0.07508180290460587\n",
      "Epoch 23, Batch 2 Loss:0.0506623238325119\n",
      "Epoch 23, Batch 3 Loss:0.07041393220424652\n",
      "Epoch 23, Batch 4 Loss:0.06689317524433136\n",
      "Epoch 23, Batch 5 Loss:0.04984315112233162\n",
      "Epoch 23, Batch 6 Loss:0.07756357640028\n",
      "Epoch 23, Batch 7 Loss:0.10101719200611115\n",
      "Epoch 23, Batch 8 Loss:0.049005236476659775\n",
      "Epoch 23, Batch 9 Loss:0.09286122024059296\n",
      "Epoch 23, Batch 10 Loss:0.05079476535320282\n",
      "Epoch 23, Batch 11 Loss:0.09539751708507538\n",
      "Epoch 23, Batch 12 Loss:0.053718533366918564\n",
      "Epoch 23, Batch 13 Loss:0.05390828102827072\n",
      "Epoch 23, Batch 14 Loss:0.062068913131952286\n",
      "Epoch 23, Batch 15 Loss:0.12068411707878113\n",
      "Epoch 23, Batch 16 Loss:0.035444993525743484\n",
      "Epoch 23, Batch 17 Loss:0.04940539970993996\n",
      "Epoch 23, Batch 18 Loss:0.044346217066049576\n",
      "Epoch 23, Batch 19 Loss:0.08524216711521149\n",
      "Epoch 23, Batch 20 Loss:0.04473109170794487\n",
      "Epoch 23, Batch 21 Loss:0.05982615426182747\n",
      "Epoch 23, Batch 22 Loss:0.05408608540892601\n",
      "Epoch 23, Batch 23 Loss:0.05866353586316109\n",
      "Epoch 23, Batch 24 Loss:0.039139360189437866\n",
      "Epoch 23, Batch 25 Loss:0.05518627166748047\n",
      "Epoch 23, Batch 26 Loss:0.09763876348733902\n",
      "Epoch 23, Batch 27 Loss:0.06959807127714157\n",
      "Epoch 23, Batch 28 Loss:0.055205706506967545\n",
      "Epoch 23, Batch 29 Loss:0.05867527797818184\n",
      "Epoch 23, Batch 30 Loss:0.09441876411437988\n",
      "Epoch 23, Batch 31 Loss:0.08666647970676422\n",
      "Epoch 23, Batch 32 Loss:0.06883884221315384\n",
      "Epoch 23, Batch 33 Loss:0.07495325058698654\n",
      "Epoch 23, Batch 34 Loss:0.048643890768289566\n",
      "Epoch 23, Batch 35 Loss:0.05776570364832878\n",
      "Epoch 23, Batch 36 Loss:0.047066766768693924\n",
      "Epoch 23, Batch 37 Loss:0.06909023970365524\n",
      "Epoch 23, Batch 38 Loss:0.03931652382016182\n",
      "Epoch 23, Batch 39 Loss:0.04163304716348648\n",
      "Epoch 23, Batch 40 Loss:0.09269917756319046\n",
      "Epoch 23, Batch 41 Loss:0.039952412247657776\n",
      "Epoch 23, Batch 42 Loss:0.0534190833568573\n",
      "Epoch 23, Batch 43 Loss:0.04632599279284477\n",
      "Epoch 23, Batch 44 Loss:0.07883717864751816\n",
      "Epoch 23, Batch 45 Loss:0.06661839038133621\n",
      "Epoch 23, Batch 46 Loss:0.04914063587784767\n",
      "Epoch 23, Batch 47 Loss:0.06502961367368698\n",
      "Epoch 23, Batch 48 Loss:0.06330884993076324\n",
      "Epoch 23, Batch 49 Loss:0.058312222361564636\n",
      "Epoch 23, Batch 50 Loss:0.05838446691632271\n",
      "Epoch 23, Batch 51 Loss:0.034799352288246155\n",
      "Epoch 23, Batch 52 Loss:0.11127609759569168\n",
      "Epoch 23, Batch 53 Loss:0.08656728267669678\n",
      "Epoch 23, Batch 54 Loss:0.13089826703071594\n",
      "Epoch 23, Batch 55 Loss:0.07226487249135971\n",
      "Epoch 23, Batch 56 Loss:0.09387136250734329\n",
      "Epoch 23, Batch 57 Loss:0.08421548455953598\n",
      "Epoch 23, Batch 58 Loss:0.050194382667541504\n",
      "Epoch 23, Batch 59 Loss:0.13999758660793304\n",
      "Epoch 23, Batch 60 Loss:0.1209309995174408\n",
      "Epoch 23, Batch 61 Loss:0.09146822988986969\n",
      "Epoch 23, Batch 62 Loss:0.08304239064455032\n",
      "Epoch 23, Batch 63 Loss:0.06538835912942886\n",
      "Epoch 23, Batch 64 Loss:0.12829379737377167\n",
      "Epoch 23, Batch 65 Loss:0.087983638048172\n",
      "Epoch 23, Batch 66 Loss:0.12292706221342087\n",
      "Epoch 23, Batch 67 Loss:0.08227421343326569\n",
      "Epoch 23, Batch 68 Loss:0.06847839057445526\n",
      "Epoch 23, Batch 69 Loss:0.08635963499546051\n",
      "Epoch 23, Batch 70 Loss:0.07768905907869339\n",
      "Epoch 23, Batch 71 Loss:0.06979880481958389\n",
      "Epoch 23, Batch 72 Loss:0.09440720081329346\n",
      "Epoch 23, Batch 73 Loss:0.06921906024217606\n",
      "Epoch 23, Batch 74 Loss:0.1173902377486229\n",
      "Epoch 23, Batch 75 Loss:0.07762930542230606\n",
      "Epoch 23, Batch 76 Loss:0.08584219962358475\n",
      "Epoch 23, Batch 77 Loss:0.12931664288043976\n",
      "Epoch 23, Batch 78 Loss:0.048266977071762085\n",
      "Epoch 23, Batch 79 Loss:0.07217826694250107\n",
      "Epoch 23, Batch 80 Loss:0.06731459498405457\n",
      "Epoch 23, Batch 81 Loss:0.0851106271147728\n",
      "Epoch 23, Batch 82 Loss:0.04664821922779083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Batch 83 Loss:0.08371414244174957\n",
      "Epoch 23, Batch 84 Loss:0.05761530622839928\n",
      "Epoch 23, Batch 85 Loss:0.07299020141363144\n",
      "Epoch 23, Batch 86 Loss:0.10295101255178452\n",
      "Epoch 23, Batch 87 Loss:0.08366412669420242\n",
      "Epoch 23, Batch 88 Loss:0.06023411825299263\n",
      "Epoch 23, Batch 89 Loss:0.06694026291370392\n",
      "Epoch 23, Batch 90 Loss:0.09211939573287964\n",
      "Epoch 23, Batch 91 Loss:0.10551594197750092\n",
      "Epoch 23, Batch 92 Loss:0.04733361303806305\n",
      "Epoch 23, Batch 93 Loss:0.052269067615270615\n",
      "Epoch 23, Batch 94 Loss:0.06275979429483414\n",
      "Epoch 23, Batch 95 Loss:0.060644425451755524\n",
      "Epoch 23, Batch 96 Loss:0.07860809564590454\n",
      "Epoch 23, Batch 97 Loss:0.085370734333992\n",
      "Epoch 23, Batch 98 Loss:0.08682482689619064\n",
      "Epoch 23, Batch 99 Loss:0.03917874023318291\n",
      "Epoch 23, Batch 100 Loss:0.03198813647031784\n",
      "Epoch 23, Batch 101 Loss:0.1026812270283699\n",
      "Epoch 23, Batch 102 Loss:0.042424701154232025\n",
      "Epoch 23, Batch 103 Loss:0.07338262349367142\n",
      "Epoch 23, Batch 104 Loss:0.05814988538622856\n",
      "Epoch 23, Batch 105 Loss:0.06677614152431488\n",
      "Epoch 23, Batch 106 Loss:0.06572750210762024\n",
      "Epoch 23, Batch 107 Loss:0.08673834800720215\n",
      "Epoch 23, Batch 108 Loss:0.05444958433508873\n",
      "Epoch 23, Batch 109 Loss:0.046897247433662415\n",
      "Epoch 23, Batch 110 Loss:0.05489892512559891\n",
      "Epoch 23, Batch 111 Loss:0.08582035452127457\n",
      "Epoch 23, Batch 112 Loss:0.05971911549568176\n",
      "Epoch 23, Batch 113 Loss:0.040062807500362396\n",
      "Epoch 23, Batch 114 Loss:0.06285412609577179\n",
      "Epoch 23, Batch 115 Loss:0.07652536779642105\n",
      "Epoch 23, Batch 116 Loss:0.09527779370546341\n",
      "Epoch 23, Batch 117 Loss:0.061938658356666565\n",
      "Epoch 23, Batch 118 Loss:0.038881171494722366\n",
      "Epoch 23, Batch 119 Loss:0.07991955429315567\n",
      "Epoch 23, Batch 120 Loss:0.06678812950849533\n",
      "Epoch 23, Batch 121 Loss:0.08049163222312927\n",
      "Epoch 23, Batch 122 Loss:0.06474006921052933\n",
      "Epoch 23, Batch 123 Loss:0.08974023163318634\n",
      "Epoch 23, Batch 124 Loss:0.09618645906448364\n",
      "Epoch 23, Batch 125 Loss:0.12371106445789337\n",
      "Epoch 23, Batch 126 Loss:0.0848214328289032\n",
      "Epoch 23, Batch 127 Loss:0.07373681664466858\n",
      "Epoch 23, Batch 128 Loss:0.07808926701545715\n",
      "Epoch 23, Batch 129 Loss:0.03684283420443535\n",
      "Epoch 23, Batch 130 Loss:0.07743947207927704\n",
      "Epoch 23, Batch 131 Loss:0.1141308844089508\n",
      "Epoch 23, Batch 132 Loss:0.06535753607749939\n",
      "Epoch 23, Batch 133 Loss:0.09314876794815063\n",
      "Epoch 23, Batch 134 Loss:0.050110142678022385\n",
      "Epoch 23, Batch 135 Loss:0.06330830603837967\n",
      "Epoch 23, Batch 136 Loss:0.0847025215625763\n",
      "Epoch 23, Batch 137 Loss:0.09975948184728622\n",
      "Epoch 23, Batch 138 Loss:0.08132825791835785\n",
      "Epoch 23, Batch 139 Loss:0.1241849809885025\n",
      "Epoch 23, Batch 140 Loss:0.041882120072841644\n",
      "Epoch 23, Batch 141 Loss:0.054181840270757675\n",
      "Epoch 23, Batch 142 Loss:0.0794885903596878\n",
      "Epoch 23, Batch 143 Loss:0.09874022006988525\n",
      "Epoch 23, Batch 144 Loss:0.10095949470996857\n",
      "Epoch 23, Batch 145 Loss:0.07288256287574768\n",
      "Epoch 23, Batch 146 Loss:0.08036552369594574\n",
      "Epoch 23, Batch 147 Loss:0.02221876196563244\n",
      "Epoch 23, Batch 148 Loss:0.09368829429149628\n",
      "Epoch 23, Batch 149 Loss:0.05377335101366043\n",
      "Epoch 23, Batch 150 Loss:0.115005262196064\n",
      "Epoch 23, Batch 151 Loss:0.13358545303344727\n",
      "Epoch 23, Batch 152 Loss:0.08192958682775497\n",
      "Epoch 23, Batch 153 Loss:0.13081902265548706\n",
      "Epoch 23, Batch 154 Loss:0.06882111728191376\n",
      "Epoch 23, Batch 155 Loss:0.06105032563209534\n",
      "Epoch 23, Batch 156 Loss:0.08244180679321289\n",
      "Epoch 23, Batch 157 Loss:0.12990683317184448\n",
      "Epoch 23, Batch 158 Loss:0.03681822493672371\n",
      "Epoch 23, Batch 159 Loss:0.07292476296424866\n",
      "Epoch 23, Batch 160 Loss:0.12095857411623001\n",
      "Epoch 23, Batch 161 Loss:0.09920360147953033\n",
      "Epoch 23, Batch 162 Loss:0.0688791275024414\n",
      "Epoch 23, Batch 163 Loss:0.09630216658115387\n",
      "Epoch 23, Batch 164 Loss:0.12354426085948944\n",
      "Epoch 23, Batch 165 Loss:0.11242736876010895\n",
      "Epoch 23, Batch 166 Loss:0.06987935304641724\n",
      "Epoch 23, Batch 167 Loss:0.07784242928028107\n",
      "Epoch 23, Batch 168 Loss:0.06315932422876358\n",
      "Epoch 23, Batch 169 Loss:0.10394655168056488\n",
      "Epoch 23, Batch 170 Loss:0.13644768297672272\n",
      "Epoch 23, Batch 171 Loss:0.08747407793998718\n",
      "Epoch 23, Batch 172 Loss:0.10732068866491318\n",
      "Epoch 23, Batch 173 Loss:0.08521939069032669\n",
      "Epoch 23, Batch 174 Loss:0.13093560934066772\n",
      "Epoch 23, Batch 175 Loss:0.09115935862064362\n",
      "Epoch 23, Batch 176 Loss:0.08749163895845413\n",
      "Epoch 23, Batch 177 Loss:0.0919092446565628\n",
      "Epoch 23, Batch 178 Loss:0.09795566648244858\n",
      "Epoch 23, Batch 179 Loss:0.14203715324401855\n",
      "Epoch 23, Batch 180 Loss:0.10777094960212708\n",
      "Epoch 23, Batch 181 Loss:0.0754246935248375\n",
      "Epoch 23, Batch 182 Loss:0.11936123669147491\n",
      "Epoch 23, Batch 183 Loss:0.0554971918463707\n",
      "Epoch 23, Batch 184 Loss:0.05866957828402519\n",
      "Epoch 23, Batch 185 Loss:0.1016639918088913\n",
      "Epoch 23, Batch 186 Loss:0.08039313554763794\n",
      "Epoch 23, Batch 187 Loss:0.1094498336315155\n",
      "Epoch 23, Batch 188 Loss:0.07243967801332474\n",
      "Epoch 23, Batch 189 Loss:0.09308796375989914\n",
      "Epoch 23, Batch 190 Loss:0.09943188726902008\n",
      "Epoch 23, Batch 191 Loss:0.13416117429733276\n",
      "Epoch 23, Batch 192 Loss:0.09686417132616043\n",
      "Epoch 23, Batch 193 Loss:0.07984419167041779\n",
      "Epoch 23, Batch 194 Loss:0.11564899235963821\n",
      "Epoch 23, Batch 195 Loss:0.08328688144683838\n",
      "Epoch 23, Batch 196 Loss:0.10621181130409241\n",
      "Epoch 23, Batch 197 Loss:0.10505735129117966\n",
      "Epoch 23, Batch 198 Loss:0.07323036342859268\n",
      "Epoch 23, Batch 199 Loss:0.11081135272979736\n",
      "Epoch 23, Batch 200 Loss:0.1126340925693512\n",
      "Epoch 23, Batch 201 Loss:0.06512851268053055\n",
      "Epoch 23, Batch 202 Loss:0.09039769321680069\n",
      "Epoch 23, Batch 203 Loss:0.11641258746385574\n",
      "Epoch 23, Batch 204 Loss:0.07594722509384155\n",
      "Epoch 23, Batch 205 Loss:0.05905810743570328\n",
      "Epoch 23, Batch 206 Loss:0.09078407287597656\n",
      "Epoch 23, Batch 207 Loss:0.11196205019950867\n",
      "Epoch 23, Batch 208 Loss:0.1463923156261444\n",
      "Epoch 23, Batch 209 Loss:0.05897150933742523\n",
      "Epoch 23, Batch 210 Loss:0.09336763620376587\n",
      "Epoch 23, Batch 211 Loss:0.07716062664985657\n",
      "Epoch 23, Batch 212 Loss:0.08597470819950104\n",
      "Epoch 23, Batch 213 Loss:0.061818160116672516\n",
      "Epoch 23, Batch 214 Loss:0.07508192211389542\n",
      "Epoch 23, Batch 215 Loss:0.06762470304965973\n",
      "Epoch 23, Batch 216 Loss:0.13230924308300018\n",
      "Epoch 23, Batch 217 Loss:0.1127658262848854\n",
      "Epoch 23, Batch 218 Loss:0.08005131036043167\n",
      "Epoch 23, Batch 219 Loss:0.0777178704738617\n",
      "Epoch 23, Batch 220 Loss:0.06835654377937317\n",
      "Epoch 23, Batch 221 Loss:0.09440241754055023\n",
      "Epoch 23, Batch 222 Loss:0.09547119587659836\n",
      "Epoch 23, Batch 223 Loss:0.0817418172955513\n",
      "Epoch 23, Batch 224 Loss:0.08544281125068665\n",
      "Epoch 23, Batch 225 Loss:0.06832695007324219\n",
      "Epoch 23, Batch 226 Loss:0.07498322427272797\n",
      "Epoch 23, Batch 227 Loss:0.07487767189741135\n",
      "Epoch 23, Batch 228 Loss:0.07434405386447906\n",
      "Epoch 23, Batch 229 Loss:0.07391102612018585\n",
      "Epoch 23, Batch 230 Loss:0.09648649394512177\n",
      "Epoch 23, Batch 231 Loss:0.062575563788414\n",
      "Epoch 23, Batch 232 Loss:0.049411460757255554\n",
      "Epoch 23, Batch 233 Loss:0.06313030421733856\n",
      "Loss in this Epoch is: 6.31303042173 %\n",
      "Accuracy in this Epoch is: 88.8100028038 %\n",
      "Epoch 24, Batch 0 Loss:0.04746584594249725\n",
      "Epoch 24, Batch 1 Loss:0.07468266785144806\n",
      "Epoch 24, Batch 2 Loss:0.06557301431894302\n",
      "Epoch 24, Batch 3 Loss:0.053021516650915146\n",
      "Epoch 24, Batch 4 Loss:0.08203952759504318\n",
      "Epoch 24, Batch 5 Loss:0.10372035205364227\n",
      "Epoch 24, Batch 6 Loss:0.04882378503680229\n",
      "Epoch 24, Batch 7 Loss:0.06673084944486618\n",
      "Epoch 24, Batch 8 Loss:0.07076627016067505\n",
      "Epoch 24, Batch 9 Loss:0.07394812256097794\n",
      "Epoch 24, Batch 10 Loss:0.05449705570936203\n",
      "Epoch 24, Batch 11 Loss:0.04749172553420067\n",
      "Epoch 24, Batch 12 Loss:0.07282107323408127\n",
      "Epoch 24, Batch 13 Loss:0.04682852700352669\n",
      "Epoch 24, Batch 14 Loss:0.08253178745508194\n",
      "Epoch 24, Batch 15 Loss:0.07329767197370529\n",
      "Epoch 24, Batch 16 Loss:0.04006724804639816\n",
      "Epoch 24, Batch 17 Loss:0.06561705470085144\n",
      "Epoch 24, Batch 18 Loss:0.07814840972423553\n",
      "Epoch 24, Batch 19 Loss:0.06253908574581146\n",
      "Epoch 24, Batch 20 Loss:0.05349703133106232\n",
      "Epoch 24, Batch 21 Loss:0.08556247502565384\n",
      "Epoch 24, Batch 22 Loss:0.0981835275888443\n",
      "Epoch 24, Batch 23 Loss:0.08767887949943542\n",
      "Epoch 24, Batch 24 Loss:0.08774878084659576\n",
      "Epoch 24, Batch 25 Loss:0.05573229864239693\n",
      "Epoch 24, Batch 26 Loss:0.058567509055137634\n",
      "Epoch 24, Batch 27 Loss:0.07026449590921402\n",
      "Epoch 24, Batch 28 Loss:0.08427247405052185\n",
      "Epoch 24, Batch 29 Loss:0.05271437019109726\n",
      "Epoch 24, Batch 30 Loss:0.030793284997344017\n",
      "Epoch 24, Batch 31 Loss:0.05271042510867119\n",
      "Epoch 24, Batch 32 Loss:0.08793006092309952\n",
      "Epoch 24, Batch 33 Loss:0.07800930738449097\n",
      "Epoch 24, Batch 34 Loss:0.05258921533823013\n",
      "Epoch 24, Batch 35 Loss:0.10013727843761444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 36 Loss:0.06486514955759048\n",
      "Epoch 24, Batch 37 Loss:0.055225588381290436\n",
      "Epoch 24, Batch 38 Loss:0.07692626863718033\n",
      "Epoch 24, Batch 39 Loss:0.05911518633365631\n",
      "Epoch 24, Batch 40 Loss:0.06937376409769058\n",
      "Epoch 24, Batch 41 Loss:0.040143292397260666\n",
      "Epoch 24, Batch 42 Loss:0.08059905469417572\n",
      "Epoch 24, Batch 43 Loss:0.09550590813159943\n",
      "Epoch 24, Batch 44 Loss:0.0846826583147049\n",
      "Epoch 24, Batch 45 Loss:0.03993106633424759\n",
      "Epoch 24, Batch 46 Loss:0.03103499300777912\n",
      "Epoch 24, Batch 47 Loss:0.06582549214363098\n",
      "Epoch 24, Batch 48 Loss:0.07930432260036469\n",
      "Epoch 24, Batch 49 Loss:0.08931446820497513\n",
      "Epoch 24, Batch 50 Loss:0.06533090770244598\n",
      "Epoch 24, Batch 51 Loss:0.05965469032526016\n",
      "Epoch 24, Batch 52 Loss:0.057787343859672546\n",
      "Epoch 24, Batch 53 Loss:0.049234721809625626\n",
      "Epoch 24, Batch 54 Loss:0.043015941977500916\n",
      "Epoch 24, Batch 55 Loss:0.04579322785139084\n",
      "Epoch 24, Batch 56 Loss:0.04932152107357979\n",
      "Epoch 24, Batch 57 Loss:0.06004031375050545\n",
      "Epoch 24, Batch 58 Loss:0.04542556405067444\n",
      "Epoch 24, Batch 59 Loss:0.08107247948646545\n",
      "Epoch 24, Batch 60 Loss:0.07339557260274887\n",
      "Epoch 24, Batch 61 Loss:0.07530441880226135\n",
      "Epoch 24, Batch 62 Loss:0.06551416218280792\n",
      "Epoch 24, Batch 63 Loss:0.06181633099913597\n",
      "Epoch 24, Batch 64 Loss:0.0880090519785881\n",
      "Epoch 24, Batch 65 Loss:0.03609328344464302\n",
      "Epoch 24, Batch 66 Loss:0.06074763834476471\n",
      "Epoch 24, Batch 67 Loss:0.07927173376083374\n",
      "Epoch 24, Batch 68 Loss:0.08343342691659927\n",
      "Epoch 24, Batch 69 Loss:0.054905179888010025\n",
      "Epoch 24, Batch 70 Loss:0.07695045322179794\n",
      "Epoch 24, Batch 71 Loss:0.061683688312768936\n",
      "Epoch 24, Batch 72 Loss:0.09293613582849503\n",
      "Epoch 24, Batch 73 Loss:0.10448460280895233\n",
      "Epoch 24, Batch 74 Loss:0.08791396021842957\n",
      "Epoch 24, Batch 75 Loss:0.04598301649093628\n",
      "Epoch 24, Batch 76 Loss:0.07034925371408463\n",
      "Epoch 24, Batch 77 Loss:0.047449927777051926\n",
      "Epoch 24, Batch 78 Loss:0.06505107879638672\n",
      "Epoch 24, Batch 79 Loss:0.04390468820929527\n",
      "Epoch 24, Batch 80 Loss:0.026392405852675438\n",
      "Epoch 24, Batch 81 Loss:0.04601287096738815\n",
      "Epoch 24, Batch 82 Loss:0.0667925551533699\n",
      "Epoch 24, Batch 83 Loss:0.08057823777198792\n",
      "Epoch 24, Batch 84 Loss:0.0548660010099411\n",
      "Epoch 24, Batch 85 Loss:0.0812080129981041\n",
      "Epoch 24, Batch 86 Loss:0.04096531495451927\n",
      "Epoch 24, Batch 87 Loss:0.03514370694756508\n",
      "Epoch 24, Batch 88 Loss:0.07937058806419373\n",
      "Epoch 24, Batch 89 Loss:0.05271359905600548\n",
      "Epoch 24, Batch 90 Loss:0.043297939002513885\n",
      "Epoch 24, Batch 91 Loss:0.039249200373888016\n",
      "Epoch 24, Batch 92 Loss:0.05669156461954117\n",
      "Epoch 24, Batch 93 Loss:0.07254734635353088\n",
      "Epoch 24, Batch 94 Loss:0.0459926538169384\n",
      "Epoch 24, Batch 95 Loss:0.045594990253448486\n",
      "Epoch 24, Batch 96 Loss:0.035064052790403366\n",
      "Epoch 24, Batch 97 Loss:0.019642556086182594\n",
      "Epoch 24, Batch 98 Loss:0.07156983017921448\n",
      "Epoch 24, Batch 99 Loss:0.06110650300979614\n",
      "Epoch 24, Batch 100 Loss:0.05424743890762329\n",
      "Epoch 24, Batch 101 Loss:0.06500204652547836\n",
      "Epoch 24, Batch 102 Loss:0.08463077992200851\n",
      "Epoch 24, Batch 103 Loss:0.044552549719810486\n",
      "Epoch 24, Batch 104 Loss:0.09401034563779831\n",
      "Epoch 24, Batch 105 Loss:0.0796629786491394\n",
      "Epoch 24, Batch 106 Loss:0.05558210238814354\n",
      "Epoch 24, Batch 107 Loss:0.06304061412811279\n",
      "Epoch 24, Batch 108 Loss:0.07680150866508484\n",
      "Epoch 24, Batch 109 Loss:0.0649009644985199\n",
      "Epoch 24, Batch 110 Loss:0.04245447367429733\n",
      "Epoch 24, Batch 111 Loss:0.02328627184033394\n",
      "Epoch 24, Batch 112 Loss:0.0645160973072052\n",
      "Epoch 24, Batch 113 Loss:0.05568359047174454\n",
      "Epoch 24, Batch 114 Loss:0.042951542884111404\n",
      "Epoch 24, Batch 115 Loss:0.0483182817697525\n",
      "Epoch 24, Batch 116 Loss:0.050930269062519073\n",
      "Epoch 24, Batch 117 Loss:0.05351373180747032\n",
      "Epoch 24, Batch 118 Loss:0.041813649237155914\n",
      "Epoch 24, Batch 119 Loss:0.06004016101360321\n",
      "Epoch 24, Batch 120 Loss:0.06601998955011368\n",
      "Epoch 24, Batch 121 Loss:0.06912294030189514\n",
      "Epoch 24, Batch 122 Loss:0.06989255547523499\n",
      "Epoch 24, Batch 123 Loss:0.03366488218307495\n",
      "Epoch 24, Batch 124 Loss:0.13703110814094543\n",
      "Epoch 24, Batch 125 Loss:0.07412511110305786\n",
      "Epoch 24, Batch 126 Loss:0.03964734822511673\n",
      "Epoch 24, Batch 127 Loss:0.05081019178032875\n",
      "Epoch 24, Batch 128 Loss:0.0906268060207367\n",
      "Epoch 24, Batch 129 Loss:0.09192438423633575\n",
      "Epoch 24, Batch 130 Loss:0.054749276489019394\n",
      "Epoch 24, Batch 131 Loss:0.03877644240856171\n",
      "Epoch 24, Batch 132 Loss:0.049761973321437836\n",
      "Epoch 24, Batch 133 Loss:0.08092078566551208\n",
      "Epoch 24, Batch 134 Loss:0.09350177645683289\n",
      "Epoch 24, Batch 135 Loss:0.0727868378162384\n",
      "Epoch 24, Batch 136 Loss:0.09758161008358002\n",
      "Epoch 24, Batch 137 Loss:0.04705366492271423\n",
      "Epoch 24, Batch 138 Loss:0.06578266620635986\n",
      "Epoch 24, Batch 139 Loss:0.07755391299724579\n",
      "Epoch 24, Batch 140 Loss:0.06665484607219696\n",
      "Epoch 24, Batch 141 Loss:0.07592390477657318\n",
      "Epoch 24, Batch 142 Loss:0.12754350900650024\n",
      "Epoch 24, Batch 143 Loss:0.1151857003569603\n",
      "Epoch 24, Batch 144 Loss:0.06538696587085724\n",
      "Epoch 24, Batch 145 Loss:0.07526366412639618\n",
      "Epoch 24, Batch 146 Loss:0.12858261168003082\n",
      "Epoch 24, Batch 147 Loss:0.058621034026145935\n",
      "Epoch 24, Batch 148 Loss:0.04934592917561531\n",
      "Epoch 24, Batch 149 Loss:0.06134598329663277\n",
      "Epoch 24, Batch 150 Loss:0.0484020859003067\n",
      "Epoch 24, Batch 151 Loss:0.07439649850130081\n",
      "Epoch 24, Batch 152 Loss:0.07589356601238251\n",
      "Epoch 24, Batch 153 Loss:0.06726539134979248\n",
      "Epoch 24, Batch 154 Loss:0.03989412263035774\n",
      "Epoch 24, Batch 155 Loss:0.07836704701185226\n",
      "Epoch 24, Batch 156 Loss:0.08107411116361618\n",
      "Epoch 24, Batch 157 Loss:0.08898580074310303\n",
      "Epoch 24, Batch 158 Loss:0.08366386592388153\n",
      "Epoch 24, Batch 159 Loss:0.08787861466407776\n",
      "Epoch 24, Batch 160 Loss:0.07023338228464127\n",
      "Epoch 24, Batch 161 Loss:0.0631006583571434\n",
      "Epoch 24, Batch 162 Loss:0.10273158550262451\n",
      "Epoch 24, Batch 163 Loss:0.07687774300575256\n",
      "Epoch 24, Batch 164 Loss:0.06670936942100525\n",
      "Epoch 24, Batch 165 Loss:0.10650218278169632\n",
      "Epoch 24, Batch 166 Loss:0.09192779660224915\n",
      "Epoch 24, Batch 167 Loss:0.0652613639831543\n",
      "Epoch 24, Batch 168 Loss:0.10398465394973755\n",
      "Epoch 24, Batch 169 Loss:0.07581779360771179\n",
      "Epoch 24, Batch 170 Loss:0.09612104296684265\n",
      "Epoch 24, Batch 171 Loss:0.05946745350956917\n",
      "Epoch 24, Batch 172 Loss:0.06908146291971207\n",
      "Epoch 24, Batch 173 Loss:0.06807153671979904\n",
      "Epoch 24, Batch 174 Loss:0.07575929909944534\n",
      "Epoch 24, Batch 175 Loss:0.06405746191740036\n",
      "Epoch 24, Batch 176 Loss:0.0412164181470871\n",
      "Epoch 24, Batch 177 Loss:0.040154844522476196\n",
      "Epoch 24, Batch 178 Loss:0.0488162524998188\n",
      "Epoch 24, Batch 179 Loss:0.06612050533294678\n",
      "Epoch 24, Batch 180 Loss:0.05843289941549301\n",
      "Epoch 24, Batch 181 Loss:0.08515557646751404\n",
      "Epoch 24, Batch 182 Loss:0.09789247065782547\n",
      "Epoch 24, Batch 183 Loss:0.0958731397986412\n",
      "Epoch 24, Batch 184 Loss:0.10037504136562347\n",
      "Epoch 24, Batch 185 Loss:0.0718168392777443\n",
      "Epoch 24, Batch 186 Loss:0.052811965346336365\n",
      "Epoch 24, Batch 187 Loss:0.09482670575380325\n",
      "Epoch 24, Batch 188 Loss:0.07375366985797882\n",
      "Epoch 24, Batch 189 Loss:0.10629954934120178\n",
      "Epoch 24, Batch 190 Loss:0.08217650651931763\n",
      "Epoch 24, Batch 191 Loss:0.07151190936565399\n",
      "Epoch 24, Batch 192 Loss:0.08930446207523346\n",
      "Epoch 24, Batch 193 Loss:0.09631023555994034\n",
      "Epoch 24, Batch 194 Loss:0.07990103214979172\n",
      "Epoch 24, Batch 195 Loss:0.08183426409959793\n",
      "Epoch 24, Batch 196 Loss:0.058379001915454865\n",
      "Epoch 24, Batch 197 Loss:0.12124893814325333\n",
      "Epoch 24, Batch 198 Loss:0.0629349946975708\n",
      "Epoch 24, Batch 199 Loss:0.07289811968803406\n",
      "Epoch 24, Batch 200 Loss:0.0578104667365551\n",
      "Epoch 24, Batch 201 Loss:0.11009117215871811\n",
      "Epoch 24, Batch 202 Loss:0.061582665890455246\n",
      "Epoch 24, Batch 203 Loss:0.04313773661851883\n",
      "Epoch 24, Batch 204 Loss:0.10867639631032944\n",
      "Epoch 24, Batch 205 Loss:0.07659292221069336\n",
      "Epoch 24, Batch 206 Loss:0.06665006279945374\n",
      "Epoch 24, Batch 207 Loss:0.0702044665813446\n",
      "Epoch 24, Batch 208 Loss:0.08592700958251953\n",
      "Epoch 24, Batch 209 Loss:0.09540797770023346\n",
      "Epoch 24, Batch 210 Loss:0.15939739346504211\n",
      "Epoch 24, Batch 211 Loss:0.07758976519107819\n",
      "Epoch 24, Batch 212 Loss:0.06723935902118683\n",
      "Epoch 24, Batch 213 Loss:0.06836970895528793\n",
      "Epoch 24, Batch 214 Loss:0.0834638699889183\n",
      "Epoch 24, Batch 215 Loss:0.05816669762134552\n",
      "Epoch 24, Batch 216 Loss:0.072788767516613\n",
      "Epoch 24, Batch 217 Loss:0.10378289967775345\n",
      "Epoch 24, Batch 218 Loss:0.07594197243452072\n",
      "Epoch 24, Batch 219 Loss:0.11003775894641876\n",
      "Epoch 24, Batch 220 Loss:0.103492870926857\n",
      "Epoch 24, Batch 221 Loss:0.07134412974119186\n",
      "Epoch 24, Batch 222 Loss:0.1088414415717125\n",
      "Epoch 24, Batch 223 Loss:0.07860353589057922\n",
      "Epoch 24, Batch 224 Loss:0.08518680930137634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Batch 225 Loss:0.07970111072063446\n",
      "Epoch 24, Batch 226 Loss:0.0662444680929184\n",
      "Epoch 24, Batch 227 Loss:0.1259395033121109\n",
      "Epoch 24, Batch 228 Loss:0.09471496939659119\n",
      "Epoch 24, Batch 229 Loss:0.08695902675390244\n",
      "Epoch 24, Batch 230 Loss:0.05160102993249893\n",
      "Epoch 24, Batch 231 Loss:0.09572615474462509\n",
      "Epoch 24, Batch 232 Loss:0.06535905599594116\n",
      "Epoch 24, Batch 233 Loss:0.07200022041797638\n",
      "Loss in this Epoch is: 7.2000220418 %\n",
      "Accuracy in this Epoch is: 88.5100007057 %\n",
      "Epoch 25, Batch 0 Loss:0.04247118905186653\n",
      "Epoch 25, Batch 1 Loss:0.05130438506603241\n",
      "Epoch 25, Batch 2 Loss:0.07865453511476517\n",
      "Epoch 25, Batch 3 Loss:0.05104386806488037\n",
      "Epoch 25, Batch 4 Loss:0.049535028636455536\n",
      "Epoch 25, Batch 5 Loss:0.053265511989593506\n",
      "Epoch 25, Batch 6 Loss:0.0385625883936882\n",
      "Epoch 25, Batch 7 Loss:0.07323414832353592\n",
      "Epoch 25, Batch 8 Loss:0.04676179587841034\n",
      "Epoch 25, Batch 9 Loss:0.06609984487295151\n",
      "Epoch 25, Batch 10 Loss:0.06996045261621475\n",
      "Epoch 25, Batch 11 Loss:0.037866972386837006\n",
      "Epoch 25, Batch 12 Loss:0.05618233606219292\n",
      "Epoch 25, Batch 13 Loss:0.05132049322128296\n",
      "Epoch 25, Batch 14 Loss:0.10972926020622253\n",
      "Epoch 25, Batch 15 Loss:0.05297423154115677\n",
      "Epoch 25, Batch 16 Loss:0.06564360111951828\n",
      "Epoch 25, Batch 17 Loss:0.06277571618556976\n",
      "Epoch 25, Batch 18 Loss:0.05682157725095749\n",
      "Epoch 25, Batch 19 Loss:0.05794962868094444\n",
      "Epoch 25, Batch 20 Loss:0.06067337468266487\n",
      "Epoch 25, Batch 21 Loss:0.03139686957001686\n",
      "Epoch 25, Batch 22 Loss:0.04160844534635544\n",
      "Epoch 25, Batch 23 Loss:0.0506291426718235\n",
      "Epoch 25, Batch 24 Loss:0.057590972632169724\n",
      "Epoch 25, Batch 25 Loss:0.03895245119929314\n",
      "Epoch 25, Batch 26 Loss:0.10040078312158585\n",
      "Epoch 25, Batch 27 Loss:0.02974044345319271\n",
      "Epoch 25, Batch 28 Loss:0.0446658656001091\n",
      "Epoch 25, Batch 29 Loss:0.0994192510843277\n",
      "Epoch 25, Batch 30 Loss:0.10207774490118027\n",
      "Epoch 25, Batch 31 Loss:0.03329518064856529\n",
      "Epoch 25, Batch 32 Loss:0.06127379089593887\n",
      "Epoch 25, Batch 33 Loss:0.07050519436597824\n",
      "Epoch 25, Batch 34 Loss:0.06651928275823593\n",
      "Epoch 25, Batch 35 Loss:0.05841036140918732\n",
      "Epoch 25, Batch 36 Loss:0.05061792954802513\n",
      "Epoch 25, Batch 37 Loss:0.06039487197995186\n",
      "Epoch 25, Batch 38 Loss:0.0599646121263504\n",
      "Epoch 25, Batch 39 Loss:0.07604213058948517\n",
      "Epoch 25, Batch 40 Loss:0.05436372384428978\n",
      "Epoch 25, Batch 41 Loss:0.08545610308647156\n",
      "Epoch 25, Batch 42 Loss:0.06636199355125427\n",
      "Epoch 25, Batch 43 Loss:0.06638439744710922\n",
      "Epoch 25, Batch 44 Loss:0.040967341512441635\n",
      "Epoch 25, Batch 45 Loss:0.06506487727165222\n",
      "Epoch 25, Batch 46 Loss:0.10254627466201782\n",
      "Epoch 25, Batch 47 Loss:0.08619554340839386\n",
      "Epoch 25, Batch 48 Loss:0.02950456365942955\n",
      "Epoch 25, Batch 49 Loss:0.036514636129140854\n",
      "Epoch 25, Batch 50 Loss:0.07301906496286392\n",
      "Epoch 25, Batch 51 Loss:0.037489719688892365\n",
      "Epoch 25, Batch 52 Loss:0.07296980172395706\n",
      "Epoch 25, Batch 53 Loss:0.06591428071260452\n",
      "Epoch 25, Batch 54 Loss:0.0877743810415268\n",
      "Epoch 25, Batch 55 Loss:0.06903037428855896\n",
      "Epoch 25, Batch 56 Loss:0.04771196469664574\n",
      "Epoch 25, Batch 57 Loss:0.06235569342970848\n",
      "Epoch 25, Batch 58 Loss:0.05658069998025894\n",
      "Epoch 25, Batch 59 Loss:0.07781201601028442\n",
      "Epoch 25, Batch 60 Loss:0.03767327219247818\n",
      "Epoch 25, Batch 61 Loss:0.06561040133237839\n",
      "Epoch 25, Batch 62 Loss:0.043120790272951126\n",
      "Epoch 25, Batch 63 Loss:0.10116539150476456\n",
      "Epoch 25, Batch 64 Loss:0.08799471706151962\n",
      "Epoch 25, Batch 65 Loss:0.06585350632667542\n",
      "Epoch 25, Batch 66 Loss:0.06759157031774521\n",
      "Epoch 25, Batch 67 Loss:0.10169659554958344\n",
      "Epoch 25, Batch 68 Loss:0.04358850046992302\n",
      "Epoch 25, Batch 69 Loss:0.07260487228631973\n",
      "Epoch 25, Batch 70 Loss:0.09766562283039093\n",
      "Epoch 25, Batch 71 Loss:0.05451151728630066\n",
      "Epoch 25, Batch 72 Loss:0.05569178983569145\n",
      "Epoch 25, Batch 73 Loss:0.05142553523182869\n",
      "Epoch 25, Batch 74 Loss:0.046151913702487946\n",
      "Epoch 25, Batch 75 Loss:0.05715801194310188\n",
      "Epoch 25, Batch 76 Loss:0.0657707154750824\n",
      "Epoch 25, Batch 77 Loss:0.07276124507188797\n",
      "Epoch 25, Batch 78 Loss:0.07273071259260178\n",
      "Epoch 25, Batch 79 Loss:0.11009079217910767\n",
      "Epoch 25, Batch 80 Loss:0.06489896774291992\n",
      "Epoch 25, Batch 81 Loss:0.039414241909980774\n",
      "Epoch 25, Batch 82 Loss:0.06991127133369446\n",
      "Epoch 25, Batch 83 Loss:0.050315309315919876\n",
      "Epoch 25, Batch 84 Loss:0.06403321027755737\n",
      "Epoch 25, Batch 85 Loss:0.07606613636016846\n",
      "Epoch 25, Batch 86 Loss:0.08844032883644104\n",
      "Epoch 25, Batch 87 Loss:0.032667264342308044\n",
      "Epoch 25, Batch 88 Loss:0.07311685383319855\n",
      "Epoch 25, Batch 89 Loss:0.07496873289346695\n",
      "Epoch 25, Batch 90 Loss:0.04580312222242355\n",
      "Epoch 25, Batch 91 Loss:0.06034320220351219\n",
      "Epoch 25, Batch 92 Loss:0.040714703500270844\n",
      "Epoch 25, Batch 93 Loss:0.050081416964530945\n",
      "Epoch 25, Batch 94 Loss:0.0639815405011177\n",
      "Epoch 25, Batch 95 Loss:0.04031370207667351\n",
      "Epoch 25, Batch 96 Loss:0.07207772135734558\n",
      "Epoch 25, Batch 97 Loss:0.03706304356455803\n",
      "Epoch 25, Batch 98 Loss:0.07150387018918991\n",
      "Epoch 25, Batch 99 Loss:0.07987301051616669\n",
      "Epoch 25, Batch 100 Loss:0.042679011821746826\n",
      "Epoch 25, Batch 101 Loss:0.09336915612220764\n",
      "Epoch 25, Batch 102 Loss:0.07013823837041855\n",
      "Epoch 25, Batch 103 Loss:0.07461746037006378\n",
      "Epoch 25, Batch 104 Loss:0.042012959718704224\n",
      "Epoch 25, Batch 105 Loss:0.0577382892370224\n",
      "Epoch 25, Batch 106 Loss:0.10350275039672852\n",
      "Epoch 25, Batch 107 Loss:0.058395590633153915\n",
      "Epoch 25, Batch 108 Loss:0.0790773555636406\n",
      "Epoch 25, Batch 109 Loss:0.09533759206533432\n",
      "Epoch 25, Batch 110 Loss:0.06781512498855591\n",
      "Epoch 25, Batch 111 Loss:0.0606418214738369\n",
      "Epoch 25, Batch 112 Loss:0.07313874363899231\n",
      "Epoch 25, Batch 113 Loss:0.1353382170200348\n",
      "Epoch 25, Batch 114 Loss:0.03503262251615524\n",
      "Epoch 25, Batch 115 Loss:0.05904218554496765\n",
      "Epoch 25, Batch 116 Loss:0.0479857474565506\n",
      "Epoch 25, Batch 117 Loss:0.07978177070617676\n",
      "Epoch 25, Batch 118 Loss:0.059109706431627274\n",
      "Epoch 25, Batch 119 Loss:0.09315131604671478\n",
      "Epoch 25, Batch 120 Loss:0.07649041712284088\n",
      "Epoch 25, Batch 121 Loss:0.05526731163263321\n",
      "Epoch 25, Batch 122 Loss:0.049688950181007385\n",
      "Epoch 25, Batch 123 Loss:0.07543709874153137\n",
      "Epoch 25, Batch 124 Loss:0.06676704436540604\n",
      "Epoch 25, Batch 125 Loss:0.10089877992868423\n",
      "Epoch 25, Batch 126 Loss:0.07178239524364471\n",
      "Epoch 25, Batch 127 Loss:0.06458345055580139\n",
      "Epoch 25, Batch 128 Loss:0.0673241913318634\n",
      "Epoch 25, Batch 129 Loss:0.09176887571811676\n",
      "Epoch 25, Batch 130 Loss:0.13907663524150848\n",
      "Epoch 25, Batch 131 Loss:0.09545619785785675\n",
      "Epoch 25, Batch 132 Loss:0.08005967736244202\n",
      "Epoch 25, Batch 133 Loss:0.10075940191745758\n",
      "Epoch 25, Batch 134 Loss:0.12263854593038559\n",
      "Epoch 25, Batch 135 Loss:0.07522763311862946\n",
      "Epoch 25, Batch 136 Loss:0.09017696976661682\n",
      "Epoch 25, Batch 137 Loss:0.09107689559459686\n",
      "Epoch 25, Batch 138 Loss:0.06158824265003204\n",
      "Epoch 25, Batch 139 Loss:0.06298189610242844\n",
      "Epoch 25, Batch 140 Loss:0.11148547381162643\n",
      "Epoch 25, Batch 141 Loss:0.05069068819284439\n",
      "Epoch 25, Batch 142 Loss:0.05488743633031845\n",
      "Epoch 25, Batch 143 Loss:0.12856604158878326\n",
      "Epoch 25, Batch 144 Loss:0.04798249900341034\n",
      "Epoch 25, Batch 145 Loss:0.05566369742155075\n",
      "Epoch 25, Batch 146 Loss:0.060248080641031265\n",
      "Epoch 25, Batch 147 Loss:0.09184010326862335\n",
      "Epoch 25, Batch 148 Loss:0.09088356792926788\n",
      "Epoch 25, Batch 149 Loss:0.037891585379838943\n",
      "Epoch 25, Batch 150 Loss:0.09454643726348877\n",
      "Epoch 25, Batch 151 Loss:0.06539358198642731\n",
      "Epoch 25, Batch 152 Loss:0.07214761525392532\n",
      "Epoch 25, Batch 153 Loss:0.09048345685005188\n",
      "Epoch 25, Batch 154 Loss:0.06307919323444366\n",
      "Epoch 25, Batch 155 Loss:0.06305453181266785\n",
      "Epoch 25, Batch 156 Loss:0.0752493217587471\n",
      "Epoch 25, Batch 157 Loss:0.08453696966171265\n",
      "Epoch 25, Batch 158 Loss:0.06305235624313354\n",
      "Epoch 25, Batch 159 Loss:0.0648028552532196\n",
      "Epoch 25, Batch 160 Loss:0.05807092785835266\n",
      "Epoch 25, Batch 161 Loss:0.06868714094161987\n",
      "Epoch 25, Batch 162 Loss:0.06848150491714478\n",
      "Epoch 25, Batch 163 Loss:0.04799506440758705\n",
      "Epoch 25, Batch 164 Loss:0.06014246866106987\n",
      "Epoch 25, Batch 165 Loss:0.12093500792980194\n",
      "Epoch 25, Batch 166 Loss:0.08042538911104202\n",
      "Epoch 25, Batch 167 Loss:0.09464210271835327\n",
      "Epoch 25, Batch 168 Loss:0.09473222494125366\n",
      "Epoch 25, Batch 169 Loss:0.06125093251466751\n",
      "Epoch 25, Batch 170 Loss:0.08170120418071747\n",
      "Epoch 25, Batch 171 Loss:0.0694117620587349\n",
      "Epoch 25, Batch 172 Loss:0.0680493488907814\n",
      "Epoch 25, Batch 173 Loss:0.03711463510990143\n",
      "Epoch 25, Batch 174 Loss:0.09194857627153397\n",
      "Epoch 25, Batch 175 Loss:0.08912003040313721\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Batch 176 Loss:0.05172305554151535\n",
      "Epoch 25, Batch 177 Loss:0.058289386332035065\n",
      "Epoch 25, Batch 178 Loss:0.05858728662133217\n",
      "Epoch 25, Batch 179 Loss:0.04015972837805748\n",
      "Epoch 25, Batch 180 Loss:0.0694766640663147\n",
      "Epoch 25, Batch 181 Loss:0.11395607143640518\n",
      "Epoch 25, Batch 182 Loss:0.1040377989411354\n",
      "Epoch 25, Batch 183 Loss:0.0885554775595665\n",
      "Epoch 25, Batch 184 Loss:0.06497383117675781\n",
      "Epoch 25, Batch 185 Loss:0.05747649818658829\n",
      "Epoch 25, Batch 186 Loss:0.049313947558403015\n",
      "Epoch 25, Batch 187 Loss:0.13808494806289673\n",
      "Epoch 25, Batch 188 Loss:0.07715146243572235\n",
      "Epoch 25, Batch 189 Loss:0.06559430062770844\n",
      "Epoch 25, Batch 190 Loss:0.061297304928302765\n",
      "Epoch 25, Batch 191 Loss:0.10372060537338257\n",
      "Epoch 25, Batch 192 Loss:0.06390029937028885\n",
      "Epoch 25, Batch 193 Loss:0.07913971692323685\n",
      "Epoch 25, Batch 194 Loss:0.10082752257585526\n",
      "Epoch 25, Batch 195 Loss:0.08098174631595612\n",
      "Epoch 25, Batch 196 Loss:0.07042437791824341\n",
      "Epoch 25, Batch 197 Loss:0.09177106618881226\n",
      "Epoch 25, Batch 198 Loss:0.04090370237827301\n",
      "Epoch 25, Batch 199 Loss:0.057932961732149124\n",
      "Epoch 25, Batch 200 Loss:0.08842825889587402\n",
      "Epoch 25, Batch 201 Loss:0.08388069272041321\n",
      "Epoch 25, Batch 202 Loss:0.06144213303923607\n",
      "Epoch 25, Batch 203 Loss:0.06424932926893234\n",
      "Epoch 25, Batch 204 Loss:0.12768369913101196\n",
      "Epoch 25, Batch 205 Loss:0.08794137835502625\n",
      "Epoch 25, Batch 206 Loss:0.0858737900853157\n",
      "Epoch 25, Batch 207 Loss:0.07245637476444244\n",
      "Epoch 25, Batch 208 Loss:0.08452636748552322\n",
      "Epoch 25, Batch 209 Loss:0.10164870321750641\n",
      "Epoch 25, Batch 210 Loss:0.09034575521945953\n",
      "Epoch 25, Batch 211 Loss:0.07880963385105133\n",
      "Epoch 25, Batch 212 Loss:0.10554689168930054\n",
      "Epoch 25, Batch 213 Loss:0.10021379590034485\n",
      "Epoch 25, Batch 214 Loss:0.07707543671131134\n",
      "Epoch 25, Batch 215 Loss:0.15901219844818115\n",
      "Epoch 25, Batch 216 Loss:0.08285272866487503\n",
      "Epoch 25, Batch 217 Loss:0.0613316148519516\n",
      "Epoch 25, Batch 218 Loss:0.07008036226034164\n",
      "Epoch 25, Batch 219 Loss:0.10531753301620483\n",
      "Epoch 25, Batch 220 Loss:0.10130177438259125\n",
      "Epoch 25, Batch 221 Loss:0.10083281248807907\n",
      "Epoch 25, Batch 222 Loss:0.09062987565994263\n",
      "Epoch 25, Batch 223 Loss:0.06087493151426315\n",
      "Epoch 25, Batch 224 Loss:0.03667204827070236\n",
      "Epoch 25, Batch 225 Loss:0.04136282950639725\n",
      "Epoch 25, Batch 226 Loss:0.1290806084871292\n",
      "Epoch 25, Batch 227 Loss:0.07923652976751328\n",
      "Epoch 25, Batch 228 Loss:0.08488447219133377\n",
      "Epoch 25, Batch 229 Loss:0.03960644081234932\n",
      "Epoch 25, Batch 230 Loss:0.05862037092447281\n",
      "Epoch 25, Batch 231 Loss:0.061696361750364304\n",
      "Epoch 25, Batch 232 Loss:0.06823690980672836\n",
      "Epoch 25, Batch 233 Loss:0.07615716755390167\n",
      "Loss in this Epoch is: 7.61571675539 %\n",
      "Accuracy in this Epoch is: 88.6500000954 %\n",
      "Epoch 26, Batch 0 Loss:0.058396805077791214\n",
      "Epoch 26, Batch 1 Loss:0.07652555406093597\n",
      "Epoch 26, Batch 2 Loss:0.060568906366825104\n",
      "Epoch 26, Batch 3 Loss:0.09450491517782211\n",
      "Epoch 26, Batch 4 Loss:0.09409621357917786\n",
      "Epoch 26, Batch 5 Loss:0.1025027185678482\n",
      "Epoch 26, Batch 6 Loss:0.04953741282224655\n",
      "Epoch 26, Batch 7 Loss:0.07729183882474899\n",
      "Epoch 26, Batch 8 Loss:0.07766827940940857\n",
      "Epoch 26, Batch 9 Loss:0.05892954394221306\n",
      "Epoch 26, Batch 10 Loss:0.06707386672496796\n",
      "Epoch 26, Batch 11 Loss:0.04324578493833542\n",
      "Epoch 26, Batch 12 Loss:0.0870799794793129\n",
      "Epoch 26, Batch 13 Loss:0.08357241749763489\n",
      "Epoch 26, Batch 14 Loss:0.09799248725175858\n",
      "Epoch 26, Batch 15 Loss:0.05897140130400658\n",
      "Epoch 26, Batch 16 Loss:0.054983966052532196\n",
      "Epoch 26, Batch 17 Loss:0.08124557137489319\n",
      "Epoch 26, Batch 18 Loss:0.10573005676269531\n",
      "Epoch 26, Batch 19 Loss:0.051630496978759766\n",
      "Epoch 26, Batch 20 Loss:0.0762535110116005\n",
      "Epoch 26, Batch 21 Loss:0.07542853057384491\n",
      "Epoch 26, Batch 22 Loss:0.09403283894062042\n",
      "Epoch 26, Batch 23 Loss:0.06661516427993774\n",
      "Epoch 26, Batch 24 Loss:0.047301191836595535\n",
      "Epoch 26, Batch 25 Loss:0.05839698389172554\n",
      "Epoch 26, Batch 26 Loss:0.06348630040884018\n",
      "Epoch 26, Batch 27 Loss:0.030469626188278198\n",
      "Epoch 26, Batch 28 Loss:0.05093182995915413\n",
      "Epoch 26, Batch 29 Loss:0.03683856129646301\n",
      "Epoch 26, Batch 30 Loss:0.03783007338643074\n",
      "Epoch 26, Batch 31 Loss:0.05758683383464813\n",
      "Epoch 26, Batch 32 Loss:0.04202887415885925\n",
      "Epoch 26, Batch 33 Loss:0.054418303072452545\n",
      "Epoch 26, Batch 34 Loss:0.042031429708004\n",
      "Epoch 26, Batch 35 Loss:0.04605330526828766\n",
      "Epoch 26, Batch 36 Loss:0.07549700886011124\n",
      "Epoch 26, Batch 37 Loss:0.06051449105143547\n",
      "Epoch 26, Batch 38 Loss:0.04451094567775726\n",
      "Epoch 26, Batch 39 Loss:0.04938363656401634\n",
      "Epoch 26, Batch 40 Loss:0.0542198047041893\n",
      "Epoch 26, Batch 41 Loss:0.061155423521995544\n",
      "Epoch 26, Batch 42 Loss:0.062132980674505234\n",
      "Epoch 26, Batch 43 Loss:0.08062858134508133\n",
      "Epoch 26, Batch 44 Loss:0.042837515473365784\n",
      "Epoch 26, Batch 45 Loss:0.09431662410497665\n",
      "Epoch 26, Batch 46 Loss:0.05559840053319931\n",
      "Epoch 26, Batch 47 Loss:0.07668458670377731\n",
      "Epoch 26, Batch 48 Loss:0.03746245056390762\n",
      "Epoch 26, Batch 49 Loss:0.09731754660606384\n",
      "Epoch 26, Batch 50 Loss:0.060711026191711426\n",
      "Epoch 26, Batch 51 Loss:0.08216214179992676\n",
      "Epoch 26, Batch 52 Loss:0.10439194738864899\n",
      "Epoch 26, Batch 53 Loss:0.03982676565647125\n",
      "Epoch 26, Batch 54 Loss:0.04974041506648064\n",
      "Epoch 26, Batch 55 Loss:0.039763230830430984\n",
      "Epoch 26, Batch 56 Loss:0.04007492586970329\n",
      "Epoch 26, Batch 57 Loss:0.06703580170869827\n",
      "Epoch 26, Batch 58 Loss:0.06384261697530746\n",
      "Epoch 26, Batch 59 Loss:0.06447538733482361\n",
      "Epoch 26, Batch 60 Loss:0.060110513120889664\n",
      "Epoch 26, Batch 61 Loss:0.06823749095201492\n",
      "Epoch 26, Batch 62 Loss:0.03185349702835083\n",
      "Epoch 26, Batch 63 Loss:0.09868726879358292\n",
      "Epoch 26, Batch 64 Loss:0.05495760962367058\n",
      "Epoch 26, Batch 65 Loss:0.05430102348327637\n",
      "Epoch 26, Batch 66 Loss:0.05135180056095123\n",
      "Epoch 26, Batch 67 Loss:0.04946610704064369\n",
      "Epoch 26, Batch 68 Loss:0.05801002308726311\n",
      "Epoch 26, Batch 69 Loss:0.056584432721138\n",
      "Epoch 26, Batch 70 Loss:0.05289420112967491\n",
      "Epoch 26, Batch 71 Loss:0.03576245903968811\n",
      "Epoch 26, Batch 72 Loss:0.04535796120762825\n",
      "Epoch 26, Batch 73 Loss:0.05873569846153259\n",
      "Epoch 26, Batch 74 Loss:0.043539706617593765\n",
      "Epoch 26, Batch 75 Loss:0.04312631115317345\n",
      "Epoch 26, Batch 76 Loss:0.038639601320028305\n",
      "Epoch 26, Batch 77 Loss:0.03129218891263008\n",
      "Epoch 26, Batch 78 Loss:0.07035592943429947\n",
      "Epoch 26, Batch 79 Loss:0.057320792227983475\n",
      "Epoch 26, Batch 80 Loss:0.04201250150799751\n",
      "Epoch 26, Batch 81 Loss:0.05745461955666542\n",
      "Epoch 26, Batch 82 Loss:0.05646466091275215\n",
      "Epoch 26, Batch 83 Loss:0.09026333689689636\n",
      "Epoch 26, Batch 84 Loss:0.05284103378653526\n",
      "Epoch 26, Batch 85 Loss:0.049635276198387146\n",
      "Epoch 26, Batch 86 Loss:0.04565085098147392\n",
      "Epoch 26, Batch 87 Loss:0.036624010652303696\n",
      "Epoch 26, Batch 88 Loss:0.04173795506358147\n",
      "Epoch 26, Batch 89 Loss:0.10366606712341309\n",
      "Epoch 26, Batch 90 Loss:0.09029345214366913\n",
      "Epoch 26, Batch 91 Loss:0.049531206488609314\n",
      "Epoch 26, Batch 92 Loss:0.05533577501773834\n",
      "Epoch 26, Batch 93 Loss:0.07965683937072754\n",
      "Epoch 26, Batch 94 Loss:0.08089787513017654\n",
      "Epoch 26, Batch 95 Loss:0.08073637634515762\n",
      "Epoch 26, Batch 96 Loss:0.04181241616606712\n",
      "Epoch 26, Batch 97 Loss:0.05505463853478432\n",
      "Epoch 26, Batch 98 Loss:0.08360986411571503\n",
      "Epoch 26, Batch 99 Loss:0.04194346070289612\n",
      "Epoch 26, Batch 100 Loss:0.07417550683021545\n",
      "Epoch 26, Batch 101 Loss:0.04842259734869003\n",
      "Epoch 26, Batch 102 Loss:0.06823648512363434\n",
      "Epoch 26, Batch 103 Loss:0.0980258584022522\n",
      "Epoch 26, Batch 104 Loss:0.07322356849908829\n",
      "Epoch 26, Batch 105 Loss:0.07639370858669281\n",
      "Epoch 26, Batch 106 Loss:0.07337778806686401\n",
      "Epoch 26, Batch 107 Loss:0.07296348363161087\n",
      "Epoch 26, Batch 108 Loss:0.0767044648528099\n",
      "Epoch 26, Batch 109 Loss:0.04837641492486\n",
      "Epoch 26, Batch 110 Loss:0.05577845498919487\n",
      "Epoch 26, Batch 111 Loss:0.022076766937971115\n",
      "Epoch 26, Batch 112 Loss:0.07194368541240692\n",
      "Epoch 26, Batch 113 Loss:0.10511999577283859\n",
      "Epoch 26, Batch 114 Loss:0.07865829765796661\n",
      "Epoch 26, Batch 115 Loss:0.0714077353477478\n",
      "Epoch 26, Batch 116 Loss:0.04996984079480171\n",
      "Epoch 26, Batch 117 Loss:0.050577133893966675\n",
      "Epoch 26, Batch 118 Loss:0.09481401741504669\n",
      "Epoch 26, Batch 119 Loss:0.07601074129343033\n",
      "Epoch 26, Batch 120 Loss:0.10521204769611359\n",
      "Epoch 26, Batch 121 Loss:0.04823845252394676\n",
      "Epoch 26, Batch 122 Loss:0.06453575193881989\n",
      "Epoch 26, Batch 123 Loss:0.0368770956993103\n",
      "Epoch 26, Batch 124 Loss:0.06973054260015488\n",
      "Epoch 26, Batch 125 Loss:0.10120207071304321\n",
      "Epoch 26, Batch 126 Loss:0.060657795518636703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Batch 127 Loss:0.05193978548049927\n",
      "Epoch 26, Batch 128 Loss:0.05852515622973442\n",
      "Epoch 26, Batch 129 Loss:0.0389760285615921\n",
      "Epoch 26, Batch 130 Loss:0.05982840061187744\n",
      "Epoch 26, Batch 131 Loss:0.11829196661710739\n",
      "Epoch 26, Batch 132 Loss:0.07252255082130432\n",
      "Epoch 26, Batch 133 Loss:0.07923014461994171\n",
      "Epoch 26, Batch 134 Loss:0.04572595655918121\n",
      "Epoch 26, Batch 135 Loss:0.08394914865493774\n",
      "Epoch 26, Batch 136 Loss:0.08000393211841583\n",
      "Epoch 26, Batch 137 Loss:0.07976201921701431\n",
      "Epoch 26, Batch 138 Loss:0.05936695635318756\n",
      "Epoch 26, Batch 139 Loss:0.10935348272323608\n",
      "Epoch 26, Batch 140 Loss:0.08017520606517792\n",
      "Epoch 26, Batch 141 Loss:0.05442206561565399\n",
      "Epoch 26, Batch 142 Loss:0.059433501213788986\n",
      "Epoch 26, Batch 143 Loss:0.04651302099227905\n",
      "Epoch 26, Batch 144 Loss:0.04767279326915741\n",
      "Epoch 26, Batch 145 Loss:0.06107822060585022\n",
      "Epoch 26, Batch 146 Loss:0.10135719925165176\n",
      "Epoch 26, Batch 147 Loss:0.06418382376432419\n",
      "Epoch 26, Batch 148 Loss:0.041702404618263245\n",
      "Epoch 26, Batch 149 Loss:0.0923488438129425\n",
      "Epoch 26, Batch 150 Loss:0.07587149739265442\n",
      "Epoch 26, Batch 151 Loss:0.1096121221780777\n",
      "Epoch 26, Batch 152 Loss:0.0771927684545517\n",
      "Epoch 26, Batch 153 Loss:0.08586882799863815\n",
      "Epoch 26, Batch 154 Loss:0.09891604632139206\n",
      "Epoch 26, Batch 155 Loss:0.10431884229183197\n",
      "Epoch 26, Batch 156 Loss:0.0708063468337059\n",
      "Epoch 26, Batch 157 Loss:0.054315514862537384\n",
      "Epoch 26, Batch 158 Loss:0.05593675374984741\n",
      "Epoch 26, Batch 159 Loss:0.05531967803835869\n",
      "Epoch 26, Batch 160 Loss:0.11514366418123245\n",
      "Epoch 26, Batch 161 Loss:0.047974616289138794\n",
      "Epoch 26, Batch 162 Loss:0.0556131973862648\n",
      "Epoch 26, Batch 163 Loss:0.08342288434505463\n",
      "Epoch 26, Batch 164 Loss:0.0719841942191124\n",
      "Epoch 26, Batch 165 Loss:0.08117450773715973\n",
      "Epoch 26, Batch 166 Loss:0.08151280134916306\n",
      "Epoch 26, Batch 167 Loss:0.09661554545164108\n",
      "Epoch 26, Batch 168 Loss:0.09676314890384674\n",
      "Epoch 26, Batch 169 Loss:0.09235488623380661\n",
      "Epoch 26, Batch 170 Loss:0.06569083034992218\n",
      "Epoch 26, Batch 171 Loss:0.07382474094629288\n",
      "Epoch 26, Batch 172 Loss:0.06354451924562454\n",
      "Epoch 26, Batch 173 Loss:0.08394449949264526\n",
      "Epoch 26, Batch 174 Loss:0.0694318562746048\n",
      "Epoch 26, Batch 175 Loss:0.061543550342321396\n",
      "Epoch 26, Batch 176 Loss:0.0464659184217453\n",
      "Epoch 26, Batch 177 Loss:0.09272988885641098\n",
      "Epoch 26, Batch 178 Loss:0.06643184274435043\n",
      "Epoch 26, Batch 179 Loss:0.1236124038696289\n",
      "Epoch 26, Batch 180 Loss:0.1158323809504509\n",
      "Epoch 26, Batch 181 Loss:0.08054212480783463\n",
      "Epoch 26, Batch 182 Loss:0.12970790266990662\n",
      "Epoch 26, Batch 183 Loss:0.06747611612081528\n",
      "Epoch 26, Batch 184 Loss:0.08092980831861496\n",
      "Epoch 26, Batch 185 Loss:0.0866081714630127\n",
      "Epoch 26, Batch 186 Loss:0.0982961431145668\n",
      "Epoch 26, Batch 187 Loss:0.11499250680208206\n",
      "Epoch 26, Batch 188 Loss:0.08878397941589355\n",
      "Epoch 26, Batch 189 Loss:0.061505477875471115\n",
      "Epoch 26, Batch 190 Loss:0.08806014060974121\n",
      "Epoch 26, Batch 191 Loss:0.11755377799272537\n",
      "Epoch 26, Batch 192 Loss:0.07705992460250854\n",
      "Epoch 26, Batch 193 Loss:0.050010234117507935\n",
      "Epoch 26, Batch 194 Loss:0.05159983038902283\n",
      "Epoch 26, Batch 195 Loss:0.07094079256057739\n",
      "Epoch 26, Batch 196 Loss:0.08200091868638992\n",
      "Epoch 26, Batch 197 Loss:0.06208682060241699\n",
      "Epoch 26, Batch 198 Loss:0.0785553902387619\n",
      "Epoch 26, Batch 199 Loss:0.07793066650629044\n",
      "Epoch 26, Batch 200 Loss:0.10582877695560455\n",
      "Epoch 26, Batch 201 Loss:0.08457658439874649\n",
      "Epoch 26, Batch 202 Loss:0.10456565022468567\n",
      "Epoch 26, Batch 203 Loss:0.07181296497583389\n",
      "Epoch 26, Batch 204 Loss:0.09064841270446777\n",
      "Epoch 26, Batch 205 Loss:0.0498068667948246\n",
      "Epoch 26, Batch 206 Loss:0.11398608982563019\n",
      "Epoch 26, Batch 207 Loss:0.09978821873664856\n",
      "Epoch 26, Batch 208 Loss:0.08105126768350601\n",
      "Epoch 26, Batch 209 Loss:0.09330083429813385\n",
      "Epoch 26, Batch 210 Loss:0.10483035445213318\n",
      "Epoch 26, Batch 211 Loss:0.07205924391746521\n",
      "Epoch 26, Batch 212 Loss:0.07002238929271698\n",
      "Epoch 26, Batch 213 Loss:0.07781827449798584\n",
      "Epoch 26, Batch 214 Loss:0.0660359114408493\n",
      "Epoch 26, Batch 215 Loss:0.07926078140735626\n",
      "Epoch 26, Batch 216 Loss:0.06832659244537354\n",
      "Epoch 26, Batch 217 Loss:0.04187901318073273\n",
      "Epoch 26, Batch 218 Loss:0.07541225105524063\n",
      "Epoch 26, Batch 219 Loss:0.08213459700345993\n",
      "Epoch 26, Batch 220 Loss:0.10338498651981354\n",
      "Epoch 26, Batch 221 Loss:0.05243749916553497\n",
      "Epoch 26, Batch 222 Loss:0.056335389614105225\n",
      "Epoch 26, Batch 223 Loss:0.052529215812683105\n",
      "Epoch 26, Batch 224 Loss:0.06466741114854813\n",
      "Epoch 26, Batch 225 Loss:0.09002925455570221\n",
      "Epoch 26, Batch 226 Loss:0.08520574122667313\n",
      "Epoch 26, Batch 227 Loss:0.06394928693771362\n",
      "Epoch 26, Batch 228 Loss:0.06257908791303635\n",
      "Epoch 26, Batch 229 Loss:0.09049976617097855\n",
      "Epoch 26, Batch 230 Loss:0.07120455801486969\n",
      "Epoch 26, Batch 231 Loss:0.06574993580579758\n",
      "Epoch 26, Batch 232 Loss:0.05723575875163078\n",
      "Epoch 26, Batch 233 Loss:0.07275371253490448\n",
      "Loss in this Epoch is: 7.27537125349 %\n",
      "Accuracy in this Epoch is: 88.8700008392 %\n",
      "Epoch 27, Batch 0 Loss:0.0376998595893383\n",
      "Epoch 27, Batch 1 Loss:0.057160280644893646\n",
      "Epoch 27, Batch 2 Loss:0.04377347230911255\n",
      "Epoch 27, Batch 3 Loss:0.0584036149084568\n",
      "Epoch 27, Batch 4 Loss:0.053556643426418304\n",
      "Epoch 27, Batch 5 Loss:0.04237272962927818\n",
      "Epoch 27, Batch 6 Loss:0.044115811586380005\n",
      "Epoch 27, Batch 7 Loss:0.06494833528995514\n",
      "Epoch 27, Batch 8 Loss:0.04380076751112938\n",
      "Epoch 27, Batch 9 Loss:0.04027390107512474\n",
      "Epoch 27, Batch 10 Loss:0.03271375223994255\n",
      "Epoch 27, Batch 11 Loss:0.03942219913005829\n",
      "Epoch 27, Batch 12 Loss:0.03149742633104324\n",
      "Epoch 27, Batch 13 Loss:0.03807886689901352\n",
      "Epoch 27, Batch 14 Loss:0.045853812247514725\n",
      "Epoch 27, Batch 15 Loss:0.05795208737254143\n",
      "Epoch 27, Batch 16 Loss:0.05033310875296593\n",
      "Epoch 27, Batch 17 Loss:0.028370007872581482\n",
      "Epoch 27, Batch 18 Loss:0.028976496309041977\n",
      "Epoch 27, Batch 19 Loss:0.04501156136393547\n",
      "Epoch 27, Batch 20 Loss:0.025398118421435356\n",
      "Epoch 27, Batch 21 Loss:0.05103204399347305\n",
      "Epoch 27, Batch 22 Loss:0.05825565755367279\n",
      "Epoch 27, Batch 23 Loss:0.05578871816396713\n",
      "Epoch 27, Batch 24 Loss:0.02623273991048336\n",
      "Epoch 27, Batch 25 Loss:0.045205991715192795\n",
      "Epoch 27, Batch 26 Loss:0.021594874560832977\n",
      "Epoch 27, Batch 27 Loss:0.06739991903305054\n",
      "Epoch 27, Batch 28 Loss:0.0853128507733345\n",
      "Epoch 27, Batch 29 Loss:0.05564499646425247\n",
      "Epoch 27, Batch 30 Loss:0.01884865015745163\n",
      "Epoch 27, Batch 31 Loss:0.07474441826343536\n",
      "Epoch 27, Batch 32 Loss:0.044222392141819\n",
      "Epoch 27, Batch 33 Loss:0.0494566410779953\n",
      "Epoch 27, Batch 34 Loss:0.02847219444811344\n",
      "Epoch 27, Batch 35 Loss:0.02214609459042549\n",
      "Epoch 27, Batch 36 Loss:0.06540137529373169\n",
      "Epoch 27, Batch 37 Loss:0.019661881029605865\n",
      "Epoch 27, Batch 38 Loss:0.08436088263988495\n",
      "Epoch 27, Batch 39 Loss:0.029711918905377388\n",
      "Epoch 27, Batch 40 Loss:0.05957537516951561\n",
      "Epoch 27, Batch 41 Loss:0.02612806111574173\n",
      "Epoch 27, Batch 42 Loss:0.08168652653694153\n",
      "Epoch 27, Batch 43 Loss:0.05358278006315231\n",
      "Epoch 27, Batch 44 Loss:0.07212352752685547\n",
      "Epoch 27, Batch 45 Loss:0.07332431524991989\n",
      "Epoch 27, Batch 46 Loss:0.057430513203144073\n",
      "Epoch 27, Batch 47 Loss:0.06668107211589813\n",
      "Epoch 27, Batch 48 Loss:0.08947477489709854\n",
      "Epoch 27, Batch 49 Loss:0.06258504837751389\n",
      "Epoch 27, Batch 50 Loss:0.08501139283180237\n",
      "Epoch 27, Batch 51 Loss:0.04363499581813812\n",
      "Epoch 27, Batch 52 Loss:0.0548425130546093\n",
      "Epoch 27, Batch 53 Loss:0.07946816086769104\n",
      "Epoch 27, Batch 54 Loss:0.0918046310544014\n",
      "Epoch 27, Batch 55 Loss:0.08403447270393372\n",
      "Epoch 27, Batch 56 Loss:0.09954409301280975\n",
      "Epoch 27, Batch 57 Loss:0.03810691460967064\n",
      "Epoch 27, Batch 58 Loss:0.04343061149120331\n",
      "Epoch 27, Batch 59 Loss:0.10190218687057495\n",
      "Epoch 27, Batch 60 Loss:0.07289756834506989\n",
      "Epoch 27, Batch 61 Loss:0.08209405094385147\n",
      "Epoch 27, Batch 62 Loss:0.08649127930402756\n",
      "Epoch 27, Batch 63 Loss:0.0600198395550251\n",
      "Epoch 27, Batch 64 Loss:0.07174695283174515\n",
      "Epoch 27, Batch 65 Loss:0.04888143390417099\n",
      "Epoch 27, Batch 66 Loss:0.0768376961350441\n",
      "Epoch 27, Batch 67 Loss:0.045401524752378464\n",
      "Epoch 27, Batch 68 Loss:0.029339026659727097\n",
      "Epoch 27, Batch 69 Loss:0.05379389598965645\n",
      "Epoch 27, Batch 70 Loss:0.08286558091640472\n",
      "Epoch 27, Batch 71 Loss:0.10783658176660538\n",
      "Epoch 27, Batch 72 Loss:0.0825248584151268\n",
      "Epoch 27, Batch 73 Loss:0.070498526096344\n",
      "Epoch 27, Batch 74 Loss:0.04506513848900795\n",
      "Epoch 27, Batch 75 Loss:0.057288896292448044\n",
      "Epoch 27, Batch 76 Loss:0.04426683858036995\n",
      "Epoch 27, Batch 77 Loss:0.04726705327630043\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Batch 78 Loss:0.05780075490474701\n",
      "Epoch 27, Batch 79 Loss:0.05288959667086601\n",
      "Epoch 27, Batch 80 Loss:0.0717456042766571\n",
      "Epoch 27, Batch 81 Loss:0.053288936614990234\n",
      "Epoch 27, Batch 82 Loss:0.07834372669458389\n",
      "Epoch 27, Batch 83 Loss:0.042665645480155945\n",
      "Epoch 27, Batch 84 Loss:0.09734032303094864\n",
      "Epoch 27, Batch 85 Loss:0.06600348651409149\n",
      "Epoch 27, Batch 86 Loss:0.06969700753688812\n",
      "Epoch 27, Batch 87 Loss:0.07265252619981766\n",
      "Epoch 27, Batch 88 Loss:0.0583907850086689\n",
      "Epoch 27, Batch 89 Loss:0.10822083055973053\n",
      "Epoch 27, Batch 90 Loss:0.039397191256284714\n",
      "Epoch 27, Batch 91 Loss:0.11879218369722366\n",
      "Epoch 27, Batch 92 Loss:0.06225205585360527\n",
      "Epoch 27, Batch 93 Loss:0.05602188780903816\n",
      "Epoch 27, Batch 94 Loss:0.06764809042215347\n",
      "Epoch 27, Batch 95 Loss:0.0630151554942131\n",
      "Epoch 27, Batch 96 Loss:0.06999525427818298\n",
      "Epoch 27, Batch 97 Loss:0.07819317281246185\n",
      "Epoch 27, Batch 98 Loss:0.04564102739095688\n",
      "Epoch 27, Batch 99 Loss:0.04983348771929741\n",
      "Epoch 27, Batch 100 Loss:0.06407706439495087\n",
      "Epoch 27, Batch 101 Loss:0.054960791021585464\n",
      "Epoch 27, Batch 102 Loss:0.06169711425900459\n",
      "Epoch 27, Batch 103 Loss:0.04717591404914856\n",
      "Epoch 27, Batch 104 Loss:0.03962454944849014\n",
      "Epoch 27, Batch 105 Loss:0.044316548854112625\n",
      "Epoch 27, Batch 106 Loss:0.05140245705842972\n",
      "Epoch 27, Batch 107 Loss:0.07307930290699005\n",
      "Epoch 27, Batch 108 Loss:0.052585795521736145\n",
      "Epoch 27, Batch 109 Loss:0.08760371804237366\n",
      "Epoch 27, Batch 110 Loss:0.05882524698972702\n",
      "Epoch 27, Batch 111 Loss:0.0470108762383461\n",
      "Epoch 27, Batch 112 Loss:0.03210846334695816\n",
      "Epoch 27, Batch 113 Loss:0.07327038794755936\n",
      "Epoch 27, Batch 114 Loss:0.11072345077991486\n",
      "Epoch 27, Batch 115 Loss:0.0357869453728199\n",
      "Epoch 27, Batch 116 Loss:0.052268076688051224\n",
      "Epoch 27, Batch 117 Loss:0.06210213154554367\n",
      "Epoch 27, Batch 118 Loss:0.0742146223783493\n",
      "Epoch 27, Batch 119 Loss:0.06390464305877686\n",
      "Epoch 27, Batch 120 Loss:0.11579754203557968\n",
      "Epoch 27, Batch 121 Loss:0.06743326783180237\n",
      "Epoch 27, Batch 122 Loss:0.0894332081079483\n",
      "Epoch 27, Batch 123 Loss:0.03221786022186279\n",
      "Epoch 27, Batch 124 Loss:0.13497453927993774\n",
      "Epoch 27, Batch 125 Loss:0.05793160945177078\n",
      "Epoch 27, Batch 126 Loss:0.0813908576965332\n",
      "Epoch 27, Batch 127 Loss:0.061179615557193756\n",
      "Epoch 27, Batch 128 Loss:0.04625001177191734\n",
      "Epoch 27, Batch 129 Loss:0.10263004153966904\n",
      "Epoch 27, Batch 130 Loss:0.06954890489578247\n",
      "Epoch 27, Batch 131 Loss:0.06404764950275421\n",
      "Epoch 27, Batch 132 Loss:0.06542468070983887\n",
      "Epoch 27, Batch 133 Loss:0.03773270174860954\n",
      "Epoch 27, Batch 134 Loss:0.07698570191860199\n",
      "Epoch 27, Batch 135 Loss:0.0526403933763504\n",
      "Epoch 27, Batch 136 Loss:0.05833369866013527\n",
      "Epoch 27, Batch 137 Loss:0.08061404526233673\n",
      "Epoch 27, Batch 138 Loss:0.04613574594259262\n",
      "Epoch 27, Batch 139 Loss:0.03109271079301834\n",
      "Epoch 27, Batch 140 Loss:0.033866189420223236\n",
      "Epoch 27, Batch 141 Loss:0.03571636602282524\n",
      "Epoch 27, Batch 142 Loss:0.08323803544044495\n",
      "Epoch 27, Batch 143 Loss:0.03501799702644348\n",
      "Epoch 27, Batch 144 Loss:0.05186467617750168\n",
      "Epoch 27, Batch 145 Loss:0.03757148236036301\n",
      "Epoch 27, Batch 146 Loss:0.06791258603334427\n",
      "Epoch 27, Batch 147 Loss:0.028441190719604492\n",
      "Epoch 27, Batch 148 Loss:0.032930657267570496\n",
      "Epoch 27, Batch 149 Loss:0.058906327933073044\n",
      "Epoch 27, Batch 150 Loss:0.06090395897626877\n",
      "Epoch 27, Batch 151 Loss:0.08163247257471085\n",
      "Epoch 27, Batch 152 Loss:0.08602574467658997\n",
      "Epoch 27, Batch 153 Loss:0.0834687128663063\n",
      "Epoch 27, Batch 154 Loss:0.04520176351070404\n",
      "Epoch 27, Batch 155 Loss:0.058167021721601486\n",
      "Epoch 27, Batch 156 Loss:0.0522744357585907\n",
      "Epoch 27, Batch 157 Loss:0.06747686862945557\n",
      "Epoch 27, Batch 158 Loss:0.03150450438261032\n",
      "Epoch 27, Batch 159 Loss:0.04077239707112312\n",
      "Epoch 27, Batch 160 Loss:0.050474733114242554\n",
      "Epoch 27, Batch 161 Loss:0.07097166031599045\n",
      "Epoch 27, Batch 162 Loss:0.04645213484764099\n",
      "Epoch 27, Batch 163 Loss:0.03614998608827591\n",
      "Epoch 27, Batch 164 Loss:0.059914588928222656\n",
      "Epoch 27, Batch 165 Loss:0.04586663842201233\n",
      "Epoch 27, Batch 166 Loss:0.041611962020397186\n",
      "Epoch 27, Batch 167 Loss:0.09081023931503296\n",
      "Epoch 27, Batch 168 Loss:0.07083232700824738\n",
      "Epoch 27, Batch 169 Loss:0.07044072449207306\n",
      "Epoch 27, Batch 170 Loss:0.06632748246192932\n",
      "Epoch 27, Batch 171 Loss:0.04976688697934151\n",
      "Epoch 27, Batch 172 Loss:0.058018192648887634\n",
      "Epoch 27, Batch 173 Loss:0.08034157007932663\n",
      "Epoch 27, Batch 174 Loss:0.06972132623195648\n",
      "Epoch 27, Batch 175 Loss:0.03889869526028633\n",
      "Epoch 27, Batch 176 Loss:0.1236286461353302\n",
      "Epoch 27, Batch 177 Loss:0.10924512147903442\n",
      "Epoch 27, Batch 178 Loss:0.08328037708997726\n",
      "Epoch 27, Batch 179 Loss:0.04679583013057709\n",
      "Epoch 27, Batch 180 Loss:0.08582081645727158\n",
      "Epoch 27, Batch 181 Loss:0.09598761796951294\n",
      "Epoch 27, Batch 182 Loss:0.038400646299123764\n",
      "Epoch 27, Batch 183 Loss:0.05936077609658241\n",
      "Epoch 27, Batch 184 Loss:0.09184467792510986\n",
      "Epoch 27, Batch 185 Loss:0.10237869620323181\n",
      "Epoch 27, Batch 186 Loss:0.06934116035699844\n",
      "Epoch 27, Batch 187 Loss:0.05595334619283676\n",
      "Epoch 27, Batch 188 Loss:0.06363756954669952\n",
      "Epoch 27, Batch 189 Loss:0.08966369926929474\n",
      "Epoch 27, Batch 190 Loss:0.0717536062002182\n",
      "Epoch 27, Batch 191 Loss:0.06390701234340668\n",
      "Epoch 27, Batch 192 Loss:0.08707645535469055\n",
      "Epoch 27, Batch 193 Loss:0.07515805959701538\n",
      "Epoch 27, Batch 194 Loss:0.1244148537516594\n",
      "Epoch 27, Batch 195 Loss:0.08242247998714447\n",
      "Epoch 27, Batch 196 Loss:0.10778634250164032\n",
      "Epoch 27, Batch 197 Loss:0.1273851990699768\n",
      "Epoch 27, Batch 198 Loss:0.08179031312465668\n",
      "Epoch 27, Batch 199 Loss:0.09079983830451965\n",
      "Epoch 27, Batch 200 Loss:0.06050547957420349\n",
      "Epoch 27, Batch 201 Loss:0.08963760733604431\n",
      "Epoch 27, Batch 202 Loss:0.10291405022144318\n",
      "Epoch 27, Batch 203 Loss:0.11424508690834045\n",
      "Epoch 27, Batch 204 Loss:0.098356693983078\n",
      "Epoch 27, Batch 205 Loss:0.06686048209667206\n",
      "Epoch 27, Batch 206 Loss:0.07212398201227188\n",
      "Epoch 27, Batch 207 Loss:0.09732604026794434\n",
      "Epoch 27, Batch 208 Loss:0.13442200422286987\n",
      "Epoch 27, Batch 209 Loss:0.08992130309343338\n",
      "Epoch 27, Batch 210 Loss:0.10452502965927124\n",
      "Epoch 27, Batch 211 Loss:0.07724837958812714\n",
      "Epoch 27, Batch 212 Loss:0.07148519903421402\n",
      "Epoch 27, Batch 213 Loss:0.13549058139324188\n",
      "Epoch 27, Batch 214 Loss:0.09333270788192749\n",
      "Epoch 27, Batch 215 Loss:0.06725908815860748\n",
      "Epoch 27, Batch 216 Loss:0.08539937436580658\n",
      "Epoch 27, Batch 217 Loss:0.06342387199401855\n",
      "Epoch 27, Batch 218 Loss:0.06022615358233452\n",
      "Epoch 27, Batch 219 Loss:0.07206233590841293\n",
      "Epoch 27, Batch 220 Loss:0.07793430984020233\n",
      "Epoch 27, Batch 221 Loss:0.028954237699508667\n",
      "Epoch 27, Batch 222 Loss:0.06443771719932556\n",
      "Epoch 27, Batch 223 Loss:0.058041349053382874\n",
      "Epoch 27, Batch 224 Loss:0.045480187982320786\n",
      "Epoch 27, Batch 225 Loss:0.04533149302005768\n",
      "Epoch 27, Batch 226 Loss:0.1037016212940216\n",
      "Epoch 27, Batch 227 Loss:0.05828918144106865\n",
      "Epoch 27, Batch 228 Loss:0.07352801412343979\n",
      "Epoch 27, Batch 229 Loss:0.03419734537601471\n",
      "Epoch 27, Batch 230 Loss:0.04714086651802063\n",
      "Epoch 27, Batch 231 Loss:0.08882022649049759\n",
      "Epoch 27, Batch 232 Loss:0.0964963287115097\n",
      "Epoch 27, Batch 233 Loss:0.07353491336107254\n",
      "Loss in this Epoch is: 7.35349133611 %\n",
      "Accuracy in this Epoch is: 88.5999977589 %\n",
      "Epoch 28, Batch 0 Loss:0.04626674950122833\n",
      "Epoch 28, Batch 1 Loss:0.07152331620454788\n",
      "Epoch 28, Batch 2 Loss:0.046086009591817856\n",
      "Epoch 28, Batch 3 Loss:0.0598880909383297\n",
      "Epoch 28, Batch 4 Loss:0.09656315296888351\n",
      "Epoch 28, Batch 5 Loss:0.06761139631271362\n",
      "Epoch 28, Batch 6 Loss:0.06006955727934837\n",
      "Epoch 28, Batch 7 Loss:0.09896223247051239\n",
      "Epoch 28, Batch 8 Loss:0.057945359498262405\n",
      "Epoch 28, Batch 9 Loss:0.11969766765832901\n",
      "Epoch 28, Batch 10 Loss:0.10565792769193649\n",
      "Epoch 28, Batch 11 Loss:0.05894427374005318\n",
      "Epoch 28, Batch 12 Loss:0.0789620578289032\n",
      "Epoch 28, Batch 13 Loss:0.045923009514808655\n",
      "Epoch 28, Batch 14 Loss:0.056442756205797195\n",
      "Epoch 28, Batch 15 Loss:0.05563676729798317\n",
      "Epoch 28, Batch 16 Loss:0.052603669464588165\n",
      "Epoch 28, Batch 17 Loss:0.04887547343969345\n",
      "Epoch 28, Batch 18 Loss:0.08511621505022049\n",
      "Epoch 28, Batch 19 Loss:0.07566791027784348\n",
      "Epoch 28, Batch 20 Loss:0.03695731610059738\n",
      "Epoch 28, Batch 21 Loss:0.10202107578516006\n",
      "Epoch 28, Batch 22 Loss:0.09228210896253586\n",
      "Epoch 28, Batch 23 Loss:0.03398162126541138\n",
      "Epoch 28, Batch 24 Loss:0.06092725321650505\n",
      "Epoch 28, Batch 25 Loss:0.07497165352106094\n",
      "Epoch 28, Batch 26 Loss:0.046108756214380264\n",
      "Epoch 28, Batch 27 Loss:0.06081242114305496\n",
      "Epoch 28, Batch 28 Loss:0.0638895034790039\n",
      "Epoch 28, Batch 29 Loss:0.04334253445267677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 30 Loss:0.10818608850240707\n",
      "Epoch 28, Batch 31 Loss:0.0800604373216629\n",
      "Epoch 28, Batch 32 Loss:0.05324799194931984\n",
      "Epoch 28, Batch 33 Loss:0.05668610334396362\n",
      "Epoch 28, Batch 34 Loss:0.03683633357286453\n",
      "Epoch 28, Batch 35 Loss:0.08834169059991837\n",
      "Epoch 28, Batch 36 Loss:0.1156175509095192\n",
      "Epoch 28, Batch 37 Loss:0.05011845752596855\n",
      "Epoch 28, Batch 38 Loss:0.050058525055646896\n",
      "Epoch 28, Batch 39 Loss:0.05444759503006935\n",
      "Epoch 28, Batch 40 Loss:0.06110977381467819\n",
      "Epoch 28, Batch 41 Loss:0.04304379224777222\n",
      "Epoch 28, Batch 42 Loss:0.046473000198602676\n",
      "Epoch 28, Batch 43 Loss:0.05149056389927864\n",
      "Epoch 28, Batch 44 Loss:0.04627538099884987\n",
      "Epoch 28, Batch 45 Loss:0.06546066701412201\n",
      "Epoch 28, Batch 46 Loss:0.06190909445285797\n",
      "Epoch 28, Batch 47 Loss:0.06773200631141663\n",
      "Epoch 28, Batch 48 Loss:0.08221318572759628\n",
      "Epoch 28, Batch 49 Loss:0.05784113332629204\n",
      "Epoch 28, Batch 50 Loss:0.05928095802664757\n",
      "Epoch 28, Batch 51 Loss:0.040126144886016846\n",
      "Epoch 28, Batch 52 Loss:0.03349427133798599\n",
      "Epoch 28, Batch 53 Loss:0.06610617786645889\n",
      "Epoch 28, Batch 54 Loss:0.054386600852012634\n",
      "Epoch 28, Batch 55 Loss:0.05325944721698761\n",
      "Epoch 28, Batch 56 Loss:0.09181714802980423\n",
      "Epoch 28, Batch 57 Loss:0.07617781311273575\n",
      "Epoch 28, Batch 58 Loss:0.044627949595451355\n",
      "Epoch 28, Batch 59 Loss:0.05743828043341637\n",
      "Epoch 28, Batch 60 Loss:0.04908113181591034\n",
      "Epoch 28, Batch 61 Loss:0.06681887060403824\n",
      "Epoch 28, Batch 62 Loss:0.04437475651502609\n",
      "Epoch 28, Batch 63 Loss:0.04374672472476959\n",
      "Epoch 28, Batch 64 Loss:0.06070183590054512\n",
      "Epoch 28, Batch 65 Loss:0.03776288032531738\n",
      "Epoch 28, Batch 66 Loss:0.07544281333684921\n",
      "Epoch 28, Batch 67 Loss:0.028047598898410797\n",
      "Epoch 28, Batch 68 Loss:0.07434730976819992\n",
      "Epoch 28, Batch 69 Loss:0.059020351618528366\n",
      "Epoch 28, Batch 70 Loss:0.058533262461423874\n",
      "Epoch 28, Batch 71 Loss:0.0420510433614254\n",
      "Epoch 28, Batch 72 Loss:0.07768061757087708\n",
      "Epoch 28, Batch 73 Loss:0.09518731385469437\n",
      "Epoch 28, Batch 74 Loss:0.0432264544069767\n",
      "Epoch 28, Batch 75 Loss:0.04203440248966217\n",
      "Epoch 28, Batch 76 Loss:0.06859476119279861\n",
      "Epoch 28, Batch 77 Loss:0.06150687858462334\n",
      "Epoch 28, Batch 78 Loss:0.02823568880558014\n",
      "Epoch 28, Batch 79 Loss:0.0381811149418354\n",
      "Epoch 28, Batch 80 Loss:0.07681585103273392\n",
      "Epoch 28, Batch 81 Loss:0.04321414232254028\n",
      "Epoch 28, Batch 82 Loss:0.07488986104726791\n",
      "Epoch 28, Batch 83 Loss:0.04520229622721672\n",
      "Epoch 28, Batch 84 Loss:0.045625366270542145\n",
      "Epoch 28, Batch 85 Loss:0.08320527523756027\n",
      "Epoch 28, Batch 86 Loss:0.03449220210313797\n",
      "Epoch 28, Batch 87 Loss:0.03240855038166046\n",
      "Epoch 28, Batch 88 Loss:0.031806424260139465\n",
      "Epoch 28, Batch 89 Loss:0.04506902024149895\n",
      "Epoch 28, Batch 90 Loss:0.057419490069150925\n",
      "Epoch 28, Batch 91 Loss:0.0505477599799633\n",
      "Epoch 28, Batch 92 Loss:0.09974630177021027\n",
      "Epoch 28, Batch 93 Loss:0.10137435793876648\n",
      "Epoch 28, Batch 94 Loss:0.06512907892465591\n",
      "Epoch 28, Batch 95 Loss:0.06084902212023735\n",
      "Epoch 28, Batch 96 Loss:0.05299167335033417\n",
      "Epoch 28, Batch 97 Loss:0.050518736243247986\n",
      "Epoch 28, Batch 98 Loss:0.05184263736009598\n",
      "Epoch 28, Batch 99 Loss:0.03392832726240158\n",
      "Epoch 28, Batch 100 Loss:0.07815414667129517\n",
      "Epoch 28, Batch 101 Loss:0.05624699592590332\n",
      "Epoch 28, Batch 102 Loss:0.06957856565713882\n",
      "Epoch 28, Batch 103 Loss:0.05481008440256119\n",
      "Epoch 28, Batch 104 Loss:0.07868602871894836\n",
      "Epoch 28, Batch 105 Loss:0.06535348296165466\n",
      "Epoch 28, Batch 106 Loss:0.045553989708423615\n",
      "Epoch 28, Batch 107 Loss:0.04296501353383064\n",
      "Epoch 28, Batch 108 Loss:0.05638318881392479\n",
      "Epoch 28, Batch 109 Loss:0.052104651927948\n",
      "Epoch 28, Batch 110 Loss:0.07178103923797607\n",
      "Epoch 28, Batch 111 Loss:0.036423083394765854\n",
      "Epoch 28, Batch 112 Loss:0.0921064168214798\n",
      "Epoch 28, Batch 113 Loss:0.06641297787427902\n",
      "Epoch 28, Batch 114 Loss:0.04921456426382065\n",
      "Epoch 28, Batch 115 Loss:0.06281109154224396\n",
      "Epoch 28, Batch 116 Loss:0.02702243998646736\n",
      "Epoch 28, Batch 117 Loss:0.06522875279188156\n",
      "Epoch 28, Batch 118 Loss:0.05408059433102608\n",
      "Epoch 28, Batch 119 Loss:0.06997700035572052\n",
      "Epoch 28, Batch 120 Loss:0.09095163643360138\n",
      "Epoch 28, Batch 121 Loss:0.060968443751335144\n",
      "Epoch 28, Batch 122 Loss:0.02883787825703621\n",
      "Epoch 28, Batch 123 Loss:0.047240398824214935\n",
      "Epoch 28, Batch 124 Loss:0.05210074409842491\n",
      "Epoch 28, Batch 125 Loss:0.0817323625087738\n",
      "Epoch 28, Batch 126 Loss:0.0699029266834259\n",
      "Epoch 28, Batch 127 Loss:0.06566356867551804\n",
      "Epoch 28, Batch 128 Loss:0.04554129019379616\n",
      "Epoch 28, Batch 129 Loss:0.04633466526865959\n",
      "Epoch 28, Batch 130 Loss:0.046523839235305786\n",
      "Epoch 28, Batch 131 Loss:0.06476295739412308\n",
      "Epoch 28, Batch 132 Loss:0.06554525345563889\n",
      "Epoch 28, Batch 133 Loss:0.04679999127984047\n",
      "Epoch 28, Batch 134 Loss:0.033525194972753525\n",
      "Epoch 28, Batch 135 Loss:0.07219845056533813\n",
      "Epoch 28, Batch 136 Loss:0.05741749703884125\n",
      "Epoch 28, Batch 137 Loss:0.04167477786540985\n",
      "Epoch 28, Batch 138 Loss:0.04270286113023758\n",
      "Epoch 28, Batch 139 Loss:0.037417877465486526\n",
      "Epoch 28, Batch 140 Loss:0.06993784755468369\n",
      "Epoch 28, Batch 141 Loss:0.05547664314508438\n",
      "Epoch 28, Batch 142 Loss:0.02913207933306694\n",
      "Epoch 28, Batch 143 Loss:0.039236851036548615\n",
      "Epoch 28, Batch 144 Loss:0.030795514583587646\n",
      "Epoch 28, Batch 145 Loss:0.037771742790937424\n",
      "Epoch 28, Batch 146 Loss:0.051153331995010376\n",
      "Epoch 28, Batch 147 Loss:0.05418732389807701\n",
      "Epoch 28, Batch 148 Loss:0.06669211387634277\n",
      "Epoch 28, Batch 149 Loss:0.07171723246574402\n",
      "Epoch 28, Batch 150 Loss:0.05093272402882576\n",
      "Epoch 28, Batch 151 Loss:0.021792249754071236\n",
      "Epoch 28, Batch 152 Loss:0.041560277342796326\n",
      "Epoch 28, Batch 153 Loss:0.029081758111715317\n",
      "Epoch 28, Batch 154 Loss:0.0626884400844574\n",
      "Epoch 28, Batch 155 Loss:0.0537499338388443\n",
      "Epoch 28, Batch 156 Loss:0.03585277497768402\n",
      "Epoch 28, Batch 157 Loss:0.0403212308883667\n",
      "Epoch 28, Batch 158 Loss:0.06435200572013855\n",
      "Epoch 28, Batch 159 Loss:0.04769189655780792\n",
      "Epoch 28, Batch 160 Loss:0.02907363511621952\n",
      "Epoch 28, Batch 161 Loss:0.0645471066236496\n",
      "Epoch 28, Batch 162 Loss:0.057291075587272644\n",
      "Epoch 28, Batch 163 Loss:0.06825312227010727\n",
      "Epoch 28, Batch 164 Loss:0.06841236352920532\n",
      "Epoch 28, Batch 165 Loss:0.08356870710849762\n",
      "Epoch 28, Batch 166 Loss:0.09094977378845215\n",
      "Epoch 28, Batch 167 Loss:0.062043920159339905\n",
      "Epoch 28, Batch 168 Loss:0.05215199291706085\n",
      "Epoch 28, Batch 169 Loss:0.05616983026266098\n",
      "Epoch 28, Batch 170 Loss:0.06589669734239578\n",
      "Epoch 28, Batch 171 Loss:0.0902571901679039\n",
      "Epoch 28, Batch 172 Loss:0.07036430388689041\n",
      "Epoch 28, Batch 173 Loss:0.08290267735719681\n",
      "Epoch 28, Batch 174 Loss:0.0775829330086708\n",
      "Epoch 28, Batch 175 Loss:0.08136281371116638\n",
      "Epoch 28, Batch 176 Loss:0.07616862654685974\n",
      "Epoch 28, Batch 177 Loss:0.08967263996601105\n",
      "Epoch 28, Batch 178 Loss:0.06813989579677582\n",
      "Epoch 28, Batch 179 Loss:0.0546959713101387\n",
      "Epoch 28, Batch 180 Loss:0.05838855728507042\n",
      "Epoch 28, Batch 181 Loss:0.06089143082499504\n",
      "Epoch 28, Batch 182 Loss:0.05938288941979408\n",
      "Epoch 28, Batch 183 Loss:0.06770993769168854\n",
      "Epoch 28, Batch 184 Loss:0.07329487055540085\n",
      "Epoch 28, Batch 185 Loss:0.0761146992444992\n",
      "Epoch 28, Batch 186 Loss:0.08909974247217178\n",
      "Epoch 28, Batch 187 Loss:0.025340843945741653\n",
      "Epoch 28, Batch 188 Loss:0.02857268787920475\n",
      "Epoch 28, Batch 189 Loss:0.043999169021844864\n",
      "Epoch 28, Batch 190 Loss:0.06010427325963974\n",
      "Epoch 28, Batch 191 Loss:0.11699250340461731\n",
      "Epoch 28, Batch 192 Loss:0.02693920023739338\n",
      "Epoch 28, Batch 193 Loss:0.053954292088747025\n",
      "Epoch 28, Batch 194 Loss:0.046665314584970474\n",
      "Epoch 28, Batch 195 Loss:0.046430911868810654\n",
      "Epoch 28, Batch 196 Loss:0.04103308543562889\n",
      "Epoch 28, Batch 197 Loss:0.050435036420822144\n",
      "Epoch 28, Batch 198 Loss:0.02798832207918167\n",
      "Epoch 28, Batch 199 Loss:0.02345278486609459\n",
      "Epoch 28, Batch 200 Loss:0.08806666731834412\n",
      "Epoch 28, Batch 201 Loss:0.04246314987540245\n",
      "Epoch 28, Batch 202 Loss:0.0867251306772232\n",
      "Epoch 28, Batch 203 Loss:0.08153093606233597\n",
      "Epoch 28, Batch 204 Loss:0.05978156998753548\n",
      "Epoch 28, Batch 205 Loss:0.05794401466846466\n",
      "Epoch 28, Batch 206 Loss:0.04150591790676117\n",
      "Epoch 28, Batch 207 Loss:0.06140375882387161\n",
      "Epoch 28, Batch 208 Loss:0.06301318854093552\n",
      "Epoch 28, Batch 209 Loss:0.058870673179626465\n",
      "Epoch 28, Batch 210 Loss:0.052621301263570786\n",
      "Epoch 28, Batch 211 Loss:0.03910475969314575\n",
      "Epoch 28, Batch 212 Loss:0.06628341972827911\n",
      "Epoch 28, Batch 213 Loss:0.08282651007175446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Batch 214 Loss:0.04285995662212372\n",
      "Epoch 28, Batch 215 Loss:0.05568266659975052\n",
      "Epoch 28, Batch 216 Loss:0.0637333020567894\n",
      "Epoch 28, Batch 217 Loss:0.0746351033449173\n",
      "Epoch 28, Batch 218 Loss:0.06965841352939606\n",
      "Epoch 28, Batch 219 Loss:0.05459184572100639\n",
      "Epoch 28, Batch 220 Loss:0.07439621537923813\n",
      "Epoch 28, Batch 221 Loss:0.03339861333370209\n",
      "Epoch 28, Batch 222 Loss:0.04242825508117676\n",
      "Epoch 28, Batch 223 Loss:0.07029514014720917\n",
      "Epoch 28, Batch 224 Loss:0.07780420780181885\n",
      "Epoch 28, Batch 225 Loss:0.05214514210820198\n",
      "Epoch 28, Batch 226 Loss:0.04764099791646004\n",
      "Epoch 28, Batch 227 Loss:0.09293641895055771\n",
      "Epoch 28, Batch 228 Loss:0.08327242732048035\n",
      "Epoch 28, Batch 229 Loss:0.062189869582653046\n",
      "Epoch 28, Batch 230 Loss:0.10759583115577698\n",
      "Epoch 28, Batch 231 Loss:0.08470909297466278\n",
      "Epoch 28, Batch 232 Loss:0.07527736574411392\n",
      "Epoch 28, Batch 233 Loss:0.06208597868680954\n",
      "Loss in this Epoch is: 6.20859786868 %\n",
      "Accuracy in this Epoch is: 88.7099981308 %\n",
      "Epoch 29, Batch 0 Loss:0.044076867401599884\n",
      "Epoch 29, Batch 1 Loss:0.0580071285367012\n",
      "Epoch 29, Batch 2 Loss:0.07539985328912735\n",
      "Epoch 29, Batch 3 Loss:0.05460675060749054\n",
      "Epoch 29, Batch 4 Loss:0.05851536989212036\n",
      "Epoch 29, Batch 5 Loss:0.053026337176561356\n",
      "Epoch 29, Batch 6 Loss:0.04084743931889534\n",
      "Epoch 29, Batch 7 Loss:0.02340829186141491\n",
      "Epoch 29, Batch 8 Loss:0.06643138080835342\n",
      "Epoch 29, Batch 9 Loss:0.06719179451465607\n",
      "Epoch 29, Batch 10 Loss:0.057350847870111465\n",
      "Epoch 29, Batch 11 Loss:0.059069667011499405\n",
      "Epoch 29, Batch 12 Loss:0.04471387341618538\n",
      "Epoch 29, Batch 13 Loss:0.05631226673722267\n",
      "Epoch 29, Batch 14 Loss:0.042451146990060806\n",
      "Epoch 29, Batch 15 Loss:0.02854764275252819\n",
      "Epoch 29, Batch 16 Loss:0.0733087807893753\n",
      "Epoch 29, Batch 17 Loss:0.05989547446370125\n",
      "Epoch 29, Batch 18 Loss:0.04199036955833435\n",
      "Epoch 29, Batch 19 Loss:0.03984741494059563\n",
      "Epoch 29, Batch 20 Loss:0.03734642639756203\n",
      "Epoch 29, Batch 21 Loss:0.04584159702062607\n",
      "Epoch 29, Batch 22 Loss:0.04667499288916588\n",
      "Epoch 29, Batch 23 Loss:0.027491625398397446\n",
      "Epoch 29, Batch 24 Loss:0.027721470221877098\n",
      "Epoch 29, Batch 25 Loss:0.034219253808259964\n",
      "Epoch 29, Batch 26 Loss:0.03093603253364563\n",
      "Epoch 29, Batch 27 Loss:0.042390353977680206\n",
      "Epoch 29, Batch 28 Loss:0.05296742543578148\n",
      "Epoch 29, Batch 29 Loss:0.01928390935063362\n",
      "Epoch 29, Batch 30 Loss:0.04928404092788696\n",
      "Epoch 29, Batch 31 Loss:0.047871168702840805\n",
      "Epoch 29, Batch 32 Loss:0.05009434372186661\n",
      "Epoch 29, Batch 33 Loss:0.0509956069290638\n",
      "Epoch 29, Batch 34 Loss:0.03056461177766323\n",
      "Epoch 29, Batch 35 Loss:0.031940001994371414\n",
      "Epoch 29, Batch 36 Loss:0.02040988579392433\n",
      "Epoch 29, Batch 37 Loss:0.06557872146368027\n",
      "Epoch 29, Batch 38 Loss:0.02523944154381752\n",
      "Epoch 29, Batch 39 Loss:0.07834611088037491\n",
      "Epoch 29, Batch 40 Loss:0.023149589076638222\n",
      "Epoch 29, Batch 41 Loss:0.06607362627983093\n",
      "Epoch 29, Batch 42 Loss:0.0417591892182827\n",
      "Epoch 29, Batch 43 Loss:0.05085238441824913\n",
      "Epoch 29, Batch 44 Loss:0.05592319741845131\n",
      "Epoch 29, Batch 45 Loss:0.014631004072725773\n",
      "Epoch 29, Batch 46 Loss:0.03350973129272461\n",
      "Epoch 29, Batch 47 Loss:0.03250358626246452\n",
      "Epoch 29, Batch 48 Loss:0.028370758518576622\n",
      "Epoch 29, Batch 49 Loss:0.04654519259929657\n",
      "Epoch 29, Batch 50 Loss:0.05353023484349251\n",
      "Epoch 29, Batch 51 Loss:0.056392062455415726\n",
      "Epoch 29, Batch 52 Loss:0.017720190808176994\n",
      "Epoch 29, Batch 53 Loss:0.06390078365802765\n",
      "Epoch 29, Batch 54 Loss:0.056636370718479156\n",
      "Epoch 29, Batch 55 Loss:0.02768605761229992\n",
      "Epoch 29, Batch 56 Loss:0.049267739057540894\n",
      "Epoch 29, Batch 57 Loss:0.024956928566098213\n",
      "Epoch 29, Batch 58 Loss:0.05475103482604027\n",
      "Epoch 29, Batch 59 Loss:0.02322394773364067\n",
      "Epoch 29, Batch 60 Loss:0.06515209376811981\n",
      "Epoch 29, Batch 61 Loss:0.04120714217424393\n",
      "Epoch 29, Batch 62 Loss:0.03644770383834839\n",
      "Epoch 29, Batch 63 Loss:0.04773189499974251\n",
      "Epoch 29, Batch 64 Loss:0.03430069983005524\n",
      "Epoch 29, Batch 65 Loss:0.027989186346530914\n",
      "Epoch 29, Batch 66 Loss:0.0296142790466547\n",
      "Epoch 29, Batch 67 Loss:0.051247432827949524\n",
      "Epoch 29, Batch 68 Loss:0.028245681896805763\n",
      "Epoch 29, Batch 69 Loss:0.03787515312433243\n",
      "Epoch 29, Batch 70 Loss:0.037898823618888855\n",
      "Epoch 29, Batch 71 Loss:0.0742986798286438\n",
      "Epoch 29, Batch 72 Loss:0.03986188769340515\n",
      "Epoch 29, Batch 73 Loss:0.04988677054643631\n",
      "Epoch 29, Batch 74 Loss:0.029629448428750038\n",
      "Epoch 29, Batch 75 Loss:0.0968373566865921\n",
      "Epoch 29, Batch 76 Loss:0.044142354279756546\n",
      "Epoch 29, Batch 77 Loss:0.04610311612486839\n",
      "Epoch 29, Batch 78 Loss:0.042621634900569916\n",
      "Epoch 29, Batch 79 Loss:0.05064732953906059\n",
      "Epoch 29, Batch 80 Loss:0.03455515578389168\n",
      "Epoch 29, Batch 81 Loss:0.04026221111416817\n",
      "Epoch 29, Batch 82 Loss:0.05295122414827347\n",
      "Epoch 29, Batch 83 Loss:0.04367123916745186\n",
      "Epoch 29, Batch 84 Loss:0.032882221043109894\n",
      "Epoch 29, Batch 85 Loss:0.02590661123394966\n",
      "Epoch 29, Batch 86 Loss:0.04357605427503586\n",
      "Epoch 29, Batch 87 Loss:0.05805783346295357\n",
      "Epoch 29, Batch 88 Loss:0.04190145060420036\n",
      "Epoch 29, Batch 89 Loss:0.033353570848703384\n",
      "Epoch 29, Batch 90 Loss:0.03742428496479988\n",
      "Epoch 29, Batch 91 Loss:0.04017812758684158\n",
      "Epoch 29, Batch 92 Loss:0.025015544146299362\n",
      "Epoch 29, Batch 93 Loss:0.03872385248541832\n",
      "Epoch 29, Batch 94 Loss:0.03187846392393112\n",
      "Epoch 29, Batch 95 Loss:0.05318724364042282\n",
      "Epoch 29, Batch 96 Loss:0.06096414476633072\n",
      "Epoch 29, Batch 97 Loss:0.052192091941833496\n",
      "Epoch 29, Batch 98 Loss:0.06484320014715195\n",
      "Epoch 29, Batch 99 Loss:0.03125060722231865\n",
      "Epoch 29, Batch 100 Loss:0.03806774318218231\n",
      "Epoch 29, Batch 101 Loss:0.034692056477069855\n",
      "Epoch 29, Batch 102 Loss:0.03700131177902222\n",
      "Epoch 29, Batch 103 Loss:0.06894553452730179\n",
      "Epoch 29, Batch 104 Loss:0.04198254272341728\n",
      "Epoch 29, Batch 105 Loss:0.04871387779712677\n",
      "Epoch 29, Batch 106 Loss:0.05152922496199608\n",
      "Epoch 29, Batch 107 Loss:0.037822287529706955\n",
      "Epoch 29, Batch 108 Loss:0.06294632703065872\n",
      "Epoch 29, Batch 109 Loss:0.04035179316997528\n",
      "Epoch 29, Batch 110 Loss:0.05201096460223198\n",
      "Epoch 29, Batch 111 Loss:0.04107875004410744\n",
      "Epoch 29, Batch 112 Loss:0.055784523487091064\n",
      "Epoch 29, Batch 113 Loss:0.06864920258522034\n",
      "Epoch 29, Batch 114 Loss:0.02813103049993515\n",
      "Epoch 29, Batch 115 Loss:0.058200903236866\n",
      "Epoch 29, Batch 116 Loss:0.0426289401948452\n",
      "Epoch 29, Batch 117 Loss:0.04802234470844269\n",
      "Epoch 29, Batch 118 Loss:0.08620209991931915\n",
      "Epoch 29, Batch 119 Loss:0.02949518896639347\n",
      "Epoch 29, Batch 120 Loss:0.04366709291934967\n",
      "Epoch 29, Batch 121 Loss:0.04061446338891983\n",
      "Epoch 29, Batch 122 Loss:0.07022416591644287\n",
      "Epoch 29, Batch 123 Loss:0.04754894599318504\n",
      "Epoch 29, Batch 124 Loss:0.06519482284784317\n",
      "Epoch 29, Batch 125 Loss:0.06379130482673645\n",
      "Epoch 29, Batch 126 Loss:0.02797022834420204\n",
      "Epoch 29, Batch 127 Loss:0.06180521100759506\n",
      "Epoch 29, Batch 128 Loss:0.04594407230615616\n",
      "Epoch 29, Batch 129 Loss:0.046796225011348724\n",
      "Epoch 29, Batch 130 Loss:0.0491652637720108\n",
      "Epoch 29, Batch 131 Loss:0.0570283941924572\n",
      "Epoch 29, Batch 132 Loss:0.02866290882229805\n",
      "Epoch 29, Batch 133 Loss:0.03703591227531433\n",
      "Epoch 29, Batch 134 Loss:0.06861268728971481\n",
      "Epoch 29, Batch 135 Loss:0.059013672173023224\n",
      "Epoch 29, Batch 136 Loss:0.03735889494419098\n",
      "Epoch 29, Batch 137 Loss:0.036220405250787735\n",
      "Epoch 29, Batch 138 Loss:0.05253862962126732\n",
      "Epoch 29, Batch 139 Loss:0.04104465991258621\n",
      "Epoch 29, Batch 140 Loss:0.04404185712337494\n",
      "Epoch 29, Batch 141 Loss:0.06594695895910263\n",
      "Epoch 29, Batch 142 Loss:0.06428486108779907\n",
      "Epoch 29, Batch 143 Loss:0.04699785262346268\n",
      "Epoch 29, Batch 144 Loss:0.09694905579090118\n",
      "Epoch 29, Batch 145 Loss:0.04202613979578018\n",
      "Epoch 29, Batch 146 Loss:0.05764634162187576\n",
      "Epoch 29, Batch 147 Loss:0.06895917654037476\n",
      "Epoch 29, Batch 148 Loss:0.02338719367980957\n",
      "Epoch 29, Batch 149 Loss:0.02077994868159294\n",
      "Epoch 29, Batch 150 Loss:0.06915763020515442\n",
      "Epoch 29, Batch 151 Loss:0.03403525799512863\n",
      "Epoch 29, Batch 152 Loss:0.08541280776262283\n",
      "Epoch 29, Batch 153 Loss:0.05979413539171219\n",
      "Epoch 29, Batch 154 Loss:0.08377572894096375\n",
      "Epoch 29, Batch 155 Loss:0.08743126690387726\n",
      "Epoch 29, Batch 156 Loss:0.042082514613866806\n",
      "Epoch 29, Batch 157 Loss:0.04355879873037338\n",
      "Epoch 29, Batch 158 Loss:0.053778838366270065\n",
      "Epoch 29, Batch 159 Loss:0.05059584975242615\n",
      "Epoch 29, Batch 160 Loss:0.08504292368888855\n",
      "Epoch 29, Batch 161 Loss:0.04482601583003998\n",
      "Epoch 29, Batch 162 Loss:0.06576903909444809\n",
      "Epoch 29, Batch 163 Loss:0.09142763912677765\n",
      "Epoch 29, Batch 164 Loss:0.02976183593273163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Batch 165 Loss:0.03817545995116234\n",
      "Epoch 29, Batch 166 Loss:0.07574120163917542\n",
      "Epoch 29, Batch 167 Loss:0.026564843952655792\n",
      "Epoch 29, Batch 168 Loss:0.05618748813867569\n",
      "Epoch 29, Batch 169 Loss:0.05778667330741882\n",
      "Epoch 29, Batch 170 Loss:0.05758233740925789\n",
      "Epoch 29, Batch 171 Loss:0.03349555283784866\n",
      "Epoch 29, Batch 172 Loss:0.05855557322502136\n",
      "Epoch 29, Batch 173 Loss:0.04432216286659241\n",
      "Epoch 29, Batch 174 Loss:0.06417183578014374\n",
      "Epoch 29, Batch 175 Loss:0.09444209933280945\n",
      "Epoch 29, Batch 176 Loss:0.061889324337244034\n",
      "Epoch 29, Batch 177 Loss:0.0883365049958229\n",
      "Epoch 29, Batch 178 Loss:0.07443475723266602\n",
      "Epoch 29, Batch 179 Loss:0.06932251155376434\n",
      "Epoch 29, Batch 180 Loss:0.05597455054521561\n",
      "Epoch 29, Batch 181 Loss:0.08604717254638672\n",
      "Epoch 29, Batch 182 Loss:0.05045342817902565\n",
      "Epoch 29, Batch 183 Loss:0.08239299803972244\n",
      "Epoch 29, Batch 184 Loss:0.05798725038766861\n",
      "Epoch 29, Batch 185 Loss:0.06887935101985931\n",
      "Epoch 29, Batch 186 Loss:0.06749065220355988\n",
      "Epoch 29, Batch 187 Loss:0.05153870955109596\n",
      "Epoch 29, Batch 188 Loss:0.044968850910663605\n",
      "Epoch 29, Batch 189 Loss:0.044653039425611496\n",
      "Epoch 29, Batch 190 Loss:0.08176268637180328\n",
      "Epoch 29, Batch 191 Loss:0.05188945680856705\n",
      "Epoch 29, Batch 192 Loss:0.1372566521167755\n",
      "Epoch 29, Batch 193 Loss:0.06394757330417633\n",
      "Epoch 29, Batch 194 Loss:0.07820636034011841\n",
      "Epoch 29, Batch 195 Loss:0.09468785673379898\n",
      "Epoch 29, Batch 196 Loss:0.09133636206388474\n",
      "Epoch 29, Batch 197 Loss:0.04040352255105972\n",
      "Epoch 29, Batch 198 Loss:0.03885144367814064\n",
      "Epoch 29, Batch 199 Loss:0.07162322103977203\n",
      "Epoch 29, Batch 200 Loss:0.08993799239397049\n",
      "Epoch 29, Batch 201 Loss:0.0594387911260128\n",
      "Epoch 29, Batch 202 Loss:0.07845751941204071\n",
      "Epoch 29, Batch 203 Loss:0.06892061233520508\n",
      "Epoch 29, Batch 204 Loss:0.055411577224731445\n",
      "Epoch 29, Batch 205 Loss:0.059705305844545364\n",
      "Epoch 29, Batch 206 Loss:0.029979821294546127\n",
      "Epoch 29, Batch 207 Loss:0.024084143340587616\n",
      "Epoch 29, Batch 208 Loss:0.08063808083534241\n",
      "Epoch 29, Batch 209 Loss:0.048681825399398804\n",
      "Epoch 29, Batch 210 Loss:0.05594458431005478\n",
      "Epoch 29, Batch 211 Loss:0.03220362216234207\n",
      "Epoch 29, Batch 212 Loss:0.12948882579803467\n",
      "Epoch 29, Batch 213 Loss:0.027216831222176552\n",
      "Epoch 29, Batch 214 Loss:0.062386754900217056\n",
      "Epoch 29, Batch 215 Loss:0.04205534979701042\n",
      "Epoch 29, Batch 216 Loss:0.08466798067092896\n",
      "Epoch 29, Batch 217 Loss:0.07279076427221298\n",
      "Epoch 29, Batch 218 Loss:0.10378666967153549\n",
      "Epoch 29, Batch 219 Loss:0.06865757703781128\n",
      "Epoch 29, Batch 220 Loss:0.10664250701665878\n",
      "Epoch 29, Batch 221 Loss:0.07286406308412552\n",
      "Epoch 29, Batch 222 Loss:0.050210438668727875\n",
      "Epoch 29, Batch 223 Loss:0.07249440997838974\n",
      "Epoch 29, Batch 224 Loss:0.10174001008272171\n",
      "Epoch 29, Batch 225 Loss:0.051994308829307556\n",
      "Epoch 29, Batch 226 Loss:0.07570207118988037\n",
      "Epoch 29, Batch 227 Loss:0.10236963629722595\n",
      "Epoch 29, Batch 228 Loss:0.06537409126758575\n",
      "Epoch 29, Batch 229 Loss:0.07793305069208145\n",
      "Epoch 29, Batch 230 Loss:0.04532665014266968\n",
      "Epoch 29, Batch 231 Loss:0.040663108229637146\n",
      "Epoch 29, Batch 232 Loss:0.031698279082775116\n",
      "Epoch 29, Batch 233 Loss:0.04100083187222481\n",
      "Loss in this Epoch is: 4.10008318722 %\n",
      "Accuracy in this Epoch is: 88.4100019932 %\n",
      "Epoch 30, Batch 0 Loss:0.03716469183564186\n",
      "Epoch 30, Batch 1 Loss:0.03251107037067413\n",
      "Epoch 30, Batch 2 Loss:0.05870972201228142\n",
      "Epoch 30, Batch 3 Loss:0.040482308715581894\n",
      "Epoch 30, Batch 4 Loss:0.04740740731358528\n",
      "Epoch 30, Batch 5 Loss:0.030746586620807648\n",
      "Epoch 30, Batch 6 Loss:0.049823418259620667\n",
      "Epoch 30, Batch 7 Loss:0.055492401123046875\n",
      "Epoch 30, Batch 8 Loss:0.04724789038300514\n",
      "Epoch 30, Batch 9 Loss:0.026074901223182678\n",
      "Epoch 30, Batch 10 Loss:0.04918092489242554\n",
      "Epoch 30, Batch 11 Loss:0.04653315618634224\n",
      "Epoch 30, Batch 12 Loss:0.05605223774909973\n",
      "Epoch 30, Batch 13 Loss:0.041824109852313995\n",
      "Epoch 30, Batch 14 Loss:0.028606228530406952\n",
      "Epoch 30, Batch 15 Loss:0.03487221151590347\n",
      "Epoch 30, Batch 16 Loss:0.03359157219529152\n",
      "Epoch 30, Batch 17 Loss:0.020470136776566505\n",
      "Epoch 30, Batch 18 Loss:0.022791611030697823\n",
      "Epoch 30, Batch 19 Loss:0.0627681091427803\n",
      "Epoch 30, Batch 20 Loss:0.045315418392419815\n",
      "Epoch 30, Batch 21 Loss:0.03567545861005783\n",
      "Epoch 30, Batch 22 Loss:0.08081825077533722\n",
      "Epoch 30, Batch 23 Loss:0.03794214129447937\n",
      "Epoch 30, Batch 24 Loss:0.02423122525215149\n",
      "Epoch 30, Batch 25 Loss:0.0370875708758831\n",
      "Epoch 30, Batch 26 Loss:0.0399448536336422\n",
      "Epoch 30, Batch 27 Loss:0.023984532803297043\n",
      "Epoch 30, Batch 28 Loss:0.017545193433761597\n",
      "Epoch 30, Batch 29 Loss:0.04354451224207878\n",
      "Epoch 30, Batch 30 Loss:0.024610845372080803\n",
      "Epoch 30, Batch 31 Loss:0.06634408980607986\n",
      "Epoch 30, Batch 32 Loss:0.027944274246692657\n",
      "Epoch 30, Batch 33 Loss:0.01573280245065689\n",
      "Epoch 30, Batch 34 Loss:0.05576622486114502\n",
      "Epoch 30, Batch 35 Loss:0.03710344806313515\n",
      "Epoch 30, Batch 36 Loss:0.036544617265462875\n",
      "Epoch 30, Batch 37 Loss:0.03416464850306511\n",
      "Epoch 30, Batch 38 Loss:0.024789277464151382\n",
      "Epoch 30, Batch 39 Loss:0.03923196718096733\n",
      "Epoch 30, Batch 40 Loss:0.045380983501672745\n",
      "Epoch 30, Batch 41 Loss:0.028927935287356377\n",
      "Epoch 30, Batch 42 Loss:0.03046315908432007\n",
      "Epoch 30, Batch 43 Loss:0.022635873407125473\n",
      "Epoch 30, Batch 44 Loss:0.037039801478385925\n",
      "Epoch 30, Batch 45 Loss:0.03764776512980461\n",
      "Epoch 30, Batch 46 Loss:0.031048642471432686\n",
      "Epoch 30, Batch 47 Loss:0.0369555689394474\n",
      "Epoch 30, Batch 48 Loss:0.023794496431946754\n",
      "Epoch 30, Batch 49 Loss:0.05096790939569473\n",
      "Epoch 30, Batch 50 Loss:0.047803983092308044\n",
      "Epoch 30, Batch 51 Loss:0.04566361755132675\n",
      "Epoch 30, Batch 52 Loss:0.04160948470234871\n",
      "Epoch 30, Batch 53 Loss:0.041532836854457855\n",
      "Epoch 30, Batch 54 Loss:0.03846808522939682\n",
      "Epoch 30, Batch 55 Loss:0.040393393486738205\n",
      "Epoch 30, Batch 56 Loss:0.05918688699603081\n",
      "Epoch 30, Batch 57 Loss:0.03982381150126457\n",
      "Epoch 30, Batch 58 Loss:0.04800219088792801\n",
      "Epoch 30, Batch 59 Loss:0.05337607115507126\n",
      "Epoch 30, Batch 60 Loss:0.02075180411338806\n",
      "Epoch 30, Batch 61 Loss:0.0331725999712944\n",
      "Epoch 30, Batch 62 Loss:0.05525818467140198\n",
      "Epoch 30, Batch 63 Loss:0.04941290244460106\n",
      "Epoch 30, Batch 64 Loss:0.033625755459070206\n",
      "Epoch 30, Batch 65 Loss:0.060023821890354156\n",
      "Epoch 30, Batch 66 Loss:0.02372843772172928\n",
      "Epoch 30, Batch 67 Loss:0.03973778709769249\n",
      "Epoch 30, Batch 68 Loss:0.06866605579853058\n",
      "Epoch 30, Batch 69 Loss:0.034973010420799255\n",
      "Epoch 30, Batch 70 Loss:0.03574209660291672\n",
      "Epoch 30, Batch 71 Loss:0.05478161945939064\n",
      "Epoch 30, Batch 72 Loss:0.024996532127261162\n",
      "Epoch 30, Batch 73 Loss:0.03409017249941826\n",
      "Epoch 30, Batch 74 Loss:0.020689211785793304\n",
      "Epoch 30, Batch 75 Loss:0.015028842724859715\n",
      "Epoch 30, Batch 76 Loss:0.040715791285037994\n",
      "Epoch 30, Batch 77 Loss:0.04228939861059189\n",
      "Epoch 30, Batch 78 Loss:0.048397909849882126\n",
      "Epoch 30, Batch 79 Loss:0.055068548768758774\n",
      "Epoch 30, Batch 80 Loss:0.06250350177288055\n",
      "Epoch 30, Batch 81 Loss:0.021614499390125275\n",
      "Epoch 30, Batch 82 Loss:0.049112025648355484\n",
      "Epoch 30, Batch 83 Loss:0.05891319736838341\n",
      "Epoch 30, Batch 84 Loss:0.06303586810827255\n",
      "Epoch 30, Batch 85 Loss:0.03416452929377556\n",
      "Epoch 30, Batch 86 Loss:0.053988441824913025\n",
      "Epoch 30, Batch 87 Loss:0.040309831500053406\n",
      "Epoch 30, Batch 88 Loss:0.03214433416724205\n",
      "Epoch 30, Batch 89 Loss:0.038401782512664795\n",
      "Epoch 30, Batch 90 Loss:0.026201659813523293\n",
      "Epoch 30, Batch 91 Loss:0.07015863806009293\n",
      "Epoch 30, Batch 92 Loss:0.02534879371523857\n",
      "Epoch 30, Batch 93 Loss:0.043996091932058334\n",
      "Epoch 30, Batch 94 Loss:0.0377948023378849\n",
      "Epoch 30, Batch 95 Loss:0.04766678065061569\n",
      "Epoch 30, Batch 96 Loss:0.05903133377432823\n",
      "Epoch 30, Batch 97 Loss:0.040352724492549896\n",
      "Epoch 30, Batch 98 Loss:0.0708802342414856\n",
      "Epoch 30, Batch 99 Loss:0.03417689725756645\n",
      "Epoch 30, Batch 100 Loss:0.05367462337017059\n",
      "Epoch 30, Batch 101 Loss:0.023751642554998398\n",
      "Epoch 30, Batch 102 Loss:0.04647833853960037\n",
      "Epoch 30, Batch 103 Loss:0.056340690702199936\n",
      "Epoch 30, Batch 104 Loss:0.0371343269944191\n",
      "Epoch 30, Batch 105 Loss:0.05676470696926117\n",
      "Epoch 30, Batch 106 Loss:0.027801211923360825\n",
      "Epoch 30, Batch 107 Loss:0.05656960606575012\n",
      "Epoch 30, Batch 108 Loss:0.03375406563282013\n",
      "Epoch 30, Batch 109 Loss:0.05889781937003136\n",
      "Epoch 30, Batch 110 Loss:0.043591082096099854\n",
      "Epoch 30, Batch 111 Loss:0.08384165912866592\n",
      "Epoch 30, Batch 112 Loss:0.09427200257778168\n",
      "Epoch 30, Batch 113 Loss:0.03807367384433746\n",
      "Epoch 30, Batch 114 Loss:0.04080410674214363\n",
      "Epoch 30, Batch 115 Loss:0.0932035744190216\n",
      "Epoch 30, Batch 116 Loss:0.06294570118188858\n",
      "Epoch 30, Batch 117 Loss:0.058991070836782455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Batch 118 Loss:0.06296511739492416\n",
      "Epoch 30, Batch 119 Loss:0.028160952031612396\n",
      "Epoch 30, Batch 120 Loss:0.07566303014755249\n",
      "Epoch 30, Batch 121 Loss:0.0788835659623146\n",
      "Epoch 30, Batch 122 Loss:0.08259537816047668\n",
      "Epoch 30, Batch 123 Loss:0.05894726514816284\n",
      "Epoch 30, Batch 124 Loss:0.04537726566195488\n",
      "Epoch 30, Batch 125 Loss:0.03543824329972267\n",
      "Epoch 30, Batch 126 Loss:0.08702461421489716\n",
      "Epoch 30, Batch 127 Loss:0.07355784624814987\n",
      "Epoch 30, Batch 128 Loss:0.03619688004255295\n",
      "Epoch 30, Batch 129 Loss:0.046905651688575745\n",
      "Epoch 30, Batch 130 Loss:0.040585391223430634\n",
      "Epoch 30, Batch 131 Loss:0.08482591807842255\n",
      "Epoch 30, Batch 132 Loss:0.04115644097328186\n",
      "Epoch 30, Batch 133 Loss:0.04170440882444382\n",
      "Epoch 30, Batch 134 Loss:0.054423630237579346\n",
      "Epoch 30, Batch 135 Loss:0.05763208121061325\n",
      "Epoch 30, Batch 136 Loss:0.04361127316951752\n",
      "Epoch 30, Batch 137 Loss:0.05608685314655304\n",
      "Epoch 30, Batch 138 Loss:0.0418679378926754\n",
      "Epoch 30, Batch 139 Loss:0.07246308028697968\n",
      "Epoch 30, Batch 140 Loss:0.04990039020776749\n",
      "Epoch 30, Batch 141 Loss:0.03227566182613373\n",
      "Epoch 30, Batch 142 Loss:0.04831111803650856\n",
      "Epoch 30, Batch 143 Loss:0.050986938178539276\n",
      "Epoch 30, Batch 144 Loss:0.07649768143892288\n",
      "Epoch 30, Batch 145 Loss:0.047369107604026794\n",
      "Epoch 30, Batch 146 Loss:0.03995167836546898\n",
      "Epoch 30, Batch 147 Loss:0.04523741826415062\n",
      "Epoch 30, Batch 148 Loss:0.03850082680583\n",
      "Epoch 30, Batch 149 Loss:0.06780669093132019\n",
      "Epoch 30, Batch 150 Loss:0.043464843183755875\n",
      "Epoch 30, Batch 151 Loss:0.030072448775172234\n",
      "Epoch 30, Batch 152 Loss:0.04045333340764046\n",
      "Epoch 30, Batch 153 Loss:0.027423452585935593\n",
      "Epoch 30, Batch 154 Loss:0.05216270312666893\n",
      "Epoch 30, Batch 155 Loss:0.07404524087905884\n",
      "Epoch 30, Batch 156 Loss:0.040845002979040146\n",
      "Epoch 30, Batch 157 Loss:0.09785236418247223\n",
      "Epoch 30, Batch 158 Loss:0.08690106123685837\n",
      "Epoch 30, Batch 159 Loss:0.03897441178560257\n",
      "Epoch 30, Batch 160 Loss:0.07584299147129059\n",
      "Epoch 30, Batch 161 Loss:0.04917532205581665\n",
      "Epoch 30, Batch 162 Loss:0.052941568195819855\n",
      "Epoch 30, Batch 163 Loss:0.05978154018521309\n",
      "Epoch 30, Batch 164 Loss:0.03894750028848648\n",
      "Epoch 30, Batch 165 Loss:0.04795759916305542\n",
      "Epoch 30, Batch 166 Loss:0.08112271130084991\n",
      "Epoch 30, Batch 167 Loss:0.053130850195884705\n",
      "Epoch 30, Batch 168 Loss:0.08411572873592377\n",
      "Epoch 30, Batch 169 Loss:0.07105635106563568\n",
      "Epoch 30, Batch 170 Loss:0.09227651357650757\n",
      "Epoch 30, Batch 171 Loss:0.0927162915468216\n",
      "Epoch 30, Batch 172 Loss:0.07222195714712143\n",
      "Epoch 30, Batch 173 Loss:0.07575611770153046\n",
      "Epoch 30, Batch 174 Loss:0.06780022382736206\n",
      "Epoch 30, Batch 175 Loss:0.07056359946727753\n",
      "Epoch 30, Batch 176 Loss:0.07818066328763962\n",
      "Epoch 30, Batch 177 Loss:0.06294311583042145\n",
      "Epoch 30, Batch 178 Loss:0.032959192991256714\n",
      "Epoch 30, Batch 179 Loss:0.06027427315711975\n",
      "Epoch 30, Batch 180 Loss:0.0708761140704155\n",
      "Epoch 30, Batch 181 Loss:0.055134326219558716\n",
      "Epoch 30, Batch 182 Loss:0.03634963929653168\n",
      "Epoch 30, Batch 183 Loss:0.024482453241944313\n",
      "Epoch 30, Batch 184 Loss:0.03639497980475426\n",
      "Epoch 30, Batch 185 Loss:0.04097535461187363\n",
      "Epoch 30, Batch 186 Loss:0.026652580127120018\n",
      "Epoch 30, Batch 187 Loss:0.07670629769563675\n",
      "Epoch 30, Batch 188 Loss:0.03726581484079361\n",
      "Epoch 30, Batch 189 Loss:0.05135722458362579\n",
      "Epoch 30, Batch 190 Loss:0.0423135980963707\n",
      "Epoch 30, Batch 191 Loss:0.051857538521289825\n",
      "Epoch 30, Batch 192 Loss:0.03616210073232651\n",
      "Epoch 30, Batch 193 Loss:0.05741901323199272\n",
      "Epoch 30, Batch 194 Loss:0.03204921633005142\n",
      "Epoch 30, Batch 195 Loss:0.028439253568649292\n",
      "Epoch 30, Batch 196 Loss:0.06152166426181793\n",
      "Epoch 30, Batch 197 Loss:0.040786974132061005\n",
      "Epoch 30, Batch 198 Loss:0.08470642566680908\n",
      "Epoch 30, Batch 199 Loss:0.09457017481327057\n",
      "Epoch 30, Batch 200 Loss:0.04543020576238632\n",
      "Epoch 30, Batch 201 Loss:0.07241262495517731\n",
      "Epoch 30, Batch 202 Loss:0.0670175850391388\n",
      "Epoch 30, Batch 203 Loss:0.04428436607122421\n",
      "Epoch 30, Batch 204 Loss:0.037719883024692535\n",
      "Epoch 30, Batch 205 Loss:0.0711926519870758\n",
      "Epoch 30, Batch 206 Loss:0.03379693254828453\n",
      "Epoch 30, Batch 207 Loss:0.07742913067340851\n",
      "Epoch 30, Batch 208 Loss:0.05679260194301605\n",
      "Epoch 30, Batch 209 Loss:0.04815308004617691\n",
      "Epoch 30, Batch 210 Loss:0.05051262304186821\n",
      "Epoch 30, Batch 211 Loss:0.06687530130147934\n",
      "Epoch 30, Batch 212 Loss:0.08188661932945251\n",
      "Epoch 30, Batch 213 Loss:0.05721275508403778\n",
      "Epoch 30, Batch 214 Loss:0.039825327694416046\n",
      "Epoch 30, Batch 215 Loss:0.05805005878210068\n",
      "Epoch 30, Batch 216 Loss:0.038995590060949326\n",
      "Epoch 30, Batch 217 Loss:0.035854414105415344\n",
      "Epoch 30, Batch 218 Loss:0.07746932655572891\n",
      "Epoch 30, Batch 219 Loss:0.033370308578014374\n",
      "Epoch 30, Batch 220 Loss:0.04563477635383606\n",
      "Epoch 30, Batch 221 Loss:0.055991046130657196\n",
      "Epoch 30, Batch 222 Loss:0.034937381744384766\n",
      "Epoch 30, Batch 223 Loss:0.03487231954932213\n",
      "Epoch 30, Batch 224 Loss:0.02730720490217209\n",
      "Epoch 30, Batch 225 Loss:0.10479888319969177\n",
      "Epoch 30, Batch 226 Loss:0.06491295993328094\n",
      "Epoch 30, Batch 227 Loss:0.09251086413860321\n",
      "Epoch 30, Batch 228 Loss:0.04553183168172836\n",
      "Epoch 30, Batch 229 Loss:0.04799337312579155\n",
      "Epoch 30, Batch 230 Loss:0.07466455549001694\n",
      "Epoch 30, Batch 231 Loss:0.0683174580335617\n",
      "Epoch 30, Batch 232 Loss:0.045262791216373444\n",
      "Epoch 30, Batch 233 Loss:0.07549123466014862\n",
      "Loss in this Epoch is: 7.54912346601 %\n",
      "Accuracy in this Epoch is: 88.630002737 %\n",
      "Epoch 31, Batch 0 Loss:0.03558189794421196\n",
      "Epoch 31, Batch 1 Loss:0.054978661239147186\n",
      "Epoch 31, Batch 2 Loss:0.035865116864442825\n",
      "Epoch 31, Batch 3 Loss:0.0729157105088234\n",
      "Epoch 31, Batch 4 Loss:0.04910120368003845\n",
      "Epoch 31, Batch 5 Loss:0.0430135540664196\n",
      "Epoch 31, Batch 6 Loss:0.03852525353431702\n",
      "Epoch 31, Batch 7 Loss:0.0582779124379158\n",
      "Epoch 31, Batch 8 Loss:0.07691270112991333\n",
      "Epoch 31, Batch 9 Loss:0.037222061306238174\n",
      "Epoch 31, Batch 10 Loss:0.078926220536232\n",
      "Epoch 31, Batch 11 Loss:0.04122481122612953\n",
      "Epoch 31, Batch 12 Loss:0.0581740066409111\n",
      "Epoch 31, Batch 13 Loss:0.04697520285844803\n",
      "Epoch 31, Batch 14 Loss:0.0253144484013319\n",
      "Epoch 31, Batch 15 Loss:0.02895205467939377\n",
      "Epoch 31, Batch 16 Loss:0.11189544200897217\n",
      "Epoch 31, Batch 17 Loss:0.08697313815355301\n",
      "Epoch 31, Batch 18 Loss:0.058420922607183456\n",
      "Epoch 31, Batch 19 Loss:0.01660832017660141\n",
      "Epoch 31, Batch 20 Loss:0.05977741256356239\n",
      "Epoch 31, Batch 21 Loss:0.04331250488758087\n",
      "Epoch 31, Batch 22 Loss:0.057168614119291306\n",
      "Epoch 31, Batch 23 Loss:0.09745284914970398\n",
      "Epoch 31, Batch 24 Loss:0.05387505888938904\n",
      "Epoch 31, Batch 25 Loss:0.09719422459602356\n",
      "Epoch 31, Batch 26 Loss:0.046937957406044006\n",
      "Epoch 31, Batch 27 Loss:0.04364325851202011\n",
      "Epoch 31, Batch 28 Loss:0.06495796144008636\n",
      "Epoch 31, Batch 29 Loss:0.07713223993778229\n",
      "Epoch 31, Batch 30 Loss:0.060076754540205\n",
      "Epoch 31, Batch 31 Loss:0.04608337581157684\n",
      "Epoch 31, Batch 32 Loss:0.07648129016160965\n",
      "Epoch 31, Batch 33 Loss:0.0706593319773674\n",
      "Epoch 31, Batch 34 Loss:0.05055500194430351\n",
      "Epoch 31, Batch 35 Loss:0.03750629350543022\n",
      "Epoch 31, Batch 36 Loss:0.04291549324989319\n",
      "Epoch 31, Batch 37 Loss:0.040825504809617996\n",
      "Epoch 31, Batch 38 Loss:0.04082663729786873\n",
      "Epoch 31, Batch 39 Loss:0.05009584501385689\n",
      "Epoch 31, Batch 40 Loss:0.04990460351109505\n",
      "Epoch 31, Batch 41 Loss:0.03697870299220085\n",
      "Epoch 31, Batch 42 Loss:0.05436716973781586\n",
      "Epoch 31, Batch 43 Loss:0.03180529177188873\n",
      "Epoch 31, Batch 44 Loss:0.04173095151782036\n",
      "Epoch 31, Batch 45 Loss:0.05819997191429138\n",
      "Epoch 31, Batch 46 Loss:0.04624892398715019\n",
      "Epoch 31, Batch 47 Loss:0.020526420325040817\n",
      "Epoch 31, Batch 48 Loss:0.0727299302816391\n",
      "Epoch 31, Batch 49 Loss:0.035186924040317535\n",
      "Epoch 31, Batch 50 Loss:0.033332183957099915\n",
      "Epoch 31, Batch 51 Loss:0.03951115161180496\n",
      "Epoch 31, Batch 52 Loss:0.03556375205516815\n",
      "Epoch 31, Batch 53 Loss:0.07384010404348373\n",
      "Epoch 31, Batch 54 Loss:0.11495431512594223\n",
      "Epoch 31, Batch 55 Loss:0.05177244916558266\n",
      "Epoch 31, Batch 56 Loss:0.025154223665595055\n",
      "Epoch 31, Batch 57 Loss:0.022351641207933426\n",
      "Epoch 31, Batch 58 Loss:0.0217197984457016\n",
      "Epoch 31, Batch 59 Loss:0.03319201618432999\n",
      "Epoch 31, Batch 60 Loss:0.03739619627594948\n",
      "Epoch 31, Batch 61 Loss:0.0670604258775711\n",
      "Epoch 31, Batch 62 Loss:0.08353676646947861\n",
      "Epoch 31, Batch 63 Loss:0.06421227008104324\n",
      "Epoch 31, Batch 64 Loss:0.042442452162504196\n",
      "Epoch 31, Batch 65 Loss:0.10258681327104568\n",
      "Epoch 31, Batch 66 Loss:0.03227822110056877\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Batch 67 Loss:0.032000765204429626\n",
      "Epoch 31, Batch 68 Loss:0.05715646594762802\n",
      "Epoch 31, Batch 69 Loss:0.06759558618068695\n",
      "Epoch 31, Batch 70 Loss:0.06363938748836517\n",
      "Epoch 31, Batch 71 Loss:0.03312509134411812\n",
      "Epoch 31, Batch 72 Loss:0.06453967839479446\n",
      "Epoch 31, Batch 73 Loss:0.0633348822593689\n",
      "Epoch 31, Batch 74 Loss:0.0709775760769844\n",
      "Epoch 31, Batch 75 Loss:0.08023393899202347\n",
      "Epoch 31, Batch 76 Loss:0.07981110364198685\n",
      "Epoch 31, Batch 77 Loss:0.04896065220236778\n",
      "Epoch 31, Batch 78 Loss:0.05625911056995392\n",
      "Epoch 31, Batch 79 Loss:0.030114104971289635\n",
      "Epoch 31, Batch 80 Loss:0.029532119631767273\n",
      "Epoch 31, Batch 81 Loss:0.04865473136305809\n",
      "Epoch 31, Batch 82 Loss:0.07804480195045471\n",
      "Epoch 31, Batch 83 Loss:0.07330016046762466\n",
      "Epoch 31, Batch 84 Loss:0.04000574350357056\n",
      "Epoch 31, Batch 85 Loss:0.05294690281152725\n",
      "Epoch 31, Batch 86 Loss:0.049118772149086\n",
      "Epoch 31, Batch 87 Loss:0.06911436468362808\n",
      "Epoch 31, Batch 88 Loss:0.07008417695760727\n",
      "Epoch 31, Batch 89 Loss:0.053520165383815765\n",
      "Epoch 31, Batch 90 Loss:0.11456875503063202\n",
      "Epoch 31, Batch 91 Loss:0.056163083761930466\n",
      "Epoch 31, Batch 92 Loss:0.05890471115708351\n",
      "Epoch 31, Batch 93 Loss:0.05045276880264282\n",
      "Epoch 31, Batch 94 Loss:0.04752720519900322\n",
      "Epoch 31, Batch 95 Loss:0.07069749385118484\n",
      "Epoch 31, Batch 96 Loss:0.034616582095623016\n",
      "Epoch 31, Batch 97 Loss:0.03821628540754318\n",
      "Epoch 31, Batch 98 Loss:0.01931888237595558\n",
      "Epoch 31, Batch 99 Loss:0.06657575070858002\n",
      "Epoch 31, Batch 100 Loss:0.05054531991481781\n",
      "Epoch 31, Batch 101 Loss:0.02612883225083351\n",
      "Epoch 31, Batch 102 Loss:0.06646545231342316\n",
      "Epoch 31, Batch 103 Loss:0.03865395113825798\n",
      "Epoch 31, Batch 104 Loss:0.06751614063978195\n",
      "Epoch 31, Batch 105 Loss:0.05157902091741562\n",
      "Epoch 31, Batch 106 Loss:0.08152420818805695\n",
      "Epoch 31, Batch 107 Loss:0.047680750489234924\n",
      "Epoch 31, Batch 108 Loss:0.04567556083202362\n",
      "Epoch 31, Batch 109 Loss:0.04330592602491379\n",
      "Epoch 31, Batch 110 Loss:0.051655806601047516\n",
      "Epoch 31, Batch 111 Loss:0.027312560006976128\n",
      "Epoch 31, Batch 112 Loss:0.06403585523366928\n",
      "Epoch 31, Batch 113 Loss:0.04748592525720596\n",
      "Epoch 31, Batch 114 Loss:0.07090194523334503\n",
      "Epoch 31, Batch 115 Loss:0.04767376929521561\n",
      "Epoch 31, Batch 116 Loss:0.04674902185797691\n",
      "Epoch 31, Batch 117 Loss:0.025529131293296814\n",
      "Epoch 31, Batch 118 Loss:0.07243122160434723\n",
      "Epoch 31, Batch 119 Loss:0.032074134796857834\n",
      "Epoch 31, Batch 120 Loss:0.0716511458158493\n",
      "Epoch 31, Batch 121 Loss:0.023823924362659454\n",
      "Epoch 31, Batch 122 Loss:0.037150345742702484\n",
      "Epoch 31, Batch 123 Loss:0.04917043447494507\n",
      "Epoch 31, Batch 124 Loss:0.035398054867982864\n",
      "Epoch 31, Batch 125 Loss:0.031011231243610382\n",
      "Epoch 31, Batch 126 Loss:0.056617848575115204\n",
      "Epoch 31, Batch 127 Loss:0.07391922920942307\n",
      "Epoch 31, Batch 128 Loss:0.022629737854003906\n",
      "Epoch 31, Batch 129 Loss:0.03814028203487396\n",
      "Epoch 31, Batch 130 Loss:0.059930913150310516\n",
      "Epoch 31, Batch 131 Loss:0.05040790140628815\n",
      "Epoch 31, Batch 132 Loss:0.0617990642786026\n",
      "Epoch 31, Batch 133 Loss:0.03009718656539917\n",
      "Epoch 31, Batch 134 Loss:0.046339280903339386\n",
      "Epoch 31, Batch 135 Loss:0.050259120762348175\n",
      "Epoch 31, Batch 136 Loss:0.0362640917301178\n",
      "Epoch 31, Batch 137 Loss:0.09215842187404633\n",
      "Epoch 31, Batch 138 Loss:0.022646136581897736\n",
      "Epoch 31, Batch 139 Loss:0.08413968980312347\n",
      "Epoch 31, Batch 140 Loss:0.0454542301595211\n",
      "Epoch 31, Batch 141 Loss:0.023592084646224976\n",
      "Epoch 31, Batch 142 Loss:0.03757132589817047\n",
      "Epoch 31, Batch 143 Loss:0.09215614199638367\n",
      "Epoch 31, Batch 144 Loss:0.06525273621082306\n",
      "Epoch 31, Batch 145 Loss:0.06282410770654678\n",
      "Epoch 31, Batch 146 Loss:0.040292903780937195\n",
      "Epoch 31, Batch 147 Loss:0.0437304750084877\n",
      "Epoch 31, Batch 148 Loss:0.05205545574426651\n",
      "Epoch 31, Batch 149 Loss:0.06143255531787872\n",
      "Epoch 31, Batch 150 Loss:0.06676460802555084\n",
      "Epoch 31, Batch 151 Loss:0.030274435877799988\n",
      "Epoch 31, Batch 152 Loss:0.04791013151407242\n",
      "Epoch 31, Batch 153 Loss:0.03260761499404907\n",
      "Epoch 31, Batch 154 Loss:0.026521718129515648\n",
      "Epoch 31, Batch 155 Loss:0.050934914499521255\n",
      "Epoch 31, Batch 156 Loss:0.06786970794200897\n",
      "Epoch 31, Batch 157 Loss:0.022498130798339844\n",
      "Epoch 31, Batch 158 Loss:0.030958697199821472\n",
      "Epoch 31, Batch 159 Loss:0.06249835342168808\n",
      "Epoch 31, Batch 160 Loss:0.04216557741165161\n",
      "Epoch 31, Batch 161 Loss:0.03275597095489502\n",
      "Epoch 31, Batch 162 Loss:0.06356148421764374\n",
      "Epoch 31, Batch 163 Loss:0.0948970690369606\n",
      "Epoch 31, Batch 164 Loss:0.03796619176864624\n",
      "Epoch 31, Batch 165 Loss:0.04836415499448776\n",
      "Epoch 31, Batch 166 Loss:0.020204488188028336\n",
      "Epoch 31, Batch 167 Loss:0.03051115572452545\n",
      "Epoch 31, Batch 168 Loss:0.0407278798520565\n",
      "Epoch 31, Batch 169 Loss:0.023164117708802223\n",
      "Epoch 31, Batch 170 Loss:0.027241960167884827\n",
      "Epoch 31, Batch 171 Loss:0.04281967878341675\n",
      "Epoch 31, Batch 172 Loss:0.027689557522535324\n",
      "Epoch 31, Batch 173 Loss:0.026251811534166336\n",
      "Epoch 31, Batch 174 Loss:0.049851495772600174\n",
      "Epoch 31, Batch 175 Loss:0.05794449895620346\n",
      "Epoch 31, Batch 176 Loss:0.022386010736227036\n",
      "Epoch 31, Batch 177 Loss:0.07327601313591003\n",
      "Epoch 31, Batch 178 Loss:0.08684580028057098\n",
      "Epoch 31, Batch 179 Loss:0.05704589933156967\n",
      "Epoch 31, Batch 180 Loss:0.04830610007047653\n",
      "Epoch 31, Batch 181 Loss:0.03754410147666931\n",
      "Epoch 31, Batch 182 Loss:0.023176204413175583\n",
      "Epoch 31, Batch 183 Loss:0.06937719881534576\n",
      "Epoch 31, Batch 184 Loss:0.04098796099424362\n",
      "Epoch 31, Batch 185 Loss:0.02728901244699955\n",
      "Epoch 31, Batch 186 Loss:0.021054498851299286\n",
      "Epoch 31, Batch 187 Loss:0.03452858328819275\n",
      "Epoch 31, Batch 188 Loss:0.04681083559989929\n",
      "Epoch 31, Batch 189 Loss:0.025540977716445923\n",
      "Epoch 31, Batch 190 Loss:0.036360934376716614\n",
      "Epoch 31, Batch 191 Loss:0.0553814060986042\n",
      "Epoch 31, Batch 192 Loss:0.061010025441646576\n",
      "Epoch 31, Batch 193 Loss:0.05520434305071831\n",
      "Epoch 31, Batch 194 Loss:0.08688867092132568\n",
      "Epoch 31, Batch 195 Loss:0.052470967173576355\n",
      "Epoch 31, Batch 196 Loss:0.0641867071390152\n",
      "Epoch 31, Batch 197 Loss:0.037718065083026886\n",
      "Epoch 31, Batch 198 Loss:0.07569032162427902\n",
      "Epoch 31, Batch 199 Loss:0.0407315231859684\n",
      "Epoch 31, Batch 200 Loss:0.05250410735607147\n",
      "Epoch 31, Batch 201 Loss:0.02973223105072975\n",
      "Epoch 31, Batch 202 Loss:0.10586587339639664\n",
      "Epoch 31, Batch 203 Loss:0.07073846459388733\n",
      "Epoch 31, Batch 204 Loss:0.05401476472616196\n",
      "Epoch 31, Batch 205 Loss:0.04565490782260895\n",
      "Epoch 31, Batch 206 Loss:0.044269710779190063\n",
      "Epoch 31, Batch 207 Loss:0.07776429504156113\n",
      "Epoch 31, Batch 208 Loss:0.04422634840011597\n",
      "Epoch 31, Batch 209 Loss:0.12004142999649048\n",
      "Epoch 31, Batch 210 Loss:0.05414127930998802\n",
      "Epoch 31, Batch 211 Loss:0.050112172961235046\n",
      "Epoch 31, Batch 212 Loss:0.03373251482844353\n",
      "Epoch 31, Batch 213 Loss:0.09835676103830338\n",
      "Epoch 31, Batch 214 Loss:0.030930349603295326\n",
      "Epoch 31, Batch 215 Loss:0.0730474591255188\n",
      "Epoch 31, Batch 216 Loss:0.04870225489139557\n",
      "Epoch 31, Batch 217 Loss:0.07640915364027023\n",
      "Epoch 31, Batch 218 Loss:0.05162364989519119\n",
      "Epoch 31, Batch 219 Loss:0.07362372428178787\n",
      "Epoch 31, Batch 220 Loss:0.0867081806063652\n",
      "Epoch 31, Batch 221 Loss:0.06907161325216293\n",
      "Epoch 31, Batch 222 Loss:0.07145844399929047\n",
      "Epoch 31, Batch 223 Loss:0.055778518319129944\n",
      "Epoch 31, Batch 224 Loss:0.027796553447842598\n",
      "Epoch 31, Batch 225 Loss:0.05867310240864754\n",
      "Epoch 31, Batch 226 Loss:0.0472484715282917\n",
      "Epoch 31, Batch 227 Loss:0.03554750606417656\n",
      "Epoch 31, Batch 228 Loss:0.03759671002626419\n",
      "Epoch 31, Batch 229 Loss:0.04778565466403961\n",
      "Epoch 31, Batch 230 Loss:0.06747523695230484\n",
      "Epoch 31, Batch 231 Loss:0.04507540538907051\n",
      "Epoch 31, Batch 232 Loss:0.1064896434545517\n",
      "Epoch 31, Batch 233 Loss:0.06700083613395691\n",
      "Loss in this Epoch is: 6.7000836134 %\n",
      "Accuracy in this Epoch is: 88.8000011444 %\n",
      "Epoch 32, Batch 0 Loss:0.0328708179295063\n",
      "Epoch 32, Batch 1 Loss:0.05157232657074928\n",
      "Epoch 32, Batch 2 Loss:0.07862196862697601\n",
      "Epoch 32, Batch 3 Loss:0.061574503779411316\n",
      "Epoch 32, Batch 4 Loss:0.04660149663686752\n",
      "Epoch 32, Batch 5 Loss:0.029615959152579308\n",
      "Epoch 32, Batch 6 Loss:0.03835373371839523\n",
      "Epoch 32, Batch 7 Loss:0.03483200445771217\n",
      "Epoch 32, Batch 8 Loss:0.03212116286158562\n",
      "Epoch 32, Batch 9 Loss:0.05028097331523895\n",
      "Epoch 32, Batch 10 Loss:0.044459737837314606\n",
      "Epoch 32, Batch 11 Loss:0.05952577292919159\n",
      "Epoch 32, Batch 12 Loss:0.029632681980729103\n",
      "Epoch 32, Batch 13 Loss:0.02878359891474247\n",
      "Epoch 32, Batch 14 Loss:0.02357953041791916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 15 Loss:0.041112154722213745\n",
      "Epoch 32, Batch 16 Loss:0.04713470861315727\n",
      "Epoch 32, Batch 17 Loss:0.03459285572171211\n",
      "Epoch 32, Batch 18 Loss:0.06325516849756241\n",
      "Epoch 32, Batch 19 Loss:0.06524071842432022\n",
      "Epoch 32, Batch 20 Loss:0.036946095526218414\n",
      "Epoch 32, Batch 21 Loss:0.03397805988788605\n",
      "Epoch 32, Batch 22 Loss:0.032062482088804245\n",
      "Epoch 32, Batch 23 Loss:0.0346841923892498\n",
      "Epoch 32, Batch 24 Loss:0.04314737766981125\n",
      "Epoch 32, Batch 25 Loss:0.0232526957988739\n",
      "Epoch 32, Batch 26 Loss:0.03273284062743187\n",
      "Epoch 32, Batch 27 Loss:0.01964540034532547\n",
      "Epoch 32, Batch 28 Loss:0.027257448062300682\n",
      "Epoch 32, Batch 29 Loss:0.09667318314313889\n",
      "Epoch 32, Batch 30 Loss:0.018380001187324524\n",
      "Epoch 32, Batch 31 Loss:0.03426932916045189\n",
      "Epoch 32, Batch 32 Loss:0.027505187317728996\n",
      "Epoch 32, Batch 33 Loss:0.019636351615190506\n",
      "Epoch 32, Batch 34 Loss:0.0546007975935936\n",
      "Epoch 32, Batch 35 Loss:0.016049224883317947\n",
      "Epoch 32, Batch 36 Loss:0.047296591103076935\n",
      "Epoch 32, Batch 37 Loss:0.04213001951575279\n",
      "Epoch 32, Batch 38 Loss:0.03658650815486908\n",
      "Epoch 32, Batch 39 Loss:0.03547555208206177\n",
      "Epoch 32, Batch 40 Loss:0.028219271451234818\n",
      "Epoch 32, Batch 41 Loss:0.04357388615608215\n",
      "Epoch 32, Batch 42 Loss:0.06901153922080994\n",
      "Epoch 32, Batch 43 Loss:0.018847648054361343\n",
      "Epoch 32, Batch 44 Loss:0.03151051700115204\n",
      "Epoch 32, Batch 45 Loss:0.022298408672213554\n",
      "Epoch 32, Batch 46 Loss:0.02007419243454933\n",
      "Epoch 32, Batch 47 Loss:0.04392047971487045\n",
      "Epoch 32, Batch 48 Loss:0.03298809751868248\n",
      "Epoch 32, Batch 49 Loss:0.013443587347865105\n",
      "Epoch 32, Batch 50 Loss:0.034556712955236435\n",
      "Epoch 32, Batch 51 Loss:0.035047635436058044\n",
      "Epoch 32, Batch 52 Loss:0.029962696135044098\n",
      "Epoch 32, Batch 53 Loss:0.051363855600357056\n",
      "Epoch 32, Batch 54 Loss:0.024730779230594635\n",
      "Epoch 32, Batch 55 Loss:0.03631044551730156\n",
      "Epoch 32, Batch 56 Loss:0.07880689948797226\n",
      "Epoch 32, Batch 57 Loss:0.0414973683655262\n",
      "Epoch 32, Batch 58 Loss:0.023097101598978043\n",
      "Epoch 32, Batch 59 Loss:0.03694342076778412\n",
      "Epoch 32, Batch 60 Loss:0.074417345225811\n",
      "Epoch 32, Batch 61 Loss:0.052828121930360794\n",
      "Epoch 32, Batch 62 Loss:0.02133576013147831\n",
      "Epoch 32, Batch 63 Loss:0.035877637565135956\n",
      "Epoch 32, Batch 64 Loss:0.03483397141098976\n",
      "Epoch 32, Batch 65 Loss:0.0628957748413086\n",
      "Epoch 32, Batch 66 Loss:0.04334490746259689\n",
      "Epoch 32, Batch 67 Loss:0.0237381923943758\n",
      "Epoch 32, Batch 68 Loss:0.04860314354300499\n",
      "Epoch 32, Batch 69 Loss:0.057528264820575714\n",
      "Epoch 32, Batch 70 Loss:0.028540682047605515\n",
      "Epoch 32, Batch 71 Loss:0.035653773695230484\n",
      "Epoch 32, Batch 72 Loss:0.04653045907616615\n",
      "Epoch 32, Batch 73 Loss:0.01784554123878479\n",
      "Epoch 32, Batch 74 Loss:0.02790394425392151\n",
      "Epoch 32, Batch 75 Loss:0.08116414397954941\n",
      "Epoch 32, Batch 76 Loss:0.019389241933822632\n",
      "Epoch 32, Batch 77 Loss:0.046696171164512634\n",
      "Epoch 32, Batch 78 Loss:0.0708608478307724\n",
      "Epoch 32, Batch 79 Loss:0.04643820598721504\n",
      "Epoch 32, Batch 80 Loss:0.036007609218358994\n",
      "Epoch 32, Batch 81 Loss:0.033300064504146576\n",
      "Epoch 32, Batch 82 Loss:0.04282161593437195\n",
      "Epoch 32, Batch 83 Loss:0.07960724085569382\n",
      "Epoch 32, Batch 84 Loss:0.05247354134917259\n",
      "Epoch 32, Batch 85 Loss:0.06109227240085602\n",
      "Epoch 32, Batch 86 Loss:0.05423394590616226\n",
      "Epoch 32, Batch 87 Loss:0.0442190058529377\n",
      "Epoch 32, Batch 88 Loss:0.07232899218797684\n",
      "Epoch 32, Batch 89 Loss:0.036040086299180984\n",
      "Epoch 32, Batch 90 Loss:0.033061206340789795\n",
      "Epoch 32, Batch 91 Loss:0.08138230443000793\n",
      "Epoch 32, Batch 92 Loss:0.03403554484248161\n",
      "Epoch 32, Batch 93 Loss:0.04052657261490822\n",
      "Epoch 32, Batch 94 Loss:0.045395564287900925\n",
      "Epoch 32, Batch 95 Loss:0.05770684406161308\n",
      "Epoch 32, Batch 96 Loss:0.03598970174789429\n",
      "Epoch 32, Batch 97 Loss:0.04201364144682884\n",
      "Epoch 32, Batch 98 Loss:0.04869472235441208\n",
      "Epoch 32, Batch 99 Loss:0.06750001013278961\n",
      "Epoch 32, Batch 100 Loss:0.047513850033283234\n",
      "Epoch 32, Batch 101 Loss:0.038262780755758286\n",
      "Epoch 32, Batch 102 Loss:0.025104304775595665\n",
      "Epoch 32, Batch 103 Loss:0.02819398231804371\n",
      "Epoch 32, Batch 104 Loss:0.029166843742132187\n",
      "Epoch 32, Batch 105 Loss:0.048798490315675735\n",
      "Epoch 32, Batch 106 Loss:0.05036298185586929\n",
      "Epoch 32, Batch 107 Loss:0.015402502380311489\n",
      "Epoch 32, Batch 108 Loss:0.04000752419233322\n",
      "Epoch 32, Batch 109 Loss:0.03031148947775364\n",
      "Epoch 32, Batch 110 Loss:0.03977418318390846\n",
      "Epoch 32, Batch 111 Loss:0.03963151574134827\n",
      "Epoch 32, Batch 112 Loss:0.056769270449876785\n",
      "Epoch 32, Batch 113 Loss:0.07278536260128021\n",
      "Epoch 32, Batch 114 Loss:0.1081576943397522\n",
      "Epoch 32, Batch 115 Loss:0.03813668340444565\n",
      "Epoch 32, Batch 116 Loss:0.08559057116508484\n",
      "Epoch 32, Batch 117 Loss:0.09158670902252197\n",
      "Epoch 32, Batch 118 Loss:0.04853692650794983\n",
      "Epoch 32, Batch 119 Loss:0.04724516719579697\n",
      "Epoch 32, Batch 120 Loss:0.044448431581258774\n",
      "Epoch 32, Batch 121 Loss:0.038772691041231155\n",
      "Epoch 32, Batch 122 Loss:0.04251667857170105\n",
      "Epoch 32, Batch 123 Loss:0.05397902429103851\n",
      "Epoch 32, Batch 124 Loss:0.06024230271577835\n",
      "Epoch 32, Batch 125 Loss:0.044386278837919235\n",
      "Epoch 32, Batch 126 Loss:0.05106917768716812\n",
      "Epoch 32, Batch 127 Loss:0.02717428095638752\n",
      "Epoch 32, Batch 128 Loss:0.030726483091711998\n",
      "Epoch 32, Batch 129 Loss:0.06812185049057007\n",
      "Epoch 32, Batch 130 Loss:0.04578752815723419\n",
      "Epoch 32, Batch 131 Loss:0.04754958301782608\n",
      "Epoch 32, Batch 132 Loss:0.04955268278717995\n",
      "Epoch 32, Batch 133 Loss:0.07370451092720032\n",
      "Epoch 32, Batch 134 Loss:0.09793777018785477\n",
      "Epoch 32, Batch 135 Loss:0.05833852291107178\n",
      "Epoch 32, Batch 136 Loss:0.02355058304965496\n",
      "Epoch 32, Batch 137 Loss:0.10127625614404678\n",
      "Epoch 32, Batch 138 Loss:0.030838778242468834\n",
      "Epoch 32, Batch 139 Loss:0.07118591666221619\n",
      "Epoch 32, Batch 140 Loss:0.062301427125930786\n",
      "Epoch 32, Batch 141 Loss:0.03355574607849121\n",
      "Epoch 32, Batch 142 Loss:0.08205041289329529\n",
      "Epoch 32, Batch 143 Loss:0.06206461042165756\n",
      "Epoch 32, Batch 144 Loss:0.06284591555595398\n",
      "Epoch 32, Batch 145 Loss:0.0669640600681305\n",
      "Epoch 32, Batch 146 Loss:0.04361696541309357\n",
      "Epoch 32, Batch 147 Loss:0.039584070444107056\n",
      "Epoch 32, Batch 148 Loss:0.06155313551425934\n",
      "Epoch 32, Batch 149 Loss:0.03826545178890228\n",
      "Epoch 32, Batch 150 Loss:0.02654716931283474\n",
      "Epoch 32, Batch 151 Loss:0.03386927768588066\n",
      "Epoch 32, Batch 152 Loss:0.04817987233400345\n",
      "Epoch 32, Batch 153 Loss:0.05912570655345917\n",
      "Epoch 32, Batch 154 Loss:0.036653898656368256\n",
      "Epoch 32, Batch 155 Loss:0.07637595385313034\n",
      "Epoch 32, Batch 156 Loss:0.028983915224671364\n",
      "Epoch 32, Batch 157 Loss:0.024135444313287735\n",
      "Epoch 32, Batch 158 Loss:0.09101025760173798\n",
      "Epoch 32, Batch 159 Loss:0.05858331173658371\n",
      "Epoch 32, Batch 160 Loss:0.041504792869091034\n",
      "Epoch 32, Batch 161 Loss:0.061987727880477905\n",
      "Epoch 32, Batch 162 Loss:0.05511432886123657\n",
      "Epoch 32, Batch 163 Loss:0.03391171246767044\n",
      "Epoch 32, Batch 164 Loss:0.06664830446243286\n",
      "Epoch 32, Batch 165 Loss:0.04985833913087845\n",
      "Epoch 32, Batch 166 Loss:0.04531971365213394\n",
      "Epoch 32, Batch 167 Loss:0.06433539092540741\n",
      "Epoch 32, Batch 168 Loss:0.034901902079582214\n",
      "Epoch 32, Batch 169 Loss:0.04113065451383591\n",
      "Epoch 32, Batch 170 Loss:0.07745756208896637\n",
      "Epoch 32, Batch 171 Loss:0.05305171385407448\n",
      "Epoch 32, Batch 172 Loss:0.039994701743125916\n",
      "Epoch 32, Batch 173 Loss:0.0468263104557991\n",
      "Epoch 32, Batch 174 Loss:0.04607872664928436\n",
      "Epoch 32, Batch 175 Loss:0.06819696724414825\n",
      "Epoch 32, Batch 176 Loss:0.05339600890874863\n",
      "Epoch 32, Batch 177 Loss:0.036414653062820435\n",
      "Epoch 32, Batch 178 Loss:0.07968343794345856\n",
      "Epoch 32, Batch 179 Loss:0.05507613718509674\n",
      "Epoch 32, Batch 180 Loss:0.02977246604859829\n",
      "Epoch 32, Batch 181 Loss:0.04460340738296509\n",
      "Epoch 32, Batch 182 Loss:0.05141272768378258\n",
      "Epoch 32, Batch 183 Loss:0.02708103321492672\n",
      "Epoch 32, Batch 184 Loss:0.12319624423980713\n",
      "Epoch 32, Batch 185 Loss:0.03225361928343773\n",
      "Epoch 32, Batch 186 Loss:0.025948941707611084\n",
      "Epoch 32, Batch 187 Loss:0.04698169231414795\n",
      "Epoch 32, Batch 188 Loss:0.04837435856461525\n",
      "Epoch 32, Batch 189 Loss:0.05749588459730148\n",
      "Epoch 32, Batch 190 Loss:0.0284260343760252\n",
      "Epoch 32, Batch 191 Loss:0.04880695790052414\n",
      "Epoch 32, Batch 192 Loss:0.02253888174891472\n",
      "Epoch 32, Batch 193 Loss:0.05400879308581352\n",
      "Epoch 32, Batch 194 Loss:0.05871224030852318\n",
      "Epoch 32, Batch 195 Loss:0.09682519733905792\n",
      "Epoch 32, Batch 196 Loss:0.02996501885354519\n",
      "Epoch 32, Batch 197 Loss:0.05843508243560791\n",
      "Epoch 32, Batch 198 Loss:0.07045333087444305\n",
      "Epoch 32, Batch 199 Loss:0.04956120625138283\n",
      "Epoch 32, Batch 200 Loss:0.04581890627741814\n",
      "Epoch 32, Batch 201 Loss:0.03607669472694397\n",
      "Epoch 32, Batch 202 Loss:0.028264980763196945\n",
      "Epoch 32, Batch 203 Loss:0.07117588073015213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Batch 204 Loss:0.04435427486896515\n",
      "Epoch 32, Batch 205 Loss:0.03495590388774872\n",
      "Epoch 32, Batch 206 Loss:0.05538205802440643\n",
      "Epoch 32, Batch 207 Loss:0.05032966285943985\n",
      "Epoch 32, Batch 208 Loss:0.030888637527823448\n",
      "Epoch 32, Batch 209 Loss:0.05692579597234726\n",
      "Epoch 32, Batch 210 Loss:0.04709557071328163\n",
      "Epoch 32, Batch 211 Loss:0.048101287335157394\n",
      "Epoch 32, Batch 212 Loss:0.08583057671785355\n",
      "Epoch 32, Batch 213 Loss:0.037305254489183426\n",
      "Epoch 32, Batch 214 Loss:0.07100741565227509\n",
      "Epoch 32, Batch 215 Loss:0.09157349169254303\n",
      "Epoch 32, Batch 216 Loss:0.03696544095873833\n",
      "Epoch 32, Batch 217 Loss:0.0686623603105545\n",
      "Epoch 32, Batch 218 Loss:0.07003595679998398\n",
      "Epoch 32, Batch 219 Loss:0.043239593505859375\n",
      "Epoch 32, Batch 220 Loss:0.062168825417757034\n",
      "Epoch 32, Batch 221 Loss:0.08131656050682068\n",
      "Epoch 32, Batch 222 Loss:0.1329805999994278\n",
      "Epoch 32, Batch 223 Loss:0.08841513842344284\n",
      "Epoch 32, Batch 224 Loss:0.05837240815162659\n",
      "Epoch 32, Batch 225 Loss:0.05975208431482315\n",
      "Epoch 32, Batch 226 Loss:0.04536966606974602\n",
      "Epoch 32, Batch 227 Loss:0.06013784557580948\n",
      "Epoch 32, Batch 228 Loss:0.047856636345386505\n",
      "Epoch 32, Batch 229 Loss:0.03161318227648735\n",
      "Epoch 32, Batch 230 Loss:0.057925984263420105\n",
      "Epoch 32, Batch 231 Loss:0.04503791034221649\n",
      "Epoch 32, Batch 232 Loss:0.049729276448488235\n",
      "Epoch 32, Batch 233 Loss:0.07177339494228363\n",
      "Loss in this Epoch is: 7.17733949423 %\n",
      "Accuracy in this Epoch is: 88.6600017548 %\n",
      "Epoch 33, Batch 0 Loss:0.02597615495324135\n",
      "Epoch 33, Batch 1 Loss:0.03325604647397995\n",
      "Epoch 33, Batch 2 Loss:0.05367228016257286\n",
      "Epoch 33, Batch 3 Loss:0.03378795459866524\n",
      "Epoch 33, Batch 4 Loss:0.04923954978585243\n",
      "Epoch 33, Batch 5 Loss:0.04679155349731445\n",
      "Epoch 33, Batch 6 Loss:0.020739050582051277\n",
      "Epoch 33, Batch 7 Loss:0.05053624138236046\n",
      "Epoch 33, Batch 8 Loss:0.020985964685678482\n",
      "Epoch 33, Batch 9 Loss:0.040374405682086945\n",
      "Epoch 33, Batch 10 Loss:0.023282337933778763\n",
      "Epoch 33, Batch 11 Loss:0.03381209820508957\n",
      "Epoch 33, Batch 12 Loss:0.014697214588522911\n",
      "Epoch 33, Batch 13 Loss:0.03346872329711914\n",
      "Epoch 33, Batch 14 Loss:0.05857480317354202\n",
      "Epoch 33, Batch 15 Loss:0.04417913779616356\n",
      "Epoch 33, Batch 16 Loss:0.05088010057806969\n",
      "Epoch 33, Batch 17 Loss:0.013995426706969738\n",
      "Epoch 33, Batch 18 Loss:0.03679191321134567\n",
      "Epoch 33, Batch 19 Loss:0.03933556377887726\n",
      "Epoch 33, Batch 20 Loss:0.054642558097839355\n",
      "Epoch 33, Batch 21 Loss:0.05939571186900139\n",
      "Epoch 33, Batch 22 Loss:0.03933468833565712\n",
      "Epoch 33, Batch 23 Loss:0.024870170280337334\n",
      "Epoch 33, Batch 24 Loss:0.028730586171150208\n",
      "Epoch 33, Batch 25 Loss:0.04123888164758682\n",
      "Epoch 33, Batch 26 Loss:0.057860393077135086\n",
      "Epoch 33, Batch 27 Loss:0.03118531033396721\n",
      "Epoch 33, Batch 28 Loss:0.03336989879608154\n",
      "Epoch 33, Batch 29 Loss:0.06900376081466675\n",
      "Epoch 33, Batch 30 Loss:0.02978583611547947\n",
      "Epoch 33, Batch 31 Loss:0.06542864441871643\n",
      "Epoch 33, Batch 32 Loss:0.0995161160826683\n",
      "Epoch 33, Batch 33 Loss:0.038747139275074005\n",
      "Epoch 33, Batch 34 Loss:0.038195036351680756\n",
      "Epoch 33, Batch 35 Loss:0.039662908762693405\n",
      "Epoch 33, Batch 36 Loss:0.036633044481277466\n",
      "Epoch 33, Batch 37 Loss:0.03461281955242157\n",
      "Epoch 33, Batch 38 Loss:0.04797818884253502\n",
      "Epoch 33, Batch 39 Loss:0.04936368763446808\n",
      "Epoch 33, Batch 40 Loss:0.04331538826227188\n",
      "Epoch 33, Batch 41 Loss:0.04461953043937683\n",
      "Epoch 33, Batch 42 Loss:0.023763028904795647\n",
      "Epoch 33, Batch 43 Loss:0.0575595386326313\n",
      "Epoch 33, Batch 44 Loss:0.0393364317715168\n",
      "Epoch 33, Batch 45 Loss:0.05732016637921333\n",
      "Epoch 33, Batch 46 Loss:0.030216936022043228\n",
      "Epoch 33, Batch 47 Loss:0.01649150811135769\n",
      "Epoch 33, Batch 48 Loss:0.06013777107000351\n",
      "Epoch 33, Batch 49 Loss:0.014271130785346031\n",
      "Epoch 33, Batch 50 Loss:0.0395304299890995\n",
      "Epoch 33, Batch 51 Loss:0.0374738983809948\n",
      "Epoch 33, Batch 52 Loss:0.027831386774778366\n",
      "Epoch 33, Batch 53 Loss:0.023492103442549706\n",
      "Epoch 33, Batch 54 Loss:0.032714977860450745\n",
      "Epoch 33, Batch 55 Loss:0.024437449872493744\n",
      "Epoch 33, Batch 56 Loss:0.04451141133904457\n",
      "Epoch 33, Batch 57 Loss:0.030574534088373184\n",
      "Epoch 33, Batch 58 Loss:0.034795667976140976\n",
      "Epoch 33, Batch 59 Loss:0.04191013425588608\n",
      "Epoch 33, Batch 60 Loss:0.029285917058587074\n",
      "Epoch 33, Batch 61 Loss:0.028930258005857468\n",
      "Epoch 33, Batch 62 Loss:0.012452724389731884\n",
      "Epoch 33, Batch 63 Loss:0.03619443625211716\n",
      "Epoch 33, Batch 64 Loss:0.0699923187494278\n",
      "Epoch 33, Batch 65 Loss:0.023967130109667778\n",
      "Epoch 33, Batch 66 Loss:0.036598093807697296\n",
      "Epoch 33, Batch 67 Loss:0.026435350999236107\n",
      "Epoch 33, Batch 68 Loss:0.040116507560014725\n",
      "Epoch 33, Batch 69 Loss:0.05222831666469574\n",
      "Epoch 33, Batch 70 Loss:0.06292089074850082\n",
      "Epoch 33, Batch 71 Loss:0.022562123835086823\n",
      "Epoch 33, Batch 72 Loss:0.038076940923929214\n",
      "Epoch 33, Batch 73 Loss:0.07040277123451233\n",
      "Epoch 33, Batch 74 Loss:0.016859551891684532\n",
      "Epoch 33, Batch 75 Loss:0.02846597507596016\n",
      "Epoch 33, Batch 76 Loss:0.026114052161574364\n",
      "Epoch 33, Batch 77 Loss:0.02997683919966221\n",
      "Epoch 33, Batch 78 Loss:0.05275354161858559\n",
      "Epoch 33, Batch 79 Loss:0.035099588334560394\n",
      "Epoch 33, Batch 80 Loss:0.030276061967015266\n",
      "Epoch 33, Batch 81 Loss:0.07483921200037003\n",
      "Epoch 33, Batch 82 Loss:0.04066653177142143\n",
      "Epoch 33, Batch 83 Loss:0.03653506934642792\n",
      "Epoch 33, Batch 84 Loss:0.04914482310414314\n",
      "Epoch 33, Batch 85 Loss:0.03279349207878113\n",
      "Epoch 33, Batch 86 Loss:0.02787347137928009\n",
      "Epoch 33, Batch 87 Loss:0.06158607080578804\n",
      "Epoch 33, Batch 88 Loss:0.03446396067738533\n",
      "Epoch 33, Batch 89 Loss:0.03779967129230499\n",
      "Epoch 33, Batch 90 Loss:0.047228071838617325\n",
      "Epoch 33, Batch 91 Loss:0.02414325624704361\n",
      "Epoch 33, Batch 92 Loss:0.02845725417137146\n",
      "Epoch 33, Batch 93 Loss:0.04703371599316597\n",
      "Epoch 33, Batch 94 Loss:0.02326326258480549\n",
      "Epoch 33, Batch 95 Loss:0.05463797226548195\n",
      "Epoch 33, Batch 96 Loss:0.028024237602949142\n",
      "Epoch 33, Batch 97 Loss:0.028769245371222496\n",
      "Epoch 33, Batch 98 Loss:0.046741124242544174\n",
      "Epoch 33, Batch 99 Loss:0.057980217039585114\n",
      "Epoch 33, Batch 100 Loss:0.02349475771188736\n",
      "Epoch 33, Batch 101 Loss:0.048865340650081635\n",
      "Epoch 33, Batch 102 Loss:0.041891735047101974\n",
      "Epoch 33, Batch 103 Loss:0.04188418760895729\n",
      "Epoch 33, Batch 104 Loss:0.04581267386674881\n",
      "Epoch 33, Batch 105 Loss:0.04103371128439903\n",
      "Epoch 33, Batch 106 Loss:0.030457710847258568\n",
      "Epoch 33, Batch 107 Loss:0.04560744762420654\n",
      "Epoch 33, Batch 108 Loss:0.06447643041610718\n",
      "Epoch 33, Batch 109 Loss:0.04118441045284271\n",
      "Epoch 33, Batch 110 Loss:0.018636900931596756\n",
      "Epoch 33, Batch 111 Loss:0.0404868945479393\n",
      "Epoch 33, Batch 112 Loss:0.029518894851207733\n",
      "Epoch 33, Batch 113 Loss:0.05545223131775856\n",
      "Epoch 33, Batch 114 Loss:0.030756870284676552\n",
      "Epoch 33, Batch 115 Loss:0.025413721799850464\n",
      "Epoch 33, Batch 116 Loss:0.026090797036886215\n",
      "Epoch 33, Batch 117 Loss:0.042401574552059174\n",
      "Epoch 33, Batch 118 Loss:0.048294711858034134\n",
      "Epoch 33, Batch 119 Loss:0.051487963646650314\n",
      "Epoch 33, Batch 120 Loss:0.0237372238188982\n",
      "Epoch 33, Batch 121 Loss:0.02245550975203514\n",
      "Epoch 33, Batch 122 Loss:0.05801161006093025\n",
      "Epoch 33, Batch 123 Loss:0.05729866400361061\n",
      "Epoch 33, Batch 124 Loss:0.02789342775940895\n",
      "Epoch 33, Batch 125 Loss:0.06127612292766571\n",
      "Epoch 33, Batch 126 Loss:0.06499908864498138\n",
      "Epoch 33, Batch 127 Loss:0.03043011762201786\n",
      "Epoch 33, Batch 128 Loss:0.022758513689041138\n",
      "Epoch 33, Batch 129 Loss:0.06448009610176086\n",
      "Epoch 33, Batch 130 Loss:0.12977415323257446\n",
      "Epoch 33, Batch 131 Loss:0.06655795872211456\n",
      "Epoch 33, Batch 132 Loss:0.11795447021722794\n",
      "Epoch 33, Batch 133 Loss:0.016033673658967018\n",
      "Epoch 33, Batch 134 Loss:0.04401948302984238\n",
      "Epoch 33, Batch 135 Loss:0.0635610967874527\n",
      "Epoch 33, Batch 136 Loss:0.0688362866640091\n",
      "Epoch 33, Batch 137 Loss:0.03737768530845642\n",
      "Epoch 33, Batch 138 Loss:0.026865780353546143\n",
      "Epoch 33, Batch 139 Loss:0.039762623608112335\n",
      "Epoch 33, Batch 140 Loss:0.048837095499038696\n",
      "Epoch 33, Batch 141 Loss:0.0824851468205452\n",
      "Epoch 33, Batch 142 Loss:0.034384701400995255\n",
      "Epoch 33, Batch 143 Loss:0.028783433139324188\n",
      "Epoch 33, Batch 144 Loss:0.03369811922311783\n",
      "Epoch 33, Batch 145 Loss:0.0470702201128006\n",
      "Epoch 33, Batch 146 Loss:0.037217915058135986\n",
      "Epoch 33, Batch 147 Loss:0.03716312348842621\n",
      "Epoch 33, Batch 148 Loss:0.053794942796230316\n",
      "Epoch 33, Batch 149 Loss:0.045086611062288284\n",
      "Epoch 33, Batch 150 Loss:0.043025944381952286\n",
      "Epoch 33, Batch 151 Loss:0.050350122153759\n",
      "Epoch 33, Batch 152 Loss:0.03832805156707764\n",
      "Epoch 33, Batch 153 Loss:0.07398095726966858\n",
      "Epoch 33, Batch 154 Loss:0.03028373047709465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Batch 155 Loss:0.03061392903327942\n",
      "Epoch 33, Batch 156 Loss:0.03219214826822281\n",
      "Epoch 33, Batch 157 Loss:0.025599133223295212\n",
      "Epoch 33, Batch 158 Loss:0.028144748881459236\n",
      "Epoch 33, Batch 159 Loss:0.05148475617170334\n",
      "Epoch 33, Batch 160 Loss:0.0240117646753788\n",
      "Epoch 33, Batch 161 Loss:0.05833252891898155\n",
      "Epoch 33, Batch 162 Loss:0.023112863302230835\n",
      "Epoch 33, Batch 163 Loss:0.03296654671430588\n",
      "Epoch 33, Batch 164 Loss:0.06076524406671524\n",
      "Epoch 33, Batch 165 Loss:0.06593664735555649\n",
      "Epoch 33, Batch 166 Loss:0.08011242747306824\n",
      "Epoch 33, Batch 167 Loss:0.017982743680477142\n",
      "Epoch 33, Batch 168 Loss:0.047000087797641754\n",
      "Epoch 33, Batch 169 Loss:0.02856026589870453\n",
      "Epoch 33, Batch 170 Loss:0.034385912120342255\n",
      "Epoch 33, Batch 171 Loss:0.06572381407022476\n",
      "Epoch 33, Batch 172 Loss:0.06423245370388031\n",
      "Epoch 33, Batch 173 Loss:0.033163249492645264\n",
      "Epoch 33, Batch 174 Loss:0.03512520343065262\n",
      "Epoch 33, Batch 175 Loss:0.028884921222925186\n",
      "Epoch 33, Batch 176 Loss:0.037289202213287354\n",
      "Epoch 33, Batch 177 Loss:0.059577520936727524\n",
      "Epoch 33, Batch 178 Loss:0.058693498373031616\n",
      "Epoch 33, Batch 179 Loss:0.06067829951643944\n",
      "Epoch 33, Batch 180 Loss:0.0425703302025795\n",
      "Epoch 33, Batch 181 Loss:0.036937061697244644\n",
      "Epoch 33, Batch 182 Loss:0.048856399953365326\n",
      "Epoch 33, Batch 183 Loss:0.03339735418558121\n",
      "Epoch 33, Batch 184 Loss:0.0543903112411499\n",
      "Epoch 33, Batch 185 Loss:0.036594733595848083\n",
      "Epoch 33, Batch 186 Loss:0.037039779126644135\n",
      "Epoch 33, Batch 187 Loss:0.04557891562581062\n",
      "Epoch 33, Batch 188 Loss:0.02918127551674843\n",
      "Epoch 33, Batch 189 Loss:0.044184133410453796\n",
      "Epoch 33, Batch 190 Loss:0.03228805959224701\n",
      "Epoch 33, Batch 191 Loss:0.03796503320336342\n",
      "Epoch 33, Batch 192 Loss:0.1624559909105301\n",
      "Epoch 33, Batch 193 Loss:0.029189251363277435\n",
      "Epoch 33, Batch 194 Loss:0.032636336982250214\n",
      "Epoch 33, Batch 195 Loss:0.04254628345370293\n",
      "Epoch 33, Batch 196 Loss:0.07068853825330734\n",
      "Epoch 33, Batch 197 Loss:0.018528342247009277\n",
      "Epoch 33, Batch 198 Loss:0.05347117781639099\n",
      "Epoch 33, Batch 199 Loss:0.04434986039996147\n",
      "Epoch 33, Batch 200 Loss:0.06308047473430634\n",
      "Epoch 33, Batch 201 Loss:0.03970256447792053\n",
      "Epoch 33, Batch 202 Loss:0.05550214275717735\n",
      "Epoch 33, Batch 203 Loss:0.03325122594833374\n",
      "Epoch 33, Batch 204 Loss:0.05121836066246033\n",
      "Epoch 33, Batch 205 Loss:0.04097660630941391\n",
      "Epoch 33, Batch 206 Loss:0.10353362560272217\n",
      "Epoch 33, Batch 207 Loss:0.056774236261844635\n",
      "Epoch 33, Batch 208 Loss:0.07411742210388184\n",
      "Epoch 33, Batch 209 Loss:0.05849180370569229\n",
      "Epoch 33, Batch 210 Loss:0.10780594497919083\n",
      "Epoch 33, Batch 211 Loss:0.04896377772092819\n",
      "Epoch 33, Batch 212 Loss:0.03577977418899536\n",
      "Epoch 33, Batch 213 Loss:0.05436166375875473\n",
      "Epoch 33, Batch 214 Loss:0.05063016340136528\n",
      "Epoch 33, Batch 215 Loss:0.06405718624591827\n",
      "Epoch 33, Batch 216 Loss:0.04837831109762192\n",
      "Epoch 33, Batch 217 Loss:0.06621581315994263\n",
      "Epoch 33, Batch 218 Loss:0.049160800874233246\n",
      "Epoch 33, Batch 219 Loss:0.02386477217078209\n",
      "Epoch 33, Batch 220 Loss:0.047404251992702484\n",
      "Epoch 33, Batch 221 Loss:0.02982266992330551\n",
      "Epoch 33, Batch 222 Loss:0.050814490765333176\n",
      "Epoch 33, Batch 223 Loss:0.042992182075977325\n",
      "Epoch 33, Batch 224 Loss:0.05706999450922012\n",
      "Epoch 33, Batch 225 Loss:0.06744597852230072\n",
      "Epoch 33, Batch 226 Loss:0.04454518109560013\n",
      "Epoch 33, Batch 227 Loss:0.055889792740345\n",
      "Epoch 33, Batch 228 Loss:0.043094657361507416\n",
      "Epoch 33, Batch 229 Loss:0.04194559156894684\n",
      "Epoch 33, Batch 230 Loss:0.05458369106054306\n",
      "Epoch 33, Batch 231 Loss:0.02924199588596821\n",
      "Epoch 33, Batch 232 Loss:0.07329194247722626\n",
      "Epoch 33, Batch 233 Loss:0.03655721992254257\n",
      "Loss in this Epoch is: 3.65572199225 %\n",
      "Accuracy in this Epoch is: 88.7799978256 %\n",
      "Epoch 34, Batch 0 Loss:0.054314371198415756\n",
      "Epoch 34, Batch 1 Loss:0.03145718574523926\n",
      "Epoch 34, Batch 2 Loss:0.05338282883167267\n",
      "Epoch 34, Batch 3 Loss:0.03469109535217285\n",
      "Epoch 34, Batch 4 Loss:0.03837895020842552\n",
      "Epoch 34, Batch 5 Loss:0.025356125086545944\n",
      "Epoch 34, Batch 6 Loss:0.05061417818069458\n",
      "Epoch 34, Batch 7 Loss:0.034339237958192825\n",
      "Epoch 34, Batch 8 Loss:0.039024513214826584\n",
      "Epoch 34, Batch 9 Loss:0.038680873811244965\n",
      "Epoch 34, Batch 10 Loss:0.05513700470328331\n",
      "Epoch 34, Batch 11 Loss:0.0349566675722599\n",
      "Epoch 34, Batch 12 Loss:0.03643779084086418\n",
      "Epoch 34, Batch 13 Loss:0.016052328050136566\n",
      "Epoch 34, Batch 14 Loss:0.045382216572761536\n",
      "Epoch 34, Batch 15 Loss:0.05111151933670044\n",
      "Epoch 34, Batch 16 Loss:0.03648873418569565\n",
      "Epoch 34, Batch 17 Loss:0.028336988762021065\n",
      "Epoch 34, Batch 18 Loss:0.026638830080628395\n",
      "Epoch 34, Batch 19 Loss:0.021007630974054337\n",
      "Epoch 34, Batch 20 Loss:0.022340325638651848\n",
      "Epoch 34, Batch 21 Loss:0.044321268796920776\n",
      "Epoch 34, Batch 22 Loss:0.021156329661607742\n",
      "Epoch 34, Batch 23 Loss:0.030141983181238174\n",
      "Epoch 34, Batch 24 Loss:0.027104007080197334\n",
      "Epoch 34, Batch 25 Loss:0.04711316153407097\n",
      "Epoch 34, Batch 26 Loss:0.031162766739726067\n",
      "Epoch 34, Batch 27 Loss:0.014562584459781647\n",
      "Epoch 34, Batch 28 Loss:0.029354047030210495\n",
      "Epoch 34, Batch 29 Loss:0.020618753507733345\n",
      "Epoch 34, Batch 30 Loss:0.04064178094267845\n",
      "Epoch 34, Batch 31 Loss:0.021193742752075195\n",
      "Epoch 34, Batch 32 Loss:0.013286983594298363\n",
      "Epoch 34, Batch 33 Loss:0.031876008957624435\n",
      "Epoch 34, Batch 34 Loss:0.016835784539580345\n",
      "Epoch 34, Batch 35 Loss:0.024951346218585968\n",
      "Epoch 34, Batch 36 Loss:0.014967658556997776\n",
      "Epoch 34, Batch 37 Loss:0.02401573583483696\n",
      "Epoch 34, Batch 38 Loss:0.012298584915697575\n",
      "Epoch 34, Batch 39 Loss:0.05576646700501442\n",
      "Epoch 34, Batch 40 Loss:0.04651660844683647\n",
      "Epoch 34, Batch 41 Loss:0.009395286440849304\n",
      "Epoch 34, Batch 42 Loss:0.01626097410917282\n",
      "Epoch 34, Batch 43 Loss:0.03353884071111679\n",
      "Epoch 34, Batch 44 Loss:0.037827979773283005\n",
      "Epoch 34, Batch 45 Loss:0.034797344356775284\n",
      "Epoch 34, Batch 46 Loss:0.04081578180193901\n",
      "Epoch 34, Batch 47 Loss:0.025594480335712433\n",
      "Epoch 34, Batch 48 Loss:0.03176359087228775\n",
      "Epoch 34, Batch 49 Loss:0.024186477065086365\n",
      "Epoch 34, Batch 50 Loss:0.08423994481563568\n",
      "Epoch 34, Batch 51 Loss:0.05063614621758461\n",
      "Epoch 34, Batch 52 Loss:0.015187840908765793\n",
      "Epoch 34, Batch 53 Loss:0.014926251024007797\n",
      "Epoch 34, Batch 54 Loss:0.03720175847411156\n",
      "Epoch 34, Batch 55 Loss:0.021455051377415657\n",
      "Epoch 34, Batch 56 Loss:0.07279850542545319\n",
      "Epoch 34, Batch 57 Loss:0.0306713804602623\n",
      "Epoch 34, Batch 58 Loss:0.04889484867453575\n",
      "Epoch 34, Batch 59 Loss:0.016478560864925385\n",
      "Epoch 34, Batch 60 Loss:0.028778282925486565\n",
      "Epoch 34, Batch 61 Loss:0.040020596235990524\n",
      "Epoch 34, Batch 62 Loss:0.03347211703658104\n",
      "Epoch 34, Batch 63 Loss:0.04036770761013031\n",
      "Epoch 34, Batch 64 Loss:0.054694488644599915\n",
      "Epoch 34, Batch 65 Loss:0.02160193957388401\n",
      "Epoch 34, Batch 66 Loss:0.05796581879258156\n",
      "Epoch 34, Batch 67 Loss:0.06164771318435669\n",
      "Epoch 34, Batch 68 Loss:0.0747690200805664\n",
      "Epoch 34, Batch 69 Loss:0.014035857282578945\n",
      "Epoch 34, Batch 70 Loss:0.04953126236796379\n",
      "Epoch 34, Batch 71 Loss:0.021811338141560555\n",
      "Epoch 34, Batch 72 Loss:0.062350329011678696\n",
      "Epoch 34, Batch 73 Loss:0.0745207816362381\n",
      "Epoch 34, Batch 74 Loss:0.07286012172698975\n",
      "Epoch 34, Batch 75 Loss:0.03389015421271324\n",
      "Epoch 34, Batch 76 Loss:0.07114893943071365\n",
      "Epoch 34, Batch 77 Loss:0.09137200564146042\n",
      "Epoch 34, Batch 78 Loss:0.06353645771741867\n",
      "Epoch 34, Batch 79 Loss:0.038103532046079636\n",
      "Epoch 34, Batch 80 Loss:0.04676525667309761\n",
      "Epoch 34, Batch 81 Loss:0.040357980877161026\n",
      "Epoch 34, Batch 82 Loss:0.030802056193351746\n",
      "Epoch 34, Batch 83 Loss:0.02665461041033268\n",
      "Epoch 34, Batch 84 Loss:0.054365430027246475\n",
      "Epoch 34, Batch 85 Loss:0.0282486230134964\n",
      "Epoch 34, Batch 86 Loss:0.024603426456451416\n",
      "Epoch 34, Batch 87 Loss:0.054240718483924866\n",
      "Epoch 34, Batch 88 Loss:0.04093215614557266\n",
      "Epoch 34, Batch 89 Loss:0.06102486327290535\n",
      "Epoch 34, Batch 90 Loss:0.03191173076629639\n",
      "Epoch 34, Batch 91 Loss:0.09978697448968887\n",
      "Epoch 34, Batch 92 Loss:0.04285641759634018\n",
      "Epoch 34, Batch 93 Loss:0.05228688195347786\n",
      "Epoch 34, Batch 94 Loss:0.038420919328927994\n",
      "Epoch 34, Batch 95 Loss:0.03600560873746872\n",
      "Epoch 34, Batch 96 Loss:0.057181619107723236\n",
      "Epoch 34, Batch 97 Loss:0.03445803374052048\n",
      "Epoch 34, Batch 98 Loss:0.057364992797374725\n",
      "Epoch 34, Batch 99 Loss:0.047970544546842575\n",
      "Epoch 34, Batch 100 Loss:0.0788855254650116\n",
      "Epoch 34, Batch 101 Loss:0.055797427892684937\n",
      "Epoch 34, Batch 102 Loss:0.03895893692970276\n",
      "Epoch 34, Batch 103 Loss:0.05686557665467262\n",
      "Epoch 34, Batch 104 Loss:0.05757066234946251\n",
      "Epoch 34, Batch 105 Loss:0.03166276589035988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Batch 106 Loss:0.044368866831064224\n",
      "Epoch 34, Batch 107 Loss:0.06107741594314575\n",
      "Epoch 34, Batch 108 Loss:0.03542908653616905\n",
      "Epoch 34, Batch 109 Loss:0.06773693114519119\n",
      "Epoch 34, Batch 110 Loss:0.03414209187030792\n",
      "Epoch 34, Batch 111 Loss:0.041726600378751755\n",
      "Epoch 34, Batch 112 Loss:0.04350505396723747\n",
      "Epoch 34, Batch 113 Loss:0.019817225635051727\n",
      "Epoch 34, Batch 114 Loss:0.028325654566287994\n",
      "Epoch 34, Batch 115 Loss:0.07552284747362137\n",
      "Epoch 34, Batch 116 Loss:0.044932205229997635\n",
      "Epoch 34, Batch 117 Loss:0.03607160598039627\n",
      "Epoch 34, Batch 118 Loss:0.06849800050258636\n",
      "Epoch 34, Batch 119 Loss:0.05317516252398491\n",
      "Epoch 34, Batch 120 Loss:0.0481964610517025\n",
      "Epoch 34, Batch 121 Loss:0.053500331938266754\n",
      "Epoch 34, Batch 122 Loss:0.03728028014302254\n",
      "Epoch 34, Batch 123 Loss:0.03888818249106407\n",
      "Epoch 34, Batch 124 Loss:0.03533267229795456\n",
      "Epoch 34, Batch 125 Loss:0.06349289417266846\n",
      "Epoch 34, Batch 126 Loss:0.0450933612883091\n",
      "Epoch 34, Batch 127 Loss:0.05351226031780243\n",
      "Epoch 34, Batch 128 Loss:0.03457925468683243\n",
      "Epoch 34, Batch 129 Loss:0.024942655116319656\n",
      "Epoch 34, Batch 130 Loss:0.04152172431349754\n",
      "Epoch 34, Batch 131 Loss:0.04837281256914139\n",
      "Epoch 34, Batch 132 Loss:0.03309411555528641\n",
      "Epoch 34, Batch 133 Loss:0.05933745205402374\n",
      "Epoch 34, Batch 134 Loss:0.04184313863515854\n",
      "Epoch 34, Batch 135 Loss:0.03765237331390381\n",
      "Epoch 34, Batch 136 Loss:0.06292114406824112\n",
      "Epoch 34, Batch 137 Loss:0.04680309817194939\n",
      "Epoch 34, Batch 138 Loss:0.02306172624230385\n",
      "Epoch 34, Batch 139 Loss:0.05796287581324577\n",
      "Epoch 34, Batch 140 Loss:0.0520474910736084\n",
      "Epoch 34, Batch 141 Loss:0.058637142181396484\n",
      "Epoch 34, Batch 142 Loss:0.059242427349090576\n",
      "Epoch 34, Batch 143 Loss:0.057529017329216\n",
      "Epoch 34, Batch 144 Loss:0.050337545573711395\n",
      "Epoch 34, Batch 145 Loss:0.07738658040761948\n",
      "Epoch 34, Batch 146 Loss:0.04512102156877518\n",
      "Epoch 34, Batch 147 Loss:0.05603887885808945\n",
      "Epoch 34, Batch 148 Loss:0.04569577798247337\n",
      "Epoch 34, Batch 149 Loss:0.05204766243696213\n",
      "Epoch 34, Batch 150 Loss:0.04245468229055405\n",
      "Epoch 34, Batch 151 Loss:0.05420294404029846\n",
      "Epoch 34, Batch 152 Loss:0.03241873160004616\n",
      "Epoch 34, Batch 153 Loss:0.0433158352971077\n",
      "Epoch 34, Batch 154 Loss:0.05225260555744171\n",
      "Epoch 34, Batch 155 Loss:0.03660931438207626\n",
      "Epoch 34, Batch 156 Loss:0.039418116211891174\n",
      "Epoch 34, Batch 157 Loss:0.02163463644683361\n",
      "Epoch 34, Batch 158 Loss:0.03799694776535034\n",
      "Epoch 34, Batch 159 Loss:0.04738213121891022\n",
      "Epoch 34, Batch 160 Loss:0.04494712874293327\n",
      "Epoch 34, Batch 161 Loss:0.08921071141958237\n",
      "Epoch 34, Batch 162 Loss:0.08463980257511139\n",
      "Epoch 34, Batch 163 Loss:0.06696078926324844\n",
      "Epoch 34, Batch 164 Loss:0.03136499226093292\n",
      "Epoch 34, Batch 165 Loss:0.04559876769781113\n",
      "Epoch 34, Batch 166 Loss:0.08547092974185944\n",
      "Epoch 34, Batch 167 Loss:0.06135624274611473\n",
      "Epoch 34, Batch 168 Loss:0.074788898229599\n",
      "Epoch 34, Batch 169 Loss:0.06820487976074219\n",
      "Epoch 34, Batch 170 Loss:0.030527273193001747\n",
      "Epoch 34, Batch 171 Loss:0.09570740908384323\n",
      "Epoch 34, Batch 172 Loss:0.054843783378601074\n",
      "Epoch 34, Batch 173 Loss:0.06537661701440811\n",
      "Epoch 34, Batch 174 Loss:0.04172651469707489\n",
      "Epoch 34, Batch 175 Loss:0.07269507646560669\n",
      "Epoch 34, Batch 176 Loss:0.03601576387882233\n",
      "Epoch 34, Batch 177 Loss:0.04345289617776871\n",
      "Epoch 34, Batch 178 Loss:0.060614213347435\n",
      "Epoch 34, Batch 179 Loss:0.03401057794690132\n",
      "Epoch 34, Batch 180 Loss:0.06269091367721558\n",
      "Epoch 34, Batch 181 Loss:0.05692394822835922\n",
      "Epoch 34, Batch 182 Loss:0.027399998158216476\n",
      "Epoch 34, Batch 183 Loss:0.09979039430618286\n",
      "Epoch 34, Batch 184 Loss:0.060256361961364746\n",
      "Epoch 34, Batch 185 Loss:0.042979493737220764\n",
      "Epoch 34, Batch 186 Loss:0.03265107423067093\n",
      "Epoch 34, Batch 187 Loss:0.027202114462852478\n",
      "Epoch 34, Batch 188 Loss:0.03527189791202545\n",
      "Epoch 34, Batch 189 Loss:0.030146079137921333\n",
      "Epoch 34, Batch 190 Loss:0.03145469352602959\n",
      "Epoch 34, Batch 191 Loss:0.04461396485567093\n",
      "Epoch 34, Batch 192 Loss:0.13299697637557983\n",
      "Epoch 34, Batch 193 Loss:0.037086259573698044\n",
      "Epoch 34, Batch 194 Loss:0.06809710711240768\n",
      "Epoch 34, Batch 195 Loss:0.09733311831951141\n",
      "Epoch 34, Batch 196 Loss:0.08197079598903656\n",
      "Epoch 34, Batch 197 Loss:0.050058938562870026\n",
      "Epoch 34, Batch 198 Loss:0.06441798806190491\n",
      "Epoch 34, Batch 199 Loss:0.06336444616317749\n",
      "Epoch 34, Batch 200 Loss:0.11237302422523499\n",
      "Epoch 34, Batch 201 Loss:0.057851869612932205\n",
      "Epoch 34, Batch 202 Loss:0.09765338897705078\n",
      "Epoch 34, Batch 203 Loss:0.05969718098640442\n",
      "Epoch 34, Batch 204 Loss:0.05978159233927727\n",
      "Epoch 34, Batch 205 Loss:0.038842350244522095\n",
      "Epoch 34, Batch 206 Loss:0.05326058715581894\n",
      "Epoch 34, Batch 207 Loss:0.03845634683966637\n",
      "Epoch 34, Batch 208 Loss:0.033328354358673096\n",
      "Epoch 34, Batch 209 Loss:0.04676498472690582\n",
      "Epoch 34, Batch 210 Loss:0.05751203000545502\n",
      "Epoch 34, Batch 211 Loss:0.044730838388204575\n",
      "Epoch 34, Batch 212 Loss:0.04455094784498215\n",
      "Epoch 34, Batch 213 Loss:0.1324063092470169\n",
      "Epoch 34, Batch 214 Loss:0.03209652006626129\n",
      "Epoch 34, Batch 215 Loss:0.0312481876462698\n",
      "Epoch 34, Batch 216 Loss:0.0596027597784996\n",
      "Epoch 34, Batch 217 Loss:0.054530683904886246\n",
      "Epoch 34, Batch 218 Loss:0.06890903413295746\n",
      "Epoch 34, Batch 219 Loss:0.03791991248726845\n",
      "Epoch 34, Batch 220 Loss:0.0729730874300003\n",
      "Epoch 34, Batch 221 Loss:0.053595490753650665\n",
      "Epoch 34, Batch 222 Loss:0.03394761681556702\n",
      "Epoch 34, Batch 223 Loss:0.035037606954574585\n",
      "Epoch 34, Batch 224 Loss:0.03726084157824516\n",
      "Epoch 34, Batch 225 Loss:0.06332157552242279\n",
      "Epoch 34, Batch 226 Loss:0.07825972139835358\n",
      "Epoch 34, Batch 227 Loss:0.06567328423261642\n",
      "Epoch 34, Batch 228 Loss:0.05019121989607811\n",
      "Epoch 34, Batch 229 Loss:0.035286325961351395\n",
      "Epoch 34, Batch 230 Loss:0.058257389813661575\n",
      "Epoch 34, Batch 231 Loss:0.053787413984537125\n",
      "Epoch 34, Batch 232 Loss:0.024812001734972\n",
      "Epoch 34, Batch 233 Loss:0.04994867742061615\n",
      "Loss in this Epoch is: 4.99486774206 %\n",
      "Accuracy in this Epoch is: 89.1900002956 %\n",
      "Epoch 35, Batch 0 Loss:0.035433124750852585\n",
      "Epoch 35, Batch 1 Loss:0.0646289587020874\n",
      "Epoch 35, Batch 2 Loss:0.016804682090878487\n",
      "Epoch 35, Batch 3 Loss:0.03889193758368492\n",
      "Epoch 35, Batch 4 Loss:0.023818695917725563\n",
      "Epoch 35, Batch 5 Loss:0.023727158084511757\n",
      "Epoch 35, Batch 6 Loss:0.09221877157688141\n",
      "Epoch 35, Batch 7 Loss:0.04587322473526001\n",
      "Epoch 35, Batch 8 Loss:0.02063904143869877\n",
      "Epoch 35, Batch 9 Loss:0.03015967085957527\n",
      "Epoch 35, Batch 10 Loss:0.03459553420543671\n",
      "Epoch 35, Batch 11 Loss:0.04126320779323578\n",
      "Epoch 35, Batch 12 Loss:0.03742600232362747\n",
      "Epoch 35, Batch 13 Loss:0.04750406742095947\n",
      "Epoch 35, Batch 14 Loss:0.037482358515262604\n",
      "Epoch 35, Batch 15 Loss:0.042181454598903656\n",
      "Epoch 35, Batch 16 Loss:0.018487002700567245\n",
      "Epoch 35, Batch 17 Loss:0.04008140787482262\n",
      "Epoch 35, Batch 18 Loss:0.024882998317480087\n",
      "Epoch 35, Batch 19 Loss:0.06416281312704086\n",
      "Epoch 35, Batch 20 Loss:0.03779060021042824\n",
      "Epoch 35, Batch 21 Loss:0.011065656319260597\n",
      "Epoch 35, Batch 22 Loss:0.04137112572789192\n",
      "Epoch 35, Batch 23 Loss:0.04074794054031372\n",
      "Epoch 35, Batch 24 Loss:0.039220359176397324\n",
      "Epoch 35, Batch 25 Loss:0.0301375649869442\n",
      "Epoch 35, Batch 26 Loss:0.027509557083249092\n",
      "Epoch 35, Batch 27 Loss:0.025120491161942482\n",
      "Epoch 35, Batch 28 Loss:0.03795965015888214\n",
      "Epoch 35, Batch 29 Loss:0.07191304117441177\n",
      "Epoch 35, Batch 30 Loss:0.00966179370880127\n",
      "Epoch 35, Batch 31 Loss:0.01734619215130806\n",
      "Epoch 35, Batch 32 Loss:0.015847602859139442\n",
      "Epoch 35, Batch 33 Loss:0.039494965225458145\n",
      "Epoch 35, Batch 34 Loss:0.014660232700407505\n",
      "Epoch 35, Batch 35 Loss:0.03318294137716293\n",
      "Epoch 35, Batch 36 Loss:0.04078133404254913\n",
      "Epoch 35, Batch 37 Loss:0.02358381636440754\n",
      "Epoch 35, Batch 38 Loss:0.018152551725506783\n",
      "Epoch 35, Batch 39 Loss:0.033464837819337845\n",
      "Epoch 35, Batch 40 Loss:0.0487518385052681\n",
      "Epoch 35, Batch 41 Loss:0.04553816467523575\n",
      "Epoch 35, Batch 42 Loss:0.014524217694997787\n",
      "Epoch 35, Batch 43 Loss:0.03003515675663948\n",
      "Epoch 35, Batch 44 Loss:0.023780716583132744\n",
      "Epoch 35, Batch 45 Loss:0.014829431660473347\n",
      "Epoch 35, Batch 46 Loss:0.052645932883024216\n",
      "Epoch 35, Batch 47 Loss:0.015112761408090591\n",
      "Epoch 35, Batch 48 Loss:0.04499002918601036\n",
      "Epoch 35, Batch 49 Loss:0.024856554344296455\n",
      "Epoch 35, Batch 50 Loss:0.010268735699355602\n",
      "Epoch 35, Batch 51 Loss:0.010919518768787384\n",
      "Epoch 35, Batch 52 Loss:0.020864607766270638\n",
      "Epoch 35, Batch 53 Loss:0.050798866897821426\n",
      "Epoch 35, Batch 54 Loss:0.013698588125407696\n",
      "Epoch 35, Batch 55 Loss:0.04792303964495659\n",
      "Epoch 35, Batch 56 Loss:0.032052017748355865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Batch 57 Loss:0.018071772530674934\n",
      "Epoch 35, Batch 58 Loss:0.07480627298355103\n",
      "Epoch 35, Batch 59 Loss:0.03420701622962952\n",
      "Epoch 35, Batch 60 Loss:0.01898408867418766\n",
      "Epoch 35, Batch 61 Loss:0.009394067339599133\n",
      "Epoch 35, Batch 62 Loss:0.038791805505752563\n",
      "Epoch 35, Batch 63 Loss:0.05436639115214348\n",
      "Epoch 35, Batch 64 Loss:0.0398494154214859\n",
      "Epoch 35, Batch 65 Loss:0.0391867458820343\n",
      "Epoch 35, Batch 66 Loss:0.019653355702757835\n",
      "Epoch 35, Batch 67 Loss:0.026325631886720657\n",
      "Epoch 35, Batch 68 Loss:0.021441346034407616\n",
      "Epoch 35, Batch 69 Loss:0.073977530002594\n",
      "Epoch 35, Batch 70 Loss:0.13794651627540588\n",
      "Epoch 35, Batch 71 Loss:0.08049279451370239\n",
      "Epoch 35, Batch 72 Loss:0.025410527363419533\n",
      "Epoch 35, Batch 73 Loss:0.018221721053123474\n",
      "Epoch 35, Batch 74 Loss:0.017459271475672722\n",
      "Epoch 35, Batch 75 Loss:0.027561591938138008\n",
      "Epoch 35, Batch 76 Loss:0.03184879198670387\n",
      "Epoch 35, Batch 77 Loss:0.07630520313978195\n",
      "Epoch 35, Batch 78 Loss:0.031197723001241684\n",
      "Epoch 35, Batch 79 Loss:0.031851571053266525\n",
      "Epoch 35, Batch 80 Loss:0.04082104563713074\n",
      "Epoch 35, Batch 81 Loss:0.031381040811538696\n",
      "Epoch 35, Batch 82 Loss:0.04776383563876152\n",
      "Epoch 35, Batch 83 Loss:0.05104526877403259\n",
      "Epoch 35, Batch 84 Loss:0.03509455546736717\n",
      "Epoch 35, Batch 85 Loss:0.04340924322605133\n",
      "Epoch 35, Batch 86 Loss:0.05928399786353111\n",
      "Epoch 35, Batch 87 Loss:0.04612939804792404\n",
      "Epoch 35, Batch 88 Loss:0.06875487416982651\n",
      "Epoch 35, Batch 89 Loss:0.06664611399173737\n",
      "Epoch 35, Batch 90 Loss:0.04379500821232796\n",
      "Epoch 35, Batch 91 Loss:0.07690578699111938\n",
      "Epoch 35, Batch 92 Loss:0.04202953726053238\n",
      "Epoch 35, Batch 93 Loss:0.03938625007867813\n",
      "Epoch 35, Batch 94 Loss:0.02479352429509163\n",
      "Epoch 35, Batch 95 Loss:0.027653910219669342\n",
      "Epoch 35, Batch 96 Loss:0.05154455825686455\n",
      "Epoch 35, Batch 97 Loss:0.05220465734601021\n",
      "Epoch 35, Batch 98 Loss:0.03594055771827698\n",
      "Epoch 35, Batch 99 Loss:0.053252823650836945\n",
      "Epoch 35, Batch 100 Loss:0.059778545051813126\n",
      "Epoch 35, Batch 101 Loss:0.026083383709192276\n",
      "Epoch 35, Batch 102 Loss:0.04916103929281235\n",
      "Epoch 35, Batch 103 Loss:0.05641734600067139\n",
      "Epoch 35, Batch 104 Loss:0.04323214665055275\n",
      "Epoch 35, Batch 105 Loss:0.051030028611421585\n",
      "Epoch 35, Batch 106 Loss:0.06593868881464005\n",
      "Epoch 35, Batch 107 Loss:0.04818784445524216\n",
      "Epoch 35, Batch 108 Loss:0.02838965505361557\n",
      "Epoch 35, Batch 109 Loss:0.03503217175602913\n",
      "Epoch 35, Batch 110 Loss:0.024624183773994446\n",
      "Epoch 35, Batch 111 Loss:0.05114580690860748\n",
      "Epoch 35, Batch 112 Loss:0.031083866953849792\n",
      "Epoch 35, Batch 113 Loss:0.03225528821349144\n",
      "Epoch 35, Batch 114 Loss:0.027827369049191475\n",
      "Epoch 35, Batch 115 Loss:0.04155977815389633\n",
      "Epoch 35, Batch 116 Loss:0.04448110610246658\n",
      "Epoch 35, Batch 117 Loss:0.020496167242527008\n",
      "Epoch 35, Batch 118 Loss:0.01832791417837143\n",
      "Epoch 35, Batch 119 Loss:0.0761101171374321\n",
      "Epoch 35, Batch 120 Loss:0.020101182162761688\n",
      "Epoch 35, Batch 121 Loss:0.044220320880413055\n",
      "Epoch 35, Batch 122 Loss:0.033726878464221954\n",
      "Epoch 35, Batch 123 Loss:0.036490924656391144\n",
      "Epoch 35, Batch 124 Loss:0.07292599976062775\n",
      "Epoch 35, Batch 125 Loss:0.025398554280400276\n",
      "Epoch 35, Batch 126 Loss:0.013431341387331486\n",
      "Epoch 35, Batch 127 Loss:0.031148545444011688\n",
      "Epoch 35, Batch 128 Loss:0.027034763246774673\n",
      "Epoch 35, Batch 129 Loss:0.04409976303577423\n",
      "Epoch 35, Batch 130 Loss:0.039329029619693756\n",
      "Epoch 35, Batch 131 Loss:0.01707524061203003\n",
      "Epoch 35, Batch 132 Loss:0.05604732409119606\n",
      "Epoch 35, Batch 133 Loss:0.01638982445001602\n",
      "Epoch 35, Batch 134 Loss:0.0431927889585495\n",
      "Epoch 35, Batch 135 Loss:0.023469896987080574\n",
      "Epoch 35, Batch 136 Loss:0.04254288226366043\n",
      "Epoch 35, Batch 137 Loss:0.0223842840641737\n",
      "Epoch 35, Batch 138 Loss:0.02465316653251648\n",
      "Epoch 35, Batch 139 Loss:0.039780180901288986\n",
      "Epoch 35, Batch 140 Loss:0.017700115218758583\n",
      "Epoch 35, Batch 141 Loss:0.02590789645910263\n",
      "Epoch 35, Batch 142 Loss:0.029051516205072403\n",
      "Epoch 35, Batch 143 Loss:0.03800889849662781\n",
      "Epoch 35, Batch 144 Loss:0.08697928488254547\n",
      "Epoch 35, Batch 145 Loss:0.05543166399002075\n",
      "Epoch 35, Batch 146 Loss:0.04070878028869629\n",
      "Epoch 35, Batch 147 Loss:0.04059455543756485\n",
      "Epoch 35, Batch 148 Loss:0.024344466626644135\n",
      "Epoch 35, Batch 149 Loss:0.03373092785477638\n",
      "Epoch 35, Batch 150 Loss:0.02627868950366974\n",
      "Epoch 35, Batch 151 Loss:0.021336058154702187\n",
      "Epoch 35, Batch 152 Loss:0.03182484209537506\n",
      "Epoch 35, Batch 153 Loss:0.03208836168050766\n",
      "Epoch 35, Batch 154 Loss:0.04831932857632637\n",
      "Epoch 35, Batch 155 Loss:0.06637542694807053\n",
      "Epoch 35, Batch 156 Loss:0.04580115154385567\n",
      "Epoch 35, Batch 157 Loss:0.015543391928076744\n",
      "Epoch 35, Batch 158 Loss:0.033100537955760956\n",
      "Epoch 35, Batch 159 Loss:0.037124477326869965\n",
      "Epoch 35, Batch 160 Loss:0.03648697957396507\n",
      "Epoch 35, Batch 161 Loss:0.02547290176153183\n",
      "Epoch 35, Batch 162 Loss:0.02107548899948597\n",
      "Epoch 35, Batch 163 Loss:0.04561549797654152\n",
      "Epoch 35, Batch 164 Loss:0.05199713259935379\n",
      "Epoch 35, Batch 165 Loss:0.0408061221241951\n",
      "Epoch 35, Batch 166 Loss:0.038664333522319794\n",
      "Epoch 35, Batch 167 Loss:0.03501109778881073\n",
      "Epoch 35, Batch 168 Loss:0.018574845045804977\n",
      "Epoch 35, Batch 169 Loss:0.038979142904281616\n",
      "Epoch 35, Batch 170 Loss:0.06075112894177437\n",
      "Epoch 35, Batch 171 Loss:0.04111434146761894\n",
      "Epoch 35, Batch 172 Loss:0.028343968093395233\n",
      "Epoch 35, Batch 173 Loss:0.04590102285146713\n",
      "Epoch 35, Batch 174 Loss:0.02889825962483883\n",
      "Epoch 35, Batch 175 Loss:0.042607080191373825\n",
      "Epoch 35, Batch 176 Loss:0.06492668390274048\n",
      "Epoch 35, Batch 177 Loss:0.023621613159775734\n",
      "Epoch 35, Batch 178 Loss:0.06648082286119461\n",
      "Epoch 35, Batch 179 Loss:0.04247306287288666\n",
      "Epoch 35, Batch 180 Loss:0.03506483510136604\n",
      "Epoch 35, Batch 181 Loss:0.09310927987098694\n",
      "Epoch 35, Batch 182 Loss:0.02811253070831299\n",
      "Epoch 35, Batch 183 Loss:0.0167994387447834\n",
      "Epoch 35, Batch 184 Loss:0.0343996062874794\n",
      "Epoch 35, Batch 185 Loss:0.033442914485931396\n",
      "Epoch 35, Batch 186 Loss:0.031644754111766815\n",
      "Epoch 35, Batch 187 Loss:0.04750879853963852\n",
      "Epoch 35, Batch 188 Loss:0.02026747725903988\n",
      "Epoch 35, Batch 189 Loss:0.03871531784534454\n",
      "Epoch 35, Batch 190 Loss:0.0980520248413086\n",
      "Epoch 35, Batch 191 Loss:0.039047472178936005\n",
      "Epoch 35, Batch 192 Loss:0.027345405891537666\n",
      "Epoch 35, Batch 193 Loss:0.03587619960308075\n",
      "Epoch 35, Batch 194 Loss:0.030606316402554512\n",
      "Epoch 35, Batch 195 Loss:0.026194781064987183\n",
      "Epoch 35, Batch 196 Loss:0.10330432653427124\n",
      "Epoch 35, Batch 197 Loss:0.0374462828040123\n",
      "Epoch 35, Batch 198 Loss:0.07018347084522247\n",
      "Epoch 35, Batch 199 Loss:0.04940999299287796\n",
      "Epoch 35, Batch 200 Loss:0.04962288588285446\n",
      "Epoch 35, Batch 201 Loss:0.03976774215698242\n",
      "Epoch 35, Batch 202 Loss:0.06593174487352371\n",
      "Epoch 35, Batch 203 Loss:0.1021520122885704\n",
      "Epoch 35, Batch 204 Loss:0.03773898631334305\n",
      "Epoch 35, Batch 205 Loss:0.04934799671173096\n",
      "Epoch 35, Batch 206 Loss:0.07271750271320343\n",
      "Epoch 35, Batch 207 Loss:0.04882880300283432\n",
      "Epoch 35, Batch 208 Loss:0.08063943684101105\n",
      "Epoch 35, Batch 209 Loss:0.03800801560282707\n",
      "Epoch 35, Batch 210 Loss:0.03424149751663208\n",
      "Epoch 35, Batch 211 Loss:0.07423907518386841\n",
      "Epoch 35, Batch 212 Loss:0.060815904289484024\n",
      "Epoch 35, Batch 213 Loss:0.04701411724090576\n",
      "Epoch 35, Batch 214 Loss:0.035458169877529144\n",
      "Epoch 35, Batch 215 Loss:0.03188175708055496\n",
      "Epoch 35, Batch 216 Loss:0.04219198226928711\n",
      "Epoch 35, Batch 217 Loss:0.06688813865184784\n",
      "Epoch 35, Batch 218 Loss:0.02888917550444603\n",
      "Epoch 35, Batch 219 Loss:0.025045184418559074\n",
      "Epoch 35, Batch 220 Loss:0.02885136753320694\n",
      "Epoch 35, Batch 221 Loss:0.024475015699863434\n",
      "Epoch 35, Batch 222 Loss:0.030083008110523224\n",
      "Epoch 35, Batch 223 Loss:0.04453581944108009\n",
      "Epoch 35, Batch 224 Loss:0.028958551585674286\n",
      "Epoch 35, Batch 225 Loss:0.0658969059586525\n",
      "Epoch 35, Batch 226 Loss:0.06649039685726166\n",
      "Epoch 35, Batch 227 Loss:0.036955416202545166\n",
      "Epoch 35, Batch 228 Loss:0.047141529619693756\n",
      "Epoch 35, Batch 229 Loss:0.03794411942362785\n",
      "Epoch 35, Batch 230 Loss:0.0660673975944519\n",
      "Epoch 35, Batch 231 Loss:0.038068920373916626\n",
      "Epoch 35, Batch 232 Loss:0.04050818830728531\n",
      "Epoch 35, Batch 233 Loss:0.03312007337808609\n",
      "Loss in this Epoch is: 3.31200733781 %\n",
      "Accuracy in this Epoch is: 88.7600004673 %\n",
      "Epoch 36, Batch 0 Loss:0.03397823125123978\n",
      "Epoch 36, Batch 1 Loss:0.03559643775224686\n",
      "Epoch 36, Batch 2 Loss:0.020289326086640358\n",
      "Epoch 36, Batch 3 Loss:0.04491068050265312\n",
      "Epoch 36, Batch 4 Loss:0.014994918368756771\n",
      "Epoch 36, Batch 5 Loss:0.038394879549741745\n",
      "Epoch 36, Batch 6 Loss:0.050635602325201035\n",
      "Epoch 36, Batch 7 Loss:0.021932540461421013\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 8 Loss:0.04314323887228966\n",
      "Epoch 36, Batch 9 Loss:0.0678197517991066\n",
      "Epoch 36, Batch 10 Loss:0.01885073259472847\n",
      "Epoch 36, Batch 11 Loss:0.03495725616812706\n",
      "Epoch 36, Batch 12 Loss:0.0372244194149971\n",
      "Epoch 36, Batch 13 Loss:0.040885791182518005\n",
      "Epoch 36, Batch 14 Loss:0.01808851771056652\n",
      "Epoch 36, Batch 15 Loss:0.06527774035930634\n",
      "Epoch 36, Batch 16 Loss:0.023134959861636162\n",
      "Epoch 36, Batch 17 Loss:0.033283837139606476\n",
      "Epoch 36, Batch 18 Loss:0.05573387071490288\n",
      "Epoch 36, Batch 19 Loss:0.03328841179609299\n",
      "Epoch 36, Batch 20 Loss:0.016039907932281494\n",
      "Epoch 36, Batch 21 Loss:0.03288964182138443\n",
      "Epoch 36, Batch 22 Loss:0.05263250693678856\n",
      "Epoch 36, Batch 23 Loss:0.030977245420217514\n",
      "Epoch 36, Batch 24 Loss:0.048840392380952835\n",
      "Epoch 36, Batch 25 Loss:0.045681752264499664\n",
      "Epoch 36, Batch 26 Loss:0.057451944798231125\n",
      "Epoch 36, Batch 27 Loss:0.01968725211918354\n",
      "Epoch 36, Batch 28 Loss:0.026316113770008087\n",
      "Epoch 36, Batch 29 Loss:0.042708080261945724\n",
      "Epoch 36, Batch 30 Loss:0.042800456285476685\n",
      "Epoch 36, Batch 31 Loss:0.019796954467892647\n",
      "Epoch 36, Batch 32 Loss:0.044099245220422745\n",
      "Epoch 36, Batch 33 Loss:0.034198664128780365\n",
      "Epoch 36, Batch 34 Loss:0.025876138359308243\n",
      "Epoch 36, Batch 35 Loss:0.06308253109455109\n",
      "Epoch 36, Batch 36 Loss:0.03129605948925018\n",
      "Epoch 36, Batch 37 Loss:0.022585975006222725\n",
      "Epoch 36, Batch 38 Loss:0.042691417038440704\n",
      "Epoch 36, Batch 39 Loss:0.02861231192946434\n",
      "Epoch 36, Batch 40 Loss:0.01995149254798889\n",
      "Epoch 36, Batch 41 Loss:0.04656637832522392\n",
      "Epoch 36, Batch 42 Loss:0.06393086165189743\n",
      "Epoch 36, Batch 43 Loss:0.024204879999160767\n",
      "Epoch 36, Batch 44 Loss:0.023964984342455864\n",
      "Epoch 36, Batch 45 Loss:0.023701058700680733\n",
      "Epoch 36, Batch 46 Loss:0.011692202650010586\n",
      "Epoch 36, Batch 47 Loss:0.03377087786793709\n",
      "Epoch 36, Batch 48 Loss:0.009464751929044724\n",
      "Epoch 36, Batch 49 Loss:0.02335541509091854\n",
      "Epoch 36, Batch 50 Loss:0.019782638177275658\n",
      "Epoch 36, Batch 51 Loss:0.013599606230854988\n",
      "Epoch 36, Batch 52 Loss:0.03315858915448189\n",
      "Epoch 36, Batch 53 Loss:0.02988196536898613\n",
      "Epoch 36, Batch 54 Loss:0.029787937179207802\n",
      "Epoch 36, Batch 55 Loss:0.035571977496147156\n",
      "Epoch 36, Batch 56 Loss:0.010940004140138626\n",
      "Epoch 36, Batch 57 Loss:0.0120162945240736\n",
      "Epoch 36, Batch 58 Loss:0.010583115741610527\n",
      "Epoch 36, Batch 59 Loss:0.03193368390202522\n",
      "Epoch 36, Batch 60 Loss:0.02294999733567238\n",
      "Epoch 36, Batch 61 Loss:0.03585704416036606\n",
      "Epoch 36, Batch 62 Loss:0.019761357456445694\n",
      "Epoch 36, Batch 63 Loss:0.013713907450437546\n",
      "Epoch 36, Batch 64 Loss:0.05588895455002785\n",
      "Epoch 36, Batch 65 Loss:0.03384575992822647\n",
      "Epoch 36, Batch 66 Loss:0.06386677920818329\n",
      "Epoch 36, Batch 67 Loss:0.013866257853806019\n",
      "Epoch 36, Batch 68 Loss:0.033867452293634415\n",
      "Epoch 36, Batch 69 Loss:0.04182029142975807\n",
      "Epoch 36, Batch 70 Loss:0.030510040000081062\n",
      "Epoch 36, Batch 71 Loss:0.017557403072714806\n",
      "Epoch 36, Batch 72 Loss:0.04416724666953087\n",
      "Epoch 36, Batch 73 Loss:0.11066415160894394\n",
      "Epoch 36, Batch 74 Loss:0.06034606695175171\n",
      "Epoch 36, Batch 75 Loss:0.06048477813601494\n",
      "Epoch 36, Batch 76 Loss:0.022077104076743126\n",
      "Epoch 36, Batch 77 Loss:0.0346619188785553\n",
      "Epoch 36, Batch 78 Loss:0.045024849474430084\n",
      "Epoch 36, Batch 79 Loss:0.03929293900728226\n",
      "Epoch 36, Batch 80 Loss:0.05198860913515091\n",
      "Epoch 36, Batch 81 Loss:0.034919314086437225\n",
      "Epoch 36, Batch 82 Loss:0.04122735187411308\n",
      "Epoch 36, Batch 83 Loss:0.0223810076713562\n",
      "Epoch 36, Batch 84 Loss:0.029930049553513527\n",
      "Epoch 36, Batch 85 Loss:0.04821528121829033\n",
      "Epoch 36, Batch 86 Loss:0.031055990606546402\n",
      "Epoch 36, Batch 87 Loss:0.061571087688207626\n",
      "Epoch 36, Batch 88 Loss:0.0585247166454792\n",
      "Epoch 36, Batch 89 Loss:0.03259672969579697\n",
      "Epoch 36, Batch 90 Loss:0.03480220213532448\n",
      "Epoch 36, Batch 91 Loss:0.030179547145962715\n",
      "Epoch 36, Batch 92 Loss:0.043751731514930725\n",
      "Epoch 36, Batch 93 Loss:0.011795548722147942\n",
      "Epoch 36, Batch 94 Loss:0.052551817148923874\n",
      "Epoch 36, Batch 95 Loss:0.03450582176446915\n",
      "Epoch 36, Batch 96 Loss:0.04420851171016693\n",
      "Epoch 36, Batch 97 Loss:0.06987413763999939\n",
      "Epoch 36, Batch 98 Loss:0.019911497831344604\n",
      "Epoch 36, Batch 99 Loss:0.022049710154533386\n",
      "Epoch 36, Batch 100 Loss:0.03264397010207176\n",
      "Epoch 36, Batch 101 Loss:0.052848175168037415\n",
      "Epoch 36, Batch 102 Loss:0.031413525342941284\n",
      "Epoch 36, Batch 103 Loss:0.017865173518657684\n",
      "Epoch 36, Batch 104 Loss:0.022319594398140907\n",
      "Epoch 36, Batch 105 Loss:0.05105124041438103\n",
      "Epoch 36, Batch 106 Loss:0.032080378383398056\n",
      "Epoch 36, Batch 107 Loss:0.032348453998565674\n",
      "Epoch 36, Batch 108 Loss:0.03311307728290558\n",
      "Epoch 36, Batch 109 Loss:0.02346188761293888\n",
      "Epoch 36, Batch 110 Loss:0.03469214215874672\n",
      "Epoch 36, Batch 111 Loss:0.04850875958800316\n",
      "Epoch 36, Batch 112 Loss:0.030820950865745544\n",
      "Epoch 36, Batch 113 Loss:0.04288595914840698\n",
      "Epoch 36, Batch 114 Loss:0.029995255172252655\n",
      "Epoch 36, Batch 115 Loss:0.02488568052649498\n",
      "Epoch 36, Batch 116 Loss:0.041123129427433014\n",
      "Epoch 36, Batch 117 Loss:0.026758460327982903\n",
      "Epoch 36, Batch 118 Loss:0.019508428871631622\n",
      "Epoch 36, Batch 119 Loss:0.027560165151953697\n",
      "Epoch 36, Batch 120 Loss:0.05502163991332054\n",
      "Epoch 36, Batch 121 Loss:0.056334201246500015\n",
      "Epoch 36, Batch 122 Loss:0.04089466109871864\n",
      "Epoch 36, Batch 123 Loss:0.032121654599905014\n",
      "Epoch 36, Batch 124 Loss:0.03278743103146553\n",
      "Epoch 36, Batch 125 Loss:0.04397363215684891\n",
      "Epoch 36, Batch 126 Loss:0.038386423140764236\n",
      "Epoch 36, Batch 127 Loss:0.04970259219408035\n",
      "Epoch 36, Batch 128 Loss:0.022445838898420334\n",
      "Epoch 36, Batch 129 Loss:0.02352234162390232\n",
      "Epoch 36, Batch 130 Loss:0.03844437003135681\n",
      "Epoch 36, Batch 131 Loss:0.03294983506202698\n",
      "Epoch 36, Batch 132 Loss:0.04556415230035782\n",
      "Epoch 36, Batch 133 Loss:0.04980368912220001\n",
      "Epoch 36, Batch 134 Loss:0.055784765630960464\n",
      "Epoch 36, Batch 135 Loss:0.031002739444375038\n",
      "Epoch 36, Batch 136 Loss:0.04674816131591797\n",
      "Epoch 36, Batch 137 Loss:0.0339733250439167\n",
      "Epoch 36, Batch 138 Loss:0.036896973848342896\n",
      "Epoch 36, Batch 139 Loss:0.022035831585526466\n",
      "Epoch 36, Batch 140 Loss:0.07963678985834122\n",
      "Epoch 36, Batch 141 Loss:0.04302205145359039\n",
      "Epoch 36, Batch 142 Loss:0.048572152853012085\n",
      "Epoch 36, Batch 143 Loss:0.025517797097563744\n",
      "Epoch 36, Batch 144 Loss:0.035808708518743515\n",
      "Epoch 36, Batch 145 Loss:0.04026980698108673\n",
      "Epoch 36, Batch 146 Loss:0.048929665237665176\n",
      "Epoch 36, Batch 147 Loss:0.04223029315471649\n",
      "Epoch 36, Batch 148 Loss:0.06623301655054092\n",
      "Epoch 36, Batch 149 Loss:0.0451781265437603\n",
      "Epoch 36, Batch 150 Loss:0.03575607389211655\n",
      "Epoch 36, Batch 151 Loss:0.044305942952632904\n",
      "Epoch 36, Batch 152 Loss:0.023750677704811096\n",
      "Epoch 36, Batch 153 Loss:0.07040268182754517\n",
      "Epoch 36, Batch 154 Loss:0.04350123926997185\n",
      "Epoch 36, Batch 155 Loss:0.05235828459262848\n",
      "Epoch 36, Batch 156 Loss:0.02723824791610241\n",
      "Epoch 36, Batch 157 Loss:0.047387175261974335\n",
      "Epoch 36, Batch 158 Loss:0.0412476472556591\n",
      "Epoch 36, Batch 159 Loss:0.052765797823667526\n",
      "Epoch 36, Batch 160 Loss:0.02253558486700058\n",
      "Epoch 36, Batch 161 Loss:0.03900938481092453\n",
      "Epoch 36, Batch 162 Loss:0.049049556255340576\n",
      "Epoch 36, Batch 163 Loss:0.09976892173290253\n",
      "Epoch 36, Batch 164 Loss:0.0488986037671566\n",
      "Epoch 36, Batch 165 Loss:0.0377441868185997\n",
      "Epoch 36, Batch 166 Loss:0.04739803820848465\n",
      "Epoch 36, Batch 167 Loss:0.04577630013227463\n",
      "Epoch 36, Batch 168 Loss:0.05418724939227104\n",
      "Epoch 36, Batch 169 Loss:0.03969272971153259\n",
      "Epoch 36, Batch 170 Loss:0.0330943688750267\n",
      "Epoch 36, Batch 171 Loss:0.06313562393188477\n",
      "Epoch 36, Batch 172 Loss:0.03229900449514389\n",
      "Epoch 36, Batch 173 Loss:0.036679286509752274\n",
      "Epoch 36, Batch 174 Loss:0.05179130658507347\n",
      "Epoch 36, Batch 175 Loss:0.03628223016858101\n",
      "Epoch 36, Batch 176 Loss:0.054513975977897644\n",
      "Epoch 36, Batch 177 Loss:0.029604794457554817\n",
      "Epoch 36, Batch 178 Loss:0.03777388855814934\n",
      "Epoch 36, Batch 179 Loss:0.04581615328788757\n",
      "Epoch 36, Batch 180 Loss:0.07280430942773819\n",
      "Epoch 36, Batch 181 Loss:0.05886177346110344\n",
      "Epoch 36, Batch 182 Loss:0.05670288950204849\n",
      "Epoch 36, Batch 183 Loss:0.03498964384198189\n",
      "Epoch 36, Batch 184 Loss:0.05013038218021393\n",
      "Epoch 36, Batch 185 Loss:0.03870844095945358\n",
      "Epoch 36, Batch 186 Loss:0.040556181222200394\n",
      "Epoch 36, Batch 187 Loss:0.04337369278073311\n",
      "Epoch 36, Batch 188 Loss:0.020042698830366135\n",
      "Epoch 36, Batch 189 Loss:0.0480223223567009\n",
      "Epoch 36, Batch 190 Loss:0.02688651904463768\n",
      "Epoch 36, Batch 191 Loss:0.03908327594399452\n",
      "Epoch 36, Batch 192 Loss:0.06454122066497803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Batch 193 Loss:0.020833177492022514\n",
      "Epoch 36, Batch 194 Loss:0.034960512071847916\n",
      "Epoch 36, Batch 195 Loss:0.0387861430644989\n",
      "Epoch 36, Batch 196 Loss:0.065221406519413\n",
      "Epoch 36, Batch 197 Loss:0.06859669834375381\n",
      "Epoch 36, Batch 198 Loss:0.03354742377996445\n",
      "Epoch 36, Batch 199 Loss:0.07917696982622147\n",
      "Epoch 36, Batch 200 Loss:0.02964995987713337\n",
      "Epoch 36, Batch 201 Loss:0.03577619045972824\n",
      "Epoch 36, Batch 202 Loss:0.06121402233839035\n",
      "Epoch 36, Batch 203 Loss:0.03587118536233902\n",
      "Epoch 36, Batch 204 Loss:0.06728661060333252\n",
      "Epoch 36, Batch 205 Loss:0.047034621238708496\n",
      "Epoch 36, Batch 206 Loss:0.024750757962465286\n",
      "Epoch 36, Batch 207 Loss:0.03555717319250107\n",
      "Epoch 36, Batch 208 Loss:0.03222274035215378\n",
      "Epoch 36, Batch 209 Loss:0.05575120076537132\n",
      "Epoch 36, Batch 210 Loss:0.039032742381095886\n",
      "Epoch 36, Batch 211 Loss:0.026827920228242874\n",
      "Epoch 36, Batch 212 Loss:0.035830721259117126\n",
      "Epoch 36, Batch 213 Loss:0.04849289357662201\n",
      "Epoch 36, Batch 214 Loss:0.033866047859191895\n",
      "Epoch 36, Batch 215 Loss:0.04710765182971954\n",
      "Epoch 36, Batch 216 Loss:0.03835349529981613\n",
      "Epoch 36, Batch 217 Loss:0.01850924640893936\n",
      "Epoch 36, Batch 218 Loss:0.03737795352935791\n",
      "Epoch 36, Batch 219 Loss:0.04154682159423828\n",
      "Epoch 36, Batch 220 Loss:0.043189942836761475\n",
      "Epoch 36, Batch 221 Loss:0.0378890335559845\n",
      "Epoch 36, Batch 222 Loss:0.05670245736837387\n",
      "Epoch 36, Batch 223 Loss:0.035203717648983\n",
      "Epoch 36, Batch 224 Loss:0.033734872937202454\n",
      "Epoch 36, Batch 225 Loss:0.054436393082141876\n",
      "Epoch 36, Batch 226 Loss:0.059978220611810684\n",
      "Epoch 36, Batch 227 Loss:0.033388786017894745\n",
      "Epoch 36, Batch 228 Loss:0.0564684234559536\n",
      "Epoch 36, Batch 229 Loss:0.033301908522844315\n",
      "Epoch 36, Batch 230 Loss:0.07241161167621613\n",
      "Epoch 36, Batch 231 Loss:0.046592168509960175\n",
      "Epoch 36, Batch 232 Loss:0.0317925401031971\n",
      "Epoch 36, Batch 233 Loss:0.03060869127511978\n",
      "Loss in this Epoch is: 3.06086912751 %\n",
      "Accuracy in this Epoch is: 88.8000011444 %\n",
      "Epoch 37, Batch 0 Loss:0.03910926356911659\n",
      "Epoch 37, Batch 1 Loss:0.02461613342165947\n",
      "Epoch 37, Batch 2 Loss:0.038433611392974854\n",
      "Epoch 37, Batch 3 Loss:0.02304505929350853\n",
      "Epoch 37, Batch 4 Loss:0.049698058515787125\n",
      "Epoch 37, Batch 5 Loss:0.04035356640815735\n",
      "Epoch 37, Batch 6 Loss:0.03609643876552582\n",
      "Epoch 37, Batch 7 Loss:0.02805236726999283\n",
      "Epoch 37, Batch 8 Loss:0.016116829589009285\n",
      "Epoch 37, Batch 9 Loss:0.03323803469538689\n",
      "Epoch 37, Batch 10 Loss:0.06344810873270035\n",
      "Epoch 37, Batch 11 Loss:0.03997835889458656\n",
      "Epoch 37, Batch 12 Loss:0.05897028371691704\n",
      "Epoch 37, Batch 13 Loss:0.06826474517583847\n",
      "Epoch 37, Batch 14 Loss:0.021421000361442566\n",
      "Epoch 37, Batch 15 Loss:0.03164035454392433\n",
      "Epoch 37, Batch 16 Loss:0.04177648574113846\n",
      "Epoch 37, Batch 17 Loss:0.045411188155412674\n",
      "Epoch 37, Batch 18 Loss:0.08788949996232986\n",
      "Epoch 37, Batch 19 Loss:0.05097930505871773\n",
      "Epoch 37, Batch 20 Loss:0.025993967428803444\n",
      "Epoch 37, Batch 21 Loss:0.03242294862866402\n",
      "Epoch 37, Batch 22 Loss:0.0348079577088356\n",
      "Epoch 37, Batch 23 Loss:0.04729887843132019\n",
      "Epoch 37, Batch 24 Loss:0.05773713067173958\n",
      "Epoch 37, Batch 25 Loss:0.03471633419394493\n",
      "Epoch 37, Batch 26 Loss:0.033701200038194656\n",
      "Epoch 37, Batch 27 Loss:0.05276116728782654\n",
      "Epoch 37, Batch 28 Loss:0.08021573722362518\n",
      "Epoch 37, Batch 29 Loss:0.022555101662874222\n",
      "Epoch 37, Batch 30 Loss:0.07155706733465195\n",
      "Epoch 37, Batch 31 Loss:0.06512311100959778\n",
      "Epoch 37, Batch 32 Loss:0.01997358351945877\n",
      "Epoch 37, Batch 33 Loss:0.05234559625387192\n",
      "Epoch 37, Batch 34 Loss:0.05545277148485184\n",
      "Epoch 37, Batch 35 Loss:0.06219860911369324\n",
      "Epoch 37, Batch 36 Loss:0.03951289504766464\n",
      "Epoch 37, Batch 37 Loss:0.03703756630420685\n",
      "Epoch 37, Batch 38 Loss:0.034835271537303925\n",
      "Epoch 37, Batch 39 Loss:0.04774292930960655\n",
      "Epoch 37, Batch 40 Loss:0.042929068207740784\n",
      "Epoch 37, Batch 41 Loss:0.05919834226369858\n",
      "Epoch 37, Batch 42 Loss:0.04447361081838608\n",
      "Epoch 37, Batch 43 Loss:0.07278171181678772\n",
      "Epoch 37, Batch 44 Loss:0.07783279567956924\n",
      "Epoch 37, Batch 45 Loss:0.05764923617243767\n",
      "Epoch 37, Batch 46 Loss:0.024993304163217545\n",
      "Epoch 37, Batch 47 Loss:0.05731295794248581\n",
      "Epoch 37, Batch 48 Loss:0.03927749767899513\n",
      "Epoch 37, Batch 49 Loss:0.03240826353430748\n",
      "Epoch 37, Batch 50 Loss:0.04346392676234245\n",
      "Epoch 37, Batch 51 Loss:0.035545334219932556\n",
      "Epoch 37, Batch 52 Loss:0.022919153794646263\n",
      "Epoch 37, Batch 53 Loss:0.04730844125151634\n",
      "Epoch 37, Batch 54 Loss:0.0512692928314209\n",
      "Epoch 37, Batch 55 Loss:0.025387825444340706\n",
      "Epoch 37, Batch 56 Loss:0.06116747483611107\n",
      "Epoch 37, Batch 57 Loss:0.0613672100007534\n",
      "Epoch 37, Batch 58 Loss:0.037435054779052734\n",
      "Epoch 37, Batch 59 Loss:0.030113594606518745\n",
      "Epoch 37, Batch 60 Loss:0.025361493229866028\n",
      "Epoch 37, Batch 61 Loss:0.010638724081218243\n",
      "Epoch 37, Batch 62 Loss:0.019737640395760536\n",
      "Epoch 37, Batch 63 Loss:0.020884759724140167\n",
      "Epoch 37, Batch 64 Loss:0.06033143028616905\n",
      "Epoch 37, Batch 65 Loss:0.029080063104629517\n",
      "Epoch 37, Batch 66 Loss:0.03701517730951309\n",
      "Epoch 37, Batch 67 Loss:0.07464554905891418\n",
      "Epoch 37, Batch 68 Loss:0.06357110291719437\n",
      "Epoch 37, Batch 69 Loss:0.05230878293514252\n",
      "Epoch 37, Batch 70 Loss:0.03564384952187538\n",
      "Epoch 37, Batch 71 Loss:0.02787843532860279\n",
      "Epoch 37, Batch 72 Loss:0.05385564640164375\n",
      "Epoch 37, Batch 73 Loss:0.03673669695854187\n",
      "Epoch 37, Batch 74 Loss:0.04225261136889458\n",
      "Epoch 37, Batch 75 Loss:0.04107886180281639\n",
      "Epoch 37, Batch 76 Loss:0.03659763187170029\n",
      "Epoch 37, Batch 77 Loss:0.04687388241291046\n",
      "Epoch 37, Batch 78 Loss:0.05424875393509865\n",
      "Epoch 37, Batch 79 Loss:0.040011078119277954\n",
      "Epoch 37, Batch 80 Loss:0.0445825532078743\n",
      "Epoch 37, Batch 81 Loss:0.05672956630587578\n",
      "Epoch 37, Batch 82 Loss:0.04614326357841492\n",
      "Epoch 37, Batch 83 Loss:0.12130852043628693\n",
      "Epoch 37, Batch 84 Loss:0.01987314783036709\n",
      "Epoch 37, Batch 85 Loss:0.027403971180319786\n",
      "Epoch 37, Batch 86 Loss:0.03193149343132973\n",
      "Epoch 37, Batch 87 Loss:0.030801208689808846\n",
      "Epoch 37, Batch 88 Loss:0.028021927922964096\n",
      "Epoch 37, Batch 89 Loss:0.02896493300795555\n",
      "Epoch 37, Batch 90 Loss:0.043065816164016724\n",
      "Epoch 37, Batch 91 Loss:0.06331343203783035\n",
      "Epoch 37, Batch 92 Loss:0.042901091277599335\n",
      "Epoch 37, Batch 93 Loss:0.03935903683304787\n",
      "Epoch 37, Batch 94 Loss:0.030076708644628525\n",
      "Epoch 37, Batch 95 Loss:0.03427114337682724\n",
      "Epoch 37, Batch 96 Loss:0.0464787594974041\n",
      "Epoch 37, Batch 97 Loss:0.05951336398720741\n",
      "Epoch 37, Batch 98 Loss:0.01919127255678177\n",
      "Epoch 37, Batch 99 Loss:0.041690677404403687\n",
      "Epoch 37, Batch 100 Loss:0.048554450273513794\n",
      "Epoch 37, Batch 101 Loss:0.039103053510189056\n",
      "Epoch 37, Batch 102 Loss:0.03239733353257179\n",
      "Epoch 37, Batch 103 Loss:0.04048171266913414\n",
      "Epoch 37, Batch 104 Loss:0.060271285474300385\n",
      "Epoch 37, Batch 105 Loss:0.04328426718711853\n",
      "Epoch 37, Batch 106 Loss:0.03301770240068436\n",
      "Epoch 37, Batch 107 Loss:0.033152345567941666\n",
      "Epoch 37, Batch 108 Loss:0.06928098201751709\n",
      "Epoch 37, Batch 109 Loss:0.052013423293828964\n",
      "Epoch 37, Batch 110 Loss:0.054684557020664215\n",
      "Epoch 37, Batch 111 Loss:0.05753655359148979\n",
      "Epoch 37, Batch 112 Loss:0.10701492428779602\n",
      "Epoch 37, Batch 113 Loss:0.04395871236920357\n",
      "Epoch 37, Batch 114 Loss:0.04346606880426407\n",
      "Epoch 37, Batch 115 Loss:0.03915339335799217\n",
      "Epoch 37, Batch 116 Loss:0.08305644243955612\n",
      "Epoch 37, Batch 117 Loss:0.07049229741096497\n",
      "Epoch 37, Batch 118 Loss:0.09720505774021149\n",
      "Epoch 37, Batch 119 Loss:0.03395310044288635\n",
      "Epoch 37, Batch 120 Loss:0.03811325132846832\n",
      "Epoch 37, Batch 121 Loss:0.07041645050048828\n",
      "Epoch 37, Batch 122 Loss:0.08185911178588867\n",
      "Epoch 37, Batch 123 Loss:0.03511966019868851\n",
      "Epoch 37, Batch 124 Loss:0.05774727091193199\n",
      "Epoch 37, Batch 125 Loss:0.02638569474220276\n",
      "Epoch 37, Batch 126 Loss:0.09014632552862167\n",
      "Epoch 37, Batch 127 Loss:0.02645638957619667\n",
      "Epoch 37, Batch 128 Loss:0.021323764696717262\n",
      "Epoch 37, Batch 129 Loss:0.07596203684806824\n",
      "Epoch 37, Batch 130 Loss:0.03718195855617523\n",
      "Epoch 37, Batch 131 Loss:0.05446670204401016\n",
      "Epoch 37, Batch 132 Loss:0.038349930197000504\n",
      "Epoch 37, Batch 133 Loss:0.05404292792081833\n",
      "Epoch 37, Batch 134 Loss:0.0504496693611145\n",
      "Epoch 37, Batch 135 Loss:0.06549857556819916\n",
      "Epoch 37, Batch 136 Loss:0.03551459684967995\n",
      "Epoch 37, Batch 137 Loss:0.05344894528388977\n",
      "Epoch 37, Batch 138 Loss:0.04157797992229462\n",
      "Epoch 37, Batch 139 Loss:0.05197235569357872\n",
      "Epoch 37, Batch 140 Loss:0.0741453766822815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Batch 141 Loss:0.022307775914669037\n",
      "Epoch 37, Batch 142 Loss:0.036474574357271194\n",
      "Epoch 37, Batch 143 Loss:0.033279117196798325\n",
      "Epoch 37, Batch 144 Loss:0.031038174405694008\n",
      "Epoch 37, Batch 145 Loss:0.04305455461144447\n",
      "Epoch 37, Batch 146 Loss:0.06614599376916885\n",
      "Epoch 37, Batch 147 Loss:0.06409228593111038\n",
      "Epoch 37, Batch 148 Loss:0.039491478353738785\n",
      "Epoch 37, Batch 149 Loss:0.02782217226922512\n",
      "Epoch 37, Batch 150 Loss:0.029903307557106018\n",
      "Epoch 37, Batch 151 Loss:0.022671692073345184\n",
      "Epoch 37, Batch 152 Loss:0.030368365347385406\n",
      "Epoch 37, Batch 153 Loss:0.04437287151813507\n",
      "Epoch 37, Batch 154 Loss:0.02935609221458435\n",
      "Epoch 37, Batch 155 Loss:0.03010435402393341\n",
      "Epoch 37, Batch 156 Loss:0.043118175119161606\n",
      "Epoch 37, Batch 157 Loss:0.06274714320898056\n",
      "Epoch 37, Batch 158 Loss:0.07173576205968857\n",
      "Epoch 37, Batch 159 Loss:0.026592304930090904\n",
      "Epoch 37, Batch 160 Loss:0.04330030083656311\n",
      "Epoch 37, Batch 161 Loss:0.035222530364990234\n",
      "Epoch 37, Batch 162 Loss:0.032766446471214294\n",
      "Epoch 37, Batch 163 Loss:0.031469397246837616\n",
      "Epoch 37, Batch 164 Loss:0.06260740011930466\n",
      "Epoch 37, Batch 165 Loss:0.048754919320344925\n",
      "Epoch 37, Batch 166 Loss:0.019909635186195374\n",
      "Epoch 37, Batch 167 Loss:0.09344276040792465\n",
      "Epoch 37, Batch 168 Loss:0.04761134460568428\n",
      "Epoch 37, Batch 169 Loss:0.07496179640293121\n",
      "Epoch 37, Batch 170 Loss:0.020283538848161697\n",
      "Epoch 37, Batch 171 Loss:0.08072847872972488\n",
      "Epoch 37, Batch 172 Loss:0.04596929997205734\n",
      "Epoch 37, Batch 173 Loss:0.04252151399850845\n",
      "Epoch 37, Batch 174 Loss:0.03461956977844238\n",
      "Epoch 37, Batch 175 Loss:0.046742722392082214\n",
      "Epoch 37, Batch 176 Loss:0.053515661507844925\n",
      "Epoch 37, Batch 177 Loss:0.07520509511232376\n",
      "Epoch 37, Batch 178 Loss:0.03510299324989319\n",
      "Epoch 37, Batch 179 Loss:0.05443189665675163\n",
      "Epoch 37, Batch 180 Loss:0.06415294110774994\n",
      "Epoch 37, Batch 181 Loss:0.018159979954361916\n",
      "Epoch 37, Batch 182 Loss:0.04765060171484947\n",
      "Epoch 37, Batch 183 Loss:0.04711474850773811\n",
      "Epoch 37, Batch 184 Loss:0.06600023061037064\n",
      "Epoch 37, Batch 185 Loss:0.03740666061639786\n",
      "Epoch 37, Batch 186 Loss:0.016077663749456406\n",
      "Epoch 37, Batch 187 Loss:0.042489193379879\n",
      "Epoch 37, Batch 188 Loss:0.027229830622673035\n",
      "Epoch 37, Batch 189 Loss:0.06922757625579834\n",
      "Epoch 37, Batch 190 Loss:0.07148097455501556\n",
      "Epoch 37, Batch 191 Loss:0.03513112664222717\n",
      "Epoch 37, Batch 192 Loss:0.05908341333270073\n",
      "Epoch 37, Batch 193 Loss:0.04389870911836624\n",
      "Epoch 37, Batch 194 Loss:0.05958666279911995\n",
      "Epoch 37, Batch 195 Loss:0.08105070888996124\n",
      "Epoch 37, Batch 196 Loss:0.04876352846622467\n",
      "Epoch 37, Batch 197 Loss:0.048006054013967514\n",
      "Epoch 37, Batch 198 Loss:0.07642623037099838\n",
      "Epoch 37, Batch 199 Loss:0.03278545290231705\n",
      "Epoch 37, Batch 200 Loss:0.10029593855142593\n",
      "Epoch 37, Batch 201 Loss:0.034102216362953186\n",
      "Epoch 37, Batch 202 Loss:0.028436128050088882\n",
      "Epoch 37, Batch 203 Loss:0.03438355773687363\n",
      "Epoch 37, Batch 204 Loss:0.05114106461405754\n",
      "Epoch 37, Batch 205 Loss:0.05225549638271332\n",
      "Epoch 37, Batch 206 Loss:0.02796126902103424\n",
      "Epoch 37, Batch 207 Loss:0.0537218376994133\n",
      "Epoch 37, Batch 208 Loss:0.07582351565361023\n",
      "Epoch 37, Batch 209 Loss:0.041194818913936615\n",
      "Epoch 37, Batch 210 Loss:0.07301268726587296\n",
      "Epoch 37, Batch 211 Loss:0.04401923343539238\n",
      "Epoch 37, Batch 212 Loss:0.043495651334524155\n",
      "Epoch 37, Batch 213 Loss:0.04355064034461975\n",
      "Epoch 37, Batch 214 Loss:0.058818917721509933\n",
      "Epoch 37, Batch 215 Loss:0.03624734282493591\n",
      "Epoch 37, Batch 216 Loss:0.03457590565085411\n",
      "Epoch 37, Batch 217 Loss:0.04156075045466423\n",
      "Epoch 37, Batch 218 Loss:0.06182453781366348\n",
      "Epoch 37, Batch 219 Loss:0.02983771078288555\n",
      "Epoch 37, Batch 220 Loss:0.07283422350883484\n",
      "Epoch 37, Batch 221 Loss:0.034087009727954865\n",
      "Epoch 37, Batch 222 Loss:0.033278390765190125\n",
      "Epoch 37, Batch 223 Loss:0.04774775356054306\n",
      "Epoch 37, Batch 224 Loss:0.0633835420012474\n",
      "Epoch 37, Batch 225 Loss:0.04839309677481651\n",
      "Epoch 37, Batch 226 Loss:0.06578163057565689\n",
      "Epoch 37, Batch 227 Loss:0.05935770273208618\n",
      "Epoch 37, Batch 228 Loss:0.03273800015449524\n",
      "Epoch 37, Batch 229 Loss:0.027537301182746887\n",
      "Epoch 37, Batch 230 Loss:0.05468113347887993\n",
      "Epoch 37, Batch 231 Loss:0.026219535619020462\n",
      "Epoch 37, Batch 232 Loss:0.04199197143316269\n",
      "Epoch 37, Batch 233 Loss:0.02318159118294716\n",
      "Loss in this Epoch is: 2.31815911829 %\n",
      "Accuracy in this Epoch is: 89.2400026321 %\n",
      "Epoch 38, Batch 0 Loss:0.01915481500327587\n",
      "Epoch 38, Batch 1 Loss:0.017720142379403114\n",
      "Epoch 38, Batch 2 Loss:0.02026987075805664\n",
      "Epoch 38, Batch 3 Loss:0.027595363557338715\n",
      "Epoch 38, Batch 4 Loss:0.03288610279560089\n",
      "Epoch 38, Batch 5 Loss:0.04020064324140549\n",
      "Epoch 38, Batch 6 Loss:0.020004069432616234\n",
      "Epoch 38, Batch 7 Loss:0.02913518436253071\n",
      "Epoch 38, Batch 8 Loss:0.02982875518500805\n",
      "Epoch 38, Batch 9 Loss:0.042048048228025436\n",
      "Epoch 38, Batch 10 Loss:0.022055910900235176\n",
      "Epoch 38, Batch 11 Loss:0.0530049130320549\n",
      "Epoch 38, Batch 12 Loss:0.0835026279091835\n",
      "Epoch 38, Batch 13 Loss:0.03299418091773987\n",
      "Epoch 38, Batch 14 Loss:0.02219472825527191\n",
      "Epoch 38, Batch 15 Loss:0.015366367995738983\n",
      "Epoch 38, Batch 16 Loss:0.026591813191771507\n",
      "Epoch 38, Batch 17 Loss:0.04872966557741165\n",
      "Epoch 38, Batch 18 Loss:0.029462886974215508\n",
      "Epoch 38, Batch 19 Loss:0.026843296363949776\n",
      "Epoch 38, Batch 20 Loss:0.03052709437906742\n",
      "Epoch 38, Batch 21 Loss:0.01844259910285473\n",
      "Epoch 38, Batch 22 Loss:0.007640291471034288\n",
      "Epoch 38, Batch 23 Loss:0.058452993631362915\n",
      "Epoch 38, Batch 24 Loss:0.048722509294748306\n",
      "Epoch 38, Batch 25 Loss:0.05771639570593834\n",
      "Epoch 38, Batch 26 Loss:0.01532123051583767\n",
      "Epoch 38, Batch 27 Loss:0.04102340340614319\n",
      "Epoch 38, Batch 28 Loss:0.018918955698609352\n",
      "Epoch 38, Batch 29 Loss:0.02679789997637272\n",
      "Epoch 38, Batch 30 Loss:0.014123431406915188\n",
      "Epoch 38, Batch 31 Loss:0.05190043896436691\n",
      "Epoch 38, Batch 32 Loss:0.039292316883802414\n",
      "Epoch 38, Batch 33 Loss:0.02851209230720997\n",
      "Epoch 38, Batch 34 Loss:0.026935642585158348\n",
      "Epoch 38, Batch 35 Loss:0.04194594919681549\n",
      "Epoch 38, Batch 36 Loss:0.02254847064614296\n",
      "Epoch 38, Batch 37 Loss:0.0454813577234745\n",
      "Epoch 38, Batch 38 Loss:0.028879018500447273\n",
      "Epoch 38, Batch 39 Loss:0.04533814638853073\n",
      "Epoch 38, Batch 40 Loss:0.01683550514280796\n",
      "Epoch 38, Batch 41 Loss:0.0445517897605896\n",
      "Epoch 38, Batch 42 Loss:0.05100256949663162\n",
      "Epoch 38, Batch 43 Loss:0.013292089104652405\n",
      "Epoch 38, Batch 44 Loss:0.036496616899967194\n",
      "Epoch 38, Batch 45 Loss:0.01878662407398224\n",
      "Epoch 38, Batch 46 Loss:0.034894466400146484\n",
      "Epoch 38, Batch 47 Loss:0.06798022985458374\n",
      "Epoch 38, Batch 48 Loss:0.06955162435770035\n",
      "Epoch 38, Batch 49 Loss:0.026652896776795387\n",
      "Epoch 38, Batch 50 Loss:0.028288546949625015\n",
      "Epoch 38, Batch 51 Loss:0.043196067214012146\n",
      "Epoch 38, Batch 52 Loss:0.01887844130396843\n",
      "Epoch 38, Batch 53 Loss:0.012880022637546062\n",
      "Epoch 38, Batch 54 Loss:0.026787055656313896\n",
      "Epoch 38, Batch 55 Loss:0.051244210451841354\n",
      "Epoch 38, Batch 56 Loss:0.02147953025996685\n",
      "Epoch 38, Batch 57 Loss:0.06489166617393494\n",
      "Epoch 38, Batch 58 Loss:0.0175195150077343\n",
      "Epoch 38, Batch 59 Loss:0.035624418407678604\n",
      "Epoch 38, Batch 60 Loss:0.03430137410759926\n",
      "Epoch 38, Batch 61 Loss:0.043906427919864655\n",
      "Epoch 38, Batch 62 Loss:0.013116954825818539\n",
      "Epoch 38, Batch 63 Loss:0.05235681310296059\n",
      "Epoch 38, Batch 64 Loss:0.03212437406182289\n",
      "Epoch 38, Batch 65 Loss:0.019349897280335426\n",
      "Epoch 38, Batch 66 Loss:0.008182701654732227\n",
      "Epoch 38, Batch 67 Loss:0.030409950762987137\n",
      "Epoch 38, Batch 68 Loss:0.0204402357339859\n",
      "Epoch 38, Batch 69 Loss:0.019499637186527252\n",
      "Epoch 38, Batch 70 Loss:0.04768875241279602\n",
      "Epoch 38, Batch 71 Loss:0.05015536770224571\n",
      "Epoch 38, Batch 72 Loss:0.021395307034254074\n",
      "Epoch 38, Batch 73 Loss:0.03772224485874176\n",
      "Epoch 38, Batch 74 Loss:0.02634311467409134\n",
      "Epoch 38, Batch 75 Loss:0.025157248601317406\n",
      "Epoch 38, Batch 76 Loss:0.0234985388815403\n",
      "Epoch 38, Batch 77 Loss:0.021749723702669144\n",
      "Epoch 38, Batch 78 Loss:0.03776247799396515\n",
      "Epoch 38, Batch 79 Loss:0.030515681952238083\n",
      "Epoch 38, Batch 80 Loss:0.029853951185941696\n",
      "Epoch 38, Batch 81 Loss:0.013110196217894554\n",
      "Epoch 38, Batch 82 Loss:0.005600069649517536\n",
      "Epoch 38, Batch 83 Loss:0.034166764467954636\n",
      "Epoch 38, Batch 84 Loss:0.024823972955346107\n",
      "Epoch 38, Batch 85 Loss:0.05397944897413254\n",
      "Epoch 38, Batch 86 Loss:0.008666174486279488\n",
      "Epoch 38, Batch 87 Loss:0.024930965155363083\n",
      "Epoch 38, Batch 88 Loss:0.036027781665325165\n",
      "Epoch 38, Batch 89 Loss:0.007963047362864017\n",
      "Epoch 38, Batch 90 Loss:0.015378745272755623\n",
      "Epoch 38, Batch 91 Loss:0.02339211292564869\n",
      "Epoch 38, Batch 92 Loss:0.034447792917490005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Batch 93 Loss:0.037241481244564056\n",
      "Epoch 38, Batch 94 Loss:0.028064820915460587\n",
      "Epoch 38, Batch 95 Loss:0.03447050228714943\n",
      "Epoch 38, Batch 96 Loss:0.027786364778876305\n",
      "Epoch 38, Batch 97 Loss:0.03063402883708477\n",
      "Epoch 38, Batch 98 Loss:0.015583413653075695\n",
      "Epoch 38, Batch 99 Loss:0.029266437515616417\n",
      "Epoch 38, Batch 100 Loss:0.021909411996603012\n",
      "Epoch 38, Batch 101 Loss:0.014256342314183712\n",
      "Epoch 38, Batch 102 Loss:0.027984216809272766\n",
      "Epoch 38, Batch 103 Loss:0.040479082614183426\n",
      "Epoch 38, Batch 104 Loss:0.045823946595191956\n",
      "Epoch 38, Batch 105 Loss:0.017970863729715347\n",
      "Epoch 38, Batch 106 Loss:0.024941641837358475\n",
      "Epoch 38, Batch 107 Loss:0.03693382069468498\n",
      "Epoch 38, Batch 108 Loss:0.031009230762720108\n",
      "Epoch 38, Batch 109 Loss:0.039888784289360046\n",
      "Epoch 38, Batch 110 Loss:0.07452026009559631\n",
      "Epoch 38, Batch 111 Loss:0.051211897283792496\n",
      "Epoch 38, Batch 112 Loss:0.024655304849147797\n",
      "Epoch 38, Batch 113 Loss:0.029219377785921097\n",
      "Epoch 38, Batch 114 Loss:0.046592265367507935\n",
      "Epoch 38, Batch 115 Loss:0.031215589493513107\n",
      "Epoch 38, Batch 116 Loss:0.05829866975545883\n",
      "Epoch 38, Batch 117 Loss:0.047173872590065\n",
      "Epoch 38, Batch 118 Loss:0.03990030288696289\n",
      "Epoch 38, Batch 119 Loss:0.015317777171730995\n",
      "Epoch 38, Batch 120 Loss:0.04265298694372177\n",
      "Epoch 38, Batch 121 Loss:0.04579337313771248\n",
      "Epoch 38, Batch 122 Loss:0.019220367074012756\n",
      "Epoch 38, Batch 123 Loss:0.04731553792953491\n",
      "Epoch 38, Batch 124 Loss:0.028867650777101517\n",
      "Epoch 38, Batch 125 Loss:0.02306661568582058\n",
      "Epoch 38, Batch 126 Loss:0.05313309282064438\n",
      "Epoch 38, Batch 127 Loss:0.050270937383174896\n",
      "Epoch 38, Batch 128 Loss:0.038465939462184906\n",
      "Epoch 38, Batch 129 Loss:0.023368990048766136\n",
      "Epoch 38, Batch 130 Loss:0.03243527188897133\n",
      "Epoch 38, Batch 131 Loss:0.046249717473983765\n",
      "Epoch 38, Batch 132 Loss:0.03667978569865227\n",
      "Epoch 38, Batch 133 Loss:0.026308756321668625\n",
      "Epoch 38, Batch 134 Loss:0.059124480932950974\n",
      "Epoch 38, Batch 135 Loss:0.022835509851574898\n",
      "Epoch 38, Batch 136 Loss:0.037953414022922516\n",
      "Epoch 38, Batch 137 Loss:0.02907482162117958\n",
      "Epoch 38, Batch 138 Loss:0.03913712128996849\n",
      "Epoch 38, Batch 139 Loss:0.059937700629234314\n",
      "Epoch 38, Batch 140 Loss:0.03674914315342903\n",
      "Epoch 38, Batch 141 Loss:0.02155786007642746\n",
      "Epoch 38, Batch 142 Loss:0.06110594421625137\n",
      "Epoch 38, Batch 143 Loss:0.03017396479845047\n",
      "Epoch 38, Batch 144 Loss:0.03584044426679611\n",
      "Epoch 38, Batch 145 Loss:0.041537798941135406\n",
      "Epoch 38, Batch 146 Loss:0.03640163689851761\n",
      "Epoch 38, Batch 147 Loss:0.05112672597169876\n",
      "Epoch 38, Batch 148 Loss:0.04468192905187607\n",
      "Epoch 38, Batch 149 Loss:0.03999753296375275\n",
      "Epoch 38, Batch 150 Loss:0.0493285246193409\n",
      "Epoch 38, Batch 151 Loss:0.034581657499074936\n",
      "Epoch 38, Batch 152 Loss:0.029399849474430084\n",
      "Epoch 38, Batch 153 Loss:0.05906891077756882\n",
      "Epoch 38, Batch 154 Loss:0.03212752193212509\n",
      "Epoch 38, Batch 155 Loss:0.04320015758275986\n",
      "Epoch 38, Batch 156 Loss:0.031896352767944336\n",
      "Epoch 38, Batch 157 Loss:0.11922825872898102\n",
      "Epoch 38, Batch 158 Loss:0.0599442683160305\n",
      "Epoch 38, Batch 159 Loss:0.07844678312540054\n",
      "Epoch 38, Batch 160 Loss:0.03760705888271332\n",
      "Epoch 38, Batch 161 Loss:0.07251016795635223\n",
      "Epoch 38, Batch 162 Loss:0.06436891853809357\n",
      "Epoch 38, Batch 163 Loss:0.05031030997633934\n",
      "Epoch 38, Batch 164 Loss:0.03499424457550049\n",
      "Epoch 38, Batch 165 Loss:0.08992201834917068\n",
      "Epoch 38, Batch 166 Loss:0.0652531087398529\n",
      "Epoch 38, Batch 167 Loss:0.036781709641218185\n",
      "Epoch 38, Batch 168 Loss:0.02028614655137062\n",
      "Epoch 38, Batch 169 Loss:0.022931767627596855\n",
      "Epoch 38, Batch 170 Loss:0.03657134622335434\n",
      "Epoch 38, Batch 171 Loss:0.04066506028175354\n",
      "Epoch 38, Batch 172 Loss:0.033529773354530334\n",
      "Epoch 38, Batch 173 Loss:0.05254064500331879\n",
      "Epoch 38, Batch 174 Loss:0.019229769706726074\n",
      "Epoch 38, Batch 175 Loss:0.043364282697439194\n",
      "Epoch 38, Batch 176 Loss:0.017205826938152313\n",
      "Epoch 38, Batch 177 Loss:0.04522383213043213\n",
      "Epoch 38, Batch 178 Loss:0.018766682595014572\n",
      "Epoch 38, Batch 179 Loss:0.09516669064760208\n",
      "Epoch 38, Batch 180 Loss:0.040495771914720535\n",
      "Epoch 38, Batch 181 Loss:0.038197338581085205\n",
      "Epoch 38, Batch 182 Loss:0.021773213520646095\n",
      "Epoch 38, Batch 183 Loss:0.044662050902843475\n",
      "Epoch 38, Batch 184 Loss:0.031457655131816864\n",
      "Epoch 38, Batch 185 Loss:0.047959841787815094\n",
      "Epoch 38, Batch 186 Loss:0.027753207832574844\n",
      "Epoch 38, Batch 187 Loss:0.02343747392296791\n",
      "Epoch 38, Batch 188 Loss:0.05844718962907791\n",
      "Epoch 38, Batch 189 Loss:0.06168706715106964\n",
      "Epoch 38, Batch 190 Loss:0.07309915125370026\n",
      "Epoch 38, Batch 191 Loss:0.012066933326423168\n",
      "Epoch 38, Batch 192 Loss:0.036758895963430405\n",
      "Epoch 38, Batch 193 Loss:0.0471993014216423\n",
      "Epoch 38, Batch 194 Loss:0.04399649053812027\n",
      "Epoch 38, Batch 195 Loss:0.06745342165231705\n",
      "Epoch 38, Batch 196 Loss:0.03724578022956848\n",
      "Epoch 38, Batch 197 Loss:0.04991170018911362\n",
      "Epoch 38, Batch 198 Loss:0.044180259108543396\n",
      "Epoch 38, Batch 199 Loss:0.08401663601398468\n",
      "Epoch 38, Batch 200 Loss:0.035131506621837616\n",
      "Epoch 38, Batch 201 Loss:0.05520205572247505\n",
      "Epoch 38, Batch 202 Loss:0.04865250736474991\n",
      "Epoch 38, Batch 203 Loss:0.032072316855192184\n",
      "Epoch 38, Batch 204 Loss:0.03342396393418312\n",
      "Epoch 38, Batch 205 Loss:0.06146417558193207\n",
      "Epoch 38, Batch 206 Loss:0.06031417474150658\n",
      "Epoch 38, Batch 207 Loss:0.040412068367004395\n",
      "Epoch 38, Batch 208 Loss:0.04954595863819122\n",
      "Epoch 38, Batch 209 Loss:0.046019021421670914\n",
      "Epoch 38, Batch 210 Loss:0.05225050449371338\n",
      "Epoch 38, Batch 211 Loss:0.04263757914304733\n",
      "Epoch 38, Batch 212 Loss:0.09502556920051575\n",
      "Epoch 38, Batch 213 Loss:0.0434405580163002\n",
      "Epoch 38, Batch 214 Loss:0.029350101947784424\n",
      "Epoch 38, Batch 215 Loss:0.02352619543671608\n",
      "Epoch 38, Batch 216 Loss:0.06774391233921051\n",
      "Epoch 38, Batch 217 Loss:0.04067734628915787\n",
      "Epoch 38, Batch 218 Loss:0.02620500698685646\n",
      "Epoch 38, Batch 219 Loss:0.04470112919807434\n",
      "Epoch 38, Batch 220 Loss:0.02043858915567398\n",
      "Epoch 38, Batch 221 Loss:0.03605631738901138\n",
      "Epoch 38, Batch 222 Loss:0.054783932864665985\n",
      "Epoch 38, Batch 223 Loss:0.06905196607112885\n",
      "Epoch 38, Batch 224 Loss:0.14443908631801605\n",
      "Epoch 38, Batch 225 Loss:0.03791675716638565\n",
      "Epoch 38, Batch 226 Loss:0.03718024492263794\n",
      "Epoch 38, Batch 227 Loss:0.027155209332704544\n",
      "Epoch 38, Batch 228 Loss:0.04496513307094574\n",
      "Epoch 38, Batch 229 Loss:0.05590546131134033\n",
      "Epoch 38, Batch 230 Loss:0.04138467460870743\n",
      "Epoch 38, Batch 231 Loss:0.032483913004398346\n",
      "Epoch 38, Batch 232 Loss:0.043832648545503616\n",
      "Epoch 38, Batch 233 Loss:0.039048366248607635\n",
      "Loss in this Epoch is: 3.90483662486 %\n",
      "Accuracy in this Epoch is: 88.8800024986 %\n",
      "Epoch 39, Batch 0 Loss:0.010328315198421478\n",
      "Epoch 39, Batch 1 Loss:0.025665350258350372\n",
      "Epoch 39, Batch 2 Loss:0.020154351368546486\n",
      "Epoch 39, Batch 3 Loss:0.03102581948041916\n",
      "Epoch 39, Batch 4 Loss:0.025314191356301308\n",
      "Epoch 39, Batch 5 Loss:0.035392895340919495\n",
      "Epoch 39, Batch 6 Loss:0.0379459410905838\n",
      "Epoch 39, Batch 7 Loss:0.03389235585927963\n",
      "Epoch 39, Batch 8 Loss:0.03232550993561745\n",
      "Epoch 39, Batch 9 Loss:0.033598288893699646\n",
      "Epoch 39, Batch 10 Loss:0.017409464344382286\n",
      "Epoch 39, Batch 11 Loss:0.03883582353591919\n",
      "Epoch 39, Batch 12 Loss:0.053416598588228226\n",
      "Epoch 39, Batch 13 Loss:0.0932874083518982\n",
      "Epoch 39, Batch 14 Loss:0.032492827624082565\n",
      "Epoch 39, Batch 15 Loss:0.046990107744932175\n",
      "Epoch 39, Batch 16 Loss:0.03828578442335129\n",
      "Epoch 39, Batch 17 Loss:0.043381933122873306\n",
      "Epoch 39, Batch 18 Loss:0.04756733775138855\n",
      "Epoch 39, Batch 19 Loss:0.038686417043209076\n",
      "Epoch 39, Batch 20 Loss:0.03176172077655792\n",
      "Epoch 39, Batch 21 Loss:0.02813783846795559\n",
      "Epoch 39, Batch 22 Loss:0.037505295127630234\n",
      "Epoch 39, Batch 23 Loss:0.03354993462562561\n",
      "Epoch 39, Batch 24 Loss:0.03687116131186485\n",
      "Epoch 39, Batch 25 Loss:0.05828753486275673\n",
      "Epoch 39, Batch 26 Loss:0.027037257328629494\n",
      "Epoch 39, Batch 27 Loss:0.06567629426717758\n",
      "Epoch 39, Batch 28 Loss:0.0355362705886364\n",
      "Epoch 39, Batch 29 Loss:0.06446468830108643\n",
      "Epoch 39, Batch 30 Loss:0.03862185776233673\n",
      "Epoch 39, Batch 31 Loss:0.056661996990442276\n",
      "Epoch 39, Batch 32 Loss:0.02673352137207985\n",
      "Epoch 39, Batch 33 Loss:0.05227801203727722\n",
      "Epoch 39, Batch 34 Loss:0.06657925248146057\n",
      "Epoch 39, Batch 35 Loss:0.042036544531583786\n",
      "Epoch 39, Batch 36 Loss:0.03191458061337471\n",
      "Epoch 39, Batch 37 Loss:0.06088893488049507\n",
      "Epoch 39, Batch 38 Loss:0.04096478223800659\n",
      "Epoch 39, Batch 39 Loss:0.04722541198134422\n",
      "Epoch 39, Batch 40 Loss:0.061306580901145935\n",
      "Epoch 39, Batch 41 Loss:0.028173498809337616\n",
      "Epoch 39, Batch 42 Loss:0.03923925384879112\n",
      "Epoch 39, Batch 43 Loss:0.03107702173292637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 44 Loss:0.023344673216342926\n",
      "Epoch 39, Batch 45 Loss:0.025672757998108864\n",
      "Epoch 39, Batch 46 Loss:0.03023039922118187\n",
      "Epoch 39, Batch 47 Loss:0.05727807804942131\n",
      "Epoch 39, Batch 48 Loss:0.018981721252202988\n",
      "Epoch 39, Batch 49 Loss:0.029943998903036118\n",
      "Epoch 39, Batch 50 Loss:0.03285066410899162\n",
      "Epoch 39, Batch 51 Loss:0.04403788968920708\n",
      "Epoch 39, Batch 52 Loss:0.03005107492208481\n",
      "Epoch 39, Batch 53 Loss:0.015423057600855827\n",
      "Epoch 39, Batch 54 Loss:0.037331435829401016\n",
      "Epoch 39, Batch 55 Loss:0.06683267652988434\n",
      "Epoch 39, Batch 56 Loss:0.0559881329536438\n",
      "Epoch 39, Batch 57 Loss:0.05866335704922676\n",
      "Epoch 39, Batch 58 Loss:0.021361222490668297\n",
      "Epoch 39, Batch 59 Loss:0.052740562707185745\n",
      "Epoch 39, Batch 60 Loss:0.05088212713599205\n",
      "Epoch 39, Batch 61 Loss:0.04507621377706528\n",
      "Epoch 39, Batch 62 Loss:0.03215900808572769\n",
      "Epoch 39, Batch 63 Loss:0.05656148120760918\n",
      "Epoch 39, Batch 64 Loss:0.08488260954618454\n",
      "Epoch 39, Batch 65 Loss:0.049062579870224\n",
      "Epoch 39, Batch 66 Loss:0.04847317188978195\n",
      "Epoch 39, Batch 67 Loss:0.03711183741688728\n",
      "Epoch 39, Batch 68 Loss:0.043808918446302414\n",
      "Epoch 39, Batch 69 Loss:0.05847195163369179\n",
      "Epoch 39, Batch 70 Loss:0.051768958568573\n",
      "Epoch 39, Batch 71 Loss:0.06606535613536835\n",
      "Epoch 39, Batch 72 Loss:0.07443004846572876\n",
      "Epoch 39, Batch 73 Loss:0.03450487554073334\n",
      "Epoch 39, Batch 74 Loss:0.03464256599545479\n",
      "Epoch 39, Batch 75 Loss:0.028372308239340782\n",
      "Epoch 39, Batch 76 Loss:0.03986271470785141\n",
      "Epoch 39, Batch 77 Loss:0.03627490997314453\n",
      "Epoch 39, Batch 78 Loss:0.03871437907218933\n",
      "Epoch 39, Batch 79 Loss:0.04469916969537735\n",
      "Epoch 39, Batch 80 Loss:0.03082701750099659\n",
      "Epoch 39, Batch 81 Loss:0.05780929699540138\n",
      "Epoch 39, Batch 82 Loss:0.030172906816005707\n",
      "Epoch 39, Batch 83 Loss:0.041802410036325455\n",
      "Epoch 39, Batch 84 Loss:0.03739718720316887\n",
      "Epoch 39, Batch 85 Loss:0.024763302877545357\n",
      "Epoch 39, Batch 86 Loss:0.028221717104315758\n",
      "Epoch 39, Batch 87 Loss:0.07768494635820389\n",
      "Epoch 39, Batch 88 Loss:0.0649263858795166\n",
      "Epoch 39, Batch 89 Loss:0.029929010197520256\n",
      "Epoch 39, Batch 90 Loss:0.027419282123446465\n",
      "Epoch 39, Batch 91 Loss:0.028855007141828537\n",
      "Epoch 39, Batch 92 Loss:0.061366576701402664\n",
      "Epoch 39, Batch 93 Loss:0.023504141718149185\n",
      "Epoch 39, Batch 94 Loss:0.027578789740800858\n",
      "Epoch 39, Batch 95 Loss:0.034755051136016846\n",
      "Epoch 39, Batch 96 Loss:0.026188304647803307\n",
      "Epoch 39, Batch 97 Loss:0.032313987612724304\n",
      "Epoch 39, Batch 98 Loss:0.012250306084752083\n",
      "Epoch 39, Batch 99 Loss:0.03759845346212387\n",
      "Epoch 39, Batch 100 Loss:0.05325759947299957\n",
      "Epoch 39, Batch 101 Loss:0.02856353111565113\n",
      "Epoch 39, Batch 102 Loss:0.04689831659197807\n",
      "Epoch 39, Batch 103 Loss:0.06893564015626907\n",
      "Epoch 39, Batch 104 Loss:0.04342921823263168\n",
      "Epoch 39, Batch 105 Loss:0.03939712047576904\n",
      "Epoch 39, Batch 106 Loss:0.01567257195711136\n",
      "Epoch 39, Batch 107 Loss:0.05613373965024948\n",
      "Epoch 39, Batch 108 Loss:0.039779551327228546\n",
      "Epoch 39, Batch 109 Loss:0.05341961234807968\n",
      "Epoch 39, Batch 110 Loss:0.03611891716718674\n",
      "Epoch 39, Batch 111 Loss:0.027854852378368378\n",
      "Epoch 39, Batch 112 Loss:0.02235124260187149\n",
      "Epoch 39, Batch 113 Loss:0.028349395841360092\n",
      "Epoch 39, Batch 114 Loss:0.044965822249650955\n",
      "Epoch 39, Batch 115 Loss:0.046295732259750366\n",
      "Epoch 39, Batch 116 Loss:0.02525027096271515\n",
      "Epoch 39, Batch 117 Loss:0.025506069883704185\n",
      "Epoch 39, Batch 118 Loss:0.017484283074736595\n",
      "Epoch 39, Batch 119 Loss:0.025132952257990837\n",
      "Epoch 39, Batch 120 Loss:0.012547887861728668\n",
      "Epoch 39, Batch 121 Loss:0.044832922518253326\n",
      "Epoch 39, Batch 122 Loss:0.0774027556180954\n",
      "Epoch 39, Batch 123 Loss:0.04883943498134613\n",
      "Epoch 39, Batch 124 Loss:0.04370424151420593\n",
      "Epoch 39, Batch 125 Loss:0.015347623266279697\n",
      "Epoch 39, Batch 126 Loss:0.04189061000943184\n",
      "Epoch 39, Batch 127 Loss:0.03073696605861187\n",
      "Epoch 39, Batch 128 Loss:0.016141118481755257\n",
      "Epoch 39, Batch 129 Loss:0.027338575571775436\n",
      "Epoch 39, Batch 130 Loss:0.04219663515686989\n",
      "Epoch 39, Batch 131 Loss:0.042864929884672165\n",
      "Epoch 39, Batch 132 Loss:0.048169780522584915\n",
      "Epoch 39, Batch 133 Loss:0.05946308746933937\n",
      "Epoch 39, Batch 134 Loss:0.05412650108337402\n",
      "Epoch 39, Batch 135 Loss:0.03376317024230957\n",
      "Epoch 39, Batch 136 Loss:0.032654911279678345\n",
      "Epoch 39, Batch 137 Loss:0.02767745405435562\n",
      "Epoch 39, Batch 138 Loss:0.05029195919632912\n",
      "Epoch 39, Batch 139 Loss:0.03100115805864334\n",
      "Epoch 39, Batch 140 Loss:0.03810753673315048\n",
      "Epoch 39, Batch 141 Loss:0.058033861219882965\n",
      "Epoch 39, Batch 142 Loss:0.048539794981479645\n",
      "Epoch 39, Batch 143 Loss:0.020225033164024353\n",
      "Epoch 39, Batch 144 Loss:0.016272613778710365\n",
      "Epoch 39, Batch 145 Loss:0.042353928089141846\n",
      "Epoch 39, Batch 146 Loss:0.040645383298397064\n",
      "Epoch 39, Batch 147 Loss:0.04663067311048508\n",
      "Epoch 39, Batch 148 Loss:0.02641313336789608\n",
      "Epoch 39, Batch 149 Loss:0.03843620792031288\n",
      "Epoch 39, Batch 150 Loss:0.028530612587928772\n",
      "Epoch 39, Batch 151 Loss:0.03765038400888443\n",
      "Epoch 39, Batch 152 Loss:0.02473585680127144\n",
      "Epoch 39, Batch 153 Loss:0.05244630575180054\n",
      "Epoch 39, Batch 154 Loss:0.03794824331998825\n",
      "Epoch 39, Batch 155 Loss:0.03539358079433441\n",
      "Epoch 39, Batch 156 Loss:0.0431465208530426\n",
      "Epoch 39, Batch 157 Loss:0.03376683220267296\n",
      "Epoch 39, Batch 158 Loss:0.01789797469973564\n",
      "Epoch 39, Batch 159 Loss:0.06594061851501465\n",
      "Epoch 39, Batch 160 Loss:0.05528847500681877\n",
      "Epoch 39, Batch 161 Loss:0.022358406335115433\n",
      "Epoch 39, Batch 162 Loss:0.01916981302201748\n",
      "Epoch 39, Batch 163 Loss:0.025701284408569336\n",
      "Epoch 39, Batch 164 Loss:0.02369425818324089\n",
      "Epoch 39, Batch 165 Loss:0.024831373244524002\n",
      "Epoch 39, Batch 166 Loss:0.0315127968788147\n",
      "Epoch 39, Batch 167 Loss:0.023269634693861008\n",
      "Epoch 39, Batch 168 Loss:0.017853284254670143\n",
      "Epoch 39, Batch 169 Loss:0.02593720331788063\n",
      "Epoch 39, Batch 170 Loss:0.05200053006410599\n",
      "Epoch 39, Batch 171 Loss:0.021076932549476624\n",
      "Epoch 39, Batch 172 Loss:0.03143708407878876\n",
      "Epoch 39, Batch 173 Loss:0.062328774482011795\n",
      "Epoch 39, Batch 174 Loss:0.05795428156852722\n",
      "Epoch 39, Batch 175 Loss:0.06309060007333755\n",
      "Epoch 39, Batch 176 Loss:0.033639825880527496\n",
      "Epoch 39, Batch 177 Loss:0.05245495215058327\n",
      "Epoch 39, Batch 178 Loss:0.05268411338329315\n",
      "Epoch 39, Batch 179 Loss:0.030060462653636932\n",
      "Epoch 39, Batch 180 Loss:0.08734075725078583\n",
      "Epoch 39, Batch 181 Loss:0.03260993957519531\n",
      "Epoch 39, Batch 182 Loss:0.05542760342359543\n",
      "Epoch 39, Batch 183 Loss:0.052019715309143066\n",
      "Epoch 39, Batch 184 Loss:0.04726043716073036\n",
      "Epoch 39, Batch 185 Loss:0.056504689157009125\n",
      "Epoch 39, Batch 186 Loss:0.025992698967456818\n",
      "Epoch 39, Batch 187 Loss:0.02700643613934517\n",
      "Epoch 39, Batch 188 Loss:0.059391312301158905\n",
      "Epoch 39, Batch 189 Loss:0.03933488577604294\n",
      "Epoch 39, Batch 190 Loss:0.04014936462044716\n",
      "Epoch 39, Batch 191 Loss:0.07617561519145966\n",
      "Epoch 39, Batch 192 Loss:0.08588903397321701\n",
      "Epoch 39, Batch 193 Loss:0.04609081894159317\n",
      "Epoch 39, Batch 194 Loss:0.06654850393533707\n",
      "Epoch 39, Batch 195 Loss:0.033654119819402695\n",
      "Epoch 39, Batch 196 Loss:0.0816001296043396\n",
      "Epoch 39, Batch 197 Loss:0.06426803767681122\n",
      "Epoch 39, Batch 198 Loss:0.053308650851249695\n",
      "Epoch 39, Batch 199 Loss:0.02087991312146187\n",
      "Epoch 39, Batch 200 Loss:0.06865391135215759\n",
      "Epoch 39, Batch 201 Loss:0.07519969344139099\n",
      "Epoch 39, Batch 202 Loss:0.06588508188724518\n",
      "Epoch 39, Batch 203 Loss:0.05248939245939255\n",
      "Epoch 39, Batch 204 Loss:0.07878077030181885\n",
      "Epoch 39, Batch 205 Loss:0.07700397819280624\n",
      "Epoch 39, Batch 206 Loss:0.04258805140852928\n",
      "Epoch 39, Batch 207 Loss:0.03454035520553589\n",
      "Epoch 39, Batch 208 Loss:0.050675299018621445\n",
      "Epoch 39, Batch 209 Loss:0.078040212392807\n",
      "Epoch 39, Batch 210 Loss:0.04961811751127243\n",
      "Epoch 39, Batch 211 Loss:0.05461575835943222\n",
      "Epoch 39, Batch 212 Loss:0.07256846129894257\n",
      "Epoch 39, Batch 213 Loss:0.042201437056064606\n",
      "Epoch 39, Batch 214 Loss:0.06545945256948471\n",
      "Epoch 39, Batch 215 Loss:0.07690101116895676\n",
      "Epoch 39, Batch 216 Loss:0.07171681523323059\n",
      "Epoch 39, Batch 217 Loss:0.060414522886276245\n",
      "Epoch 39, Batch 218 Loss:0.03176377713680267\n",
      "Epoch 39, Batch 219 Loss:0.03668161481618881\n",
      "Epoch 39, Batch 220 Loss:0.030924882739782333\n",
      "Epoch 39, Batch 221 Loss:0.05226491391658783\n",
      "Epoch 39, Batch 222 Loss:0.028959553688764572\n",
      "Epoch 39, Batch 223 Loss:0.050101473927497864\n",
      "Epoch 39, Batch 224 Loss:0.04779458045959473\n",
      "Epoch 39, Batch 225 Loss:0.070432648062706\n",
      "Epoch 39, Batch 226 Loss:0.05060938745737076\n",
      "Epoch 39, Batch 227 Loss:0.0632249265909195\n",
      "Epoch 39, Batch 228 Loss:0.0284322127699852\n",
      "Epoch 39, Batch 229 Loss:0.03645067289471626\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Batch 230 Loss:0.03202154114842415\n",
      "Epoch 39, Batch 231 Loss:0.04779893159866333\n",
      "Epoch 39, Batch 232 Loss:0.024943560361862183\n",
      "Epoch 39, Batch 233 Loss:0.014807743020355701\n",
      "Loss in this Epoch is: 1.48077430204 %\n",
      "Accuracy in this Epoch is: 88.6099994183 %\n",
      "Epoch 40, Batch 0 Loss:0.03776589035987854\n",
      "Epoch 40, Batch 1 Loss:0.027289265766739845\n",
      "Epoch 40, Batch 2 Loss:0.032673753798007965\n",
      "Epoch 40, Batch 3 Loss:0.049289483577013016\n",
      "Epoch 40, Batch 4 Loss:0.016132516786456108\n",
      "Epoch 40, Batch 5 Loss:0.035181187093257904\n",
      "Epoch 40, Batch 6 Loss:0.03979600965976715\n",
      "Epoch 40, Batch 7 Loss:0.07437051832675934\n",
      "Epoch 40, Batch 8 Loss:0.02158944308757782\n",
      "Epoch 40, Batch 9 Loss:0.035645101219415665\n",
      "Epoch 40, Batch 10 Loss:0.03762760013341904\n",
      "Epoch 40, Batch 11 Loss:0.014270270243287086\n",
      "Epoch 40, Batch 12 Loss:0.048307765275239944\n",
      "Epoch 40, Batch 13 Loss:0.06050410121679306\n",
      "Epoch 40, Batch 14 Loss:0.021456416696310043\n",
      "Epoch 40, Batch 15 Loss:0.014936822466552258\n",
      "Epoch 40, Batch 16 Loss:0.04755726829171181\n",
      "Epoch 40, Batch 17 Loss:0.052769605070352554\n",
      "Epoch 40, Batch 18 Loss:0.0220081377774477\n",
      "Epoch 40, Batch 19 Loss:0.053940851241350174\n",
      "Epoch 40, Batch 20 Loss:0.0779823362827301\n",
      "Epoch 40, Batch 21 Loss:0.023063810542225838\n",
      "Epoch 40, Batch 22 Loss:0.03314848989248276\n",
      "Epoch 40, Batch 23 Loss:0.0317402146756649\n",
      "Epoch 40, Batch 24 Loss:0.01595522277057171\n",
      "Epoch 40, Batch 25 Loss:0.041280269622802734\n",
      "Epoch 40, Batch 26 Loss:0.05770132318139076\n",
      "Epoch 40, Batch 27 Loss:0.04070901498198509\n",
      "Epoch 40, Batch 28 Loss:0.031331758946180344\n",
      "Epoch 40, Batch 29 Loss:0.03376001864671707\n",
      "Epoch 40, Batch 30 Loss:0.02467300370335579\n",
      "Epoch 40, Batch 31 Loss:0.06423094123601913\n",
      "Epoch 40, Batch 32 Loss:0.02083691582083702\n",
      "Epoch 40, Batch 33 Loss:0.0731070414185524\n",
      "Epoch 40, Batch 34 Loss:0.050934139639139175\n",
      "Epoch 40, Batch 35 Loss:0.023247897624969482\n",
      "Epoch 40, Batch 36 Loss:0.06571811437606812\n",
      "Epoch 40, Batch 37 Loss:0.04185030981898308\n",
      "Epoch 40, Batch 38 Loss:0.04266694560647011\n",
      "Epoch 40, Batch 39 Loss:0.019939446821808815\n",
      "Epoch 40, Batch 40 Loss:0.028673842549324036\n",
      "Epoch 40, Batch 41 Loss:0.030225491151213646\n",
      "Epoch 40, Batch 42 Loss:0.019320964813232422\n",
      "Epoch 40, Batch 43 Loss:0.02829126827418804\n",
      "Epoch 40, Batch 44 Loss:0.07909976691007614\n",
      "Epoch 40, Batch 45 Loss:0.03219039738178253\n",
      "Epoch 40, Batch 46 Loss:0.02628546766936779\n",
      "Epoch 40, Batch 47 Loss:0.06612824648618698\n",
      "Epoch 40, Batch 48 Loss:0.01322901714593172\n",
      "Epoch 40, Batch 49 Loss:0.01931718736886978\n",
      "Epoch 40, Batch 50 Loss:0.035093970596790314\n",
      "Epoch 40, Batch 51 Loss:0.0676669105887413\n",
      "Epoch 40, Batch 52 Loss:0.03896490857005119\n",
      "Epoch 40, Batch 53 Loss:0.017708804458379745\n",
      "Epoch 40, Batch 54 Loss:0.03657381236553192\n",
      "Epoch 40, Batch 55 Loss:0.05071614310145378\n",
      "Epoch 40, Batch 56 Loss:0.031978633254766464\n",
      "Epoch 40, Batch 57 Loss:0.02812911942601204\n",
      "Epoch 40, Batch 58 Loss:0.0227765254676342\n",
      "Epoch 40, Batch 59 Loss:0.012996152974665165\n",
      "Epoch 40, Batch 60 Loss:0.030055468901991844\n",
      "Epoch 40, Batch 61 Loss:0.03616252914071083\n",
      "Epoch 40, Batch 62 Loss:0.04827586188912392\n",
      "Epoch 40, Batch 63 Loss:0.03250226750969887\n",
      "Epoch 40, Batch 64 Loss:0.03386243060231209\n",
      "Epoch 40, Batch 65 Loss:0.012464339844882488\n",
      "Epoch 40, Batch 66 Loss:0.04196557775139809\n",
      "Epoch 40, Batch 67 Loss:0.022101368755102158\n",
      "Epoch 40, Batch 68 Loss:0.012226203456521034\n",
      "Epoch 40, Batch 69 Loss:0.026213129982352257\n",
      "Epoch 40, Batch 70 Loss:0.029522357508540154\n",
      "Epoch 40, Batch 71 Loss:0.03175080940127373\n",
      "Epoch 40, Batch 72 Loss:0.03666422888636589\n",
      "Epoch 40, Batch 73 Loss:0.03590673580765724\n",
      "Epoch 40, Batch 74 Loss:0.03691332787275314\n",
      "Epoch 40, Batch 75 Loss:0.023067589849233627\n",
      "Epoch 40, Batch 76 Loss:0.036437541246414185\n",
      "Epoch 40, Batch 77 Loss:0.04996238276362419\n",
      "Epoch 40, Batch 78 Loss:0.010890541598200798\n",
      "Epoch 40, Batch 79 Loss:0.031311679631471634\n",
      "Epoch 40, Batch 80 Loss:0.044837746769189835\n",
      "Epoch 40, Batch 81 Loss:0.026432810351252556\n",
      "Epoch 40, Batch 82 Loss:0.013723274692893028\n",
      "Epoch 40, Batch 83 Loss:0.03220926597714424\n",
      "Epoch 40, Batch 84 Loss:0.019289622083306313\n",
      "Epoch 40, Batch 85 Loss:0.03903802111744881\n",
      "Epoch 40, Batch 86 Loss:0.022576510906219482\n",
      "Epoch 40, Batch 87 Loss:0.018582498654723167\n",
      "Epoch 40, Batch 88 Loss:0.03084215708076954\n",
      "Epoch 40, Batch 89 Loss:0.03330248221755028\n",
      "Epoch 40, Batch 90 Loss:0.05182965099811554\n",
      "Epoch 40, Batch 91 Loss:0.018985901027917862\n",
      "Epoch 40, Batch 92 Loss:0.04328172653913498\n",
      "Epoch 40, Batch 93 Loss:0.03766891360282898\n",
      "Epoch 40, Batch 94 Loss:0.020936325192451477\n",
      "Epoch 40, Batch 95 Loss:0.023978987708687782\n",
      "Epoch 40, Batch 96 Loss:0.021934229880571365\n",
      "Epoch 40, Batch 97 Loss:0.02753205969929695\n",
      "Epoch 40, Batch 98 Loss:0.038329556584358215\n",
      "Epoch 40, Batch 99 Loss:0.030725844204425812\n",
      "Epoch 40, Batch 100 Loss:0.007772400509566069\n",
      "Epoch 40, Batch 101 Loss:0.018090765923261642\n",
      "Epoch 40, Batch 102 Loss:0.03778015822172165\n",
      "Epoch 40, Batch 103 Loss:0.037911031395196915\n",
      "Epoch 40, Batch 104 Loss:0.03651082515716553\n",
      "Epoch 40, Batch 105 Loss:0.007105003111064434\n",
      "Epoch 40, Batch 106 Loss:0.051960788667201996\n",
      "Epoch 40, Batch 107 Loss:0.03539386764168739\n",
      "Epoch 40, Batch 108 Loss:0.019783614203333855\n",
      "Epoch 40, Batch 109 Loss:0.010139822028577328\n",
      "Epoch 40, Batch 110 Loss:0.023006416857242584\n",
      "Epoch 40, Batch 111 Loss:0.015641070902347565\n",
      "Epoch 40, Batch 112 Loss:0.03997671604156494\n",
      "Epoch 40, Batch 113 Loss:0.04608637094497681\n",
      "Epoch 40, Batch 114 Loss:0.026662584394216537\n",
      "Epoch 40, Batch 115 Loss:0.036453746259212494\n",
      "Epoch 40, Batch 116 Loss:0.02158162370324135\n",
      "Epoch 40, Batch 117 Loss:0.06147152930498123\n",
      "Epoch 40, Batch 118 Loss:0.06399042904376984\n",
      "Epoch 40, Batch 119 Loss:0.027109593152999878\n",
      "Epoch 40, Batch 120 Loss:0.02991129830479622\n",
      "Epoch 40, Batch 121 Loss:0.030938107520341873\n",
      "Epoch 40, Batch 122 Loss:0.023850079625844955\n",
      "Epoch 40, Batch 123 Loss:0.042973268777132034\n",
      "Epoch 40, Batch 124 Loss:0.04089327156543732\n",
      "Epoch 40, Batch 125 Loss:0.034488528966903687\n",
      "Epoch 40, Batch 126 Loss:0.013081135228276253\n",
      "Epoch 40, Batch 127 Loss:0.02771560102701187\n",
      "Epoch 40, Batch 128 Loss:0.041896507143974304\n",
      "Epoch 40, Batch 129 Loss:0.02086384780704975\n",
      "Epoch 40, Batch 130 Loss:0.0612715482711792\n",
      "Epoch 40, Batch 131 Loss:0.026900580152869225\n",
      "Epoch 40, Batch 132 Loss:0.029905695468187332\n",
      "Epoch 40, Batch 133 Loss:0.027296466752886772\n",
      "Epoch 40, Batch 134 Loss:0.03301325440406799\n",
      "Epoch 40, Batch 135 Loss:0.04415605217218399\n",
      "Epoch 40, Batch 136 Loss:0.05360010266304016\n",
      "Epoch 40, Batch 137 Loss:0.0367487296462059\n",
      "Epoch 40, Batch 138 Loss:0.010052435100078583\n",
      "Epoch 40, Batch 139 Loss:0.02226429618895054\n",
      "Epoch 40, Batch 140 Loss:0.031185243278741837\n",
      "Epoch 40, Batch 141 Loss:0.02964687906205654\n",
      "Epoch 40, Batch 142 Loss:0.009615108370780945\n",
      "Epoch 40, Batch 143 Loss:0.03460771217942238\n",
      "Epoch 40, Batch 144 Loss:0.021255459636449814\n",
      "Epoch 40, Batch 145 Loss:0.03588908910751343\n",
      "Epoch 40, Batch 146 Loss:0.03613266721367836\n",
      "Epoch 40, Batch 147 Loss:0.025085903704166412\n",
      "Epoch 40, Batch 148 Loss:0.02218833938241005\n",
      "Epoch 40, Batch 149 Loss:0.02270158752799034\n",
      "Epoch 40, Batch 150 Loss:0.012100194580852985\n",
      "Epoch 40, Batch 151 Loss:0.029188182204961777\n",
      "Epoch 40, Batch 152 Loss:0.019369641318917274\n",
      "Epoch 40, Batch 153 Loss:0.019502846524119377\n",
      "Epoch 40, Batch 154 Loss:0.02057502418756485\n",
      "Epoch 40, Batch 155 Loss:0.027628306299448013\n",
      "Epoch 40, Batch 156 Loss:0.10569926351308823\n",
      "Epoch 40, Batch 157 Loss:0.018992695957422256\n",
      "Epoch 40, Batch 158 Loss:0.02143038995563984\n",
      "Epoch 40, Batch 159 Loss:0.020165394991636276\n",
      "Epoch 40, Batch 160 Loss:0.09702487289905548\n",
      "Epoch 40, Batch 161 Loss:0.02248309552669525\n",
      "Epoch 40, Batch 162 Loss:0.05571306496858597\n",
      "Epoch 40, Batch 163 Loss:0.04589139297604561\n",
      "Epoch 40, Batch 164 Loss:0.0122369434684515\n",
      "Epoch 40, Batch 165 Loss:0.04595491662621498\n",
      "Epoch 40, Batch 166 Loss:0.038298558443784714\n",
      "Epoch 40, Batch 167 Loss:0.027549225836992264\n",
      "Epoch 40, Batch 168 Loss:0.08146457374095917\n",
      "Epoch 40, Batch 169 Loss:0.028355496004223824\n",
      "Epoch 40, Batch 170 Loss:0.022659290581941605\n",
      "Epoch 40, Batch 171 Loss:0.03987303748726845\n",
      "Epoch 40, Batch 172 Loss:0.04942452162504196\n",
      "Epoch 40, Batch 173 Loss:0.016196059063076973\n",
      "Epoch 40, Batch 174 Loss:0.027299804612994194\n",
      "Epoch 40, Batch 175 Loss:0.03232516348361969\n",
      "Epoch 40, Batch 176 Loss:0.01948091760277748\n",
      "Epoch 40, Batch 177 Loss:0.04735008254647255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Batch 178 Loss:0.03587234020233154\n",
      "Epoch 40, Batch 179 Loss:0.01904163882136345\n",
      "Epoch 40, Batch 180 Loss:0.010866092517971992\n",
      "Epoch 40, Batch 181 Loss:0.02812858484685421\n",
      "Epoch 40, Batch 182 Loss:0.03869987651705742\n",
      "Epoch 40, Batch 183 Loss:0.030333437025547028\n",
      "Epoch 40, Batch 184 Loss:0.04032744839787483\n",
      "Epoch 40, Batch 185 Loss:0.05907095968723297\n",
      "Epoch 40, Batch 186 Loss:0.038472745567560196\n",
      "Epoch 40, Batch 187 Loss:0.048331938683986664\n",
      "Epoch 40, Batch 188 Loss:0.11369284987449646\n",
      "Epoch 40, Batch 189 Loss:0.029841411858797073\n",
      "Epoch 40, Batch 190 Loss:0.024183016270399094\n",
      "Epoch 40, Batch 191 Loss:0.030830463394522667\n",
      "Epoch 40, Batch 192 Loss:0.012792915105819702\n",
      "Epoch 40, Batch 193 Loss:0.02971234917640686\n",
      "Epoch 40, Batch 194 Loss:0.025509241968393326\n",
      "Epoch 40, Batch 195 Loss:0.03468341380357742\n",
      "Epoch 40, Batch 196 Loss:0.03978480398654938\n",
      "Epoch 40, Batch 197 Loss:0.053505461663007736\n",
      "Epoch 40, Batch 198 Loss:0.06701204180717468\n",
      "Epoch 40, Batch 199 Loss:0.05439166724681854\n",
      "Epoch 40, Batch 200 Loss:0.06106942519545555\n",
      "Epoch 40, Batch 201 Loss:0.030032843351364136\n",
      "Epoch 40, Batch 202 Loss:0.06297942996025085\n",
      "Epoch 40, Batch 203 Loss:0.033876121044158936\n",
      "Epoch 40, Batch 204 Loss:0.0662669911980629\n",
      "Epoch 40, Batch 205 Loss:0.030175667256116867\n",
      "Epoch 40, Batch 206 Loss:0.012232238426804543\n",
      "Epoch 40, Batch 207 Loss:0.03983309864997864\n",
      "Epoch 40, Batch 208 Loss:0.03856798633933067\n",
      "Epoch 40, Batch 209 Loss:0.035721562802791595\n",
      "Epoch 40, Batch 210 Loss:0.0471377819776535\n",
      "Epoch 40, Batch 211 Loss:0.030379101634025574\n",
      "Epoch 40, Batch 212 Loss:0.040167436003685\n",
      "Epoch 40, Batch 213 Loss:0.06747692823410034\n",
      "Epoch 40, Batch 214 Loss:0.05333632230758667\n",
      "Epoch 40, Batch 215 Loss:0.06619131565093994\n",
      "Epoch 40, Batch 216 Loss:0.048859983682632446\n",
      "Epoch 40, Batch 217 Loss:0.05865601450204849\n",
      "Epoch 40, Batch 218 Loss:0.056297097355127335\n",
      "Epoch 40, Batch 219 Loss:0.07620850950479507\n",
      "Epoch 40, Batch 220 Loss:0.0673927515745163\n",
      "Epoch 40, Batch 221 Loss:0.022712869569659233\n",
      "Epoch 40, Batch 222 Loss:0.06374098360538483\n",
      "Epoch 40, Batch 223 Loss:0.047475144267082214\n",
      "Epoch 40, Batch 224 Loss:0.027512557804584503\n",
      "Epoch 40, Batch 225 Loss:0.039608895778656006\n",
      "Epoch 40, Batch 226 Loss:0.026954319328069687\n",
      "Epoch 40, Batch 227 Loss:0.02727387845516205\n",
      "Epoch 40, Batch 228 Loss:0.06162971258163452\n",
      "Epoch 40, Batch 229 Loss:0.03576785698533058\n",
      "Epoch 40, Batch 230 Loss:0.04372318089008331\n",
      "Epoch 40, Batch 231 Loss:0.04185985401272774\n",
      "Epoch 40, Batch 232 Loss:0.06310983002185822\n",
      "Epoch 40, Batch 233 Loss:0.07245010882616043\n",
      "Loss in this Epoch is: 7.24501088262 %\n",
      "Accuracy in this Epoch is: 88.9199972153 %\n",
      "Epoch 41, Batch 0 Loss:0.022455483675003052\n",
      "Epoch 41, Batch 1 Loss:0.014670916832983494\n",
      "Epoch 41, Batch 2 Loss:0.028188923373818398\n",
      "Epoch 41, Batch 3 Loss:0.04820434749126434\n",
      "Epoch 41, Batch 4 Loss:0.058187395334243774\n",
      "Epoch 41, Batch 5 Loss:0.04315540939569473\n",
      "Epoch 41, Batch 6 Loss:0.04848991706967354\n",
      "Epoch 41, Batch 7 Loss:0.02392777055501938\n",
      "Epoch 41, Batch 8 Loss:0.029454965144395828\n",
      "Epoch 41, Batch 9 Loss:0.021101079881191254\n",
      "Epoch 41, Batch 10 Loss:0.05654657259583473\n",
      "Epoch 41, Batch 11 Loss:0.03146008029580116\n",
      "Epoch 41, Batch 12 Loss:0.0266915000975132\n",
      "Epoch 41, Batch 13 Loss:0.01821182481944561\n",
      "Epoch 41, Batch 14 Loss:0.037775132805109024\n",
      "Epoch 41, Batch 15 Loss:0.04916591942310333\n",
      "Epoch 41, Batch 16 Loss:0.010414643213152885\n",
      "Epoch 41, Batch 17 Loss:0.0514913909137249\n",
      "Epoch 41, Batch 18 Loss:0.03427460417151451\n",
      "Epoch 41, Batch 19 Loss:0.045194245874881744\n",
      "Epoch 41, Batch 20 Loss:0.030661052092909813\n",
      "Epoch 41, Batch 21 Loss:0.022557174786925316\n",
      "Epoch 41, Batch 22 Loss:0.01668020337820053\n",
      "Epoch 41, Batch 23 Loss:0.030037039890885353\n",
      "Epoch 41, Batch 24 Loss:0.0336383655667305\n",
      "Epoch 41, Batch 25 Loss:0.0631415918469429\n",
      "Epoch 41, Batch 26 Loss:0.03195246681571007\n",
      "Epoch 41, Batch 27 Loss:0.052514202892780304\n",
      "Epoch 41, Batch 28 Loss:0.014524814672768116\n",
      "Epoch 41, Batch 29 Loss:0.05427660793066025\n",
      "Epoch 41, Batch 30 Loss:0.02273663505911827\n",
      "Epoch 41, Batch 31 Loss:0.048631876707077026\n",
      "Epoch 41, Batch 32 Loss:0.024064885452389717\n",
      "Epoch 41, Batch 33 Loss:0.02759370021522045\n",
      "Epoch 41, Batch 34 Loss:0.047914400696754456\n",
      "Epoch 41, Batch 35 Loss:0.034552380442619324\n",
      "Epoch 41, Batch 36 Loss:0.02072126604616642\n",
      "Epoch 41, Batch 37 Loss:0.02631688304245472\n",
      "Epoch 41, Batch 38 Loss:0.023258524015545845\n",
      "Epoch 41, Batch 39 Loss:0.04236157611012459\n",
      "Epoch 41, Batch 40 Loss:0.044765740633010864\n",
      "Epoch 41, Batch 41 Loss:0.032547157257795334\n",
      "Epoch 41, Batch 42 Loss:0.04002177715301514\n",
      "Epoch 41, Batch 43 Loss:0.029210930690169334\n",
      "Epoch 41, Batch 44 Loss:0.04690280556678772\n",
      "Epoch 41, Batch 45 Loss:0.039901357144117355\n",
      "Epoch 41, Batch 46 Loss:0.04744698852300644\n",
      "Epoch 41, Batch 47 Loss:0.0699077695608139\n",
      "Epoch 41, Batch 48 Loss:0.02303122729063034\n",
      "Epoch 41, Batch 49 Loss:0.13236096501350403\n",
      "Epoch 41, Batch 50 Loss:0.12561607360839844\n",
      "Epoch 41, Batch 51 Loss:0.04177059605717659\n",
      "Epoch 41, Batch 52 Loss:0.0660010576248169\n",
      "Epoch 41, Batch 53 Loss:0.06804446876049042\n",
      "Epoch 41, Batch 54 Loss:0.04019651561975479\n",
      "Epoch 41, Batch 55 Loss:0.04005087912082672\n",
      "Epoch 41, Batch 56 Loss:0.024225104600191116\n",
      "Epoch 41, Batch 57 Loss:0.030049366876482964\n",
      "Epoch 41, Batch 58 Loss:0.031015433371067047\n",
      "Epoch 41, Batch 59 Loss:0.04669732227921486\n",
      "Epoch 41, Batch 60 Loss:0.0439290851354599\n",
      "Epoch 41, Batch 61 Loss:0.062417514622211456\n",
      "Epoch 41, Batch 62 Loss:0.07734675705432892\n",
      "Epoch 41, Batch 63 Loss:0.058826085180044174\n",
      "Epoch 41, Batch 64 Loss:0.059785395860672\n",
      "Epoch 41, Batch 65 Loss:0.05416177585721016\n",
      "Epoch 41, Batch 66 Loss:0.0658896267414093\n",
      "Epoch 41, Batch 67 Loss:0.06475982069969177\n",
      "Epoch 41, Batch 68 Loss:0.03994166478514671\n",
      "Epoch 41, Batch 69 Loss:0.0341637022793293\n",
      "Epoch 41, Batch 70 Loss:0.06785865128040314\n",
      "Epoch 41, Batch 71 Loss:0.09774762392044067\n",
      "Epoch 41, Batch 72 Loss:0.03915140777826309\n",
      "Epoch 41, Batch 73 Loss:0.07363862544298172\n",
      "Epoch 41, Batch 74 Loss:0.06522593647241592\n",
      "Epoch 41, Batch 75 Loss:0.027545830234885216\n",
      "Epoch 41, Batch 76 Loss:0.04277053475379944\n",
      "Epoch 41, Batch 77 Loss:0.040410496294498444\n",
      "Epoch 41, Batch 78 Loss:0.079213447868824\n",
      "Epoch 41, Batch 79 Loss:0.09904640913009644\n",
      "Epoch 41, Batch 80 Loss:0.040409497916698456\n",
      "Epoch 41, Batch 81 Loss:0.0323135145008564\n",
      "Epoch 41, Batch 82 Loss:0.036549363285303116\n",
      "Epoch 41, Batch 83 Loss:0.06308884173631668\n",
      "Epoch 41, Batch 84 Loss:0.03604549169540405\n",
      "Epoch 41, Batch 85 Loss:0.023157460615038872\n",
      "Epoch 41, Batch 86 Loss:0.03129703179001808\n",
      "Epoch 41, Batch 87 Loss:0.024189693853259087\n",
      "Epoch 41, Batch 88 Loss:0.06631352752447128\n",
      "Epoch 41, Batch 89 Loss:0.051748357713222504\n",
      "Epoch 41, Batch 90 Loss:0.0579032227396965\n",
      "Epoch 41, Batch 91 Loss:0.04436514526605606\n",
      "Epoch 41, Batch 92 Loss:0.033722978085279465\n",
      "Epoch 41, Batch 93 Loss:0.04060672968626022\n",
      "Epoch 41, Batch 94 Loss:0.015201173722743988\n",
      "Epoch 41, Batch 95 Loss:0.0953303650021553\n",
      "Epoch 41, Batch 96 Loss:0.0409674309194088\n",
      "Epoch 41, Batch 97 Loss:0.04939975589513779\n",
      "Epoch 41, Batch 98 Loss:0.030642937868833542\n",
      "Epoch 41, Batch 99 Loss:0.01883821003139019\n",
      "Epoch 41, Batch 100 Loss:0.025887735188007355\n",
      "Epoch 41, Batch 101 Loss:0.054044000804424286\n",
      "Epoch 41, Batch 102 Loss:0.04314737394452095\n",
      "Epoch 41, Batch 103 Loss:0.031064473092556\n",
      "Epoch 41, Batch 104 Loss:0.030922990292310715\n",
      "Epoch 41, Batch 105 Loss:0.053246770054101944\n",
      "Epoch 41, Batch 106 Loss:0.03460847958922386\n",
      "Epoch 41, Batch 107 Loss:0.06211401894688606\n",
      "Epoch 41, Batch 108 Loss:0.048025719821453094\n",
      "Epoch 41, Batch 109 Loss:0.04251744598150253\n",
      "Epoch 41, Batch 110 Loss:0.01981002651154995\n",
      "Epoch 41, Batch 111 Loss:0.04601741582155228\n",
      "Epoch 41, Batch 112 Loss:0.033671338111162186\n",
      "Epoch 41, Batch 113 Loss:0.014017453417181969\n",
      "Epoch 41, Batch 114 Loss:0.04052010551095009\n",
      "Epoch 41, Batch 115 Loss:0.04678814858198166\n",
      "Epoch 41, Batch 116 Loss:0.02842770889401436\n",
      "Epoch 41, Batch 117 Loss:0.011528819799423218\n",
      "Epoch 41, Batch 118 Loss:0.04613522067666054\n",
      "Epoch 41, Batch 119 Loss:0.009260228835046291\n",
      "Epoch 41, Batch 120 Loss:0.019047608599066734\n",
      "Epoch 41, Batch 121 Loss:0.020502138882875443\n",
      "Epoch 41, Batch 122 Loss:0.021223338320851326\n",
      "Epoch 41, Batch 123 Loss:0.05387946963310242\n",
      "Epoch 41, Batch 124 Loss:0.06097247079014778\n",
      "Epoch 41, Batch 125 Loss:0.023194923996925354\n",
      "Epoch 41, Batch 126 Loss:0.028920862823724747\n",
      "Epoch 41, Batch 127 Loss:0.019933879375457764\n",
      "Epoch 41, Batch 128 Loss:0.028180250898003578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Batch 129 Loss:0.014650838449597359\n",
      "Epoch 41, Batch 130 Loss:0.03297901153564453\n",
      "Epoch 41, Batch 131 Loss:0.03363897651433945\n",
      "Epoch 41, Batch 132 Loss:0.05899322032928467\n",
      "Epoch 41, Batch 133 Loss:0.02256309986114502\n",
      "Epoch 41, Batch 134 Loss:0.05484311282634735\n",
      "Epoch 41, Batch 135 Loss:0.012765701860189438\n",
      "Epoch 41, Batch 136 Loss:0.04050745815038681\n",
      "Epoch 41, Batch 137 Loss:0.06377702206373215\n",
      "Epoch 41, Batch 138 Loss:0.028589770197868347\n",
      "Epoch 41, Batch 139 Loss:0.0349607840180397\n",
      "Epoch 41, Batch 140 Loss:0.018171211704611778\n",
      "Epoch 41, Batch 141 Loss:0.010968240909278393\n",
      "Epoch 41, Batch 142 Loss:0.035624366253614426\n",
      "Epoch 41, Batch 143 Loss:0.024469345808029175\n",
      "Epoch 41, Batch 144 Loss:0.015363912098109722\n",
      "Epoch 41, Batch 145 Loss:0.06041926518082619\n",
      "Epoch 41, Batch 146 Loss:0.06789776682853699\n",
      "Epoch 41, Batch 147 Loss:0.07007703930139542\n",
      "Epoch 41, Batch 148 Loss:0.039780668914318085\n",
      "Epoch 41, Batch 149 Loss:0.036390520632267\n",
      "Epoch 41, Batch 150 Loss:0.02204236015677452\n",
      "Epoch 41, Batch 151 Loss:0.012720980681478977\n",
      "Epoch 41, Batch 152 Loss:0.06571389734745026\n",
      "Epoch 41, Batch 153 Loss:0.023631256073713303\n",
      "Epoch 41, Batch 154 Loss:0.03150247409939766\n",
      "Epoch 41, Batch 155 Loss:0.06189146637916565\n",
      "Epoch 41, Batch 156 Loss:0.024046510457992554\n",
      "Epoch 41, Batch 157 Loss:0.018463829532265663\n",
      "Epoch 41, Batch 158 Loss:0.03593045100569725\n",
      "Epoch 41, Batch 159 Loss:0.03720401972532272\n",
      "Epoch 41, Batch 160 Loss:0.039432354271411896\n",
      "Epoch 41, Batch 161 Loss:0.027485869824886322\n",
      "Epoch 41, Batch 162 Loss:0.06445278972387314\n",
      "Epoch 41, Batch 163 Loss:0.028068907558918\n",
      "Epoch 41, Batch 164 Loss:0.02858796902000904\n",
      "Epoch 41, Batch 165 Loss:0.04468924179673195\n",
      "Epoch 41, Batch 166 Loss:0.03261189162731171\n",
      "Epoch 41, Batch 167 Loss:0.02507302537560463\n",
      "Epoch 41, Batch 168 Loss:0.03461363539099693\n",
      "Epoch 41, Batch 169 Loss:0.021940670907497406\n",
      "Epoch 41, Batch 170 Loss:0.025598065927624702\n",
      "Epoch 41, Batch 171 Loss:0.0499013252556324\n",
      "Epoch 41, Batch 172 Loss:0.0371650829911232\n",
      "Epoch 41, Batch 173 Loss:0.03669054061174393\n",
      "Epoch 41, Batch 174 Loss:0.021337326616048813\n",
      "Epoch 41, Batch 175 Loss:0.04743105545639992\n",
      "Epoch 41, Batch 176 Loss:0.04095330089330673\n",
      "Epoch 41, Batch 177 Loss:0.020935434848070145\n",
      "Epoch 41, Batch 178 Loss:0.03809131681919098\n",
      "Epoch 41, Batch 179 Loss:0.031782492995262146\n",
      "Epoch 41, Batch 180 Loss:0.028658665716648102\n",
      "Epoch 41, Batch 181 Loss:0.027828391641378403\n",
      "Epoch 41, Batch 182 Loss:0.04618098959326744\n",
      "Epoch 41, Batch 183 Loss:0.01838504895567894\n",
      "Epoch 41, Batch 184 Loss:0.03538460284471512\n",
      "Epoch 41, Batch 185 Loss:0.044760145246982574\n",
      "Epoch 41, Batch 186 Loss:0.03695164993405342\n",
      "Epoch 41, Batch 187 Loss:0.032080575823783875\n",
      "Epoch 41, Batch 188 Loss:0.07361415028572083\n",
      "Epoch 41, Batch 189 Loss:0.02227088436484337\n",
      "Epoch 41, Batch 190 Loss:0.05541268736124039\n",
      "Epoch 41, Batch 191 Loss:0.06501278281211853\n",
      "Epoch 41, Batch 192 Loss:0.03706873953342438\n",
      "Epoch 41, Batch 193 Loss:0.026607614010572433\n",
      "Epoch 41, Batch 194 Loss:0.05426613241434097\n",
      "Epoch 41, Batch 195 Loss:0.055330995470285416\n",
      "Epoch 41, Batch 196 Loss:0.05774335935711861\n",
      "Epoch 41, Batch 197 Loss:0.03168618306517601\n",
      "Epoch 41, Batch 198 Loss:0.05638277530670166\n",
      "Epoch 41, Batch 199 Loss:0.034416232258081436\n",
      "Epoch 41, Batch 200 Loss:0.08702012896537781\n",
      "Epoch 41, Batch 201 Loss:0.042704448103904724\n",
      "Epoch 41, Batch 202 Loss:0.01754804328083992\n",
      "Epoch 41, Batch 203 Loss:0.032175034284591675\n",
      "Epoch 41, Batch 204 Loss:0.051437199115753174\n",
      "Epoch 41, Batch 205 Loss:0.03666902333498001\n",
      "Epoch 41, Batch 206 Loss:0.05410188063979149\n",
      "Epoch 41, Batch 207 Loss:0.06052670627832413\n",
      "Epoch 41, Batch 208 Loss:0.027417223900556564\n",
      "Epoch 41, Batch 209 Loss:0.03724243491888046\n",
      "Epoch 41, Batch 210 Loss:0.023934654891490936\n",
      "Epoch 41, Batch 211 Loss:0.02514052949845791\n",
      "Epoch 41, Batch 212 Loss:0.013337640091776848\n",
      "Epoch 41, Batch 213 Loss:0.02479895018041134\n",
      "Epoch 41, Batch 214 Loss:0.01628468930721283\n",
      "Epoch 41, Batch 215 Loss:0.024103302508592606\n",
      "Epoch 41, Batch 216 Loss:0.021070385351777077\n",
      "Epoch 41, Batch 217 Loss:0.03196507692337036\n",
      "Epoch 41, Batch 218 Loss:0.03497679531574249\n",
      "Epoch 41, Batch 219 Loss:0.048800066113471985\n",
      "Epoch 41, Batch 220 Loss:0.03746640309691429\n",
      "Epoch 41, Batch 221 Loss:0.030137572437524796\n",
      "Epoch 41, Batch 222 Loss:0.06822104007005692\n",
      "Epoch 41, Batch 223 Loss:0.05364356189966202\n",
      "Epoch 41, Batch 224 Loss:0.059285134077072144\n",
      "Epoch 41, Batch 225 Loss:0.03990061581134796\n",
      "Epoch 41, Batch 226 Loss:0.046084992587566376\n",
      "Epoch 41, Batch 227 Loss:0.023827042430639267\n",
      "Epoch 41, Batch 228 Loss:0.047547586262226105\n",
      "Epoch 41, Batch 229 Loss:0.033658988773822784\n",
      "Epoch 41, Batch 230 Loss:0.02485467866063118\n",
      "Epoch 41, Batch 231 Loss:0.053201571106910706\n",
      "Epoch 41, Batch 232 Loss:0.04092613607645035\n",
      "Epoch 41, Batch 233 Loss:0.018364492803812027\n",
      "Loss in this Epoch is: 1.83644928038 %\n",
      "Accuracy in this Epoch is: 88.7600004673 %\n",
      "Epoch 42, Batch 0 Loss:0.019247449934482574\n",
      "Epoch 42, Batch 1 Loss:0.05771585926413536\n",
      "Epoch 42, Batch 2 Loss:0.04494354873895645\n",
      "Epoch 42, Batch 3 Loss:0.03312961384654045\n",
      "Epoch 42, Batch 4 Loss:0.012265821918845177\n",
      "Epoch 42, Batch 5 Loss:0.028452936559915543\n",
      "Epoch 42, Batch 6 Loss:0.022491799667477608\n",
      "Epoch 42, Batch 7 Loss:0.015456924214959145\n",
      "Epoch 42, Batch 8 Loss:0.03403663635253906\n",
      "Epoch 42, Batch 9 Loss:0.01406801026314497\n",
      "Epoch 42, Batch 10 Loss:0.018922720104455948\n",
      "Epoch 42, Batch 11 Loss:0.021612413227558136\n",
      "Epoch 42, Batch 12 Loss:0.020301759243011475\n",
      "Epoch 42, Batch 13 Loss:0.010103815235197544\n",
      "Epoch 42, Batch 14 Loss:0.03254156932234764\n",
      "Epoch 42, Batch 15 Loss:0.0249188169836998\n",
      "Epoch 42, Batch 16 Loss:0.019063081592321396\n",
      "Epoch 42, Batch 17 Loss:0.030871016904711723\n",
      "Epoch 42, Batch 18 Loss:0.02712659351527691\n",
      "Epoch 42, Batch 19 Loss:0.02423887327313423\n",
      "Epoch 42, Batch 20 Loss:0.026502829045057297\n",
      "Epoch 42, Batch 21 Loss:0.030734607949852943\n",
      "Epoch 42, Batch 22 Loss:0.019076144322752953\n",
      "Epoch 42, Batch 23 Loss:0.04006456211209297\n",
      "Epoch 42, Batch 24 Loss:0.019883893430233\n",
      "Epoch 42, Batch 25 Loss:0.048748861998319626\n",
      "Epoch 42, Batch 26 Loss:0.02220296487212181\n",
      "Epoch 42, Batch 27 Loss:0.015853237360715866\n",
      "Epoch 42, Batch 28 Loss:0.021980082616209984\n",
      "Epoch 42, Batch 29 Loss:0.04615043103694916\n",
      "Epoch 42, Batch 30 Loss:0.015889590606093407\n",
      "Epoch 42, Batch 31 Loss:0.022690007463097572\n",
      "Epoch 42, Batch 32 Loss:0.013305778615176678\n",
      "Epoch 42, Batch 33 Loss:0.0203643050044775\n",
      "Epoch 42, Batch 34 Loss:0.0373409204185009\n",
      "Epoch 42, Batch 35 Loss:0.020319851115345955\n",
      "Epoch 42, Batch 36 Loss:0.011495268903672695\n",
      "Epoch 42, Batch 37 Loss:0.013207855634391308\n",
      "Epoch 42, Batch 38 Loss:0.011261294595897198\n",
      "Epoch 42, Batch 39 Loss:0.02916327863931656\n",
      "Epoch 42, Batch 40 Loss:0.03128529712557793\n",
      "Epoch 42, Batch 41 Loss:0.007179774343967438\n",
      "Epoch 42, Batch 42 Loss:0.05776422470808029\n",
      "Epoch 42, Batch 43 Loss:0.023806309327483177\n",
      "Epoch 42, Batch 44 Loss:0.014408158138394356\n",
      "Epoch 42, Batch 45 Loss:0.03511115908622742\n",
      "Epoch 42, Batch 46 Loss:0.02539983205497265\n",
      "Epoch 42, Batch 47 Loss:0.034228742122650146\n",
      "Epoch 42, Batch 48 Loss:0.01363319344818592\n",
      "Epoch 42, Batch 49 Loss:0.008472809568047523\n",
      "Epoch 42, Batch 50 Loss:0.017883677035570145\n",
      "Epoch 42, Batch 51 Loss:0.03271763399243355\n",
      "Epoch 42, Batch 52 Loss:0.037046514451503754\n",
      "Epoch 42, Batch 53 Loss:0.046749673783779144\n",
      "Epoch 42, Batch 54 Loss:0.016958722844719887\n",
      "Epoch 42, Batch 55 Loss:0.006869880948215723\n",
      "Epoch 42, Batch 56 Loss:0.020779840648174286\n",
      "Epoch 42, Batch 57 Loss:0.019446194171905518\n",
      "Epoch 42, Batch 58 Loss:0.029880456626415253\n",
      "Epoch 42, Batch 59 Loss:0.02603995054960251\n",
      "Epoch 42, Batch 60 Loss:0.02198132872581482\n",
      "Epoch 42, Batch 61 Loss:0.017347054556012154\n",
      "Epoch 42, Batch 62 Loss:0.020746968686580658\n",
      "Epoch 42, Batch 63 Loss:0.022766858339309692\n",
      "Epoch 42, Batch 64 Loss:0.029679708182811737\n",
      "Epoch 42, Batch 65 Loss:0.057548895478248596\n",
      "Epoch 42, Batch 66 Loss:0.019050635397434235\n",
      "Epoch 42, Batch 67 Loss:0.04375264421105385\n",
      "Epoch 42, Batch 68 Loss:0.02600734867155552\n",
      "Epoch 42, Batch 69 Loss:0.024195192381739616\n",
      "Epoch 42, Batch 70 Loss:0.03363128751516342\n",
      "Epoch 42, Batch 71 Loss:0.017617901787161827\n",
      "Epoch 42, Batch 72 Loss:0.029897315427660942\n",
      "Epoch 42, Batch 73 Loss:0.013778476044535637\n",
      "Epoch 42, Batch 74 Loss:0.016140306368470192\n",
      "Epoch 42, Batch 75 Loss:0.01691277138888836\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Batch 76 Loss:0.034687262028455734\n",
      "Epoch 42, Batch 77 Loss:0.027009978890419006\n",
      "Epoch 42, Batch 78 Loss:0.017475489526987076\n",
      "Epoch 42, Batch 79 Loss:0.006910137832164764\n",
      "Epoch 42, Batch 80 Loss:0.01582544483244419\n",
      "Epoch 42, Batch 81 Loss:0.02181055024266243\n",
      "Epoch 42, Batch 82 Loss:0.017813697457313538\n",
      "Epoch 42, Batch 83 Loss:0.0250226017087698\n",
      "Epoch 42, Batch 84 Loss:0.011240358464419842\n",
      "Epoch 42, Batch 85 Loss:0.02620578557252884\n",
      "Epoch 42, Batch 86 Loss:0.029843486845493317\n",
      "Epoch 42, Batch 87 Loss:0.009068659506738186\n",
      "Epoch 42, Batch 88 Loss:0.017097586765885353\n",
      "Epoch 42, Batch 89 Loss:0.010273769497871399\n",
      "Epoch 42, Batch 90 Loss:0.013267572969198227\n",
      "Epoch 42, Batch 91 Loss:0.04388757422566414\n",
      "Epoch 42, Batch 92 Loss:0.05228755623102188\n",
      "Epoch 42, Batch 93 Loss:0.021669544279575348\n",
      "Epoch 42, Batch 94 Loss:0.03956407681107521\n",
      "Epoch 42, Batch 95 Loss:0.013056434690952301\n",
      "Epoch 42, Batch 96 Loss:0.07827393710613251\n",
      "Epoch 42, Batch 97 Loss:0.021803179755806923\n",
      "Epoch 42, Batch 98 Loss:0.010207390412688255\n",
      "Epoch 42, Batch 99 Loss:0.0486735962331295\n",
      "Epoch 42, Batch 100 Loss:0.018401306122541428\n",
      "Epoch 42, Batch 101 Loss:0.014916596934199333\n",
      "Epoch 42, Batch 102 Loss:0.019200146198272705\n",
      "Epoch 42, Batch 103 Loss:0.010358146391808987\n",
      "Epoch 42, Batch 104 Loss:0.01203174702823162\n",
      "Epoch 42, Batch 105 Loss:0.016740044578909874\n",
      "Epoch 42, Batch 106 Loss:0.05405491590499878\n",
      "Epoch 42, Batch 107 Loss:0.01641836389899254\n",
      "Epoch 42, Batch 108 Loss:0.01829305663704872\n",
      "Epoch 42, Batch 109 Loss:0.04380502551794052\n",
      "Epoch 42, Batch 110 Loss:0.029196567833423615\n",
      "Epoch 42, Batch 111 Loss:0.03853650018572807\n",
      "Epoch 42, Batch 112 Loss:0.019333630800247192\n",
      "Epoch 42, Batch 113 Loss:0.04518217593431473\n",
      "Epoch 42, Batch 114 Loss:0.022547807544469833\n",
      "Epoch 42, Batch 115 Loss:0.021709449589252472\n",
      "Epoch 42, Batch 116 Loss:0.019258441403508186\n",
      "Epoch 42, Batch 117 Loss:0.03204980492591858\n",
      "Epoch 42, Batch 118 Loss:0.018409451469779015\n",
      "Epoch 42, Batch 119 Loss:0.025540698319673538\n",
      "Epoch 42, Batch 120 Loss:0.05497748404741287\n",
      "Epoch 42, Batch 121 Loss:0.013394812121987343\n",
      "Epoch 42, Batch 122 Loss:0.012623623013496399\n",
      "Epoch 42, Batch 123 Loss:0.023750748485326767\n",
      "Epoch 42, Batch 124 Loss:0.013663861900568008\n",
      "Epoch 42, Batch 125 Loss:0.028038226068019867\n",
      "Epoch 42, Batch 126 Loss:0.010683240368962288\n",
      "Epoch 42, Batch 127 Loss:0.00602816604077816\n",
      "Epoch 42, Batch 128 Loss:0.03728365898132324\n",
      "Epoch 42, Batch 129 Loss:0.03022979386150837\n",
      "Epoch 42, Batch 130 Loss:0.013166049495339394\n",
      "Epoch 42, Batch 131 Loss:0.01566024124622345\n",
      "Epoch 42, Batch 132 Loss:0.01188867911696434\n",
      "Epoch 42, Batch 133 Loss:0.015387799590826035\n",
      "Epoch 42, Batch 134 Loss:0.03297363966703415\n",
      "Epoch 42, Batch 135 Loss:0.011808867566287518\n",
      "Epoch 42, Batch 136 Loss:0.02854945696890354\n",
      "Epoch 42, Batch 137 Loss:0.00788327120244503\n",
      "Epoch 42, Batch 138 Loss:0.015507573261857033\n",
      "Epoch 42, Batch 139 Loss:0.009643206372857094\n",
      "Epoch 42, Batch 140 Loss:0.01150858961045742\n",
      "Epoch 42, Batch 141 Loss:0.01321735791862011\n",
      "Epoch 42, Batch 142 Loss:0.017202982679009438\n",
      "Epoch 42, Batch 143 Loss:0.01585615798830986\n",
      "Epoch 42, Batch 144 Loss:0.01847568154335022\n",
      "Epoch 42, Batch 145 Loss:0.019021816551685333\n",
      "Epoch 42, Batch 146 Loss:0.01642836257815361\n",
      "Epoch 42, Batch 147 Loss:0.0141324233263731\n",
      "Epoch 42, Batch 148 Loss:0.01576346717774868\n",
      "Epoch 42, Batch 149 Loss:0.015770306810736656\n",
      "Epoch 42, Batch 150 Loss:0.007261411286890507\n",
      "Epoch 42, Batch 151 Loss:0.01082245260477066\n",
      "Epoch 42, Batch 152 Loss:0.035858072340488434\n",
      "Epoch 42, Batch 153 Loss:0.025042451918125153\n",
      "Epoch 42, Batch 154 Loss:0.01143957395106554\n",
      "Epoch 42, Batch 155 Loss:0.03391217440366745\n",
      "Epoch 42, Batch 156 Loss:0.02480187453329563\n",
      "Epoch 42, Batch 157 Loss:0.014019553549587727\n",
      "Epoch 42, Batch 158 Loss:0.018160128965973854\n",
      "Epoch 42, Batch 159 Loss:0.05682514235377312\n",
      "Epoch 42, Batch 160 Loss:0.01531759649515152\n",
      "Epoch 42, Batch 161 Loss:0.025152457877993584\n",
      "Epoch 42, Batch 162 Loss:0.024145862087607384\n",
      "Epoch 42, Batch 163 Loss:0.02881060168147087\n",
      "Epoch 42, Batch 164 Loss:0.04732557013630867\n",
      "Epoch 42, Batch 165 Loss:0.022881416603922844\n",
      "Epoch 42, Batch 166 Loss:0.034647032618522644\n",
      "Epoch 42, Batch 167 Loss:0.028285551816225052\n",
      "Epoch 42, Batch 168 Loss:0.017495354637503624\n",
      "Epoch 42, Batch 169 Loss:0.03440737724304199\n",
      "Epoch 42, Batch 170 Loss:0.0214169230312109\n",
      "Epoch 42, Batch 171 Loss:0.03378518298268318\n",
      "Epoch 42, Batch 172 Loss:0.03347291052341461\n",
      "Epoch 42, Batch 173 Loss:0.028963560238480568\n",
      "Epoch 42, Batch 174 Loss:0.0061978306621313095\n",
      "Epoch 42, Batch 175 Loss:0.03494034707546234\n",
      "Epoch 42, Batch 176 Loss:0.02525770664215088\n",
      "Epoch 42, Batch 177 Loss:0.048741258680820465\n",
      "Epoch 42, Batch 178 Loss:0.019372103735804558\n",
      "Epoch 42, Batch 179 Loss:0.033382318913936615\n",
      "Epoch 42, Batch 180 Loss:0.013536576181650162\n",
      "Epoch 42, Batch 181 Loss:0.04559272155165672\n",
      "Epoch 42, Batch 182 Loss:0.018851956352591515\n",
      "Epoch 42, Batch 183 Loss:0.038956571370363235\n",
      "Epoch 42, Batch 184 Loss:0.03832024335861206\n",
      "Epoch 42, Batch 185 Loss:0.024592559784650803\n",
      "Epoch 42, Batch 186 Loss:0.03747696802020073\n",
      "Epoch 42, Batch 187 Loss:0.02883681282401085\n",
      "Epoch 42, Batch 188 Loss:0.022936463356018066\n",
      "Epoch 42, Batch 189 Loss:0.07369842380285263\n",
      "Epoch 42, Batch 190 Loss:0.029841823503375053\n",
      "Epoch 42, Batch 191 Loss:0.0167696550488472\n",
      "Epoch 42, Batch 192 Loss:0.02666538581252098\n",
      "Epoch 42, Batch 193 Loss:0.03724420815706253\n",
      "Epoch 42, Batch 194 Loss:0.051686741411685944\n",
      "Epoch 42, Batch 195 Loss:0.032823868095874786\n",
      "Epoch 42, Batch 196 Loss:0.020054541528224945\n",
      "Epoch 42, Batch 197 Loss:0.03685552626848221\n",
      "Epoch 42, Batch 198 Loss:0.03504626825451851\n",
      "Epoch 42, Batch 199 Loss:0.049296893179416656\n",
      "Epoch 42, Batch 200 Loss:0.015457253903150558\n",
      "Epoch 42, Batch 201 Loss:0.039890676736831665\n",
      "Epoch 42, Batch 202 Loss:0.04309207201004028\n",
      "Epoch 42, Batch 203 Loss:0.015139543451368809\n",
      "Epoch 42, Batch 204 Loss:0.02674558013677597\n",
      "Epoch 42, Batch 205 Loss:0.02562197670340538\n",
      "Epoch 42, Batch 206 Loss:0.048116717487573624\n",
      "Epoch 42, Batch 207 Loss:0.015425893478095531\n",
      "Epoch 42, Batch 208 Loss:0.03203333169221878\n",
      "Epoch 42, Batch 209 Loss:0.04734911769628525\n",
      "Epoch 42, Batch 210 Loss:0.027330107986927032\n",
      "Epoch 42, Batch 211 Loss:0.022941166535019875\n",
      "Epoch 42, Batch 212 Loss:0.059876225888729095\n",
      "Epoch 42, Batch 213 Loss:0.02226824127137661\n",
      "Epoch 42, Batch 214 Loss:0.06666897237300873\n",
      "Epoch 42, Batch 215 Loss:0.04114484041929245\n",
      "Epoch 42, Batch 216 Loss:0.054726146161556244\n",
      "Epoch 42, Batch 217 Loss:0.04415642470121384\n",
      "Epoch 42, Batch 218 Loss:0.026691731065511703\n",
      "Epoch 42, Batch 219 Loss:0.036461181938648224\n",
      "Epoch 42, Batch 220 Loss:0.015854332596063614\n",
      "Epoch 42, Batch 221 Loss:0.06180036813020706\n",
      "Epoch 42, Batch 222 Loss:0.04915586858987808\n",
      "Epoch 42, Batch 223 Loss:0.04420486092567444\n",
      "Epoch 42, Batch 224 Loss:0.05188795179128647\n",
      "Epoch 42, Batch 225 Loss:0.04513031244277954\n",
      "Epoch 42, Batch 226 Loss:0.036842040717601776\n",
      "Epoch 42, Batch 227 Loss:0.027867941185832024\n",
      "Epoch 42, Batch 228 Loss:0.03232891485095024\n",
      "Epoch 42, Batch 229 Loss:0.03975199908018112\n",
      "Epoch 42, Batch 230 Loss:0.024352623149752617\n",
      "Epoch 42, Batch 231 Loss:0.04581429436802864\n",
      "Epoch 42, Batch 232 Loss:0.023955296725034714\n",
      "Epoch 42, Batch 233 Loss:0.033822715282440186\n",
      "Loss in this Epoch is: 3.38227152824 %\n",
      "Accuracy in this Epoch is: 88.3800029755 %\n",
      "Epoch 43, Batch 0 Loss:0.0314454548060894\n",
      "Epoch 43, Batch 1 Loss:0.031117359176278114\n",
      "Epoch 43, Batch 2 Loss:0.020068474113941193\n",
      "Epoch 43, Batch 3 Loss:0.05400068312883377\n",
      "Epoch 43, Batch 4 Loss:0.010031037963926792\n",
      "Epoch 43, Batch 5 Loss:0.016037655994296074\n",
      "Epoch 43, Batch 6 Loss:0.028146876022219658\n",
      "Epoch 43, Batch 7 Loss:0.026036636903882027\n",
      "Epoch 43, Batch 8 Loss:0.04272150248289108\n",
      "Epoch 43, Batch 9 Loss:0.027830488979816437\n",
      "Epoch 43, Batch 10 Loss:0.02985822968184948\n",
      "Epoch 43, Batch 11 Loss:0.017561808228492737\n",
      "Epoch 43, Batch 12 Loss:0.04199667647480965\n",
      "Epoch 43, Batch 13 Loss:0.008216887712478638\n",
      "Epoch 43, Batch 14 Loss:0.030290547758340836\n",
      "Epoch 43, Batch 15 Loss:0.02489069476723671\n",
      "Epoch 43, Batch 16 Loss:0.03782026842236519\n",
      "Epoch 43, Batch 17 Loss:0.016167370602488518\n",
      "Epoch 43, Batch 18 Loss:0.0248402189463377\n",
      "Epoch 43, Batch 19 Loss:0.038845133036375046\n",
      "Epoch 43, Batch 20 Loss:0.05780184268951416\n",
      "Epoch 43, Batch 21 Loss:0.012555994093418121\n",
      "Epoch 43, Batch 22 Loss:0.026817385107278824\n",
      "Epoch 43, Batch 23 Loss:0.019317293539643288\n",
      "Epoch 43, Batch 24 Loss:0.03127463907003403\n",
      "Epoch 43, Batch 25 Loss:0.038014184683561325\n",
      "Epoch 43, Batch 26 Loss:0.03442183509469032\n",
      "Epoch 43, Batch 27 Loss:0.02621012181043625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Batch 28 Loss:0.01920109987258911\n",
      "Epoch 43, Batch 29 Loss:0.021273000165820122\n",
      "Epoch 43, Batch 30 Loss:0.03350137546658516\n",
      "Epoch 43, Batch 31 Loss:0.014048919081687927\n",
      "Epoch 43, Batch 32 Loss:0.037500061094760895\n",
      "Epoch 43, Batch 33 Loss:0.00973633024841547\n",
      "Epoch 43, Batch 34 Loss:0.04093720391392708\n",
      "Epoch 43, Batch 35 Loss:0.03456113114953041\n",
      "Epoch 43, Batch 36 Loss:0.031176945194602013\n",
      "Epoch 43, Batch 37 Loss:0.01725793443620205\n",
      "Epoch 43, Batch 38 Loss:0.007735199294984341\n",
      "Epoch 43, Batch 39 Loss:0.02953457087278366\n",
      "Epoch 43, Batch 40 Loss:0.03091328777372837\n",
      "Epoch 43, Batch 41 Loss:0.01037837564945221\n",
      "Epoch 43, Batch 42 Loss:0.005815732292830944\n",
      "Epoch 43, Batch 43 Loss:0.009854456409811974\n",
      "Epoch 43, Batch 44 Loss:0.01732071116566658\n",
      "Epoch 43, Batch 45 Loss:0.01582002267241478\n",
      "Epoch 43, Batch 46 Loss:0.004763898905366659\n",
      "Epoch 43, Batch 47 Loss:0.03808783367276192\n",
      "Epoch 43, Batch 48 Loss:0.047695647925138474\n",
      "Epoch 43, Batch 49 Loss:0.037122052162885666\n",
      "Epoch 43, Batch 50 Loss:0.016760583966970444\n",
      "Epoch 43, Batch 51 Loss:0.022075466811656952\n",
      "Epoch 43, Batch 52 Loss:0.01592806726694107\n",
      "Epoch 43, Batch 53 Loss:0.016801901161670685\n",
      "Epoch 43, Batch 54 Loss:0.024883385747671127\n",
      "Epoch 43, Batch 55 Loss:0.022564604878425598\n",
      "Epoch 43, Batch 56 Loss:0.008638051338493824\n",
      "Epoch 43, Batch 57 Loss:0.019538871943950653\n",
      "Epoch 43, Batch 58 Loss:0.07069584727287292\n",
      "Epoch 43, Batch 59 Loss:0.02869287133216858\n",
      "Epoch 43, Batch 60 Loss:0.06735724210739136\n",
      "Epoch 43, Batch 61 Loss:0.006528178229928017\n",
      "Epoch 43, Batch 62 Loss:0.0539558120071888\n",
      "Epoch 43, Batch 63 Loss:0.03399885818362236\n",
      "Epoch 43, Batch 64 Loss:0.03549475595355034\n",
      "Epoch 43, Batch 65 Loss:0.03539544716477394\n",
      "Epoch 43, Batch 66 Loss:0.027539804577827454\n",
      "Epoch 43, Batch 67 Loss:0.015040525235235691\n",
      "Epoch 43, Batch 68 Loss:0.024866383522748947\n",
      "Epoch 43, Batch 69 Loss:0.03733140975236893\n",
      "Epoch 43, Batch 70 Loss:0.04228435084223747\n",
      "Epoch 43, Batch 71 Loss:0.02331787534058094\n",
      "Epoch 43, Batch 72 Loss:0.04284893348813057\n",
      "Epoch 43, Batch 73 Loss:0.031800322234630585\n",
      "Epoch 43, Batch 74 Loss:0.012457730248570442\n",
      "Epoch 43, Batch 75 Loss:0.03491241857409477\n",
      "Epoch 43, Batch 76 Loss:0.013550599105656147\n",
      "Epoch 43, Batch 77 Loss:0.04594865068793297\n",
      "Epoch 43, Batch 78 Loss:0.03186052292585373\n",
      "Epoch 43, Batch 79 Loss:0.018879057839512825\n",
      "Epoch 43, Batch 80 Loss:0.01797376573085785\n",
      "Epoch 43, Batch 81 Loss:0.04024071618914604\n",
      "Epoch 43, Batch 82 Loss:0.057739127427339554\n",
      "Epoch 43, Batch 83 Loss:0.03536084666848183\n",
      "Epoch 43, Batch 84 Loss:0.03158583119511604\n",
      "Epoch 43, Batch 85 Loss:0.04088735580444336\n",
      "Epoch 43, Batch 86 Loss:0.04233784228563309\n",
      "Epoch 43, Batch 87 Loss:0.019004909321665764\n",
      "Epoch 43, Batch 88 Loss:0.023439904674887657\n",
      "Epoch 43, Batch 89 Loss:0.05365762487053871\n",
      "Epoch 43, Batch 90 Loss:0.03766389936208725\n",
      "Epoch 43, Batch 91 Loss:0.04165160283446312\n",
      "Epoch 43, Batch 92 Loss:0.020331712439656258\n",
      "Epoch 43, Batch 93 Loss:0.05306323617696762\n",
      "Epoch 43, Batch 94 Loss:0.0774502232670784\n",
      "Epoch 43, Batch 95 Loss:0.04347359016537666\n",
      "Epoch 43, Batch 96 Loss:0.025817422196269035\n",
      "Epoch 43, Batch 97 Loss:0.03080561012029648\n",
      "Epoch 43, Batch 98 Loss:0.013655513525009155\n",
      "Epoch 43, Batch 99 Loss:0.05054868385195732\n",
      "Epoch 43, Batch 100 Loss:0.03188486024737358\n",
      "Epoch 43, Batch 101 Loss:0.028191514313220978\n",
      "Epoch 43, Batch 102 Loss:0.015236414968967438\n",
      "Epoch 43, Batch 103 Loss:0.02309834584593773\n",
      "Epoch 43, Batch 104 Loss:0.014707113616168499\n",
      "Epoch 43, Batch 105 Loss:0.034663401544094086\n",
      "Epoch 43, Batch 106 Loss:0.05376680940389633\n",
      "Epoch 43, Batch 107 Loss:0.03581082075834274\n",
      "Epoch 43, Batch 108 Loss:0.05619420111179352\n",
      "Epoch 43, Batch 109 Loss:0.01584872230887413\n",
      "Epoch 43, Batch 110 Loss:0.034978192299604416\n",
      "Epoch 43, Batch 111 Loss:0.022199055179953575\n",
      "Epoch 43, Batch 112 Loss:0.04911571741104126\n",
      "Epoch 43, Batch 113 Loss:0.04849790036678314\n",
      "Epoch 43, Batch 114 Loss:0.02646590769290924\n",
      "Epoch 43, Batch 115 Loss:0.016747914254665375\n",
      "Epoch 43, Batch 116 Loss:0.04456969350576401\n",
      "Epoch 43, Batch 117 Loss:0.008304732851684093\n",
      "Epoch 43, Batch 118 Loss:0.02674100361764431\n",
      "Epoch 43, Batch 119 Loss:0.046732962131500244\n",
      "Epoch 43, Batch 120 Loss:0.0443410724401474\n",
      "Epoch 43, Batch 121 Loss:0.02350073680281639\n",
      "Epoch 43, Batch 122 Loss:0.02104085683822632\n",
      "Epoch 43, Batch 123 Loss:0.04430731385946274\n",
      "Epoch 43, Batch 124 Loss:0.03716176748275757\n",
      "Epoch 43, Batch 125 Loss:0.034120112657547\n",
      "Epoch 43, Batch 126 Loss:0.021627511829137802\n",
      "Epoch 43, Batch 127 Loss:0.016787245869636536\n",
      "Epoch 43, Batch 128 Loss:0.012024937197566032\n",
      "Epoch 43, Batch 129 Loss:0.011090043932199478\n",
      "Epoch 43, Batch 130 Loss:0.02286536432802677\n",
      "Epoch 43, Batch 131 Loss:0.016306739300489426\n",
      "Epoch 43, Batch 132 Loss:0.016414247453212738\n",
      "Epoch 43, Batch 133 Loss:0.05348263680934906\n",
      "Epoch 43, Batch 134 Loss:0.017284169793128967\n",
      "Epoch 43, Batch 135 Loss:0.021095480769872665\n",
      "Epoch 43, Batch 136 Loss:0.04302226006984711\n",
      "Epoch 43, Batch 137 Loss:0.021456247195601463\n",
      "Epoch 43, Batch 138 Loss:0.046805910766124725\n",
      "Epoch 43, Batch 139 Loss:0.02050432749092579\n",
      "Epoch 43, Batch 140 Loss:0.04183804988861084\n",
      "Epoch 43, Batch 141 Loss:0.017370054498314857\n",
      "Epoch 43, Batch 142 Loss:0.026180259883403778\n",
      "Epoch 43, Batch 143 Loss:0.03597157076001167\n",
      "Epoch 43, Batch 144 Loss:0.06760373711585999\n",
      "Epoch 43, Batch 145 Loss:0.02308196760714054\n",
      "Epoch 43, Batch 146 Loss:0.039570197463035583\n",
      "Epoch 43, Batch 147 Loss:0.03680901601910591\n",
      "Epoch 43, Batch 148 Loss:0.058081820607185364\n",
      "Epoch 43, Batch 149 Loss:0.04097267612814903\n",
      "Epoch 43, Batch 150 Loss:0.045895569026470184\n",
      "Epoch 43, Batch 151 Loss:0.019250642508268356\n",
      "Epoch 43, Batch 152 Loss:0.03599771857261658\n",
      "Epoch 43, Batch 153 Loss:0.025146484375\n",
      "Epoch 43, Batch 154 Loss:0.023813102394342422\n",
      "Epoch 43, Batch 155 Loss:0.050463221967220306\n",
      "Epoch 43, Batch 156 Loss:0.0562826469540596\n",
      "Epoch 43, Batch 157 Loss:0.014623277820646763\n",
      "Epoch 43, Batch 158 Loss:0.02504856325685978\n",
      "Epoch 43, Batch 159 Loss:0.02632927894592285\n",
      "Epoch 43, Batch 160 Loss:0.022044016048312187\n",
      "Epoch 43, Batch 161 Loss:0.038087692111730576\n",
      "Epoch 43, Batch 162 Loss:0.07702110707759857\n",
      "Epoch 43, Batch 163 Loss:0.057693783193826675\n",
      "Epoch 43, Batch 164 Loss:0.053402334451675415\n",
      "Epoch 43, Batch 165 Loss:0.024901455268263817\n",
      "Epoch 43, Batch 166 Loss:0.03911948949098587\n",
      "Epoch 43, Batch 167 Loss:0.03951500728726387\n",
      "Epoch 43, Batch 168 Loss:0.012999971397221088\n",
      "Epoch 43, Batch 169 Loss:0.039849743247032166\n",
      "Epoch 43, Batch 170 Loss:0.03155260160565376\n",
      "Epoch 43, Batch 171 Loss:0.06687764823436737\n",
      "Epoch 43, Batch 172 Loss:0.017013302072882652\n",
      "Epoch 43, Batch 173 Loss:0.036754854023456573\n",
      "Epoch 43, Batch 174 Loss:0.03845583647489548\n",
      "Epoch 43, Batch 175 Loss:0.018099140375852585\n",
      "Epoch 43, Batch 176 Loss:0.05373535305261612\n",
      "Epoch 43, Batch 177 Loss:0.07152978330850601\n",
      "Epoch 43, Batch 178 Loss:0.020997706800699234\n",
      "Epoch 43, Batch 179 Loss:0.03488713502883911\n",
      "Epoch 43, Batch 180 Loss:0.052646078169345856\n",
      "Epoch 43, Batch 181 Loss:0.02516254037618637\n",
      "Epoch 43, Batch 182 Loss:0.02927268110215664\n",
      "Epoch 43, Batch 183 Loss:0.033949729055166245\n",
      "Epoch 43, Batch 184 Loss:0.03385826572775841\n",
      "Epoch 43, Batch 185 Loss:0.025025805458426476\n",
      "Epoch 43, Batch 186 Loss:0.04559999704360962\n",
      "Epoch 43, Batch 187 Loss:0.028576165437698364\n",
      "Epoch 43, Batch 188 Loss:0.05727091431617737\n",
      "Epoch 43, Batch 189 Loss:0.029044101014733315\n",
      "Epoch 43, Batch 190 Loss:0.053158827126026154\n",
      "Epoch 43, Batch 191 Loss:0.04417281597852707\n",
      "Epoch 43, Batch 192 Loss:0.03176344931125641\n",
      "Epoch 43, Batch 193 Loss:0.044857725501060486\n",
      "Epoch 43, Batch 194 Loss:0.029434915632009506\n",
      "Epoch 43, Batch 195 Loss:0.039724260568618774\n",
      "Epoch 43, Batch 196 Loss:0.05281390994787216\n",
      "Epoch 43, Batch 197 Loss:0.025264650583267212\n",
      "Epoch 43, Batch 198 Loss:0.03542546182870865\n",
      "Epoch 43, Batch 199 Loss:0.04603090137243271\n",
      "Epoch 43, Batch 200 Loss:0.026696735993027687\n",
      "Epoch 43, Batch 201 Loss:0.03240599110722542\n",
      "Epoch 43, Batch 202 Loss:0.021512720733880997\n",
      "Epoch 43, Batch 203 Loss:0.06143924966454506\n",
      "Epoch 43, Batch 204 Loss:0.03189129754900932\n",
      "Epoch 43, Batch 205 Loss:0.02398410439491272\n",
      "Epoch 43, Batch 206 Loss:0.025552615523338318\n",
      "Epoch 43, Batch 207 Loss:0.03530779480934143\n",
      "Epoch 43, Batch 208 Loss:0.034216634929180145\n",
      "Epoch 43, Batch 209 Loss:0.019561590626835823\n",
      "Epoch 43, Batch 210 Loss:0.025262346491217613\n",
      "Epoch 43, Batch 211 Loss:0.02620202675461769\n",
      "Epoch 43, Batch 212 Loss:0.03232640027999878\n",
      "Epoch 43, Batch 213 Loss:0.053005486726760864\n",
      "Epoch 43, Batch 214 Loss:0.02979464828968048\n",
      "Epoch 43, Batch 215 Loss:0.046225182712078094\n",
      "Epoch 43, Batch 216 Loss:0.046804845333099365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Batch 217 Loss:0.04168105870485306\n",
      "Epoch 43, Batch 218 Loss:0.02314966730773449\n",
      "Epoch 43, Batch 219 Loss:0.03298243135213852\n",
      "Epoch 43, Batch 220 Loss:0.02811562269926071\n",
      "Epoch 43, Batch 221 Loss:0.023438740521669388\n",
      "Epoch 43, Batch 222 Loss:0.04310973733663559\n",
      "Epoch 43, Batch 223 Loss:0.02403857931494713\n",
      "Epoch 43, Batch 224 Loss:0.07865467667579651\n",
      "Epoch 43, Batch 225 Loss:0.03999730572104454\n",
      "Epoch 43, Batch 226 Loss:0.037827711552381516\n",
      "Epoch 43, Batch 227 Loss:0.026936087757349014\n",
      "Epoch 43, Batch 228 Loss:0.03032803349196911\n",
      "Epoch 43, Batch 229 Loss:0.045633748173713684\n",
      "Epoch 43, Batch 230 Loss:0.03403618931770325\n",
      "Epoch 43, Batch 231 Loss:0.05895726755261421\n",
      "Epoch 43, Batch 232 Loss:0.019687024876475334\n",
      "Epoch 43, Batch 233 Loss:0.0342564731836319\n",
      "Loss in this Epoch is: 3.42564731836 %\n",
      "Accuracy in this Epoch is: 88.3800029755 %\n",
      "Epoch 44, Batch 0 Loss:0.034769393503665924\n",
      "Epoch 44, Batch 1 Loss:0.03125666081905365\n",
      "Epoch 44, Batch 2 Loss:0.024821162223815918\n",
      "Epoch 44, Batch 3 Loss:0.009552106261253357\n",
      "Epoch 44, Batch 4 Loss:0.010534633882343769\n",
      "Epoch 44, Batch 5 Loss:0.01334867812693119\n",
      "Epoch 44, Batch 6 Loss:0.050924476236104965\n",
      "Epoch 44, Batch 7 Loss:0.01321330200880766\n",
      "Epoch 44, Batch 8 Loss:0.012913496233522892\n",
      "Epoch 44, Batch 9 Loss:0.01728920266032219\n",
      "Epoch 44, Batch 10 Loss:0.021512610837817192\n",
      "Epoch 44, Batch 11 Loss:0.018747463822364807\n",
      "Epoch 44, Batch 12 Loss:0.018677210435271263\n",
      "Epoch 44, Batch 13 Loss:0.0263562873005867\n",
      "Epoch 44, Batch 14 Loss:0.01072381529957056\n",
      "Epoch 44, Batch 15 Loss:0.04022972658276558\n",
      "Epoch 44, Batch 16 Loss:0.029979990795254707\n",
      "Epoch 44, Batch 17 Loss:0.023201482370495796\n",
      "Epoch 44, Batch 18 Loss:0.0321027897298336\n",
      "Epoch 44, Batch 19 Loss:0.008202409371733665\n",
      "Epoch 44, Batch 20 Loss:0.02969668246805668\n",
      "Epoch 44, Batch 21 Loss:0.020570507273077965\n",
      "Epoch 44, Batch 22 Loss:0.007165940012782812\n",
      "Epoch 44, Batch 23 Loss:0.015929605811834335\n",
      "Epoch 44, Batch 24 Loss:0.04649122431874275\n",
      "Epoch 44, Batch 25 Loss:0.02330513298511505\n",
      "Epoch 44, Batch 26 Loss:0.027646128088235855\n",
      "Epoch 44, Batch 27 Loss:0.007404240779578686\n",
      "Epoch 44, Batch 28 Loss:0.01810963824391365\n",
      "Epoch 44, Batch 29 Loss:0.03144575655460358\n",
      "Epoch 44, Batch 30 Loss:0.023353811353445053\n",
      "Epoch 44, Batch 31 Loss:0.026554809883236885\n",
      "Epoch 44, Batch 32 Loss:0.011412596330046654\n",
      "Epoch 44, Batch 33 Loss:0.006218662019819021\n",
      "Epoch 44, Batch 34 Loss:0.007081393618136644\n",
      "Epoch 44, Batch 35 Loss:0.02280808612704277\n",
      "Epoch 44, Batch 36 Loss:0.015729861333966255\n",
      "Epoch 44, Batch 37 Loss:0.012209207750856876\n",
      "Epoch 44, Batch 38 Loss:0.04348849505186081\n",
      "Epoch 44, Batch 39 Loss:0.015905218198895454\n",
      "Epoch 44, Batch 40 Loss:0.024200282990932465\n",
      "Epoch 44, Batch 41 Loss:0.028442801907658577\n",
      "Epoch 44, Batch 42 Loss:0.015633750706911087\n",
      "Epoch 44, Batch 43 Loss:0.01552698016166687\n",
      "Epoch 44, Batch 44 Loss:0.015666501596570015\n",
      "Epoch 44, Batch 45 Loss:0.009800154715776443\n",
      "Epoch 44, Batch 46 Loss:0.010827949270606041\n",
      "Epoch 44, Batch 47 Loss:0.04195051267743111\n",
      "Epoch 44, Batch 48 Loss:0.014383688569068909\n",
      "Epoch 44, Batch 49 Loss:0.04181577265262604\n",
      "Epoch 44, Batch 50 Loss:0.026932228356599808\n",
      "Epoch 44, Batch 51 Loss:0.010818389244377613\n",
      "Epoch 44, Batch 52 Loss:0.007770892698317766\n",
      "Epoch 44, Batch 53 Loss:0.011747379787266254\n",
      "Epoch 44, Batch 54 Loss:0.013886474072933197\n",
      "Epoch 44, Batch 55 Loss:0.010612793266773224\n",
      "Epoch 44, Batch 56 Loss:0.027497144415974617\n",
      "Epoch 44, Batch 57 Loss:0.013588509522378445\n",
      "Epoch 44, Batch 58 Loss:0.015608479268848896\n",
      "Epoch 44, Batch 59 Loss:0.04982833191752434\n",
      "Epoch 44, Batch 60 Loss:0.02146768756210804\n",
      "Epoch 44, Batch 61 Loss:0.023447463288903236\n",
      "Epoch 44, Batch 62 Loss:0.020732920616865158\n",
      "Epoch 44, Batch 63 Loss:0.023361289873719215\n",
      "Epoch 44, Batch 64 Loss:0.007803380489349365\n",
      "Epoch 44, Batch 65 Loss:0.01401947345584631\n",
      "Epoch 44, Batch 66 Loss:0.014967536553740501\n",
      "Epoch 44, Batch 67 Loss:0.01811066083610058\n",
      "Epoch 44, Batch 68 Loss:0.015347922220826149\n",
      "Epoch 44, Batch 69 Loss:0.04587705060839653\n",
      "Epoch 44, Batch 70 Loss:0.005282863043248653\n",
      "Epoch 44, Batch 71 Loss:0.017868991941213608\n",
      "Epoch 44, Batch 72 Loss:0.0198403000831604\n",
      "Epoch 44, Batch 73 Loss:0.010758329182863235\n",
      "Epoch 44, Batch 74 Loss:0.012742631137371063\n",
      "Epoch 44, Batch 75 Loss:0.03282412514090538\n",
      "Epoch 44, Batch 76 Loss:0.04424474388360977\n",
      "Epoch 44, Batch 77 Loss:0.01183843519538641\n",
      "Epoch 44, Batch 78 Loss:0.020632294937968254\n",
      "Epoch 44, Batch 79 Loss:0.022875551134347916\n",
      "Epoch 44, Batch 80 Loss:0.023533497005701065\n",
      "Epoch 44, Batch 81 Loss:0.012645469047129154\n",
      "Epoch 44, Batch 82 Loss:0.01805977150797844\n",
      "Epoch 44, Batch 83 Loss:0.007765904534608126\n",
      "Epoch 44, Batch 84 Loss:0.01210614014416933\n",
      "Epoch 44, Batch 85 Loss:0.006793254986405373\n",
      "Epoch 44, Batch 86 Loss:0.0434156097471714\n",
      "Epoch 44, Batch 87 Loss:0.017641527578234673\n",
      "Epoch 44, Batch 88 Loss:0.03074711002409458\n",
      "Epoch 44, Batch 89 Loss:0.03932014852762222\n",
      "Epoch 44, Batch 90 Loss:0.014630455523729324\n",
      "Epoch 44, Batch 91 Loss:0.015473103150725365\n",
      "Epoch 44, Batch 92 Loss:0.02259521186351776\n",
      "Epoch 44, Batch 93 Loss:0.02516559697687626\n",
      "Epoch 44, Batch 94 Loss:0.023147346451878548\n",
      "Epoch 44, Batch 95 Loss:0.016231456771492958\n",
      "Epoch 44, Batch 96 Loss:0.010576201602816582\n",
      "Epoch 44, Batch 97 Loss:0.00497790053486824\n",
      "Epoch 44, Batch 98 Loss:0.01344382669776678\n",
      "Epoch 44, Batch 99 Loss:0.013134012930095196\n",
      "Epoch 44, Batch 100 Loss:0.008053623139858246\n",
      "Epoch 44, Batch 101 Loss:0.024565506726503372\n",
      "Epoch 44, Batch 102 Loss:0.04825656861066818\n",
      "Epoch 44, Batch 103 Loss:0.017497092485427856\n",
      "Epoch 44, Batch 104 Loss:0.04584120213985443\n",
      "Epoch 44, Batch 105 Loss:0.011819683015346527\n",
      "Epoch 44, Batch 106 Loss:0.017219725996255875\n",
      "Epoch 44, Batch 107 Loss:0.025041736662387848\n",
      "Epoch 44, Batch 108 Loss:0.010535030625760555\n",
      "Epoch 44, Batch 109 Loss:0.02623184584081173\n",
      "Epoch 44, Batch 110 Loss:0.022849418222904205\n",
      "Epoch 44, Batch 111 Loss:0.00873003900051117\n",
      "Epoch 44, Batch 112 Loss:0.01857239566743374\n",
      "Epoch 44, Batch 113 Loss:0.015796534717082977\n",
      "Epoch 44, Batch 114 Loss:0.011348807252943516\n",
      "Epoch 44, Batch 115 Loss:0.0054426034912467\n",
      "Epoch 44, Batch 116 Loss:0.022134829312562943\n",
      "Epoch 44, Batch 117 Loss:0.025916285812854767\n",
      "Epoch 44, Batch 118 Loss:0.018636539578437805\n",
      "Epoch 44, Batch 119 Loss:0.022843802347779274\n",
      "Epoch 44, Batch 120 Loss:0.00761153269559145\n",
      "Epoch 44, Batch 121 Loss:0.01794324815273285\n",
      "Epoch 44, Batch 122 Loss:0.02405025251209736\n",
      "Epoch 44, Batch 123 Loss:0.048852674663066864\n",
      "Epoch 44, Batch 124 Loss:0.013574022799730301\n",
      "Epoch 44, Batch 125 Loss:0.050402551889419556\n",
      "Epoch 44, Batch 126 Loss:0.008815685287117958\n",
      "Epoch 44, Batch 127 Loss:0.050347328186035156\n",
      "Epoch 44, Batch 128 Loss:0.054660018533468246\n",
      "Epoch 44, Batch 129 Loss:0.059487082064151764\n",
      "Epoch 44, Batch 130 Loss:0.06793837249279022\n",
      "Epoch 44, Batch 131 Loss:0.049944423139095306\n",
      "Epoch 44, Batch 132 Loss:0.014843501150608063\n",
      "Epoch 44, Batch 133 Loss:0.03540251404047012\n",
      "Epoch 44, Batch 134 Loss:0.040275126695632935\n",
      "Epoch 44, Batch 135 Loss:0.05727151036262512\n",
      "Epoch 44, Batch 136 Loss:0.017286516726017\n",
      "Epoch 44, Batch 137 Loss:0.04033040627837181\n",
      "Epoch 44, Batch 138 Loss:0.02272326685488224\n",
      "Epoch 44, Batch 139 Loss:0.04586651548743248\n",
      "Epoch 44, Batch 140 Loss:0.02442816086113453\n",
      "Epoch 44, Batch 141 Loss:0.055594414472579956\n",
      "Epoch 44, Batch 142 Loss:0.04977764934301376\n",
      "Epoch 44, Batch 143 Loss:0.04950957000255585\n",
      "Epoch 44, Batch 144 Loss:0.043097011744976044\n",
      "Epoch 44, Batch 145 Loss:0.025248777121305466\n",
      "Epoch 44, Batch 146 Loss:0.01535093691200018\n",
      "Epoch 44, Batch 147 Loss:0.029560988768935204\n",
      "Epoch 44, Batch 148 Loss:0.024901948869228363\n",
      "Epoch 44, Batch 149 Loss:0.018942750990390778\n",
      "Epoch 44, Batch 150 Loss:0.0438137948513031\n",
      "Epoch 44, Batch 151 Loss:0.040880993008613586\n",
      "Epoch 44, Batch 152 Loss:0.017311327159404755\n",
      "Epoch 44, Batch 153 Loss:0.018748503178358078\n",
      "Epoch 44, Batch 154 Loss:0.018889235332608223\n",
      "Epoch 44, Batch 155 Loss:0.04640600457787514\n",
      "Epoch 44, Batch 156 Loss:0.04114086553454399\n",
      "Epoch 44, Batch 157 Loss:0.02500518225133419\n",
      "Epoch 44, Batch 158 Loss:0.031241947785019875\n",
      "Epoch 44, Batch 159 Loss:0.06463055312633514\n",
      "Epoch 44, Batch 160 Loss:0.06459274888038635\n",
      "Epoch 44, Batch 161 Loss:0.021002070978283882\n",
      "Epoch 44, Batch 162 Loss:0.026002895087003708\n",
      "Epoch 44, Batch 163 Loss:0.041963573545217514\n",
      "Epoch 44, Batch 164 Loss:0.025083176791667938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Batch 165 Loss:0.015976611524820328\n",
      "Epoch 44, Batch 166 Loss:0.032180219888687134\n",
      "Epoch 44, Batch 167 Loss:0.03185151517391205\n",
      "Epoch 44, Batch 168 Loss:0.049880340695381165\n",
      "Epoch 44, Batch 169 Loss:0.03405632823705673\n",
      "Epoch 44, Batch 170 Loss:0.035005342215299606\n",
      "Epoch 44, Batch 171 Loss:0.01644929125905037\n",
      "Epoch 44, Batch 172 Loss:0.015644382685422897\n",
      "Epoch 44, Batch 173 Loss:0.041892290115356445\n",
      "Epoch 44, Batch 174 Loss:0.033367305994033813\n",
      "Epoch 44, Batch 175 Loss:0.06948180496692657\n",
      "Epoch 44, Batch 176 Loss:0.009079517796635628\n",
      "Epoch 44, Batch 177 Loss:0.026000062003731728\n",
      "Epoch 44, Batch 178 Loss:0.026797449216246605\n",
      "Epoch 44, Batch 179 Loss:0.0387088879942894\n",
      "Epoch 44, Batch 180 Loss:0.03348914906382561\n",
      "Epoch 44, Batch 181 Loss:0.01729603484272957\n",
      "Epoch 44, Batch 182 Loss:0.011689599603414536\n",
      "Epoch 44, Batch 183 Loss:0.008862858638167381\n",
      "Epoch 44, Batch 184 Loss:0.026553871110081673\n",
      "Epoch 44, Batch 185 Loss:0.021535638719797134\n",
      "Epoch 44, Batch 186 Loss:0.013496886938810349\n",
      "Epoch 44, Batch 187 Loss:0.04939392954111099\n",
      "Epoch 44, Batch 188 Loss:0.026160381734371185\n",
      "Epoch 44, Batch 189 Loss:0.023065339773893356\n",
      "Epoch 44, Batch 190 Loss:0.03874841704964638\n",
      "Epoch 44, Batch 191 Loss:0.05276888608932495\n",
      "Epoch 44, Batch 192 Loss:0.01736578345298767\n",
      "Epoch 44, Batch 193 Loss:0.05072500929236412\n",
      "Epoch 44, Batch 194 Loss:0.030987601727247238\n",
      "Epoch 44, Batch 195 Loss:0.01656142622232437\n",
      "Epoch 44, Batch 196 Loss:0.01549336314201355\n",
      "Epoch 44, Batch 197 Loss:0.02975962497293949\n",
      "Epoch 44, Batch 198 Loss:0.029742524027824402\n",
      "Epoch 44, Batch 199 Loss:0.021381011232733727\n",
      "Epoch 44, Batch 200 Loss:0.022488784044981003\n",
      "Epoch 44, Batch 201 Loss:0.016489911824464798\n",
      "Epoch 44, Batch 202 Loss:0.016954544931650162\n",
      "Epoch 44, Batch 203 Loss:0.0364486500620842\n",
      "Epoch 44, Batch 204 Loss:0.024094823747873306\n",
      "Epoch 44, Batch 205 Loss:0.04326719790697098\n",
      "Epoch 44, Batch 206 Loss:0.05072441324591637\n",
      "Epoch 44, Batch 207 Loss:0.042068980634212494\n",
      "Epoch 44, Batch 208 Loss:0.02082195319235325\n",
      "Epoch 44, Batch 209 Loss:0.028801873326301575\n",
      "Epoch 44, Batch 210 Loss:0.02033540979027748\n",
      "Epoch 44, Batch 211 Loss:0.05279978737235069\n",
      "Epoch 44, Batch 212 Loss:0.024294335395097733\n",
      "Epoch 44, Batch 213 Loss:0.007894882000982761\n",
      "Epoch 44, Batch 214 Loss:0.03214610368013382\n",
      "Epoch 44, Batch 215 Loss:0.021540459245443344\n",
      "Epoch 44, Batch 216 Loss:0.01922536827623844\n",
      "Epoch 44, Batch 217 Loss:0.012300830334424973\n",
      "Epoch 44, Batch 218 Loss:0.05101305991411209\n",
      "Epoch 44, Batch 219 Loss:0.021724024787545204\n",
      "Epoch 44, Batch 220 Loss:0.026848748326301575\n",
      "Epoch 44, Batch 221 Loss:0.014614829793572426\n",
      "Epoch 44, Batch 222 Loss:0.020967228338122368\n",
      "Epoch 44, Batch 223 Loss:0.021579120308160782\n",
      "Epoch 44, Batch 224 Loss:0.041296303272247314\n",
      "Epoch 44, Batch 225 Loss:0.02347543276846409\n",
      "Epoch 44, Batch 226 Loss:0.023514891043305397\n",
      "Epoch 44, Batch 227 Loss:0.021842241287231445\n",
      "Epoch 44, Batch 228 Loss:0.044658537954092026\n",
      "Epoch 44, Batch 229 Loss:0.02715889737010002\n",
      "Epoch 44, Batch 230 Loss:0.03792539983987808\n",
      "Epoch 44, Batch 231 Loss:0.06569681316614151\n",
      "Epoch 44, Batch 232 Loss:0.023450706154108047\n",
      "Epoch 44, Batch 233 Loss:0.093684121966362\n",
      "Loss in this Epoch is: 9.36841219664 %\n",
      "Accuracy in this Epoch is: 88.4599983692 %\n",
      "Epoch 45, Batch 0 Loss:0.016479330137372017\n",
      "Epoch 45, Batch 1 Loss:0.024366531521081924\n",
      "Epoch 45, Batch 2 Loss:0.03539889305830002\n",
      "Epoch 45, Batch 3 Loss:0.03451956808567047\n",
      "Epoch 45, Batch 4 Loss:0.06902583688497543\n",
      "Epoch 45, Batch 5 Loss:0.025873076170682907\n",
      "Epoch 45, Batch 6 Loss:0.03684615716338158\n",
      "Epoch 45, Batch 7 Loss:0.016708439216017723\n",
      "Epoch 45, Batch 8 Loss:0.04685664176940918\n",
      "Epoch 45, Batch 9 Loss:0.008625314570963383\n",
      "Epoch 45, Batch 10 Loss:0.030685042962431908\n",
      "Epoch 45, Batch 11 Loss:0.024550460278987885\n",
      "Epoch 45, Batch 12 Loss:0.011826365254819393\n",
      "Epoch 45, Batch 13 Loss:0.01609325036406517\n",
      "Epoch 45, Batch 14 Loss:0.015393958427011967\n",
      "Epoch 45, Batch 15 Loss:0.024034986272454262\n",
      "Epoch 45, Batch 16 Loss:0.022429410368204117\n",
      "Epoch 45, Batch 17 Loss:0.01813516765832901\n",
      "Epoch 45, Batch 18 Loss:0.02626882679760456\n",
      "Epoch 45, Batch 19 Loss:0.027152014896273613\n",
      "Epoch 45, Batch 20 Loss:0.011337175965309143\n",
      "Epoch 45, Batch 21 Loss:0.027879150584340096\n",
      "Epoch 45, Batch 22 Loss:0.010171775706112385\n",
      "Epoch 45, Batch 23 Loss:0.02376287616789341\n",
      "Epoch 45, Batch 24 Loss:0.04635995626449585\n",
      "Epoch 45, Batch 25 Loss:0.02948874793946743\n",
      "Epoch 45, Batch 26 Loss:0.009716988541185856\n",
      "Epoch 45, Batch 27 Loss:0.01555902510881424\n",
      "Epoch 45, Batch 28 Loss:0.021777896210551262\n",
      "Epoch 45, Batch 29 Loss:0.019188296049833298\n",
      "Epoch 45, Batch 30 Loss:0.018860334530472755\n",
      "Epoch 45, Batch 31 Loss:0.02228504791855812\n",
      "Epoch 45, Batch 32 Loss:0.025839487090706825\n",
      "Epoch 45, Batch 33 Loss:0.018008209764957428\n",
      "Epoch 45, Batch 34 Loss:0.027633478865027428\n",
      "Epoch 45, Batch 35 Loss:0.00854276493191719\n",
      "Epoch 45, Batch 36 Loss:0.04296313598752022\n",
      "Epoch 45, Batch 37 Loss:0.02469209022819996\n",
      "Epoch 45, Batch 38 Loss:0.022487420588731766\n",
      "Epoch 45, Batch 39 Loss:0.011097395792603493\n",
      "Epoch 45, Batch 40 Loss:0.04210055619478226\n",
      "Epoch 45, Batch 41 Loss:0.020089875906705856\n",
      "Epoch 45, Batch 42 Loss:0.01680665649473667\n",
      "Epoch 45, Batch 43 Loss:0.02105623669922352\n",
      "Epoch 45, Batch 44 Loss:0.006866447161883116\n",
      "Epoch 45, Batch 45 Loss:0.015541015192866325\n",
      "Epoch 45, Batch 46 Loss:0.02440056949853897\n",
      "Epoch 45, Batch 47 Loss:0.01975133642554283\n",
      "Epoch 45, Batch 48 Loss:0.010665477253496647\n",
      "Epoch 45, Batch 49 Loss:0.02918456494808197\n",
      "Epoch 45, Batch 50 Loss:0.02220122329890728\n",
      "Epoch 45, Batch 51 Loss:0.00863860547542572\n",
      "Epoch 45, Batch 52 Loss:0.011958828195929527\n",
      "Epoch 45, Batch 53 Loss:0.014721709303557873\n",
      "Epoch 45, Batch 54 Loss:0.06090381741523743\n",
      "Epoch 45, Batch 55 Loss:0.014572938904166222\n",
      "Epoch 45, Batch 56 Loss:0.020176047459244728\n",
      "Epoch 45, Batch 57 Loss:0.017367541790008545\n",
      "Epoch 45, Batch 58 Loss:0.01581811159849167\n",
      "Epoch 45, Batch 59 Loss:0.004956706892699003\n",
      "Epoch 45, Batch 60 Loss:0.010609540157020092\n",
      "Epoch 45, Batch 61 Loss:0.01614399626851082\n",
      "Epoch 45, Batch 62 Loss:0.009646231308579445\n",
      "Epoch 45, Batch 63 Loss:0.014388146810233593\n",
      "Epoch 45, Batch 64 Loss:0.0166480615735054\n",
      "Epoch 45, Batch 65 Loss:0.017196249216794968\n",
      "Epoch 45, Batch 66 Loss:0.02307913824915886\n",
      "Epoch 45, Batch 67 Loss:0.010134541429579258\n",
      "Epoch 45, Batch 68 Loss:0.041884563863277435\n",
      "Epoch 45, Batch 69 Loss:0.026935633271932602\n",
      "Epoch 45, Batch 70 Loss:0.022344013676047325\n",
      "Epoch 45, Batch 71 Loss:0.029774807393550873\n",
      "Epoch 45, Batch 72 Loss:0.014495592564344406\n",
      "Epoch 45, Batch 73 Loss:0.03985336795449257\n",
      "Epoch 45, Batch 74 Loss:0.025025401264429092\n",
      "Epoch 45, Batch 75 Loss:0.03383380174636841\n",
      "Epoch 45, Batch 76 Loss:0.029977483674883842\n",
      "Epoch 45, Batch 77 Loss:0.022840986028313637\n",
      "Epoch 45, Batch 78 Loss:0.02769959159195423\n",
      "Epoch 45, Batch 79 Loss:0.019414011389017105\n",
      "Epoch 45, Batch 80 Loss:0.024596279487013817\n",
      "Epoch 45, Batch 81 Loss:0.01572890393435955\n",
      "Epoch 45, Batch 82 Loss:0.05608160421252251\n",
      "Epoch 45, Batch 83 Loss:0.01661268249154091\n",
      "Epoch 45, Batch 84 Loss:0.03204258158802986\n",
      "Epoch 45, Batch 85 Loss:0.016138695180416107\n",
      "Epoch 45, Batch 86 Loss:0.012189612723886967\n",
      "Epoch 45, Batch 87 Loss:0.02253362163901329\n",
      "Epoch 45, Batch 88 Loss:0.012674638070166111\n",
      "Epoch 45, Batch 89 Loss:0.013578562997281551\n",
      "Epoch 45, Batch 90 Loss:0.011369956657290459\n",
      "Epoch 45, Batch 91 Loss:0.011982548050582409\n",
      "Epoch 45, Batch 92 Loss:0.026118379086256027\n",
      "Epoch 45, Batch 93 Loss:0.033070385456085205\n",
      "Epoch 45, Batch 94 Loss:0.018539583310484886\n",
      "Epoch 45, Batch 95 Loss:0.01993398740887642\n",
      "Epoch 45, Batch 96 Loss:0.010959314182400703\n",
      "Epoch 45, Batch 97 Loss:0.029921632260084152\n",
      "Epoch 45, Batch 98 Loss:0.011289060115814209\n",
      "Epoch 45, Batch 99 Loss:0.01086474396288395\n",
      "Epoch 45, Batch 100 Loss:0.004736236296594143\n",
      "Epoch 45, Batch 101 Loss:0.05742114409804344\n",
      "Epoch 45, Batch 102 Loss:0.00940584484487772\n",
      "Epoch 45, Batch 103 Loss:0.01244896836578846\n",
      "Epoch 45, Batch 104 Loss:0.04339791461825371\n",
      "Epoch 45, Batch 105 Loss:0.011899517849087715\n",
      "Epoch 45, Batch 106 Loss:0.012568537145853043\n",
      "Epoch 45, Batch 107 Loss:0.015342607162892818\n",
      "Epoch 45, Batch 108 Loss:0.010660112835466862\n",
      "Epoch 45, Batch 109 Loss:0.036893606185913086\n",
      "Epoch 45, Batch 110 Loss:0.025679748505353928\n",
      "Epoch 45, Batch 111 Loss:0.0275050550699234\n",
      "Epoch 45, Batch 112 Loss:0.011448466219007969\n",
      "Epoch 45, Batch 113 Loss:0.054467327892780304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Batch 114 Loss:0.01775839552283287\n",
      "Epoch 45, Batch 115 Loss:0.021023795008659363\n",
      "Epoch 45, Batch 116 Loss:0.06245178356766701\n",
      "Epoch 45, Batch 117 Loss:0.012047181837260723\n",
      "Epoch 45, Batch 118 Loss:0.02499958872795105\n",
      "Epoch 45, Batch 119 Loss:0.03898545727133751\n",
      "Epoch 45, Batch 120 Loss:0.02258465439081192\n",
      "Epoch 45, Batch 121 Loss:0.016119644045829773\n",
      "Epoch 45, Batch 122 Loss:0.03320702910423279\n",
      "Epoch 45, Batch 123 Loss:0.025330841541290283\n",
      "Epoch 45, Batch 124 Loss:0.021414712071418762\n",
      "Epoch 45, Batch 125 Loss:0.04856225475668907\n",
      "Epoch 45, Batch 126 Loss:0.027340445667505264\n",
      "Epoch 45, Batch 127 Loss:0.009827710688114166\n",
      "Epoch 45, Batch 128 Loss:0.00851333886384964\n",
      "Epoch 45, Batch 129 Loss:0.026026420295238495\n",
      "Epoch 45, Batch 130 Loss:0.019079146906733513\n",
      "Epoch 45, Batch 131 Loss:0.03153747692704201\n",
      "Epoch 45, Batch 132 Loss:0.041615646332502365\n",
      "Epoch 45, Batch 133 Loss:0.03443386033177376\n",
      "Epoch 45, Batch 134 Loss:0.026334360241889954\n",
      "Epoch 45, Batch 135 Loss:0.029016975313425064\n",
      "Epoch 45, Batch 136 Loss:0.08777351677417755\n",
      "Epoch 45, Batch 137 Loss:0.03478709235787392\n",
      "Epoch 45, Batch 138 Loss:0.02574702352285385\n",
      "Epoch 45, Batch 139 Loss:0.015136867761611938\n",
      "Epoch 45, Batch 140 Loss:0.010154440999031067\n",
      "Epoch 45, Batch 141 Loss:0.00400638859719038\n",
      "Epoch 45, Batch 142 Loss:0.023705169558525085\n",
      "Epoch 45, Batch 143 Loss:0.033542126417160034\n",
      "Epoch 45, Batch 144 Loss:0.022616349160671234\n",
      "Epoch 45, Batch 145 Loss:0.012729681096971035\n",
      "Epoch 45, Batch 146 Loss:0.020593706518411636\n",
      "Epoch 45, Batch 147 Loss:0.013949843123555183\n",
      "Epoch 45, Batch 148 Loss:0.01043644268065691\n",
      "Epoch 45, Batch 149 Loss:0.03328750282526016\n",
      "Epoch 45, Batch 150 Loss:0.015168079175055027\n",
      "Epoch 45, Batch 151 Loss:0.030513720586895943\n",
      "Epoch 45, Batch 152 Loss:0.02328154444694519\n",
      "Epoch 45, Batch 153 Loss:0.03829449415206909\n",
      "Epoch 45, Batch 154 Loss:0.026393994688987732\n",
      "Epoch 45, Batch 155 Loss:0.026861369609832764\n",
      "Epoch 45, Batch 156 Loss:0.03857066482305527\n",
      "Epoch 45, Batch 157 Loss:0.03043690323829651\n",
      "Epoch 45, Batch 158 Loss:0.02301931194961071\n",
      "Epoch 45, Batch 159 Loss:0.015352054499089718\n",
      "Epoch 45, Batch 160 Loss:0.029674164950847626\n",
      "Epoch 45, Batch 161 Loss:0.020235951989889145\n",
      "Epoch 45, Batch 162 Loss:0.04237643629312515\n",
      "Epoch 45, Batch 163 Loss:0.028476282954216003\n",
      "Epoch 45, Batch 164 Loss:0.02314305305480957\n",
      "Epoch 45, Batch 165 Loss:0.02523834817111492\n",
      "Epoch 45, Batch 166 Loss:0.025894105434417725\n",
      "Epoch 45, Batch 167 Loss:0.012485215440392494\n",
      "Epoch 45, Batch 168 Loss:0.04438789188861847\n",
      "Epoch 45, Batch 169 Loss:0.01676483452320099\n",
      "Epoch 45, Batch 170 Loss:0.03683618828654289\n",
      "Epoch 45, Batch 171 Loss:0.027622025460004807\n",
      "Epoch 45, Batch 172 Loss:0.041352808475494385\n",
      "Epoch 45, Batch 173 Loss:0.02104146033525467\n",
      "Epoch 45, Batch 174 Loss:0.020219480618834496\n",
      "Epoch 45, Batch 175 Loss:0.039868105202913284\n",
      "Epoch 45, Batch 176 Loss:0.02640105038881302\n",
      "Epoch 45, Batch 177 Loss:0.014042013324797153\n",
      "Epoch 45, Batch 178 Loss:0.006381826940923929\n",
      "Epoch 45, Batch 179 Loss:0.02012736350297928\n",
      "Epoch 45, Batch 180 Loss:0.04382573068141937\n",
      "Epoch 45, Batch 181 Loss:0.016123462468385696\n",
      "Epoch 45, Batch 182 Loss:0.027595922350883484\n",
      "Epoch 45, Batch 183 Loss:0.029990673065185547\n",
      "Epoch 45, Batch 184 Loss:0.020258832722902298\n",
      "Epoch 45, Batch 185 Loss:0.02374453656375408\n",
      "Epoch 45, Batch 186 Loss:0.016980357468128204\n",
      "Epoch 45, Batch 187 Loss:0.03532740846276283\n",
      "Epoch 45, Batch 188 Loss:0.03901943564414978\n",
      "Epoch 45, Batch 189 Loss:0.04173196852207184\n",
      "Epoch 45, Batch 190 Loss:0.0324980691075325\n",
      "Epoch 45, Batch 191 Loss:0.07817722856998444\n",
      "Epoch 45, Batch 192 Loss:0.025165963917970657\n",
      "Epoch 45, Batch 193 Loss:0.028695419430732727\n",
      "Epoch 45, Batch 194 Loss:0.05787556618452072\n",
      "Epoch 45, Batch 195 Loss:0.068064384162426\n",
      "Epoch 45, Batch 196 Loss:0.04361999034881592\n",
      "Epoch 45, Batch 197 Loss:0.02754540741443634\n",
      "Epoch 45, Batch 198 Loss:0.02965296432375908\n",
      "Epoch 45, Batch 199 Loss:0.10893578827381134\n",
      "Epoch 45, Batch 200 Loss:0.050691451877355576\n",
      "Epoch 45, Batch 201 Loss:0.02699526958167553\n",
      "Epoch 45, Batch 202 Loss:0.04436350241303444\n",
      "Epoch 45, Batch 203 Loss:0.020521042868494987\n",
      "Epoch 45, Batch 204 Loss:0.01923752948641777\n",
      "Epoch 45, Batch 205 Loss:0.08162926882505417\n",
      "Epoch 45, Batch 206 Loss:0.026789367198944092\n",
      "Epoch 45, Batch 207 Loss:0.03398509696125984\n",
      "Epoch 45, Batch 208 Loss:0.06730593740940094\n",
      "Epoch 45, Batch 209 Loss:0.05412142351269722\n",
      "Epoch 45, Batch 210 Loss:0.0473540797829628\n",
      "Epoch 45, Batch 211 Loss:0.06526410579681396\n",
      "Epoch 45, Batch 212 Loss:0.03625018894672394\n",
      "Epoch 45, Batch 213 Loss:0.04619167745113373\n",
      "Epoch 45, Batch 214 Loss:0.028251154348254204\n",
      "Epoch 45, Batch 215 Loss:0.037075962871313095\n",
      "Epoch 45, Batch 216 Loss:0.051437199115753174\n",
      "Epoch 45, Batch 217 Loss:0.08989937603473663\n",
      "Epoch 45, Batch 218 Loss:0.05897236987948418\n",
      "Epoch 45, Batch 219 Loss:0.04548049718141556\n",
      "Epoch 45, Batch 220 Loss:0.04892828315496445\n",
      "Epoch 45, Batch 221 Loss:0.03608859330415726\n",
      "Epoch 45, Batch 222 Loss:0.03991572558879852\n",
      "Epoch 45, Batch 223 Loss:0.041797928512096405\n",
      "Epoch 45, Batch 224 Loss:0.04429032653570175\n",
      "Epoch 45, Batch 225 Loss:0.04617312550544739\n",
      "Epoch 45, Batch 226 Loss:0.02216542884707451\n",
      "Epoch 45, Batch 227 Loss:0.021207207813858986\n",
      "Epoch 45, Batch 228 Loss:0.020286913961172104\n",
      "Epoch 45, Batch 229 Loss:0.05587587505578995\n",
      "Epoch 45, Batch 230 Loss:0.035846687853336334\n",
      "Epoch 45, Batch 231 Loss:0.011723446659743786\n",
      "Epoch 45, Batch 232 Loss:0.044372133910655975\n",
      "Epoch 45, Batch 233 Loss:0.033444035798311234\n",
      "Loss in this Epoch is: 3.34440357983 %\n",
      "Accuracy in this Epoch is: 88.6200010777 %\n",
      "Epoch 46, Batch 0 Loss:0.02937930077314377\n",
      "Epoch 46, Batch 1 Loss:0.03861508518457413\n",
      "Epoch 46, Batch 2 Loss:0.007935035042464733\n",
      "Epoch 46, Batch 3 Loss:0.020054638385772705\n",
      "Epoch 46, Batch 4 Loss:0.01335929799824953\n",
      "Epoch 46, Batch 5 Loss:0.02829151786863804\n",
      "Epoch 46, Batch 6 Loss:0.05516636744141579\n",
      "Epoch 46, Batch 7 Loss:0.008899657055735588\n",
      "Epoch 46, Batch 8 Loss:0.019987359642982483\n",
      "Epoch 46, Batch 9 Loss:0.04623594135046005\n",
      "Epoch 46, Batch 10 Loss:0.02152566984295845\n",
      "Epoch 46, Batch 11 Loss:0.018422991037368774\n",
      "Epoch 46, Batch 12 Loss:0.017137981951236725\n",
      "Epoch 46, Batch 13 Loss:0.015443475916981697\n",
      "Epoch 46, Batch 14 Loss:0.037462253123521805\n",
      "Epoch 46, Batch 15 Loss:0.01149159949272871\n",
      "Epoch 46, Batch 16 Loss:0.037162210792303085\n",
      "Epoch 46, Batch 17 Loss:0.009024146944284439\n",
      "Epoch 46, Batch 18 Loss:0.012391781434416771\n",
      "Epoch 46, Batch 19 Loss:0.01341584324836731\n",
      "Epoch 46, Batch 20 Loss:0.020971305668354034\n",
      "Epoch 46, Batch 21 Loss:0.019053148105740547\n",
      "Epoch 46, Batch 22 Loss:0.021623581647872925\n",
      "Epoch 46, Batch 23 Loss:0.011486797593533993\n",
      "Epoch 46, Batch 24 Loss:0.04609912261366844\n",
      "Epoch 46, Batch 25 Loss:0.033312905579805374\n",
      "Epoch 46, Batch 26 Loss:0.01721027120947838\n",
      "Epoch 46, Batch 27 Loss:0.03897016867995262\n",
      "Epoch 46, Batch 28 Loss:0.04324387386441231\n",
      "Epoch 46, Batch 29 Loss:0.018446994945406914\n",
      "Epoch 46, Batch 30 Loss:0.02185511589050293\n",
      "Epoch 46, Batch 31 Loss:0.019455108791589737\n",
      "Epoch 46, Batch 32 Loss:0.024135425686836243\n",
      "Epoch 46, Batch 33 Loss:0.06352492421865463\n",
      "Epoch 46, Batch 34 Loss:0.05094233527779579\n",
      "Epoch 46, Batch 35 Loss:0.025240454822778702\n",
      "Epoch 46, Batch 36 Loss:0.03745237737894058\n",
      "Epoch 46, Batch 37 Loss:0.04572318494319916\n",
      "Epoch 46, Batch 38 Loss:0.033357713371515274\n",
      "Epoch 46, Batch 39 Loss:0.05436467379331589\n",
      "Epoch 46, Batch 40 Loss:0.043927017599344254\n",
      "Epoch 46, Batch 41 Loss:0.04800973832607269\n",
      "Epoch 46, Batch 42 Loss:0.02485562302172184\n",
      "Epoch 46, Batch 43 Loss:0.019134409725666046\n",
      "Epoch 46, Batch 44 Loss:0.06440635025501251\n",
      "Epoch 46, Batch 45 Loss:0.031006699427962303\n",
      "Epoch 46, Batch 46 Loss:0.05241876095533371\n",
      "Epoch 46, Batch 47 Loss:0.026488782837986946\n",
      "Epoch 46, Batch 48 Loss:0.017591528594493866\n",
      "Epoch 46, Batch 49 Loss:0.01691584475338459\n",
      "Epoch 46, Batch 50 Loss:0.0216742642223835\n",
      "Epoch 46, Batch 51 Loss:0.022554218769073486\n",
      "Epoch 46, Batch 52 Loss:0.0391288697719574\n",
      "Epoch 46, Batch 53 Loss:0.014002433978021145\n",
      "Epoch 46, Batch 54 Loss:0.04194774106144905\n",
      "Epoch 46, Batch 55 Loss:0.028957344591617584\n",
      "Epoch 46, Batch 56 Loss:0.03169659897685051\n",
      "Epoch 46, Batch 57 Loss:0.03260498866438866\n",
      "Epoch 46, Batch 58 Loss:0.025004448369145393\n",
      "Epoch 46, Batch 59 Loss:0.019601592794060707\n",
      "Epoch 46, Batch 60 Loss:0.038737088441848755\n",
      "Epoch 46, Batch 61 Loss:0.0562576949596405\n",
      "Epoch 46, Batch 62 Loss:0.030302619561553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Batch 63 Loss:0.02123677358031273\n",
      "Epoch 46, Batch 64 Loss:0.030505651608109474\n",
      "Epoch 46, Batch 65 Loss:0.017062176018953323\n",
      "Epoch 46, Batch 66 Loss:0.030381495133042336\n",
      "Epoch 46, Batch 67 Loss:0.022744741290807724\n",
      "Epoch 46, Batch 68 Loss:0.030868617817759514\n",
      "Epoch 46, Batch 69 Loss:0.032712310552597046\n",
      "Epoch 46, Batch 70 Loss:0.016849175095558167\n",
      "Epoch 46, Batch 71 Loss:0.07012016326189041\n",
      "Epoch 46, Batch 72 Loss:0.02312171831727028\n",
      "Epoch 46, Batch 73 Loss:0.024875661358237267\n",
      "Epoch 46, Batch 74 Loss:0.02822883613407612\n",
      "Epoch 46, Batch 75 Loss:0.02333478443324566\n",
      "Epoch 46, Batch 76 Loss:0.02974672242999077\n",
      "Epoch 46, Batch 77 Loss:0.022797761484980583\n",
      "Epoch 46, Batch 78 Loss:0.01772787794470787\n",
      "Epoch 46, Batch 79 Loss:0.020333215594291687\n",
      "Epoch 46, Batch 80 Loss:0.04379627853631973\n",
      "Epoch 46, Batch 81 Loss:0.02021520957350731\n",
      "Epoch 46, Batch 82 Loss:0.03438297286629677\n",
      "Epoch 46, Batch 83 Loss:0.04620196670293808\n",
      "Epoch 46, Batch 84 Loss:0.059160102158784866\n",
      "Epoch 46, Batch 85 Loss:0.06693483889102936\n",
      "Epoch 46, Batch 86 Loss:0.03388189524412155\n",
      "Epoch 46, Batch 87 Loss:0.050808414816856384\n",
      "Epoch 46, Batch 88 Loss:0.02185695618391037\n",
      "Epoch 46, Batch 89 Loss:0.03298543393611908\n",
      "Epoch 46, Batch 90 Loss:0.03439287468791008\n",
      "Epoch 46, Batch 91 Loss:0.04175316542387009\n",
      "Epoch 46, Batch 92 Loss:0.03673238679766655\n",
      "Epoch 46, Batch 93 Loss:0.018201198428869247\n",
      "Epoch 46, Batch 94 Loss:0.019443457946181297\n",
      "Epoch 46, Batch 95 Loss:0.034752584993839264\n",
      "Epoch 46, Batch 96 Loss:0.019071241840720177\n",
      "Epoch 46, Batch 97 Loss:0.03879781812429428\n",
      "Epoch 46, Batch 98 Loss:0.025457270443439484\n",
      "Epoch 46, Batch 99 Loss:0.054722752422094345\n",
      "Epoch 46, Batch 100 Loss:0.01159733533859253\n",
      "Epoch 46, Batch 101 Loss:0.013627476990222931\n",
      "Epoch 46, Batch 102 Loss:0.03121943771839142\n",
      "Epoch 46, Batch 103 Loss:0.03702492639422417\n",
      "Epoch 46, Batch 104 Loss:0.04925285279750824\n",
      "Epoch 46, Batch 105 Loss:0.019875159487128258\n",
      "Epoch 46, Batch 106 Loss:0.10181555151939392\n",
      "Epoch 46, Batch 107 Loss:0.03007546067237854\n",
      "Epoch 46, Batch 108 Loss:0.029296744614839554\n",
      "Epoch 46, Batch 109 Loss:0.021275687962770462\n",
      "Epoch 46, Batch 110 Loss:0.011106057092547417\n",
      "Epoch 46, Batch 111 Loss:0.021384399384260178\n",
      "Epoch 46, Batch 112 Loss:0.03582068160176277\n",
      "Epoch 46, Batch 113 Loss:0.028355076909065247\n",
      "Epoch 46, Batch 114 Loss:0.01739734783768654\n",
      "Epoch 46, Batch 115 Loss:0.01878904365003109\n",
      "Epoch 46, Batch 116 Loss:0.015084616839885712\n",
      "Epoch 46, Batch 117 Loss:0.044974036514759064\n",
      "Epoch 46, Batch 118 Loss:0.00922461599111557\n",
      "Epoch 46, Batch 119 Loss:0.06369579583406448\n",
      "Epoch 46, Batch 120 Loss:0.06006716564297676\n",
      "Epoch 46, Batch 121 Loss:0.01196342520415783\n",
      "Epoch 46, Batch 122 Loss:0.01770380511879921\n",
      "Epoch 46, Batch 123 Loss:0.0356338806450367\n",
      "Epoch 46, Batch 124 Loss:0.029716186225414276\n",
      "Epoch 46, Batch 125 Loss:0.013945817947387695\n",
      "Epoch 46, Batch 126 Loss:0.04129677265882492\n",
      "Epoch 46, Batch 127 Loss:0.04718463867902756\n",
      "Epoch 46, Batch 128 Loss:0.034621674567461014\n",
      "Epoch 46, Batch 129 Loss:0.034174688160419464\n",
      "Epoch 46, Batch 130 Loss:0.035139866173267365\n",
      "Epoch 46, Batch 131 Loss:0.06937651336193085\n",
      "Epoch 46, Batch 132 Loss:0.04650984704494476\n",
      "Epoch 46, Batch 133 Loss:0.009641675278544426\n",
      "Epoch 46, Batch 134 Loss:0.02765009179711342\n",
      "Epoch 46, Batch 135 Loss:0.022043779492378235\n",
      "Epoch 46, Batch 136 Loss:0.032243791967630386\n",
      "Epoch 46, Batch 137 Loss:0.027292301878333092\n",
      "Epoch 46, Batch 138 Loss:0.04736161231994629\n",
      "Epoch 46, Batch 139 Loss:0.044492609798908234\n",
      "Epoch 46, Batch 140 Loss:0.02188687212765217\n",
      "Epoch 46, Batch 141 Loss:0.021681057289242744\n",
      "Epoch 46, Batch 142 Loss:0.023848731070756912\n",
      "Epoch 46, Batch 143 Loss:0.022699877619743347\n",
      "Epoch 46, Batch 144 Loss:0.02769528143107891\n",
      "Epoch 46, Batch 145 Loss:0.026134708896279335\n",
      "Epoch 46, Batch 146 Loss:0.04375190660357475\n",
      "Epoch 46, Batch 147 Loss:0.04577377066016197\n",
      "Epoch 46, Batch 148 Loss:0.026625150814652443\n",
      "Epoch 46, Batch 149 Loss:0.0565551295876503\n",
      "Epoch 46, Batch 150 Loss:0.023501085117459297\n",
      "Epoch 46, Batch 151 Loss:0.053246185183525085\n",
      "Epoch 46, Batch 152 Loss:0.02957061119377613\n",
      "Epoch 46, Batch 153 Loss:0.04566749930381775\n",
      "Epoch 46, Batch 154 Loss:0.06144826486706734\n",
      "Epoch 46, Batch 155 Loss:0.024289637804031372\n",
      "Epoch 46, Batch 156 Loss:0.027340641245245934\n",
      "Epoch 46, Batch 157 Loss:0.04223145544528961\n",
      "Epoch 46, Batch 158 Loss:0.022535867989063263\n",
      "Epoch 46, Batch 159 Loss:0.0181058868765831\n",
      "Epoch 46, Batch 160 Loss:0.031644806265830994\n",
      "Epoch 46, Batch 161 Loss:0.029699066653847694\n",
      "Epoch 46, Batch 162 Loss:0.030148325487971306\n",
      "Epoch 46, Batch 163 Loss:0.041104402393102646\n",
      "Epoch 46, Batch 164 Loss:0.08743637800216675\n",
      "Epoch 46, Batch 165 Loss:0.033785268664360046\n",
      "Epoch 46, Batch 166 Loss:0.03728988394141197\n",
      "Epoch 46, Batch 167 Loss:0.02845635451376438\n",
      "Epoch 46, Batch 168 Loss:0.029788654297590256\n",
      "Epoch 46, Batch 169 Loss:0.019558526575565338\n",
      "Epoch 46, Batch 170 Loss:0.021725621074438095\n",
      "Epoch 46, Batch 171 Loss:0.045687220990657806\n",
      "Epoch 46, Batch 172 Loss:0.03318652883172035\n",
      "Epoch 46, Batch 173 Loss:0.05556757375597954\n",
      "Epoch 46, Batch 174 Loss:0.016904138028621674\n",
      "Epoch 46, Batch 175 Loss:0.04518500342965126\n",
      "Epoch 46, Batch 176 Loss:0.0420302078127861\n",
      "Epoch 46, Batch 177 Loss:0.052668165415525436\n",
      "Epoch 46, Batch 178 Loss:0.03247305005788803\n",
      "Epoch 46, Batch 179 Loss:0.021708127111196518\n",
      "Epoch 46, Batch 180 Loss:0.07951179891824722\n",
      "Epoch 46, Batch 181 Loss:0.024585414677858353\n",
      "Epoch 46, Batch 182 Loss:0.03289360553026199\n",
      "Epoch 46, Batch 183 Loss:0.024413641542196274\n",
      "Epoch 46, Batch 184 Loss:0.03966326266527176\n",
      "Epoch 46, Batch 185 Loss:0.021099582314491272\n",
      "Epoch 46, Batch 186 Loss:0.06331100314855576\n",
      "Epoch 46, Batch 187 Loss:0.03360371291637421\n",
      "Epoch 46, Batch 188 Loss:0.029448777437210083\n",
      "Epoch 46, Batch 189 Loss:0.03898904472589493\n",
      "Epoch 46, Batch 190 Loss:0.028338201344013214\n",
      "Epoch 46, Batch 191 Loss:0.04705848544836044\n",
      "Epoch 46, Batch 192 Loss:0.03268693387508392\n",
      "Epoch 46, Batch 193 Loss:0.05861866474151611\n",
      "Epoch 46, Batch 194 Loss:0.0664898157119751\n",
      "Epoch 46, Batch 195 Loss:0.04941391199827194\n",
      "Epoch 46, Batch 196 Loss:0.05672898143529892\n",
      "Epoch 46, Batch 197 Loss:0.05398891121149063\n",
      "Epoch 46, Batch 198 Loss:0.06886167824268341\n",
      "Epoch 46, Batch 199 Loss:0.07837498188018799\n",
      "Epoch 46, Batch 200 Loss:0.08090692013502121\n",
      "Epoch 46, Batch 201 Loss:0.0596916526556015\n",
      "Epoch 46, Batch 202 Loss:0.07851862162351608\n",
      "Epoch 46, Batch 203 Loss:0.02145438641309738\n",
      "Epoch 46, Batch 204 Loss:0.03963463753461838\n",
      "Epoch 46, Batch 205 Loss:0.0810488760471344\n",
      "Epoch 46, Batch 206 Loss:0.03809460997581482\n",
      "Epoch 46, Batch 207 Loss:0.04937630146741867\n",
      "Epoch 46, Batch 208 Loss:0.05806838721036911\n",
      "Epoch 46, Batch 209 Loss:0.07996545732021332\n",
      "Epoch 46, Batch 210 Loss:0.021827351301908493\n",
      "Epoch 46, Batch 211 Loss:0.05541027709841728\n",
      "Epoch 46, Batch 212 Loss:0.05923491716384888\n",
      "Epoch 46, Batch 213 Loss:0.11696072667837143\n",
      "Epoch 46, Batch 214 Loss:0.0491204671561718\n",
      "Epoch 46, Batch 215 Loss:0.02185247465968132\n",
      "Epoch 46, Batch 216 Loss:0.041001904755830765\n",
      "Epoch 46, Batch 217 Loss:0.032015360891819\n",
      "Epoch 46, Batch 218 Loss:0.053053759038448334\n",
      "Epoch 46, Batch 219 Loss:0.06538734585046768\n",
      "Epoch 46, Batch 220 Loss:0.05318142846226692\n",
      "Epoch 46, Batch 221 Loss:0.05658674240112305\n",
      "Epoch 46, Batch 222 Loss:0.06142701208591461\n",
      "Epoch 46, Batch 223 Loss:0.06157587468624115\n",
      "Epoch 46, Batch 224 Loss:0.052162639796733856\n",
      "Epoch 46, Batch 225 Loss:0.08210007846355438\n",
      "Epoch 46, Batch 226 Loss:0.04087593033909798\n",
      "Epoch 46, Batch 227 Loss:0.06891340762376785\n",
      "Epoch 46, Batch 228 Loss:0.033258356153964996\n",
      "Epoch 46, Batch 229 Loss:0.037796586751937866\n",
      "Epoch 46, Batch 230 Loss:0.06430315971374512\n",
      "Epoch 46, Batch 231 Loss:0.06267639249563217\n",
      "Epoch 46, Batch 232 Loss:0.06928007304668427\n",
      "Epoch 46, Batch 233 Loss:0.10782692581415176\n",
      "Loss in this Epoch is: 10.7826925814 %\n",
      "Accuracy in this Epoch is: 88.1399989128 %\n",
      "Epoch 47, Batch 0 Loss:0.07018423825502396\n",
      "Epoch 47, Batch 1 Loss:0.06802075356245041\n",
      "Epoch 47, Batch 2 Loss:0.062114931643009186\n",
      "Epoch 47, Batch 3 Loss:0.055993933230638504\n",
      "Epoch 47, Batch 4 Loss:0.0571296326816082\n",
      "Epoch 47, Batch 5 Loss:0.08102557063102722\n",
      "Epoch 47, Batch 6 Loss:0.04220050945878029\n",
      "Epoch 47, Batch 7 Loss:0.06793277710676193\n",
      "Epoch 47, Batch 8 Loss:0.023346470668911934\n",
      "Epoch 47, Batch 9 Loss:0.02491740696132183\n",
      "Epoch 47, Batch 10 Loss:0.056520912796258926\n",
      "Epoch 47, Batch 11 Loss:0.050655294209718704\n",
      "Epoch 47, Batch 12 Loss:0.05110125243663788\n",
      "Epoch 47, Batch 13 Loss:0.044118732213974\n",
      "Epoch 47, Batch 14 Loss:0.0216786228120327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Batch 15 Loss:0.011033707298338413\n",
      "Epoch 47, Batch 16 Loss:0.02725488506257534\n",
      "Epoch 47, Batch 17 Loss:0.023569321259856224\n",
      "Epoch 47, Batch 18 Loss:0.028759365901350975\n",
      "Epoch 47, Batch 19 Loss:0.03176970034837723\n",
      "Epoch 47, Batch 20 Loss:0.013468465767800808\n",
      "Epoch 47, Batch 21 Loss:0.03926704078912735\n",
      "Epoch 47, Batch 22 Loss:0.021343613043427467\n",
      "Epoch 47, Batch 23 Loss:0.022764142602682114\n",
      "Epoch 47, Batch 24 Loss:0.0678754448890686\n",
      "Epoch 47, Batch 25 Loss:0.028132736682891846\n",
      "Epoch 47, Batch 26 Loss:0.013492941856384277\n",
      "Epoch 47, Batch 27 Loss:0.015950966626405716\n",
      "Epoch 47, Batch 28 Loss:0.03947703167796135\n",
      "Epoch 47, Batch 29 Loss:0.011400656774640083\n",
      "Epoch 47, Batch 30 Loss:0.021217647939920425\n",
      "Epoch 47, Batch 31 Loss:0.02543250471353531\n",
      "Epoch 47, Batch 32 Loss:0.013173654675483704\n",
      "Epoch 47, Batch 33 Loss:0.019199417904019356\n",
      "Epoch 47, Batch 34 Loss:0.06098482012748718\n",
      "Epoch 47, Batch 35 Loss:0.04924324154853821\n",
      "Epoch 47, Batch 36 Loss:0.03081532195210457\n",
      "Epoch 47, Batch 37 Loss:0.016585342586040497\n",
      "Epoch 47, Batch 38 Loss:0.017207272350788116\n",
      "Epoch 47, Batch 39 Loss:0.02799774706363678\n",
      "Epoch 47, Batch 40 Loss:0.017247147858142853\n",
      "Epoch 47, Batch 41 Loss:0.03652070462703705\n",
      "Epoch 47, Batch 42 Loss:0.04207124188542366\n",
      "Epoch 47, Batch 43 Loss:0.028820443898439407\n",
      "Epoch 47, Batch 44 Loss:0.020621569827198982\n",
      "Epoch 47, Batch 45 Loss:0.030773144215345383\n",
      "Epoch 47, Batch 46 Loss:0.04271660000085831\n",
      "Epoch 47, Batch 47 Loss:0.007663752883672714\n",
      "Epoch 47, Batch 48 Loss:0.010704876855015755\n",
      "Epoch 47, Batch 49 Loss:0.04662913456559181\n",
      "Epoch 47, Batch 50 Loss:0.03743556886911392\n",
      "Epoch 47, Batch 51 Loss:0.021566033363342285\n",
      "Epoch 47, Batch 52 Loss:0.018432196229696274\n",
      "Epoch 47, Batch 53 Loss:0.017732582986354828\n",
      "Epoch 47, Batch 54 Loss:0.015621484257280827\n",
      "Epoch 47, Batch 55 Loss:0.016347043216228485\n",
      "Epoch 47, Batch 56 Loss:0.012291123159229755\n",
      "Epoch 47, Batch 57 Loss:0.03900410607457161\n",
      "Epoch 47, Batch 58 Loss:0.01037345640361309\n",
      "Epoch 47, Batch 59 Loss:0.01914764940738678\n",
      "Epoch 47, Batch 60 Loss:0.014491752721369267\n",
      "Epoch 47, Batch 61 Loss:0.01406505424529314\n",
      "Epoch 47, Batch 62 Loss:0.033630356192588806\n",
      "Epoch 47, Batch 63 Loss:0.011061948724091053\n",
      "Epoch 47, Batch 64 Loss:0.05479712784290314\n",
      "Epoch 47, Batch 65 Loss:0.03338542953133583\n",
      "Epoch 47, Batch 66 Loss:0.008948860689997673\n",
      "Epoch 47, Batch 67 Loss:0.010529537685215473\n",
      "Epoch 47, Batch 68 Loss:0.013710030354559422\n",
      "Epoch 47, Batch 69 Loss:0.025772960856556892\n",
      "Epoch 47, Batch 70 Loss:0.049924030900001526\n",
      "Epoch 47, Batch 71 Loss:0.02471795864403248\n",
      "Epoch 47, Batch 72 Loss:0.027310697361826897\n",
      "Epoch 47, Batch 73 Loss:0.030598193407058716\n",
      "Epoch 47, Batch 74 Loss:0.07651341706514359\n",
      "Epoch 47, Batch 75 Loss:0.039867326617240906\n",
      "Epoch 47, Batch 76 Loss:0.009925895370543003\n",
      "Epoch 47, Batch 77 Loss:0.02460508421063423\n",
      "Epoch 47, Batch 78 Loss:0.10261617600917816\n",
      "Epoch 47, Batch 79 Loss:0.007467939984053373\n",
      "Epoch 47, Batch 80 Loss:0.0194474495947361\n",
      "Epoch 47, Batch 81 Loss:0.05886697769165039\n",
      "Epoch 47, Batch 82 Loss:0.03313746675848961\n",
      "Epoch 47, Batch 83 Loss:0.0541180782020092\n",
      "Epoch 47, Batch 84 Loss:0.010515106841921806\n",
      "Epoch 47, Batch 85 Loss:0.03781244903802872\n",
      "Epoch 47, Batch 86 Loss:0.023798247799277306\n",
      "Epoch 47, Batch 87 Loss:0.040123019367456436\n",
      "Epoch 47, Batch 88 Loss:0.05015737563371658\n",
      "Epoch 47, Batch 89 Loss:0.02378806285560131\n",
      "Epoch 47, Batch 90 Loss:0.03733837604522705\n",
      "Epoch 47, Batch 91 Loss:0.060711123049259186\n",
      "Epoch 47, Batch 92 Loss:0.052290964871644974\n",
      "Epoch 47, Batch 93 Loss:0.04137980937957764\n",
      "Epoch 47, Batch 94 Loss:0.04880734160542488\n",
      "Epoch 47, Batch 95 Loss:0.030764294788241386\n",
      "Epoch 47, Batch 96 Loss:0.01781904697418213\n",
      "Epoch 47, Batch 97 Loss:0.034189414232969284\n",
      "Epoch 47, Batch 98 Loss:0.014625932089984417\n",
      "Epoch 47, Batch 99 Loss:0.030453503131866455\n",
      "Epoch 47, Batch 100 Loss:0.030643196776509285\n",
      "Epoch 47, Batch 101 Loss:0.029147882014513016\n",
      "Epoch 47, Batch 102 Loss:0.03287382796406746\n",
      "Epoch 47, Batch 103 Loss:0.010857056826353073\n",
      "Epoch 47, Batch 104 Loss:0.02199389599263668\n",
      "Epoch 47, Batch 105 Loss:0.024161014705896378\n",
      "Epoch 47, Batch 106 Loss:0.02143871784210205\n",
      "Epoch 47, Batch 107 Loss:0.04920009896159172\n",
      "Epoch 47, Batch 108 Loss:0.023765385150909424\n",
      "Epoch 47, Batch 109 Loss:0.029874954372644424\n",
      "Epoch 47, Batch 110 Loss:0.01755385473370552\n",
      "Epoch 47, Batch 111 Loss:0.022675542160868645\n",
      "Epoch 47, Batch 112 Loss:0.06056461110711098\n",
      "Epoch 47, Batch 113 Loss:0.062447626143693924\n",
      "Epoch 47, Batch 114 Loss:0.03020482510328293\n",
      "Epoch 47, Batch 115 Loss:0.08254682272672653\n",
      "Epoch 47, Batch 116 Loss:0.017576094716787338\n",
      "Epoch 47, Batch 117 Loss:0.012161283753812313\n",
      "Epoch 47, Batch 118 Loss:0.017035631462931633\n",
      "Epoch 47, Batch 119 Loss:0.0432351753115654\n",
      "Epoch 47, Batch 120 Loss:0.020855627954006195\n",
      "Epoch 47, Batch 121 Loss:0.019541170448064804\n",
      "Epoch 47, Batch 122 Loss:0.0424046590924263\n",
      "Epoch 47, Batch 123 Loss:0.01796051487326622\n",
      "Epoch 47, Batch 124 Loss:0.022962164133787155\n",
      "Epoch 47, Batch 125 Loss:0.015218948945403099\n",
      "Epoch 47, Batch 126 Loss:0.015596271492540836\n",
      "Epoch 47, Batch 127 Loss:0.01228468120098114\n",
      "Epoch 47, Batch 128 Loss:0.06226447969675064\n",
      "Epoch 47, Batch 129 Loss:0.04031124338507652\n",
      "Epoch 47, Batch 130 Loss:0.028552398085594177\n",
      "Epoch 47, Batch 131 Loss:0.023658253252506256\n",
      "Epoch 47, Batch 132 Loss:0.02999451942741871\n",
      "Epoch 47, Batch 133 Loss:0.03207110986113548\n",
      "Epoch 47, Batch 134 Loss:0.025838496163487434\n",
      "Epoch 47, Batch 135 Loss:0.04176999628543854\n",
      "Epoch 47, Batch 136 Loss:0.05594964325428009\n",
      "Epoch 47, Batch 137 Loss:0.03490300849080086\n",
      "Epoch 47, Batch 138 Loss:0.03561966121196747\n",
      "Epoch 47, Batch 139 Loss:0.04815998673439026\n",
      "Epoch 47, Batch 140 Loss:0.11756940186023712\n",
      "Epoch 47, Batch 141 Loss:0.035118527710437775\n",
      "Epoch 47, Batch 142 Loss:0.04192175716161728\n",
      "Epoch 47, Batch 143 Loss:0.03396575152873993\n",
      "Epoch 47, Batch 144 Loss:0.04374706745147705\n",
      "Epoch 47, Batch 145 Loss:0.07416783273220062\n",
      "Epoch 47, Batch 146 Loss:0.015320724807679653\n",
      "Epoch 47, Batch 147 Loss:0.03285614401102066\n",
      "Epoch 47, Batch 148 Loss:0.05390947312116623\n",
      "Epoch 47, Batch 149 Loss:0.04186807945370674\n",
      "Epoch 47, Batch 150 Loss:0.02800159901380539\n",
      "Epoch 47, Batch 151 Loss:0.05923861637711525\n",
      "Epoch 47, Batch 152 Loss:0.04580879583954811\n",
      "Epoch 47, Batch 153 Loss:0.050884444266557693\n",
      "Epoch 47, Batch 154 Loss:0.020453840494155884\n",
      "Epoch 47, Batch 155 Loss:0.021485954523086548\n",
      "Epoch 47, Batch 156 Loss:0.016128836199641228\n",
      "Epoch 47, Batch 157 Loss:0.019829876720905304\n",
      "Epoch 47, Batch 158 Loss:0.06130504608154297\n",
      "Epoch 47, Batch 159 Loss:0.033918753266334534\n",
      "Epoch 47, Batch 160 Loss:0.058423712849617004\n",
      "Epoch 47, Batch 161 Loss:0.0229436494410038\n",
      "Epoch 47, Batch 162 Loss:0.08392463624477386\n",
      "Epoch 47, Batch 163 Loss:0.04617803916335106\n",
      "Epoch 47, Batch 164 Loss:0.03840683400630951\n",
      "Epoch 47, Batch 165 Loss:0.04013410210609436\n",
      "Epoch 47, Batch 166 Loss:0.05109433829784393\n",
      "Epoch 47, Batch 167 Loss:0.03585394099354744\n",
      "Epoch 47, Batch 168 Loss:0.040377017110586166\n",
      "Epoch 47, Batch 169 Loss:0.03996748849749565\n",
      "Epoch 47, Batch 170 Loss:0.030275756493210793\n",
      "Epoch 47, Batch 171 Loss:0.05210622027516365\n",
      "Epoch 47, Batch 172 Loss:0.03715597465634346\n",
      "Epoch 47, Batch 173 Loss:0.022746063768863678\n",
      "Epoch 47, Batch 174 Loss:0.026998277753591537\n",
      "Epoch 47, Batch 175 Loss:0.05280701816082001\n",
      "Epoch 47, Batch 176 Loss:0.03307709842920303\n",
      "Epoch 47, Batch 177 Loss:0.032451458275318146\n",
      "Epoch 47, Batch 178 Loss:0.08026612550020218\n",
      "Epoch 47, Batch 179 Loss:0.026216987520456314\n",
      "Epoch 47, Batch 180 Loss:0.031737320125103\n",
      "Epoch 47, Batch 181 Loss:0.028084076941013336\n",
      "Epoch 47, Batch 182 Loss:0.022385112941265106\n",
      "Epoch 47, Batch 183 Loss:0.024218695238232613\n",
      "Epoch 47, Batch 184 Loss:0.04427139461040497\n",
      "Epoch 47, Batch 185 Loss:0.03245576098561287\n",
      "Epoch 47, Batch 186 Loss:0.013125408440828323\n",
      "Epoch 47, Batch 187 Loss:0.037823982536792755\n",
      "Epoch 47, Batch 188 Loss:0.0850420743227005\n",
      "Epoch 47, Batch 189 Loss:0.021449226886034012\n",
      "Epoch 47, Batch 190 Loss:0.031144775450229645\n",
      "Epoch 47, Batch 191 Loss:0.05899035185575485\n",
      "Epoch 47, Batch 192 Loss:0.053518667817115784\n",
      "Epoch 47, Batch 193 Loss:0.03454156592488289\n",
      "Epoch 47, Batch 194 Loss:0.0247537549585104\n",
      "Epoch 47, Batch 195 Loss:0.028710223734378815\n",
      "Epoch 47, Batch 196 Loss:0.022513026371598244\n",
      "Epoch 47, Batch 197 Loss:0.03968390077352524\n",
      "Epoch 47, Batch 198 Loss:0.029933849349617958\n",
      "Epoch 47, Batch 199 Loss:0.014351362362504005\n",
      "Epoch 47, Batch 200 Loss:0.045418016612529755\n",
      "Epoch 47, Batch 201 Loss:0.025851715356111526\n",
      "Epoch 47, Batch 202 Loss:0.020649246871471405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Batch 203 Loss:0.04016994312405586\n",
      "Epoch 47, Batch 204 Loss:0.014281054958701134\n",
      "Epoch 47, Batch 205 Loss:0.05956733226776123\n",
      "Epoch 47, Batch 206 Loss:0.06984648108482361\n",
      "Epoch 47, Batch 207 Loss:0.029690079391002655\n",
      "Epoch 47, Batch 208 Loss:0.019708454608917236\n",
      "Epoch 47, Batch 209 Loss:0.04757069796323776\n",
      "Epoch 47, Batch 210 Loss:0.03523033857345581\n",
      "Epoch 47, Batch 211 Loss:0.02536989375948906\n",
      "Epoch 47, Batch 212 Loss:0.04406464099884033\n",
      "Epoch 47, Batch 213 Loss:0.022391822189092636\n",
      "Epoch 47, Batch 214 Loss:0.055885232985019684\n",
      "Epoch 47, Batch 215 Loss:0.041413743048906326\n",
      "Epoch 47, Batch 216 Loss:0.014299053698778152\n",
      "Epoch 47, Batch 217 Loss:0.020738929510116577\n",
      "Epoch 47, Batch 218 Loss:0.026343239471316338\n",
      "Epoch 47, Batch 219 Loss:0.07422816008329391\n",
      "Epoch 47, Batch 220 Loss:0.030746638774871826\n",
      "Epoch 47, Batch 221 Loss:0.02617737278342247\n",
      "Epoch 47, Batch 222 Loss:0.03519685938954353\n",
      "Epoch 47, Batch 223 Loss:0.028093233704566956\n",
      "Epoch 47, Batch 224 Loss:0.007015584502369165\n",
      "Epoch 47, Batch 225 Loss:0.02025650441646576\n",
      "Epoch 47, Batch 226 Loss:0.05214380472898483\n",
      "Epoch 47, Batch 227 Loss:0.054059822112321854\n",
      "Epoch 47, Batch 228 Loss:0.07668992131948471\n",
      "Epoch 47, Batch 229 Loss:0.016810782253742218\n",
      "Epoch 47, Batch 230 Loss:0.05390918254852295\n",
      "Epoch 47, Batch 231 Loss:0.05466160178184509\n",
      "Epoch 47, Batch 232 Loss:0.027491137385368347\n",
      "Epoch 47, Batch 233 Loss:0.07590144127607346\n",
      "Loss in this Epoch is: 7.59014412761 %\n",
      "Accuracy in this Epoch is: 88.7499988079 %\n",
      "Epoch 48, Batch 0 Loss:0.0586366206407547\n",
      "Epoch 48, Batch 1 Loss:0.02203013002872467\n",
      "Epoch 48, Batch 2 Loss:0.03314938396215439\n",
      "Epoch 48, Batch 3 Loss:0.04547150060534477\n",
      "Epoch 48, Batch 4 Loss:0.04871170222759247\n",
      "Epoch 48, Batch 5 Loss:0.01988694630563259\n",
      "Epoch 48, Batch 6 Loss:0.02728629857301712\n",
      "Epoch 48, Batch 7 Loss:0.009495679289102554\n",
      "Epoch 48, Batch 8 Loss:0.03487241268157959\n",
      "Epoch 48, Batch 9 Loss:0.029822437092661858\n",
      "Epoch 48, Batch 10 Loss:0.014738996513187885\n",
      "Epoch 48, Batch 11 Loss:0.02654244937002659\n",
      "Epoch 48, Batch 12 Loss:0.022003505378961563\n",
      "Epoch 48, Batch 13 Loss:0.010991189628839493\n",
      "Epoch 48, Batch 14 Loss:0.03988714888691902\n",
      "Epoch 48, Batch 15 Loss:0.026133116334676743\n",
      "Epoch 48, Batch 16 Loss:0.03573380410671234\n",
      "Epoch 48, Batch 17 Loss:0.05688484013080597\n",
      "Epoch 48, Batch 18 Loss:0.03381115198135376\n",
      "Epoch 48, Batch 19 Loss:0.018190013244748116\n",
      "Epoch 48, Batch 20 Loss:0.030507711693644524\n",
      "Epoch 48, Batch 21 Loss:0.04577161744236946\n",
      "Epoch 48, Batch 22 Loss:0.028318608179688454\n",
      "Epoch 48, Batch 23 Loss:0.01552269421517849\n",
      "Epoch 48, Batch 24 Loss:0.02658681757748127\n",
      "Epoch 48, Batch 25 Loss:0.011578096076846123\n",
      "Epoch 48, Batch 26 Loss:0.02105826325714588\n",
      "Epoch 48, Batch 27 Loss:0.01695835217833519\n",
      "Epoch 48, Batch 28 Loss:0.02592906728386879\n",
      "Epoch 48, Batch 29 Loss:0.00861167348921299\n",
      "Epoch 48, Batch 30 Loss:0.043076932430267334\n",
      "Epoch 48, Batch 31 Loss:0.05797184631228447\n",
      "Epoch 48, Batch 32 Loss:0.023722244426608086\n",
      "Epoch 48, Batch 33 Loss:0.019504427909851074\n",
      "Epoch 48, Batch 34 Loss:0.014607570134103298\n",
      "Epoch 48, Batch 35 Loss:0.03455328196287155\n",
      "Epoch 48, Batch 36 Loss:0.041711434721946716\n",
      "Epoch 48, Batch 37 Loss:0.040118586272001266\n",
      "Epoch 48, Batch 38 Loss:0.023047829046845436\n",
      "Epoch 48, Batch 39 Loss:0.023724643513560295\n",
      "Epoch 48, Batch 40 Loss:0.03401978313922882\n",
      "Epoch 48, Batch 41 Loss:0.035184383392333984\n",
      "Epoch 48, Batch 42 Loss:0.02675778791308403\n",
      "Epoch 48, Batch 43 Loss:0.025654517114162445\n",
      "Epoch 48, Batch 44 Loss:0.05551713705062866\n",
      "Epoch 48, Batch 45 Loss:0.046591758728027344\n",
      "Epoch 48, Batch 46 Loss:0.0189191997051239\n",
      "Epoch 48, Batch 47 Loss:0.025207726284861565\n",
      "Epoch 48, Batch 48 Loss:0.03310861811041832\n",
      "Epoch 48, Batch 49 Loss:0.027546653524041176\n",
      "Epoch 48, Batch 50 Loss:0.021031025797128677\n",
      "Epoch 48, Batch 51 Loss:0.0260234996676445\n",
      "Epoch 48, Batch 52 Loss:0.042905542999506\n",
      "Epoch 48, Batch 53 Loss:0.07080210000276566\n",
      "Epoch 48, Batch 54 Loss:0.024980992078781128\n",
      "Epoch 48, Batch 55 Loss:0.03204815089702606\n",
      "Epoch 48, Batch 56 Loss:0.040442388504743576\n",
      "Epoch 48, Batch 57 Loss:0.030232790857553482\n",
      "Epoch 48, Batch 58 Loss:0.030347011983394623\n",
      "Epoch 48, Batch 59 Loss:0.017291763797402382\n",
      "Epoch 48, Batch 60 Loss:0.021078692749142647\n",
      "Epoch 48, Batch 61 Loss:0.03747786581516266\n",
      "Epoch 48, Batch 62 Loss:0.03310392424464226\n",
      "Epoch 48, Batch 63 Loss:0.024040838703513145\n",
      "Epoch 48, Batch 64 Loss:0.025732390582561493\n",
      "Epoch 48, Batch 65 Loss:0.014879909344017506\n",
      "Epoch 48, Batch 66 Loss:0.041723161935806274\n",
      "Epoch 48, Batch 67 Loss:0.015970829874277115\n",
      "Epoch 48, Batch 68 Loss:0.019910655915737152\n",
      "Epoch 48, Batch 69 Loss:0.013224652037024498\n",
      "Epoch 48, Batch 70 Loss:0.03838396817445755\n",
      "Epoch 48, Batch 71 Loss:0.018644893541932106\n",
      "Epoch 48, Batch 72 Loss:0.033075541257858276\n",
      "Epoch 48, Batch 73 Loss:0.017885444685816765\n",
      "Epoch 48, Batch 74 Loss:0.028197333216667175\n",
      "Epoch 48, Batch 75 Loss:0.01169519405812025\n",
      "Epoch 48, Batch 76 Loss:0.03773152083158493\n",
      "Epoch 48, Batch 77 Loss:0.05629285052418709\n",
      "Epoch 48, Batch 78 Loss:0.031390368938446045\n",
      "Epoch 48, Batch 79 Loss:0.03556337580084801\n",
      "Epoch 48, Batch 80 Loss:0.03756602481007576\n",
      "Epoch 48, Batch 81 Loss:0.054527588188648224\n",
      "Epoch 48, Batch 82 Loss:0.015292024239897728\n",
      "Epoch 48, Batch 83 Loss:0.026072319597005844\n",
      "Epoch 48, Batch 84 Loss:0.05822288617491722\n",
      "Epoch 48, Batch 85 Loss:0.02806265652179718\n",
      "Epoch 48, Batch 86 Loss:0.01913856342434883\n",
      "Epoch 48, Batch 87 Loss:0.05039915814995766\n",
      "Epoch 48, Batch 88 Loss:0.045217592269182205\n",
      "Epoch 48, Batch 89 Loss:0.025813771411776543\n",
      "Epoch 48, Batch 90 Loss:0.033610887825489044\n",
      "Epoch 48, Batch 91 Loss:0.022310830652713776\n",
      "Epoch 48, Batch 92 Loss:0.021080074831843376\n",
      "Epoch 48, Batch 93 Loss:0.09338603168725967\n",
      "Epoch 48, Batch 94 Loss:0.046050846576690674\n",
      "Epoch 48, Batch 95 Loss:0.03885902464389801\n",
      "Epoch 48, Batch 96 Loss:0.0340416356921196\n",
      "Epoch 48, Batch 97 Loss:0.021183915436267853\n",
      "Epoch 48, Batch 98 Loss:0.04000410810112953\n",
      "Epoch 48, Batch 99 Loss:0.02592753991484642\n",
      "Epoch 48, Batch 100 Loss:0.027676161378622055\n",
      "Epoch 48, Batch 101 Loss:0.017491281032562256\n",
      "Epoch 48, Batch 102 Loss:0.045878514647483826\n",
      "Epoch 48, Batch 103 Loss:0.013687901198863983\n",
      "Epoch 48, Batch 104 Loss:0.0387275367975235\n",
      "Epoch 48, Batch 105 Loss:0.028740912675857544\n",
      "Epoch 48, Batch 106 Loss:0.032509829849004745\n",
      "Epoch 48, Batch 107 Loss:0.03365302085876465\n",
      "Epoch 48, Batch 108 Loss:0.024985894560813904\n",
      "Epoch 48, Batch 109 Loss:0.015193687751889229\n",
      "Epoch 48, Batch 110 Loss:0.01789313182234764\n",
      "Epoch 48, Batch 111 Loss:0.023833895102143288\n",
      "Epoch 48, Batch 112 Loss:0.034857939928770065\n",
      "Epoch 48, Batch 113 Loss:0.018623150885105133\n",
      "Epoch 48, Batch 114 Loss:0.034102752804756165\n",
      "Epoch 48, Batch 115 Loss:0.046454329043626785\n",
      "Epoch 48, Batch 116 Loss:0.03882824257016182\n",
      "Epoch 48, Batch 117 Loss:0.07084070146083832\n",
      "Epoch 48, Batch 118 Loss:0.03748291730880737\n",
      "Epoch 48, Batch 119 Loss:0.024197746068239212\n",
      "Epoch 48, Batch 120 Loss:0.02890953980386257\n",
      "Epoch 48, Batch 121 Loss:0.02102397009730339\n",
      "Epoch 48, Batch 122 Loss:0.032071325927972794\n",
      "Epoch 48, Batch 123 Loss:0.03799322247505188\n",
      "Epoch 48, Batch 124 Loss:0.024869881570339203\n",
      "Epoch 48, Batch 125 Loss:0.021621141582727432\n",
      "Epoch 48, Batch 126 Loss:0.04935768246650696\n",
      "Epoch 48, Batch 127 Loss:0.017730606719851494\n",
      "Epoch 48, Batch 128 Loss:0.02677927166223526\n",
      "Epoch 48, Batch 129 Loss:0.013945612125098705\n",
      "Epoch 48, Batch 130 Loss:0.02158106118440628\n",
      "Epoch 48, Batch 131 Loss:0.029092298820614815\n",
      "Epoch 48, Batch 132 Loss:0.02360597997903824\n",
      "Epoch 48, Batch 133 Loss:0.024975093081593513\n",
      "Epoch 48, Batch 134 Loss:0.04630095884203911\n",
      "Epoch 48, Batch 135 Loss:0.01889234036207199\n",
      "Epoch 48, Batch 136 Loss:0.04469866305589676\n",
      "Epoch 48, Batch 137 Loss:0.020926136523485184\n",
      "Epoch 48, Batch 138 Loss:0.02879098430275917\n",
      "Epoch 48, Batch 139 Loss:0.06368570774793625\n",
      "Epoch 48, Batch 140 Loss:0.04623086005449295\n",
      "Epoch 48, Batch 141 Loss:0.08478927612304688\n",
      "Epoch 48, Batch 142 Loss:0.02284243516623974\n",
      "Epoch 48, Batch 143 Loss:0.022874709218740463\n",
      "Epoch 48, Batch 144 Loss:0.030073167756199837\n",
      "Epoch 48, Batch 145 Loss:0.019344566389918327\n",
      "Epoch 48, Batch 146 Loss:0.016826238483190536\n",
      "Epoch 48, Batch 147 Loss:0.026048246771097183\n",
      "Epoch 48, Batch 148 Loss:0.022229813039302826\n",
      "Epoch 48, Batch 149 Loss:0.025290701538324356\n",
      "Epoch 48, Batch 150 Loss:0.043069027364254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Batch 151 Loss:0.021377934142947197\n",
      "Epoch 48, Batch 152 Loss:0.03229031339287758\n",
      "Epoch 48, Batch 153 Loss:0.009926717728376389\n",
      "Epoch 48, Batch 154 Loss:0.0407646968960762\n",
      "Epoch 48, Batch 155 Loss:0.02961677685379982\n",
      "Epoch 48, Batch 156 Loss:0.018357250839471817\n",
      "Epoch 48, Batch 157 Loss:0.0218332689255476\n",
      "Epoch 48, Batch 158 Loss:0.02094550058245659\n",
      "Epoch 48, Batch 159 Loss:0.006965090055018663\n",
      "Epoch 48, Batch 160 Loss:0.028155580163002014\n",
      "Epoch 48, Batch 161 Loss:0.018770333379507065\n",
      "Epoch 48, Batch 162 Loss:0.030798645690083504\n",
      "Epoch 48, Batch 163 Loss:0.06544607877731323\n",
      "Epoch 48, Batch 164 Loss:0.09131118655204773\n",
      "Epoch 48, Batch 165 Loss:0.025092052295804024\n",
      "Epoch 48, Batch 166 Loss:0.05431034415960312\n",
      "Epoch 48, Batch 167 Loss:0.024766109883785248\n",
      "Epoch 48, Batch 168 Loss:0.04989282041788101\n",
      "Epoch 48, Batch 169 Loss:0.03119467943906784\n",
      "Epoch 48, Batch 170 Loss:0.061750367283821106\n",
      "Epoch 48, Batch 171 Loss:0.03615481033921242\n",
      "Epoch 48, Batch 172 Loss:0.028629345819354057\n",
      "Epoch 48, Batch 173 Loss:0.02940172329545021\n",
      "Epoch 48, Batch 174 Loss:0.020359234884381294\n",
      "Epoch 48, Batch 175 Loss:0.010853031650185585\n",
      "Epoch 48, Batch 176 Loss:0.05426529794931412\n",
      "Epoch 48, Batch 177 Loss:0.041351255029439926\n",
      "Epoch 48, Batch 178 Loss:0.014894310384988785\n",
      "Epoch 48, Batch 179 Loss:0.040789663791656494\n",
      "Epoch 48, Batch 180 Loss:0.014738669618964195\n",
      "Epoch 48, Batch 181 Loss:0.01962217316031456\n",
      "Epoch 48, Batch 182 Loss:0.030332528054714203\n",
      "Epoch 48, Batch 183 Loss:0.0388268381357193\n",
      "Epoch 48, Batch 184 Loss:0.031178787350654602\n",
      "Epoch 48, Batch 185 Loss:0.016964543610811234\n",
      "Epoch 48, Batch 186 Loss:0.027595842257142067\n",
      "Epoch 48, Batch 187 Loss:0.01867794245481491\n",
      "Epoch 48, Batch 188 Loss:0.029902316629886627\n",
      "Epoch 48, Batch 189 Loss:0.02157234027981758\n",
      "Epoch 48, Batch 190 Loss:0.01615022122859955\n",
      "Epoch 48, Batch 191 Loss:0.014512555673718452\n",
      "Epoch 48, Batch 192 Loss:0.03071696311235428\n",
      "Epoch 48, Batch 193 Loss:0.03629978373646736\n",
      "Epoch 48, Batch 194 Loss:0.015731921419501305\n",
      "Epoch 48, Batch 195 Loss:0.01461276225745678\n",
      "Epoch 48, Batch 196 Loss:0.03020494617521763\n",
      "Epoch 48, Batch 197 Loss:0.04329651594161987\n",
      "Epoch 48, Batch 198 Loss:0.02813398279249668\n",
      "Epoch 48, Batch 199 Loss:0.029297906905412674\n",
      "Epoch 48, Batch 200 Loss:0.050662800669670105\n",
      "Epoch 48, Batch 201 Loss:0.014375475235283375\n",
      "Epoch 48, Batch 202 Loss:0.021059751510620117\n",
      "Epoch 48, Batch 203 Loss:0.014081933535635471\n",
      "Epoch 48, Batch 204 Loss:0.012394584715366364\n",
      "Epoch 48, Batch 205 Loss:0.022670865058898926\n",
      "Epoch 48, Batch 206 Loss:0.019942741841077805\n",
      "Epoch 48, Batch 207 Loss:0.023439601063728333\n",
      "Epoch 48, Batch 208 Loss:0.005164356902241707\n",
      "Epoch 48, Batch 209 Loss:0.03688874468207359\n",
      "Epoch 48, Batch 210 Loss:0.02530747465789318\n",
      "Epoch 48, Batch 211 Loss:0.05934065207839012\n",
      "Epoch 48, Batch 212 Loss:0.020890863612294197\n",
      "Epoch 48, Batch 213 Loss:0.06845767796039581\n",
      "Epoch 48, Batch 214 Loss:0.05313443765044212\n",
      "Epoch 48, Batch 215 Loss:0.013373669236898422\n",
      "Epoch 48, Batch 216 Loss:0.027515817433595657\n",
      "Epoch 48, Batch 217 Loss:0.015397651121020317\n",
      "Epoch 48, Batch 218 Loss:0.03892131149768829\n",
      "Epoch 48, Batch 219 Loss:0.025619346648454666\n",
      "Epoch 48, Batch 220 Loss:0.012526245787739754\n",
      "Epoch 48, Batch 221 Loss:0.03686479479074478\n",
      "Epoch 48, Batch 222 Loss:0.019039157778024673\n",
      "Epoch 48, Batch 223 Loss:0.02211400307714939\n",
      "Epoch 48, Batch 224 Loss:0.018345490097999573\n",
      "Epoch 48, Batch 225 Loss:0.020651275292038918\n",
      "Epoch 48, Batch 226 Loss:0.011759735643863678\n",
      "Epoch 48, Batch 227 Loss:0.039869606494903564\n",
      "Epoch 48, Batch 228 Loss:0.018402669578790665\n",
      "Epoch 48, Batch 229 Loss:0.02320169471204281\n",
      "Epoch 48, Batch 230 Loss:0.04648212343454361\n",
      "Epoch 48, Batch 231 Loss:0.051220111548900604\n",
      "Epoch 48, Batch 232 Loss:0.03966592624783516\n",
      "Epoch 48, Batch 233 Loss:0.027894608676433563\n",
      "Loss in this Epoch is: 2.78946086764 %\n",
      "Accuracy in this Epoch is: 88.9999985695 %\n",
      "Epoch 49, Batch 0 Loss:0.025128647685050964\n",
      "Epoch 49, Batch 1 Loss:0.0196833536028862\n",
      "Epoch 49, Batch 2 Loss:0.02309347875416279\n",
      "Epoch 49, Batch 3 Loss:0.017731625586748123\n",
      "Epoch 49, Batch 4 Loss:0.023858817294239998\n",
      "Epoch 49, Batch 5 Loss:0.059574149549007416\n",
      "Epoch 49, Batch 6 Loss:0.03049982152879238\n",
      "Epoch 49, Batch 7 Loss:0.0317031592130661\n",
      "Epoch 49, Batch 8 Loss:0.014945032075047493\n",
      "Epoch 49, Batch 9 Loss:0.02330048568546772\n",
      "Epoch 49, Batch 10 Loss:0.03583630174398422\n",
      "Epoch 49, Batch 11 Loss:0.02728523500263691\n",
      "Epoch 49, Batch 12 Loss:0.03108658455312252\n",
      "Epoch 49, Batch 13 Loss:0.023585880175232887\n",
      "Epoch 49, Batch 14 Loss:0.027699638158082962\n",
      "Epoch 49, Batch 15 Loss:0.018498754128813744\n",
      "Epoch 49, Batch 16 Loss:0.019170040264725685\n",
      "Epoch 49, Batch 17 Loss:0.010470990091562271\n",
      "Epoch 49, Batch 18 Loss:0.027594996616244316\n",
      "Epoch 49, Batch 19 Loss:0.020123369991779327\n",
      "Epoch 49, Batch 20 Loss:0.020612044259905815\n",
      "Epoch 49, Batch 21 Loss:0.02737811952829361\n",
      "Epoch 49, Batch 22 Loss:0.012535681016743183\n",
      "Epoch 49, Batch 23 Loss:0.015797175467014313\n",
      "Epoch 49, Batch 24 Loss:0.023774592205882072\n",
      "Epoch 49, Batch 25 Loss:0.0303556639701128\n",
      "Epoch 49, Batch 26 Loss:0.02802601270377636\n",
      "Epoch 49, Batch 27 Loss:0.02097775973379612\n",
      "Epoch 49, Batch 28 Loss:0.011577239260077477\n",
      "Epoch 49, Batch 29 Loss:0.014259262010455132\n",
      "Epoch 49, Batch 30 Loss:0.021174557507038116\n",
      "Epoch 49, Batch 31 Loss:0.012663239613175392\n",
      "Epoch 49, Batch 32 Loss:0.020915012806653976\n",
      "Epoch 49, Batch 33 Loss:0.010192041285336018\n",
      "Epoch 49, Batch 34 Loss:0.023778840899467468\n",
      "Epoch 49, Batch 35 Loss:0.028504477813839912\n",
      "Epoch 49, Batch 36 Loss:0.054676443338394165\n",
      "Epoch 49, Batch 37 Loss:0.013541856780648232\n",
      "Epoch 49, Batch 38 Loss:0.006969703361392021\n",
      "Epoch 49, Batch 39 Loss:0.019700339064002037\n",
      "Epoch 49, Batch 40 Loss:0.008780749514698982\n",
      "Epoch 49, Batch 41 Loss:0.013880760408937931\n",
      "Epoch 49, Batch 42 Loss:0.012727446854114532\n",
      "Epoch 49, Batch 43 Loss:0.017618050798773766\n",
      "Epoch 49, Batch 44 Loss:0.006425112020224333\n",
      "Epoch 49, Batch 45 Loss:0.014545941725373268\n",
      "Epoch 49, Batch 46 Loss:0.00919438898563385\n",
      "Epoch 49, Batch 47 Loss:0.0050795841962099075\n",
      "Epoch 49, Batch 48 Loss:0.01739245280623436\n",
      "Epoch 49, Batch 49 Loss:0.019096510484814644\n",
      "Epoch 49, Batch 50 Loss:0.017529284581542015\n",
      "Epoch 49, Batch 51 Loss:0.02257002703845501\n",
      "Epoch 49, Batch 52 Loss:0.01222418062388897\n",
      "Epoch 49, Batch 53 Loss:0.017498651519417763\n",
      "Epoch 49, Batch 54 Loss:0.014129695482552052\n",
      "Epoch 49, Batch 55 Loss:0.014477592892944813\n",
      "Epoch 49, Batch 56 Loss:0.02805686742067337\n",
      "Epoch 49, Batch 57 Loss:0.04915216937661171\n",
      "Epoch 49, Batch 58 Loss:0.01052760612219572\n",
      "Epoch 49, Batch 59 Loss:0.01980491355061531\n",
      "Epoch 49, Batch 60 Loss:0.04272797331213951\n",
      "Epoch 49, Batch 61 Loss:0.017677098512649536\n",
      "Epoch 49, Batch 62 Loss:0.0055450391955673695\n",
      "Epoch 49, Batch 63 Loss:0.03574049472808838\n",
      "Epoch 49, Batch 64 Loss:0.04057547077536583\n",
      "Epoch 49, Batch 65 Loss:0.009777206927537918\n",
      "Epoch 49, Batch 66 Loss:0.02814224362373352\n",
      "Epoch 49, Batch 67 Loss:0.0038569695316255093\n",
      "Epoch 49, Batch 68 Loss:0.016908898949623108\n",
      "Epoch 49, Batch 69 Loss:0.028807368129491806\n",
      "Epoch 49, Batch 70 Loss:0.04163404926657677\n",
      "Epoch 49, Batch 71 Loss:0.04022010415792465\n",
      "Epoch 49, Batch 72 Loss:0.05060921609401703\n",
      "Epoch 49, Batch 73 Loss:0.03014378622174263\n",
      "Epoch 49, Batch 74 Loss:0.027432328090071678\n",
      "Epoch 49, Batch 75 Loss:0.008594810962677002\n",
      "Epoch 49, Batch 76 Loss:0.021075312048196793\n",
      "Epoch 49, Batch 77 Loss:0.011904986575245857\n",
      "Epoch 49, Batch 78 Loss:0.04973946884274483\n",
      "Epoch 49, Batch 79 Loss:0.025177525356411934\n",
      "Epoch 49, Batch 80 Loss:0.025044864043593407\n",
      "Epoch 49, Batch 81 Loss:0.02695022150874138\n",
      "Epoch 49, Batch 82 Loss:0.04569912329316139\n",
      "Epoch 49, Batch 83 Loss:0.0604882575571537\n",
      "Epoch 49, Batch 84 Loss:0.046051912009716034\n",
      "Epoch 49, Batch 85 Loss:0.0527871735394001\n",
      "Epoch 49, Batch 86 Loss:0.07834949344396591\n",
      "Epoch 49, Batch 87 Loss:0.04094739630818367\n",
      "Epoch 49, Batch 88 Loss:0.035289324820041656\n",
      "Epoch 49, Batch 89 Loss:0.025008682161569595\n",
      "Epoch 49, Batch 90 Loss:0.0366433784365654\n",
      "Epoch 49, Batch 91 Loss:0.032368648797273636\n",
      "Epoch 49, Batch 92 Loss:0.035891491919755936\n",
      "Epoch 49, Batch 93 Loss:0.027399079874157906\n",
      "Epoch 49, Batch 94 Loss:0.0349133275449276\n",
      "Epoch 49, Batch 95 Loss:0.022880185395479202\n",
      "Epoch 49, Batch 96 Loss:0.03134912997484207\n",
      "Epoch 49, Batch 97 Loss:0.05511097609996796\n",
      "Epoch 49, Batch 98 Loss:0.019211281090974808\n",
      "Epoch 49, Batch 99 Loss:0.030861539766192436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Batch 100 Loss:0.044783979654312134\n",
      "Epoch 49, Batch 101 Loss:0.03055119514465332\n",
      "Epoch 49, Batch 102 Loss:0.06616443395614624\n",
      "Epoch 49, Batch 103 Loss:0.031364280730485916\n",
      "Epoch 49, Batch 104 Loss:0.034901849925518036\n",
      "Epoch 49, Batch 105 Loss:0.07700660824775696\n",
      "Epoch 49, Batch 106 Loss:0.032115764915943146\n",
      "Epoch 49, Batch 107 Loss:0.033378712832927704\n",
      "Epoch 49, Batch 108 Loss:0.02831488475203514\n",
      "Epoch 49, Batch 109 Loss:0.03577973693609238\n",
      "Epoch 49, Batch 110 Loss:0.035784970968961716\n",
      "Epoch 49, Batch 111 Loss:0.024248505011200905\n",
      "Epoch 49, Batch 112 Loss:0.06045767664909363\n",
      "Epoch 49, Batch 113 Loss:0.052566178143024445\n",
      "Epoch 49, Batch 114 Loss:0.031998418271541595\n",
      "Epoch 49, Batch 115 Loss:0.04830286651849747\n",
      "Epoch 49, Batch 116 Loss:0.028932560235261917\n",
      "Epoch 49, Batch 117 Loss:0.029907140880823135\n",
      "Epoch 49, Batch 118 Loss:0.05919806659221649\n",
      "Epoch 49, Batch 119 Loss:0.02947341836988926\n",
      "Epoch 49, Batch 120 Loss:0.005989662371575832\n",
      "Epoch 49, Batch 121 Loss:0.07571886479854584\n",
      "Epoch 49, Batch 122 Loss:0.053626421838998795\n",
      "Epoch 49, Batch 123 Loss:0.020873984321951866\n",
      "Epoch 49, Batch 124 Loss:0.017641227692365646\n",
      "Epoch 49, Batch 125 Loss:0.03461453318595886\n",
      "Epoch 49, Batch 126 Loss:0.050746604800224304\n",
      "Epoch 49, Batch 127 Loss:0.023391809314489365\n",
      "Epoch 49, Batch 128 Loss:0.024331938475370407\n",
      "Epoch 49, Batch 129 Loss:0.033322542905807495\n",
      "Epoch 49, Batch 130 Loss:0.012338146567344666\n",
      "Epoch 49, Batch 131 Loss:0.016443971544504166\n",
      "Epoch 49, Batch 132 Loss:0.04998818784952164\n",
      "Epoch 49, Batch 133 Loss:0.037480734288692474\n",
      "Epoch 49, Batch 134 Loss:0.051369331777095795\n",
      "Epoch 49, Batch 135 Loss:0.04115203768014908\n",
      "Epoch 49, Batch 136 Loss:0.029280787333846092\n",
      "Epoch 49, Batch 137 Loss:0.033805616199970245\n",
      "Epoch 49, Batch 138 Loss:0.0804586261510849\n",
      "Epoch 49, Batch 139 Loss:0.034496597945690155\n",
      "Epoch 49, Batch 140 Loss:0.024176500737667084\n",
      "Epoch 49, Batch 141 Loss:0.03753972798585892\n",
      "Epoch 49, Batch 142 Loss:0.008678678423166275\n",
      "Epoch 49, Batch 143 Loss:0.0072949836030602455\n",
      "Epoch 49, Batch 144 Loss:0.0536276251077652\n",
      "Epoch 49, Batch 145 Loss:0.03817707672715187\n",
      "Epoch 49, Batch 146 Loss:0.023011185228824615\n",
      "Epoch 49, Batch 147 Loss:0.038341961801052094\n",
      "Epoch 49, Batch 148 Loss:0.04767158627510071\n",
      "Epoch 49, Batch 149 Loss:0.011195540428161621\n",
      "Epoch 49, Batch 150 Loss:0.03032280132174492\n",
      "Epoch 49, Batch 151 Loss:0.03718205913901329\n",
      "Epoch 49, Batch 152 Loss:0.027526460587978363\n",
      "Epoch 49, Batch 153 Loss:0.033761586993932724\n",
      "Epoch 49, Batch 154 Loss:0.025975143536925316\n",
      "Epoch 49, Batch 155 Loss:0.022245638072490692\n",
      "Epoch 49, Batch 156 Loss:0.02659999206662178\n",
      "Epoch 49, Batch 157 Loss:0.043707944452762604\n",
      "Epoch 49, Batch 158 Loss:0.049846891313791275\n",
      "Epoch 49, Batch 159 Loss:0.03807199373841286\n",
      "Epoch 49, Batch 160 Loss:0.013045499101281166\n",
      "Epoch 49, Batch 161 Loss:0.02597120776772499\n",
      "Epoch 49, Batch 162 Loss:0.01543552614748478\n",
      "Epoch 49, Batch 163 Loss:0.027232740074396133\n",
      "Epoch 49, Batch 164 Loss:0.04855829104781151\n",
      "Epoch 49, Batch 165 Loss:0.01693483255803585\n",
      "Epoch 49, Batch 166 Loss:0.017929499968886375\n",
      "Epoch 49, Batch 167 Loss:0.01308102160692215\n",
      "Epoch 49, Batch 168 Loss:0.02771703526377678\n",
      "Epoch 49, Batch 169 Loss:0.02665937878191471\n",
      "Epoch 49, Batch 170 Loss:0.03359717130661011\n",
      "Epoch 49, Batch 171 Loss:0.020045574754476547\n",
      "Epoch 49, Batch 172 Loss:0.008362296968698502\n",
      "Epoch 49, Batch 173 Loss:0.026354579254984856\n",
      "Epoch 49, Batch 174 Loss:0.005919803865253925\n",
      "Epoch 49, Batch 175 Loss:0.017098870128393173\n",
      "Epoch 49, Batch 176 Loss:0.015839921310544014\n",
      "Epoch 49, Batch 177 Loss:0.016868693754076958\n",
      "Epoch 49, Batch 178 Loss:0.029400356113910675\n",
      "Epoch 49, Batch 179 Loss:0.014879955910146236\n",
      "Epoch 49, Batch 180 Loss:0.017673827707767487\n",
      "Epoch 49, Batch 181 Loss:0.012186587788164616\n",
      "Epoch 49, Batch 182 Loss:0.038236141204833984\n",
      "Epoch 49, Batch 183 Loss:0.03136488422751427\n",
      "Epoch 49, Batch 184 Loss:0.007181756664067507\n",
      "Epoch 49, Batch 185 Loss:0.022881420329213142\n",
      "Epoch 49, Batch 186 Loss:0.05834514647722244\n",
      "Epoch 49, Batch 187 Loss:0.01647859439253807\n",
      "Epoch 49, Batch 188 Loss:0.028850505128502846\n",
      "Epoch 49, Batch 189 Loss:0.014697670936584473\n",
      "Epoch 49, Batch 190 Loss:0.01931106671690941\n",
      "Epoch 49, Batch 191 Loss:0.04895105957984924\n",
      "Epoch 49, Batch 192 Loss:0.02645636722445488\n",
      "Epoch 49, Batch 193 Loss:0.03575746715068817\n",
      "Epoch 49, Batch 194 Loss:0.0056281425058841705\n",
      "Epoch 49, Batch 195 Loss:0.028501294553279877\n",
      "Epoch 49, Batch 196 Loss:0.023348204791545868\n",
      "Epoch 49, Batch 197 Loss:0.039931222796440125\n",
      "Epoch 49, Batch 198 Loss:0.04033651947975159\n",
      "Epoch 49, Batch 199 Loss:0.027994772419333458\n",
      "Epoch 49, Batch 200 Loss:0.0382612906396389\n",
      "Epoch 49, Batch 201 Loss:0.05180307850241661\n",
      "Epoch 49, Batch 202 Loss:0.02503874897956848\n",
      "Epoch 49, Batch 203 Loss:0.02798403799533844\n",
      "Epoch 49, Batch 204 Loss:0.029004760086536407\n",
      "Epoch 49, Batch 205 Loss:0.027878567576408386\n",
      "Epoch 49, Batch 206 Loss:0.027564438059926033\n",
      "Epoch 49, Batch 207 Loss:0.06309880316257477\n",
      "Epoch 49, Batch 208 Loss:0.01724742352962494\n",
      "Epoch 49, Batch 209 Loss:0.02571139670908451\n",
      "Epoch 49, Batch 210 Loss:0.03126220405101776\n",
      "Epoch 49, Batch 211 Loss:0.009745280258357525\n",
      "Epoch 49, Batch 212 Loss:0.018138080835342407\n",
      "Epoch 49, Batch 213 Loss:0.014482978731393814\n",
      "Epoch 49, Batch 214 Loss:0.022637754678726196\n",
      "Epoch 49, Batch 215 Loss:0.04165752977132797\n",
      "Epoch 49, Batch 216 Loss:0.013467744924128056\n",
      "Epoch 49, Batch 217 Loss:0.021997831761837006\n",
      "Epoch 49, Batch 218 Loss:0.01868625171482563\n",
      "Epoch 49, Batch 219 Loss:0.050001680850982666\n",
      "Epoch 49, Batch 220 Loss:0.05191236734390259\n",
      "Epoch 49, Batch 221 Loss:0.04444459080696106\n",
      "Epoch 49, Batch 222 Loss:0.01789579913020134\n",
      "Epoch 49, Batch 223 Loss:0.0351705439388752\n",
      "Epoch 49, Batch 224 Loss:0.018468841910362244\n",
      "Epoch 49, Batch 225 Loss:0.028554735705256462\n",
      "Epoch 49, Batch 226 Loss:0.040136970579624176\n",
      "Epoch 49, Batch 227 Loss:0.012402428314089775\n",
      "Epoch 49, Batch 228 Loss:0.06242920085787773\n",
      "Epoch 49, Batch 229 Loss:0.05311612784862518\n",
      "Epoch 49, Batch 230 Loss:0.06970122456550598\n",
      "Epoch 49, Batch 231 Loss:0.019726483151316643\n",
      "Epoch 49, Batch 232 Loss:0.020311493426561356\n",
      "Epoch 49, Batch 233 Loss:0.0152285136282444\n",
      "Loss in this Epoch is: 1.52285136282 %\n",
      "Accuracy in this Epoch is: 88.4599983692 %\n",
      "Epoch 50, Batch 0 Loss:0.0078887939453125\n",
      "Epoch 50, Batch 1 Loss:0.030231419950723648\n",
      "Epoch 50, Batch 2 Loss:0.02169906534254551\n",
      "Epoch 50, Batch 3 Loss:0.020655538886785507\n",
      "Epoch 50, Batch 4 Loss:0.026720235124230385\n",
      "Epoch 50, Batch 5 Loss:0.014306457713246346\n",
      "Epoch 50, Batch 6 Loss:0.03412652015686035\n",
      "Epoch 50, Batch 7 Loss:0.013491329737007618\n",
      "Epoch 50, Batch 8 Loss:0.03503282368183136\n",
      "Epoch 50, Batch 9 Loss:0.00905562099069357\n",
      "Epoch 50, Batch 10 Loss:0.010767273604869843\n",
      "Epoch 50, Batch 11 Loss:0.01530471257865429\n",
      "Epoch 50, Batch 12 Loss:0.0110159358009696\n",
      "Epoch 50, Batch 13 Loss:0.011761466972529888\n",
      "Epoch 50, Batch 14 Loss:0.006524026393890381\n",
      "Epoch 50, Batch 15 Loss:0.01875184290111065\n",
      "Epoch 50, Batch 16 Loss:0.011935116723179817\n",
      "Epoch 50, Batch 17 Loss:0.012609593570232391\n",
      "Epoch 50, Batch 18 Loss:0.021641375496983528\n",
      "Epoch 50, Batch 19 Loss:0.010397053323686123\n",
      "Epoch 50, Batch 20 Loss:0.016087569296360016\n",
      "Epoch 50, Batch 21 Loss:0.006857944652438164\n",
      "Epoch 50, Batch 22 Loss:0.03406672924757004\n",
      "Epoch 50, Batch 23 Loss:0.02731628715991974\n",
      "Epoch 50, Batch 24 Loss:0.011653471738100052\n",
      "Epoch 50, Batch 25 Loss:0.013281095772981644\n",
      "Epoch 50, Batch 26 Loss:0.008431044407188892\n",
      "Epoch 50, Batch 27 Loss:0.014460541307926178\n",
      "Epoch 50, Batch 28 Loss:0.012770491652190685\n",
      "Epoch 50, Batch 29 Loss:0.024305284023284912\n",
      "Epoch 50, Batch 30 Loss:0.023371320217847824\n",
      "Epoch 50, Batch 31 Loss:0.01918966881930828\n",
      "Epoch 50, Batch 32 Loss:0.0087828878313303\n",
      "Epoch 50, Batch 33 Loss:0.027966151013970375\n",
      "Epoch 50, Batch 34 Loss:0.007136743050068617\n",
      "Epoch 50, Batch 35 Loss:0.007900245487689972\n",
      "Epoch 50, Batch 36 Loss:0.00814042892307043\n",
      "Epoch 50, Batch 37 Loss:0.014192590489983559\n",
      "Epoch 50, Batch 38 Loss:0.012569474056363106\n",
      "Epoch 50, Batch 39 Loss:0.00915468204766512\n",
      "Epoch 50, Batch 40 Loss:0.005213269032537937\n",
      "Epoch 50, Batch 41 Loss:0.012554008513689041\n",
      "Epoch 50, Batch 42 Loss:0.010448171757161617\n",
      "Epoch 50, Batch 43 Loss:0.006855685263872147\n",
      "Epoch 50, Batch 44 Loss:0.016339097172021866\n",
      "Epoch 50, Batch 45 Loss:0.0208763275295496\n",
      "Epoch 50, Batch 46 Loss:0.018888114020228386\n",
      "Epoch 50, Batch 47 Loss:0.02716914191842079\n",
      "Epoch 50, Batch 48 Loss:0.009662994183599949\n",
      "Epoch 50, Batch 49 Loss:0.008995112031698227\n",
      "Epoch 50, Batch 50 Loss:0.03361217677593231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Batch 51 Loss:0.007187305949628353\n",
      "Epoch 50, Batch 52 Loss:0.015671657398343086\n",
      "Epoch 50, Batch 53 Loss:0.03542131930589676\n",
      "Epoch 50, Batch 54 Loss:0.026257773861289024\n",
      "Epoch 50, Batch 55 Loss:0.018064457923173904\n",
      "Epoch 50, Batch 56 Loss:0.009844678454101086\n",
      "Epoch 50, Batch 57 Loss:0.011688507162034512\n",
      "Epoch 50, Batch 58 Loss:0.019858837127685547\n",
      "Epoch 50, Batch 59 Loss:0.01502842828631401\n",
      "Epoch 50, Batch 60 Loss:0.020308615639805794\n",
      "Epoch 50, Batch 61 Loss:0.006711127236485481\n",
      "Epoch 50, Batch 62 Loss:0.049790527671575546\n",
      "Epoch 50, Batch 63 Loss:0.0037715155631303787\n",
      "Epoch 50, Batch 64 Loss:0.015111836604773998\n",
      "Epoch 50, Batch 65 Loss:0.011485266499221325\n",
      "Epoch 50, Batch 66 Loss:0.008951827883720398\n",
      "Epoch 50, Batch 67 Loss:0.027822570875287056\n",
      "Epoch 50, Batch 68 Loss:0.015990056097507477\n",
      "Epoch 50, Batch 69 Loss:0.008403128944337368\n",
      "Epoch 50, Batch 70 Loss:0.019410070031881332\n",
      "Epoch 50, Batch 71 Loss:0.040151432156562805\n",
      "Epoch 50, Batch 72 Loss:0.027850108221173286\n",
      "Epoch 50, Batch 73 Loss:0.015609150752425194\n",
      "Epoch 50, Batch 74 Loss:0.02108047902584076\n",
      "Epoch 50, Batch 75 Loss:0.014307278208434582\n",
      "Epoch 50, Batch 76 Loss:0.018671298399567604\n",
      "Epoch 50, Batch 77 Loss:0.009739120490849018\n",
      "Epoch 50, Batch 78 Loss:0.02154897153377533\n",
      "Epoch 50, Batch 79 Loss:0.0169770997017622\n",
      "Epoch 50, Batch 80 Loss:0.005848521366715431\n",
      "Epoch 50, Batch 81 Loss:0.02160000056028366\n",
      "Epoch 50, Batch 82 Loss:0.014862548559904099\n",
      "Epoch 50, Batch 83 Loss:0.014933940954506397\n",
      "Epoch 50, Batch 84 Loss:0.012614891864359379\n",
      "Epoch 50, Batch 85 Loss:0.015389038249850273\n",
      "Epoch 50, Batch 86 Loss:0.012526053003966808\n",
      "Epoch 50, Batch 87 Loss:0.012902571819722652\n",
      "Epoch 50, Batch 88 Loss:0.005208401475101709\n",
      "Epoch 50, Batch 89 Loss:0.009365200996398926\n",
      "Epoch 50, Batch 90 Loss:0.01306584570556879\n",
      "Epoch 50, Batch 91 Loss:0.005477547645568848\n",
      "Epoch 50, Batch 92 Loss:0.02604653127491474\n",
      "Epoch 50, Batch 93 Loss:0.007954410277307034\n",
      "Epoch 50, Batch 94 Loss:0.03768351674079895\n",
      "Epoch 50, Batch 95 Loss:0.004622078035026789\n",
      "Epoch 50, Batch 96 Loss:0.012441348284482956\n",
      "Epoch 50, Batch 97 Loss:0.022506816312670708\n",
      "Epoch 50, Batch 98 Loss:0.025991270318627357\n",
      "Epoch 50, Batch 99 Loss:0.007220203522592783\n",
      "Epoch 50, Batch 100 Loss:0.023675084114074707\n",
      "Epoch 50, Batch 101 Loss:0.014019789174199104\n",
      "Epoch 50, Batch 102 Loss:0.008088536560535431\n",
      "Epoch 50, Batch 103 Loss:0.016064224764704704\n",
      "Epoch 50, Batch 104 Loss:0.008377578109502792\n",
      "Epoch 50, Batch 105 Loss:0.014973130077123642\n",
      "Epoch 50, Batch 106 Loss:0.011820167303085327\n",
      "Epoch 50, Batch 107 Loss:0.07115622609853745\n",
      "Epoch 50, Batch 108 Loss:0.04258989542722702\n",
      "Epoch 50, Batch 109 Loss:0.005309822503477335\n",
      "Epoch 50, Batch 110 Loss:0.008256688714027405\n",
      "Epoch 50, Batch 111 Loss:0.013675853610038757\n",
      "Epoch 50, Batch 112 Loss:0.018755991011857986\n",
      "Epoch 50, Batch 113 Loss:0.007915997877717018\n",
      "Epoch 50, Batch 114 Loss:0.05699083209037781\n",
      "Epoch 50, Batch 115 Loss:0.006711930967867374\n",
      "Epoch 50, Batch 116 Loss:0.033180732280015945\n",
      "Epoch 50, Batch 117 Loss:0.023735031485557556\n",
      "Epoch 50, Batch 118 Loss:0.021656585857272148\n",
      "Epoch 50, Batch 119 Loss:0.01892024278640747\n",
      "Epoch 50, Batch 120 Loss:0.011867458000779152\n",
      "Epoch 50, Batch 121 Loss:0.04053806513547897\n",
      "Epoch 50, Batch 122 Loss:0.007461164146661758\n",
      "Epoch 50, Batch 123 Loss:0.013534828089177608\n",
      "Epoch 50, Batch 124 Loss:0.009493071585893631\n",
      "Epoch 50, Batch 125 Loss:0.00890553742647171\n",
      "Epoch 50, Batch 126 Loss:0.00984586589038372\n",
      "Epoch 50, Batch 127 Loss:0.01956438459455967\n",
      "Epoch 50, Batch 128 Loss:0.01895993761718273\n",
      "Epoch 50, Batch 129 Loss:0.036939144134521484\n",
      "Epoch 50, Batch 130 Loss:0.01103781908750534\n",
      "Epoch 50, Batch 131 Loss:0.03133736178278923\n",
      "Epoch 50, Batch 132 Loss:0.01909511908888817\n",
      "Epoch 50, Batch 133 Loss:0.0072859241627156734\n",
      "Epoch 50, Batch 134 Loss:0.010543055832386017\n",
      "Epoch 50, Batch 135 Loss:0.014000956900417805\n",
      "Epoch 50, Batch 136 Loss:0.01239580288529396\n",
      "Epoch 50, Batch 137 Loss:0.04466967284679413\n",
      "Epoch 50, Batch 138 Loss:0.029283223673701286\n",
      "Epoch 50, Batch 139 Loss:0.019296210259199142\n",
      "Epoch 50, Batch 140 Loss:0.02596270851790905\n",
      "Epoch 50, Batch 141 Loss:0.017560552805662155\n",
      "Epoch 50, Batch 142 Loss:0.00864325650036335\n",
      "Epoch 50, Batch 143 Loss:0.03454286605119705\n",
      "Epoch 50, Batch 144 Loss:0.014135978184640408\n",
      "Epoch 50, Batch 145 Loss:0.028504371643066406\n",
      "Epoch 50, Batch 146 Loss:0.024747254326939583\n",
      "Epoch 50, Batch 147 Loss:0.039870694279670715\n",
      "Epoch 50, Batch 148 Loss:0.0066234213300049305\n",
      "Epoch 50, Batch 149 Loss:0.008370152674615383\n",
      "Epoch 50, Batch 150 Loss:0.026307906955480576\n",
      "Epoch 50, Batch 151 Loss:0.03699933737516403\n",
      "Epoch 50, Batch 152 Loss:0.04025581479072571\n",
      "Epoch 50, Batch 153 Loss:0.02960704267024994\n",
      "Epoch 50, Batch 154 Loss:0.012354626320302486\n",
      "Epoch 50, Batch 155 Loss:0.023518407717347145\n",
      "Epoch 50, Batch 156 Loss:0.015754666179418564\n",
      "Epoch 50, Batch 157 Loss:0.040174856781959534\n",
      "Epoch 50, Batch 158 Loss:0.01129579171538353\n",
      "Epoch 50, Batch 159 Loss:0.018572675064206123\n",
      "Epoch 50, Batch 160 Loss:0.03489834442734718\n",
      "Epoch 50, Batch 161 Loss:0.019748367369174957\n",
      "Epoch 50, Batch 162 Loss:0.028349827975034714\n",
      "Epoch 50, Batch 163 Loss:0.057749271392822266\n",
      "Epoch 50, Batch 164 Loss:0.009785317815840244\n",
      "Epoch 50, Batch 165 Loss:0.018068697303533554\n",
      "Epoch 50, Batch 166 Loss:0.06519407033920288\n",
      "Epoch 50, Batch 167 Loss:0.018841233104467392\n",
      "Epoch 50, Batch 168 Loss:0.016956493258476257\n",
      "Epoch 50, Batch 169 Loss:0.013782616704702377\n",
      "Epoch 50, Batch 170 Loss:0.0047750030644237995\n",
      "Epoch 50, Batch 171 Loss:0.07720619440078735\n",
      "Epoch 50, Batch 172 Loss:0.04999592900276184\n",
      "Epoch 50, Batch 173 Loss:0.015070617198944092\n",
      "Epoch 50, Batch 174 Loss:0.022704068571329117\n",
      "Epoch 50, Batch 175 Loss:0.04777497425675392\n",
      "Epoch 50, Batch 176 Loss:0.0366540364921093\n",
      "Epoch 50, Batch 177 Loss:0.010956693440675735\n",
      "Epoch 50, Batch 178 Loss:0.014761733822524548\n",
      "Epoch 50, Batch 179 Loss:0.03492489084601402\n",
      "Epoch 50, Batch 180 Loss:0.0371878445148468\n",
      "Epoch 50, Batch 181 Loss:0.032158706337213516\n",
      "Epoch 50, Batch 182 Loss:0.024842681363224983\n",
      "Epoch 50, Batch 183 Loss:0.008424168452620506\n",
      "Epoch 50, Batch 184 Loss:0.011602271348237991\n",
      "Epoch 50, Batch 185 Loss:0.02408282458782196\n",
      "Epoch 50, Batch 186 Loss:0.023114943876862526\n",
      "Epoch 50, Batch 187 Loss:0.019401468336582184\n",
      "Epoch 50, Batch 188 Loss:0.030772985890507698\n",
      "Epoch 50, Batch 189 Loss:0.00894513726234436\n",
      "Epoch 50, Batch 190 Loss:0.016407618299126625\n",
      "Epoch 50, Batch 191 Loss:0.03265971317887306\n",
      "Epoch 50, Batch 192 Loss:0.06920424848794937\n",
      "Epoch 50, Batch 193 Loss:0.020828913897275925\n",
      "Epoch 50, Batch 194 Loss:0.015675993636250496\n",
      "Epoch 50, Batch 195 Loss:0.02019946649670601\n",
      "Epoch 50, Batch 196 Loss:0.02186761423945427\n",
      "Epoch 50, Batch 197 Loss:0.005575108341872692\n",
      "Epoch 50, Batch 198 Loss:0.025593074038624763\n",
      "Epoch 50, Batch 199 Loss:0.009015430696308613\n",
      "Epoch 50, Batch 200 Loss:0.011550736613571644\n",
      "Epoch 50, Batch 201 Loss:0.02968829683959484\n",
      "Epoch 50, Batch 202 Loss:0.03053661808371544\n",
      "Epoch 50, Batch 203 Loss:0.017261400818824768\n",
      "Epoch 50, Batch 204 Loss:0.018831567838788033\n",
      "Epoch 50, Batch 205 Loss:0.017175033688545227\n",
      "Epoch 50, Batch 206 Loss:0.017417188733816147\n",
      "Epoch 50, Batch 207 Loss:0.016714952886104584\n",
      "Epoch 50, Batch 208 Loss:0.015429140999913216\n",
      "Epoch 50, Batch 209 Loss:0.010918332263827324\n",
      "Epoch 50, Batch 210 Loss:0.019970200955867767\n",
      "Epoch 50, Batch 211 Loss:0.051860928535461426\n",
      "Epoch 50, Batch 212 Loss:0.030096568167209625\n",
      "Epoch 50, Batch 213 Loss:0.032121337950229645\n",
      "Epoch 50, Batch 214 Loss:0.027497783303260803\n",
      "Epoch 50, Batch 215 Loss:0.026038870215415955\n",
      "Epoch 50, Batch 216 Loss:0.0321715772151947\n",
      "Epoch 50, Batch 217 Loss:0.009044782258570194\n",
      "Epoch 50, Batch 218 Loss:0.03759419545531273\n",
      "Epoch 50, Batch 219 Loss:0.027521692216396332\n",
      "Epoch 50, Batch 220 Loss:0.04275519400835037\n",
      "Epoch 50, Batch 221 Loss:0.010106208734214306\n",
      "Epoch 50, Batch 222 Loss:0.011106569319963455\n",
      "Epoch 50, Batch 223 Loss:0.04190349578857422\n",
      "Epoch 50, Batch 224 Loss:0.03089980036020279\n",
      "Epoch 50, Batch 225 Loss:0.007427670061588287\n",
      "Epoch 50, Batch 226 Loss:0.03774547576904297\n",
      "Epoch 50, Batch 227 Loss:0.022472750395536423\n",
      "Epoch 50, Batch 228 Loss:0.011079467833042145\n",
      "Epoch 50, Batch 229 Loss:0.05448329448699951\n",
      "Epoch 50, Batch 230 Loss:0.00885497871786356\n",
      "Epoch 50, Batch 231 Loss:0.04750332981348038\n",
      "Epoch 50, Batch 232 Loss:0.02094271220266819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Batch 233 Loss:0.02363332360982895\n",
      "Loss in this Epoch is: 2.36333236098 %\n",
      "Accuracy in this Epoch is: 88.7300014496 %\n",
      "Epoch 51, Batch 0 Loss:0.03616821765899658\n",
      "Epoch 51, Batch 1 Loss:0.02109733410179615\n",
      "Epoch 51, Batch 2 Loss:0.022323844954371452\n",
      "Epoch 51, Batch 3 Loss:0.0077032060362398624\n",
      "Epoch 51, Batch 4 Loss:0.03723590075969696\n",
      "Epoch 51, Batch 5 Loss:0.01707538403570652\n",
      "Epoch 51, Batch 6 Loss:0.015243910253047943\n",
      "Epoch 51, Batch 7 Loss:0.034639567136764526\n",
      "Epoch 51, Batch 8 Loss:0.01471283845603466\n",
      "Epoch 51, Batch 9 Loss:0.018398018553853035\n",
      "Epoch 51, Batch 10 Loss:0.020448971539735794\n",
      "Epoch 51, Batch 11 Loss:0.0115465447306633\n",
      "Epoch 51, Batch 12 Loss:0.046730589121580124\n",
      "Epoch 51, Batch 13 Loss:0.04299040511250496\n",
      "Epoch 51, Batch 14 Loss:0.032268233597278595\n",
      "Epoch 51, Batch 15 Loss:0.01706537790596485\n",
      "Epoch 51, Batch 16 Loss:0.020864907652139664\n",
      "Epoch 51, Batch 17 Loss:0.009307987056672573\n",
      "Epoch 51, Batch 18 Loss:0.04886329174041748\n",
      "Epoch 51, Batch 19 Loss:0.021229511126875877\n",
      "Epoch 51, Batch 20 Loss:0.01985825039446354\n",
      "Epoch 51, Batch 21 Loss:0.018876254558563232\n",
      "Epoch 51, Batch 22 Loss:0.016290083527565002\n",
      "Epoch 51, Batch 23 Loss:0.010277338325977325\n",
      "Epoch 51, Batch 24 Loss:0.02260596491396427\n",
      "Epoch 51, Batch 25 Loss:0.0392957367002964\n",
      "Epoch 51, Batch 26 Loss:0.011979203671216965\n",
      "Epoch 51, Batch 27 Loss:0.003974123392254114\n",
      "Epoch 51, Batch 28 Loss:0.037073321640491486\n",
      "Epoch 51, Batch 29 Loss:0.023082397878170013\n",
      "Epoch 51, Batch 30 Loss:0.021221620962023735\n",
      "Epoch 51, Batch 31 Loss:0.0119353998452425\n",
      "Epoch 51, Batch 32 Loss:0.013264227658510208\n",
      "Epoch 51, Batch 33 Loss:0.016092823818325996\n",
      "Epoch 51, Batch 34 Loss:0.016059864312410355\n",
      "Epoch 51, Batch 35 Loss:0.025382932275533676\n",
      "Epoch 51, Batch 36 Loss:0.017775213345885277\n",
      "Epoch 51, Batch 37 Loss:0.019931679591536522\n",
      "Epoch 51, Batch 38 Loss:0.012702371925115585\n",
      "Epoch 51, Batch 39 Loss:0.010042090900242329\n",
      "Epoch 51, Batch 40 Loss:0.0246313214302063\n",
      "Epoch 51, Batch 41 Loss:0.012425215914845467\n",
      "Epoch 51, Batch 42 Loss:0.022142643108963966\n",
      "Epoch 51, Batch 43 Loss:0.008133779279887676\n",
      "Epoch 51, Batch 44 Loss:0.037344951182603836\n",
      "Epoch 51, Batch 45 Loss:0.013458427973091602\n",
      "Epoch 51, Batch 46 Loss:0.03800326585769653\n",
      "Epoch 51, Batch 47 Loss:0.009080769494175911\n",
      "Epoch 51, Batch 48 Loss:0.012766845524311066\n",
      "Epoch 51, Batch 49 Loss:0.00964230578392744\n",
      "Epoch 51, Batch 50 Loss:0.024605533108115196\n",
      "Epoch 51, Batch 51 Loss:0.011667277663946152\n",
      "Epoch 51, Batch 52 Loss:0.009169618599116802\n",
      "Epoch 51, Batch 53 Loss:0.012447040528059006\n",
      "Epoch 51, Batch 54 Loss:0.013125812634825706\n",
      "Epoch 51, Batch 55 Loss:0.013680463656783104\n",
      "Epoch 51, Batch 56 Loss:0.028650173917412758\n",
      "Epoch 51, Batch 57 Loss:0.01015944592654705\n",
      "Epoch 51, Batch 58 Loss:0.04901527613401413\n",
      "Epoch 51, Batch 59 Loss:0.014201081357896328\n",
      "Epoch 51, Batch 60 Loss:0.011896880343556404\n",
      "Epoch 51, Batch 61 Loss:0.008420026861131191\n",
      "Epoch 51, Batch 62 Loss:0.011721959337592125\n",
      "Epoch 51, Batch 63 Loss:0.005560239311307669\n",
      "Epoch 51, Batch 64 Loss:0.009064316749572754\n",
      "Epoch 51, Batch 65 Loss:0.029057862237095833\n",
      "Epoch 51, Batch 66 Loss:0.00958544947206974\n",
      "Epoch 51, Batch 67 Loss:0.005585247650742531\n",
      "Epoch 51, Batch 68 Loss:0.004668389912694693\n",
      "Epoch 51, Batch 69 Loss:0.0032320022583007812\n",
      "Epoch 51, Batch 70 Loss:0.019378121942281723\n",
      "Epoch 51, Batch 71 Loss:0.010588662698864937\n",
      "Epoch 51, Batch 72 Loss:0.016914963722229004\n",
      "Epoch 51, Batch 73 Loss:0.0069436500780284405\n",
      "Epoch 51, Batch 74 Loss:0.0057333121076226234\n",
      "Epoch 51, Batch 75 Loss:0.022646460682153702\n",
      "Epoch 51, Batch 76 Loss:0.005620275624096394\n",
      "Epoch 51, Batch 77 Loss:0.037256453186273575\n",
      "Epoch 51, Batch 78 Loss:0.04229721799492836\n",
      "Epoch 51, Batch 79 Loss:0.022174859419465065\n",
      "Epoch 51, Batch 80 Loss:0.00989430584013462\n",
      "Epoch 51, Batch 81 Loss:0.014679876156151295\n",
      "Epoch 51, Batch 82 Loss:0.013271044008433819\n",
      "Epoch 51, Batch 83 Loss:0.022049706429243088\n",
      "Epoch 51, Batch 84 Loss:0.030738698318600655\n",
      "Epoch 51, Batch 85 Loss:0.05503535643219948\n",
      "Epoch 51, Batch 86 Loss:0.010221371427178383\n",
      "Epoch 51, Batch 87 Loss:0.009307226166129112\n",
      "Epoch 51, Batch 88 Loss:0.00527550233528018\n",
      "Epoch 51, Batch 89 Loss:0.0227364432066679\n",
      "Epoch 51, Batch 90 Loss:0.03052056021988392\n",
      "Epoch 51, Batch 91 Loss:0.027119392529129982\n",
      "Epoch 51, Batch 92 Loss:0.005094379652291536\n",
      "Epoch 51, Batch 93 Loss:0.02845505066215992\n",
      "Epoch 51, Batch 94 Loss:0.013624323531985283\n",
      "Epoch 51, Batch 95 Loss:0.017320098355412483\n",
      "Epoch 51, Batch 96 Loss:0.03207598999142647\n",
      "Epoch 51, Batch 97 Loss:0.009752989746630192\n",
      "Epoch 51, Batch 98 Loss:0.009513620287179947\n",
      "Epoch 51, Batch 99 Loss:0.06716741621494293\n",
      "Epoch 51, Batch 100 Loss:0.014466701075434685\n",
      "Epoch 51, Batch 101 Loss:0.07553219795227051\n",
      "Epoch 51, Batch 102 Loss:0.008072379976511002\n",
      "Epoch 51, Batch 103 Loss:0.007176693063229322\n",
      "Epoch 51, Batch 104 Loss:0.02538980171084404\n",
      "Epoch 51, Batch 105 Loss:0.016278276219964027\n",
      "Epoch 51, Batch 106 Loss:0.02138577587902546\n",
      "Epoch 51, Batch 107 Loss:0.032623689621686935\n",
      "Epoch 51, Batch 108 Loss:0.02502434328198433\n",
      "Epoch 51, Batch 109 Loss:0.040390774607658386\n",
      "Epoch 51, Batch 110 Loss:0.01216472964733839\n",
      "Epoch 51, Batch 111 Loss:0.015063472092151642\n",
      "Epoch 51, Batch 112 Loss:0.010010886006057262\n",
      "Epoch 51, Batch 113 Loss:0.04259471222758293\n",
      "Epoch 51, Batch 114 Loss:0.03115517646074295\n",
      "Epoch 51, Batch 115 Loss:0.012278137728571892\n",
      "Epoch 51, Batch 116 Loss:0.06325486302375793\n",
      "Epoch 51, Batch 117 Loss:0.06295313686132431\n",
      "Epoch 51, Batch 118 Loss:0.015052825212478638\n",
      "Epoch 51, Batch 119 Loss:0.027640921995043755\n",
      "Epoch 51, Batch 120 Loss:0.05121168866753578\n",
      "Epoch 51, Batch 121 Loss:0.03258500248193741\n",
      "Epoch 51, Batch 122 Loss:0.006027234718203545\n",
      "Epoch 51, Batch 123 Loss:0.03214389085769653\n",
      "Epoch 51, Batch 124 Loss:0.04313945025205612\n",
      "Epoch 51, Batch 125 Loss:0.0536673478782177\n",
      "Epoch 51, Batch 126 Loss:0.013323575258255005\n",
      "Epoch 51, Batch 127 Loss:0.03418470919132233\n",
      "Epoch 51, Batch 128 Loss:0.018839463591575623\n",
      "Epoch 51, Batch 129 Loss:0.04461273178458214\n",
      "Epoch 51, Batch 130 Loss:0.010264594107866287\n",
      "Epoch 51, Batch 131 Loss:0.020877780392766\n",
      "Epoch 51, Batch 132 Loss:0.07028859108686447\n",
      "Epoch 51, Batch 133 Loss:0.021209605038166046\n",
      "Epoch 51, Batch 134 Loss:0.03544297069311142\n",
      "Epoch 51, Batch 135 Loss:0.02746904268860817\n",
      "Epoch 51, Batch 136 Loss:0.025838710367679596\n",
      "Epoch 51, Batch 137 Loss:0.06357552856206894\n",
      "Epoch 51, Batch 138 Loss:0.02104603312909603\n",
      "Epoch 51, Batch 139 Loss:0.038618456572294235\n",
      "Epoch 51, Batch 140 Loss:0.04088272899389267\n",
      "Epoch 51, Batch 141 Loss:0.01358255185186863\n",
      "Epoch 51, Batch 142 Loss:0.01771998405456543\n",
      "Epoch 51, Batch 143 Loss:0.008268339559435844\n",
      "Epoch 51, Batch 144 Loss:0.01750044897198677\n",
      "Epoch 51, Batch 145 Loss:0.008787759579718113\n",
      "Epoch 51, Batch 146 Loss:0.03477958217263222\n",
      "Epoch 51, Batch 147 Loss:0.03628930449485779\n",
      "Epoch 51, Batch 148 Loss:0.035492394119501114\n",
      "Epoch 51, Batch 149 Loss:0.0322750024497509\n",
      "Epoch 51, Batch 150 Loss:0.04035908728837967\n",
      "Epoch 51, Batch 151 Loss:0.01667412370443344\n",
      "Epoch 51, Batch 152 Loss:0.02233739197254181\n",
      "Epoch 51, Batch 153 Loss:0.006362439598888159\n",
      "Epoch 51, Batch 154 Loss:0.01993047446012497\n",
      "Epoch 51, Batch 155 Loss:0.024316128343343735\n",
      "Epoch 51, Batch 156 Loss:0.007105846889317036\n",
      "Epoch 51, Batch 157 Loss:0.010597039014101028\n",
      "Epoch 51, Batch 158 Loss:0.015601299703121185\n",
      "Epoch 51, Batch 159 Loss:0.02962918020784855\n",
      "Epoch 51, Batch 160 Loss:0.006640627980232239\n",
      "Epoch 51, Batch 161 Loss:0.013455168344080448\n",
      "Epoch 51, Batch 162 Loss:0.015613383613526821\n",
      "Epoch 51, Batch 163 Loss:0.01373850554227829\n",
      "Epoch 51, Batch 164 Loss:0.006083228159695864\n",
      "Epoch 51, Batch 165 Loss:0.016647741198539734\n",
      "Epoch 51, Batch 166 Loss:0.014202428981661797\n",
      "Epoch 51, Batch 167 Loss:0.011942815035581589\n",
      "Epoch 51, Batch 168 Loss:0.005115503445267677\n",
      "Epoch 51, Batch 169 Loss:0.009451780468225479\n",
      "Epoch 51, Batch 170 Loss:0.004966260865330696\n",
      "Epoch 51, Batch 171 Loss:0.022786820307374\n",
      "Epoch 51, Batch 172 Loss:0.022697776556015015\n",
      "Epoch 51, Batch 173 Loss:0.0612449049949646\n",
      "Epoch 51, Batch 174 Loss:0.03447680547833443\n",
      "Epoch 51, Batch 175 Loss:0.040239349007606506\n",
      "Epoch 51, Batch 176 Loss:0.027478964999318123\n",
      "Epoch 51, Batch 177 Loss:0.0343911349773407\n",
      "Epoch 51, Batch 178 Loss:0.03708619624376297\n",
      "Epoch 51, Batch 179 Loss:0.007630034349858761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51, Batch 180 Loss:0.01681971363723278\n",
      "Epoch 51, Batch 181 Loss:0.017244897782802582\n",
      "Epoch 51, Batch 182 Loss:0.019962096586823463\n",
      "Epoch 51, Batch 183 Loss:0.02570350095629692\n",
      "Epoch 51, Batch 184 Loss:0.02593236044049263\n",
      "Epoch 51, Batch 185 Loss:0.02292771264910698\n",
      "Epoch 51, Batch 186 Loss:0.012983830645680428\n",
      "Epoch 51, Batch 187 Loss:0.016949068754911423\n",
      "Epoch 51, Batch 188 Loss:0.009908533655107021\n",
      "Epoch 51, Batch 189 Loss:0.040393516421318054\n",
      "Epoch 51, Batch 190 Loss:0.022009309381246567\n",
      "Epoch 51, Batch 191 Loss:0.026563148945569992\n",
      "Epoch 51, Batch 192 Loss:0.012907849624752998\n",
      "Epoch 51, Batch 193 Loss:0.027690956369042397\n",
      "Epoch 51, Batch 194 Loss:0.027859292924404144\n",
      "Epoch 51, Batch 195 Loss:0.021015875041484833\n",
      "Epoch 51, Batch 196 Loss:0.012634135782718658\n",
      "Epoch 51, Batch 197 Loss:0.023285575211048126\n",
      "Epoch 51, Batch 198 Loss:0.03469811752438545\n",
      "Epoch 51, Batch 199 Loss:0.032247282564640045\n",
      "Epoch 51, Batch 200 Loss:0.017348438501358032\n",
      "Epoch 51, Batch 201 Loss:0.06540151685476303\n",
      "Epoch 51, Batch 202 Loss:0.033008869737386703\n",
      "Epoch 51, Batch 203 Loss:0.029094241559505463\n",
      "Epoch 51, Batch 204 Loss:0.027695629745721817\n",
      "Epoch 51, Batch 205 Loss:0.011427285149693489\n",
      "Epoch 51, Batch 206 Loss:0.018210576847195625\n",
      "Epoch 51, Batch 207 Loss:0.03764132037758827\n",
      "Epoch 51, Batch 208 Loss:0.027754563838243484\n",
      "Epoch 51, Batch 209 Loss:0.01855016127228737\n",
      "Epoch 51, Batch 210 Loss:0.025900982320308685\n",
      "Epoch 51, Batch 211 Loss:0.092365562915802\n",
      "Epoch 51, Batch 212 Loss:0.015312579460442066\n",
      "Epoch 51, Batch 213 Loss:0.02224157005548477\n",
      "Epoch 51, Batch 214 Loss:0.026579078286886215\n",
      "Epoch 51, Batch 215 Loss:0.006203445140272379\n",
      "Epoch 51, Batch 216 Loss:0.013516414910554886\n",
      "Epoch 51, Batch 217 Loss:0.016575369983911514\n",
      "Epoch 51, Batch 218 Loss:0.02298101969063282\n",
      "Epoch 51, Batch 219 Loss:0.04932919144630432\n",
      "Epoch 51, Batch 220 Loss:0.018191101029515266\n",
      "Epoch 51, Batch 221 Loss:0.03953962400555611\n",
      "Epoch 51, Batch 222 Loss:0.01254917774349451\n",
      "Epoch 51, Batch 223 Loss:0.014531347900629044\n",
      "Epoch 51, Batch 224 Loss:0.0499684102833271\n",
      "Epoch 51, Batch 225 Loss:0.049310654401779175\n",
      "Epoch 51, Batch 226 Loss:0.056219376623630524\n",
      "Epoch 51, Batch 227 Loss:0.024882860481739044\n",
      "Epoch 51, Batch 228 Loss:0.009029651060700417\n",
      "Epoch 51, Batch 229 Loss:0.011766880750656128\n",
      "Epoch 51, Batch 230 Loss:0.02192986197769642\n",
      "Epoch 51, Batch 231 Loss:0.045853570103645325\n",
      "Epoch 51, Batch 232 Loss:0.012984117493033409\n",
      "Epoch 51, Batch 233 Loss:0.01280254777520895\n",
      "Loss in this Epoch is: 1.28025477752 %\n",
      "Accuracy in this Epoch is: 88.7799978256 %\n",
      "Epoch 52, Batch 0 Loss:0.013354628346860409\n",
      "Epoch 52, Batch 1 Loss:0.030073042958974838\n",
      "Epoch 52, Batch 2 Loss:0.02922810986638069\n",
      "Epoch 52, Batch 3 Loss:0.019086861982941628\n",
      "Epoch 52, Batch 4 Loss:0.012149924412369728\n",
      "Epoch 52, Batch 5 Loss:0.031165290623903275\n",
      "Epoch 52, Batch 6 Loss:0.010104055516421795\n",
      "Epoch 52, Batch 7 Loss:0.018244989216327667\n",
      "Epoch 52, Batch 8 Loss:0.018871348351240158\n",
      "Epoch 52, Batch 9 Loss:0.01891501247882843\n",
      "Epoch 52, Batch 10 Loss:0.043011680245399475\n",
      "Epoch 52, Batch 11 Loss:0.022018762305378914\n",
      "Epoch 52, Batch 12 Loss:0.02335551381111145\n",
      "Epoch 52, Batch 13 Loss:0.014802920632064342\n",
      "Epoch 52, Batch 14 Loss:0.031470607966184616\n",
      "Epoch 52, Batch 15 Loss:0.01048754807561636\n",
      "Epoch 52, Batch 16 Loss:0.022803964093327522\n",
      "Epoch 52, Batch 17 Loss:0.013683432713150978\n",
      "Epoch 52, Batch 18 Loss:0.02054595574736595\n",
      "Epoch 52, Batch 19 Loss:0.010264819487929344\n",
      "Epoch 52, Batch 20 Loss:0.026586612686514854\n",
      "Epoch 52, Batch 21 Loss:0.03160984441637993\n",
      "Epoch 52, Batch 22 Loss:0.021881738677620888\n",
      "Epoch 52, Batch 23 Loss:0.01855001598596573\n",
      "Epoch 52, Batch 24 Loss:0.024776890873908997\n",
      "Epoch 52, Batch 25 Loss:0.00726196588948369\n",
      "Epoch 52, Batch 26 Loss:0.008414728567004204\n",
      "Epoch 52, Batch 27 Loss:0.020478181540966034\n",
      "Epoch 52, Batch 28 Loss:0.01345743890851736\n",
      "Epoch 52, Batch 29 Loss:0.009300988167524338\n",
      "Epoch 52, Batch 30 Loss:0.01016234327107668\n",
      "Epoch 52, Batch 31 Loss:0.00963196437805891\n",
      "Epoch 52, Batch 32 Loss:0.008902708999812603\n",
      "Epoch 52, Batch 33 Loss:0.0070386226288974285\n",
      "Epoch 52, Batch 34 Loss:0.03533661738038063\n",
      "Epoch 52, Batch 35 Loss:0.0049382164143025875\n",
      "Epoch 52, Batch 36 Loss:0.006101483013480902\n",
      "Epoch 52, Batch 37 Loss:0.010589012876152992\n",
      "Epoch 52, Batch 38 Loss:0.010195933282375336\n",
      "Epoch 52, Batch 39 Loss:0.00603401567786932\n",
      "Epoch 52, Batch 40 Loss:0.016752207651734352\n",
      "Epoch 52, Batch 41 Loss:0.00944122951477766\n",
      "Epoch 52, Batch 42 Loss:0.0034218139480799437\n",
      "Epoch 52, Batch 43 Loss:0.006130557972937822\n",
      "Epoch 52, Batch 44 Loss:0.017209263518452644\n",
      "Epoch 52, Batch 45 Loss:0.006490474101155996\n",
      "Epoch 52, Batch 46 Loss:0.012365704402327538\n",
      "Epoch 52, Batch 47 Loss:0.010733198374509811\n",
      "Epoch 52, Batch 48 Loss:0.016651974990963936\n",
      "Epoch 52, Batch 49 Loss:0.02791403979063034\n",
      "Epoch 52, Batch 50 Loss:0.02118724398314953\n",
      "Epoch 52, Batch 51 Loss:0.019884636625647545\n",
      "Epoch 52, Batch 52 Loss:0.01551937498152256\n",
      "Epoch 52, Batch 53 Loss:0.005593701731413603\n",
      "Epoch 52, Batch 54 Loss:0.02248437888920307\n",
      "Epoch 52, Batch 55 Loss:0.008140862919390202\n",
      "Epoch 52, Batch 56 Loss:0.013665989972651005\n",
      "Epoch 52, Batch 57 Loss:0.019112979993224144\n",
      "Epoch 52, Batch 58 Loss:0.03133154660463333\n",
      "Epoch 52, Batch 59 Loss:0.004633596166968346\n",
      "Epoch 52, Batch 60 Loss:0.016598712652921677\n",
      "Epoch 52, Batch 61 Loss:0.0030656044837087393\n",
      "Epoch 52, Batch 62 Loss:0.060654427856206894\n",
      "Epoch 52, Batch 63 Loss:0.006709956098347902\n",
      "Epoch 52, Batch 64 Loss:0.008707850240170956\n",
      "Epoch 52, Batch 65 Loss:0.008764614351093769\n",
      "Epoch 52, Batch 66 Loss:0.005322964861989021\n",
      "Epoch 52, Batch 67 Loss:0.0164297204464674\n",
      "Epoch 52, Batch 68 Loss:0.008260482922196388\n",
      "Epoch 52, Batch 69 Loss:0.021219072863459587\n",
      "Epoch 52, Batch 70 Loss:0.018555741757154465\n",
      "Epoch 52, Batch 71 Loss:0.028488874435424805\n",
      "Epoch 52, Batch 72 Loss:0.007414829917252064\n",
      "Epoch 52, Batch 73 Loss:0.069875068962574\n",
      "Epoch 52, Batch 74 Loss:0.0461135171353817\n",
      "Epoch 52, Batch 75 Loss:0.0148753821849823\n",
      "Epoch 52, Batch 76 Loss:0.035090599209070206\n",
      "Epoch 52, Batch 77 Loss:0.012686284258961678\n",
      "Epoch 52, Batch 78 Loss:0.010559488087892532\n",
      "Epoch 52, Batch 79 Loss:0.005710081662982702\n",
      "Epoch 52, Batch 80 Loss:0.020619194954633713\n",
      "Epoch 52, Batch 81 Loss:0.03754878416657448\n",
      "Epoch 52, Batch 82 Loss:0.039984531700611115\n",
      "Epoch 52, Batch 83 Loss:0.008935987949371338\n",
      "Epoch 52, Batch 84 Loss:0.04464132338762283\n",
      "Epoch 52, Batch 85 Loss:0.04980161786079407\n",
      "Epoch 52, Batch 86 Loss:0.04372202605009079\n",
      "Epoch 52, Batch 87 Loss:0.02791493758559227\n",
      "Epoch 52, Batch 88 Loss:0.010692060925066471\n",
      "Epoch 52, Batch 89 Loss:0.017299655824899673\n",
      "Epoch 52, Batch 90 Loss:0.012799723073840141\n",
      "Epoch 52, Batch 91 Loss:0.01173684373497963\n",
      "Epoch 52, Batch 92 Loss:0.013728097081184387\n",
      "Epoch 52, Batch 93 Loss:0.047454312443733215\n",
      "Epoch 52, Batch 94 Loss:0.02338661439716816\n",
      "Epoch 52, Batch 95 Loss:0.028044123202562332\n",
      "Epoch 52, Batch 96 Loss:0.03960436210036278\n",
      "Epoch 52, Batch 97 Loss:0.00639755092561245\n",
      "Epoch 52, Batch 98 Loss:0.02480846270918846\n",
      "Epoch 52, Batch 99 Loss:0.0462365448474884\n",
      "Epoch 52, Batch 100 Loss:0.01688038930296898\n",
      "Epoch 52, Batch 101 Loss:0.048216596245765686\n",
      "Epoch 52, Batch 102 Loss:0.030736079439520836\n",
      "Epoch 52, Batch 103 Loss:0.012955762445926666\n",
      "Epoch 52, Batch 104 Loss:0.09028718620538712\n",
      "Epoch 52, Batch 105 Loss:0.057723745703697205\n",
      "Epoch 52, Batch 106 Loss:0.015300720930099487\n",
      "Epoch 52, Batch 107 Loss:0.03202679753303528\n",
      "Epoch 52, Batch 108 Loss:0.049493208527565\n",
      "Epoch 52, Batch 109 Loss:0.0364224947988987\n",
      "Epoch 52, Batch 110 Loss:0.08453354239463806\n",
      "Epoch 52, Batch 111 Loss:0.016756491735577583\n",
      "Epoch 52, Batch 112 Loss:0.01292799785733223\n",
      "Epoch 52, Batch 113 Loss:0.0601007416844368\n",
      "Epoch 52, Batch 114 Loss:0.020588785409927368\n",
      "Epoch 52, Batch 115 Loss:0.024086477234959602\n",
      "Epoch 52, Batch 116 Loss:0.016471777111291885\n",
      "Epoch 52, Batch 117 Loss:0.026421919465065002\n",
      "Epoch 52, Batch 118 Loss:0.03132862597703934\n",
      "Epoch 52, Batch 119 Loss:0.020549897104501724\n",
      "Epoch 52, Batch 120 Loss:0.009689703583717346\n",
      "Epoch 52, Batch 121 Loss:0.03741846978664398\n",
      "Epoch 52, Batch 122 Loss:0.052348870784044266\n",
      "Epoch 52, Batch 123 Loss:0.04399595782160759\n",
      "Epoch 52, Batch 124 Loss:0.04722553491592407\n",
      "Epoch 52, Batch 125 Loss:0.036537475883960724\n",
      "Epoch 52, Batch 126 Loss:0.01742880791425705\n",
      "Epoch 52, Batch 127 Loss:0.021120477467775345\n",
      "Epoch 52, Batch 128 Loss:0.021269403398036957\n",
      "Epoch 52, Batch 129 Loss:0.04484066739678383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52, Batch 130 Loss:0.042382240295410156\n",
      "Epoch 52, Batch 131 Loss:0.02448597364127636\n",
      "Epoch 52, Batch 132 Loss:0.02549501694738865\n",
      "Epoch 52, Batch 133 Loss:0.013414137065410614\n",
      "Epoch 52, Batch 134 Loss:0.031224915757775307\n",
      "Epoch 52, Batch 135 Loss:0.011249306611716747\n",
      "Epoch 52, Batch 136 Loss:0.03715212643146515\n",
      "Epoch 52, Batch 137 Loss:0.023786619305610657\n",
      "Epoch 52, Batch 138 Loss:0.023592043668031693\n",
      "Epoch 52, Batch 139 Loss:0.021412154659628868\n",
      "Epoch 52, Batch 140 Loss:0.036595430225133896\n",
      "Epoch 52, Batch 141 Loss:0.009848902933299541\n",
      "Epoch 52, Batch 142 Loss:0.007635106332600117\n",
      "Epoch 52, Batch 143 Loss:0.021594539284706116\n",
      "Epoch 52, Batch 144 Loss:0.03189500793814659\n",
      "Epoch 52, Batch 145 Loss:0.007943261414766312\n",
      "Epoch 52, Batch 146 Loss:0.03607049956917763\n",
      "Epoch 52, Batch 147 Loss:0.017316749319434166\n",
      "Epoch 52, Batch 148 Loss:0.010038616135716438\n",
      "Epoch 52, Batch 149 Loss:0.013509230688214302\n",
      "Epoch 52, Batch 150 Loss:0.011546989902853966\n",
      "Epoch 52, Batch 151 Loss:0.023205094039440155\n",
      "Epoch 52, Batch 152 Loss:0.04940531402826309\n",
      "Epoch 52, Batch 153 Loss:0.016545893624424934\n",
      "Epoch 52, Batch 154 Loss:0.015593481250107288\n",
      "Epoch 52, Batch 155 Loss:0.034960661083459854\n",
      "Epoch 52, Batch 156 Loss:0.0355626717209816\n",
      "Epoch 52, Batch 157 Loss:0.007966192439198494\n",
      "Epoch 52, Batch 158 Loss:0.016293583437800407\n",
      "Epoch 52, Batch 159 Loss:0.028071023523807526\n",
      "Epoch 52, Batch 160 Loss:0.04033832252025604\n",
      "Epoch 52, Batch 161 Loss:0.011886007152497768\n",
      "Epoch 52, Batch 162 Loss:0.013003677129745483\n",
      "Epoch 52, Batch 163 Loss:0.04344187676906586\n",
      "Epoch 52, Batch 164 Loss:0.034801237285137177\n",
      "Epoch 52, Batch 165 Loss:0.06854034215211868\n",
      "Epoch 52, Batch 166 Loss:0.010854881256818771\n",
      "Epoch 52, Batch 167 Loss:0.009434368461370468\n",
      "Epoch 52, Batch 168 Loss:0.029651932418346405\n",
      "Epoch 52, Batch 169 Loss:0.015478728339076042\n",
      "Epoch 52, Batch 170 Loss:0.010737509466707706\n",
      "Epoch 52, Batch 171 Loss:0.02406071312725544\n",
      "Epoch 52, Batch 172 Loss:0.00848148763179779\n",
      "Epoch 52, Batch 173 Loss:0.018063971772789955\n",
      "Epoch 52, Batch 174 Loss:0.02715086191892624\n",
      "Epoch 52, Batch 175 Loss:0.03427007049322128\n",
      "Epoch 52, Batch 176 Loss:0.028474777936935425\n",
      "Epoch 52, Batch 177 Loss:0.023559125140309334\n",
      "Epoch 52, Batch 178 Loss:0.03358118236064911\n",
      "Epoch 52, Batch 179 Loss:0.019646884873509407\n",
      "Epoch 52, Batch 180 Loss:0.06601469218730927\n",
      "Epoch 52, Batch 181 Loss:0.024202773347496986\n",
      "Epoch 52, Batch 182 Loss:0.035146504640579224\n",
      "Epoch 52, Batch 183 Loss:0.020322248339653015\n",
      "Epoch 52, Batch 184 Loss:0.026657722890377045\n",
      "Epoch 52, Batch 185 Loss:0.019659850746393204\n",
      "Epoch 52, Batch 186 Loss:0.03999019041657448\n",
      "Epoch 52, Batch 187 Loss:0.036147795617580414\n",
      "Epoch 52, Batch 188 Loss:0.02718767151236534\n",
      "Epoch 52, Batch 189 Loss:0.03877178579568863\n",
      "Epoch 52, Batch 190 Loss:0.037453506141901016\n",
      "Epoch 52, Batch 191 Loss:0.02541164681315422\n",
      "Epoch 52, Batch 192 Loss:0.014352218247950077\n",
      "Epoch 52, Batch 193 Loss:0.027938734740018845\n",
      "Epoch 52, Batch 194 Loss:0.034933000802993774\n",
      "Epoch 52, Batch 195 Loss:0.01879059709608555\n",
      "Epoch 52, Batch 196 Loss:0.027375683188438416\n",
      "Epoch 52, Batch 197 Loss:0.029955295845866203\n",
      "Epoch 52, Batch 198 Loss:0.04138275235891342\n",
      "Epoch 52, Batch 199 Loss:0.045435838401317596\n",
      "Epoch 52, Batch 200 Loss:0.013514721766114235\n",
      "Epoch 52, Batch 201 Loss:0.030788583680987358\n",
      "Epoch 52, Batch 202 Loss:0.025228211656212807\n",
      "Epoch 52, Batch 203 Loss:0.03374432772397995\n",
      "Epoch 52, Batch 204 Loss:0.004510205239057541\n",
      "Epoch 52, Batch 205 Loss:0.025153201073408127\n",
      "Epoch 52, Batch 206 Loss:0.01604219153523445\n",
      "Epoch 52, Batch 207 Loss:0.019591379910707474\n",
      "Epoch 52, Batch 208 Loss:0.022051572799682617\n",
      "Epoch 52, Batch 209 Loss:0.013562527485191822\n",
      "Epoch 52, Batch 210 Loss:0.039236314594745636\n",
      "Epoch 52, Batch 211 Loss:0.032851770520210266\n",
      "Epoch 52, Batch 212 Loss:0.027476046234369278\n",
      "Epoch 52, Batch 213 Loss:0.007388122379779816\n",
      "Epoch 52, Batch 214 Loss:0.024920634925365448\n",
      "Epoch 52, Batch 215 Loss:0.017000433057546616\n",
      "Epoch 52, Batch 216 Loss:0.020903069525957108\n",
      "Epoch 52, Batch 217 Loss:0.031524527817964554\n",
      "Epoch 52, Batch 218 Loss:0.05062200129032135\n",
      "Epoch 52, Batch 219 Loss:0.012233540415763855\n",
      "Epoch 52, Batch 220 Loss:0.009710006415843964\n",
      "Epoch 52, Batch 221 Loss:0.00782506912946701\n",
      "Epoch 52, Batch 222 Loss:0.03491701930761337\n",
      "Epoch 52, Batch 223 Loss:0.010592425242066383\n",
      "Epoch 52, Batch 224 Loss:0.00810435600578785\n",
      "Epoch 52, Batch 225 Loss:0.02539568766951561\n",
      "Epoch 52, Batch 226 Loss:0.0063476660288870335\n",
      "Epoch 52, Batch 227 Loss:0.014905592426657677\n",
      "Epoch 52, Batch 228 Loss:0.03515031188726425\n",
      "Epoch 52, Batch 229 Loss:0.005569441244006157\n",
      "Epoch 52, Batch 230 Loss:0.023280717432498932\n",
      "Epoch 52, Batch 231 Loss:0.024702634662389755\n",
      "Epoch 52, Batch 232 Loss:0.03786875680088997\n",
      "Epoch 52, Batch 233 Loss:0.0247032567858696\n",
      "Loss in this Epoch is: 2.47032567859 %\n",
      "Accuracy in this Epoch is: 88.3199989796 %\n",
      "Epoch 53, Batch 0 Loss:0.011057919822633266\n",
      "Epoch 53, Batch 1 Loss:0.008892027661204338\n",
      "Epoch 53, Batch 2 Loss:0.005779057741165161\n",
      "Epoch 53, Batch 3 Loss:0.0277745109051466\n",
      "Epoch 53, Batch 4 Loss:0.026208927854895592\n",
      "Epoch 53, Batch 5 Loss:0.08314944803714752\n",
      "Epoch 53, Batch 6 Loss:0.012417477555572987\n",
      "Epoch 53, Batch 7 Loss:0.010148848406970501\n",
      "Epoch 53, Batch 8 Loss:0.029700275510549545\n",
      "Epoch 53, Batch 9 Loss:0.005009746178984642\n",
      "Epoch 53, Batch 10 Loss:0.015566661022603512\n",
      "Epoch 53, Batch 11 Loss:0.08332057297229767\n",
      "Epoch 53, Batch 12 Loss:0.03640545904636383\n",
      "Epoch 53, Batch 13 Loss:0.010270001366734505\n",
      "Epoch 53, Batch 14 Loss:0.006870783865451813\n",
      "Epoch 53, Batch 15 Loss:0.01277604978531599\n",
      "Epoch 53, Batch 16 Loss:0.010844655334949493\n",
      "Epoch 53, Batch 17 Loss:0.012866612523794174\n",
      "Epoch 53, Batch 18 Loss:0.010753728449344635\n",
      "Epoch 53, Batch 19 Loss:0.01940915547311306\n",
      "Epoch 53, Batch 20 Loss:0.02722232975065708\n",
      "Epoch 53, Batch 21 Loss:0.008037997409701347\n",
      "Epoch 53, Batch 22 Loss:0.027839450165629387\n",
      "Epoch 53, Batch 23 Loss:0.027596846222877502\n",
      "Epoch 53, Batch 24 Loss:0.036632515490055084\n",
      "Epoch 53, Batch 25 Loss:0.024749716743826866\n",
      "Epoch 53, Batch 26 Loss:0.041770629584789276\n",
      "Epoch 53, Batch 27 Loss:0.01563839428126812\n",
      "Epoch 53, Batch 28 Loss:0.01682894118130207\n",
      "Epoch 53, Batch 29 Loss:0.01180368009954691\n",
      "Epoch 53, Batch 30 Loss:0.011470274068415165\n",
      "Epoch 53, Batch 31 Loss:0.008332459256052971\n",
      "Epoch 53, Batch 32 Loss:0.01275609340518713\n",
      "Epoch 53, Batch 33 Loss:0.026817481964826584\n",
      "Epoch 53, Batch 34 Loss:0.011680513620376587\n",
      "Epoch 53, Batch 35 Loss:0.036228299140930176\n",
      "Epoch 53, Batch 36 Loss:0.008798128925263882\n",
      "Epoch 53, Batch 37 Loss:0.016322078183293343\n",
      "Epoch 53, Batch 38 Loss:0.026500364765524864\n",
      "Epoch 53, Batch 39 Loss:0.02055266872048378\n",
      "Epoch 53, Batch 40 Loss:0.012663455680012703\n",
      "Epoch 53, Batch 41 Loss:0.024415602907538414\n",
      "Epoch 53, Batch 42 Loss:0.043307337909936905\n",
      "Epoch 53, Batch 43 Loss:0.01857764460146427\n",
      "Epoch 53, Batch 44 Loss:0.02356312982738018\n",
      "Epoch 53, Batch 45 Loss:0.01647496595978737\n",
      "Epoch 53, Batch 46 Loss:0.018402431160211563\n",
      "Epoch 53, Batch 47 Loss:0.01150716282427311\n",
      "Epoch 53, Batch 48 Loss:0.03646568953990936\n",
      "Epoch 53, Batch 49 Loss:0.039540067315101624\n",
      "Epoch 53, Batch 50 Loss:0.05463031306862831\n",
      "Epoch 53, Batch 51 Loss:0.010660264641046524\n",
      "Epoch 53, Batch 52 Loss:0.012942497618496418\n",
      "Epoch 53, Batch 53 Loss:0.03190821036696434\n",
      "Epoch 53, Batch 54 Loss:0.03089042380452156\n",
      "Epoch 53, Batch 55 Loss:0.019724322482943535\n",
      "Epoch 53, Batch 56 Loss:0.05188870429992676\n",
      "Epoch 53, Batch 57 Loss:0.0460306741297245\n",
      "Epoch 53, Batch 58 Loss:0.04552804306149483\n",
      "Epoch 53, Batch 59 Loss:0.03419401869177818\n",
      "Epoch 53, Batch 60 Loss:0.06657595187425613\n",
      "Epoch 53, Batch 61 Loss:0.03895983099937439\n",
      "Epoch 53, Batch 62 Loss:0.012645774520933628\n",
      "Epoch 53, Batch 63 Loss:0.015217111445963383\n",
      "Epoch 53, Batch 64 Loss:0.015454241074621677\n",
      "Epoch 53, Batch 65 Loss:0.015414991416037083\n",
      "Epoch 53, Batch 66 Loss:0.02323838509619236\n",
      "Epoch 53, Batch 67 Loss:0.019676312804222107\n",
      "Epoch 53, Batch 68 Loss:0.028520653024315834\n",
      "Epoch 53, Batch 69 Loss:0.04844725504517555\n",
      "Epoch 53, Batch 70 Loss:0.017791075631976128\n",
      "Epoch 53, Batch 71 Loss:0.04289816692471504\n",
      "Epoch 53, Batch 72 Loss:0.011208898387849331\n",
      "Epoch 53, Batch 73 Loss:0.015700779855251312\n",
      "Epoch 53, Batch 74 Loss:0.02197658270597458\n",
      "Epoch 53, Batch 75 Loss:0.023832285776734352\n",
      "Epoch 53, Batch 76 Loss:0.05515197291970253\n",
      "Epoch 53, Batch 77 Loss:0.014724274165928364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53, Batch 78 Loss:0.03825514018535614\n",
      "Epoch 53, Batch 79 Loss:0.030337532982230186\n",
      "Epoch 53, Batch 80 Loss:0.016968585550785065\n",
      "Epoch 53, Batch 81 Loss:0.008527890779078007\n",
      "Epoch 53, Batch 82 Loss:0.04845643788576126\n",
      "Epoch 53, Batch 83 Loss:0.01007925346493721\n",
      "Epoch 53, Batch 84 Loss:0.04088102653622627\n",
      "Epoch 53, Batch 85 Loss:0.019272735342383385\n",
      "Epoch 53, Batch 86 Loss:0.0335896871984005\n",
      "Epoch 53, Batch 87 Loss:0.0423341803252697\n",
      "Epoch 53, Batch 88 Loss:0.010211847722530365\n",
      "Epoch 53, Batch 89 Loss:0.039603978395462036\n",
      "Epoch 53, Batch 90 Loss:0.013311238959431648\n",
      "Epoch 53, Batch 91 Loss:0.03238604962825775\n",
      "Epoch 53, Batch 92 Loss:0.018524667248129845\n",
      "Epoch 53, Batch 93 Loss:0.02861080877482891\n",
      "Epoch 53, Batch 94 Loss:0.05285916477441788\n",
      "Epoch 53, Batch 95 Loss:0.02234787866473198\n",
      "Epoch 53, Batch 96 Loss:0.011429301463067532\n",
      "Epoch 53, Batch 97 Loss:0.06100213900208473\n",
      "Epoch 53, Batch 98 Loss:0.06449504941701889\n",
      "Epoch 53, Batch 99 Loss:0.028652695938944817\n",
      "Epoch 53, Batch 100 Loss:0.030936066061258316\n",
      "Epoch 53, Batch 101 Loss:0.052253223955631256\n",
      "Epoch 53, Batch 102 Loss:0.04549138993024826\n",
      "Epoch 53, Batch 103 Loss:0.041569784283638\n",
      "Epoch 53, Batch 104 Loss:0.011742337606847286\n",
      "Epoch 53, Batch 105 Loss:0.07239999622106552\n",
      "Epoch 53, Batch 106 Loss:0.008504752069711685\n",
      "Epoch 53, Batch 107 Loss:0.015620775520801544\n",
      "Epoch 53, Batch 108 Loss:0.011417364701628685\n",
      "Epoch 53, Batch 109 Loss:0.027780592441558838\n",
      "Epoch 53, Batch 110 Loss:0.014368562027812004\n",
      "Epoch 53, Batch 111 Loss:0.01840599626302719\n",
      "Epoch 53, Batch 112 Loss:0.019862238317728043\n",
      "Epoch 53, Batch 113 Loss:0.018682796508073807\n",
      "Epoch 53, Batch 114 Loss:0.033733516931533813\n",
      "Epoch 53, Batch 115 Loss:0.027195988222956657\n",
      "Epoch 53, Batch 116 Loss:0.0344347320497036\n",
      "Epoch 53, Batch 117 Loss:0.04371136054396629\n",
      "Epoch 53, Batch 118 Loss:0.032360538840293884\n",
      "Epoch 53, Batch 119 Loss:0.020231783390045166\n",
      "Epoch 53, Batch 120 Loss:0.033801183104515076\n",
      "Epoch 53, Batch 121 Loss:0.025453785434365273\n",
      "Epoch 53, Batch 122 Loss:0.02482115849852562\n",
      "Epoch 53, Batch 123 Loss:0.01485566608607769\n",
      "Epoch 53, Batch 124 Loss:0.01279025711119175\n",
      "Epoch 53, Batch 125 Loss:0.03468712419271469\n",
      "Epoch 53, Batch 126 Loss:0.026224283501505852\n",
      "Epoch 53, Batch 127 Loss:0.024800997227430344\n",
      "Epoch 53, Batch 128 Loss:0.016927707940340042\n",
      "Epoch 53, Batch 129 Loss:0.029234468936920166\n",
      "Epoch 53, Batch 130 Loss:0.025266829878091812\n",
      "Epoch 53, Batch 131 Loss:0.030727528035640717\n",
      "Epoch 53, Batch 132 Loss:0.017333898693323135\n",
      "Epoch 53, Batch 133 Loss:0.016972988843917847\n",
      "Epoch 53, Batch 134 Loss:0.018790844827890396\n",
      "Epoch 53, Batch 135 Loss:0.019818471744656563\n",
      "Epoch 53, Batch 136 Loss:0.02425733394920826\n",
      "Epoch 53, Batch 137 Loss:0.02674228325486183\n",
      "Epoch 53, Batch 138 Loss:0.014574386179447174\n",
      "Epoch 53, Batch 139 Loss:0.03656916320323944\n",
      "Epoch 53, Batch 140 Loss:0.013049021363258362\n",
      "Epoch 53, Batch 141 Loss:0.0259709432721138\n",
      "Epoch 53, Batch 142 Loss:0.03198429197072983\n",
      "Epoch 53, Batch 143 Loss:0.032134756445884705\n",
      "Epoch 53, Batch 144 Loss:0.02319924160838127\n",
      "Epoch 53, Batch 145 Loss:0.015959810465574265\n",
      "Epoch 53, Batch 146 Loss:0.011613691225647926\n",
      "Epoch 53, Batch 147 Loss:0.006014290265738964\n",
      "Epoch 53, Batch 148 Loss:0.029673544690012932\n",
      "Epoch 53, Batch 149 Loss:0.015231618657708168\n",
      "Epoch 53, Batch 150 Loss:0.017679013311862946\n",
      "Epoch 53, Batch 151 Loss:0.02631690539419651\n",
      "Epoch 53, Batch 152 Loss:0.01566830836236477\n",
      "Epoch 53, Batch 153 Loss:0.03987347334623337\n",
      "Epoch 53, Batch 154 Loss:0.019959840923547745\n",
      "Epoch 53, Batch 155 Loss:0.02197083830833435\n",
      "Epoch 53, Batch 156 Loss:0.018523626029491425\n",
      "Epoch 53, Batch 157 Loss:0.022152118384838104\n",
      "Epoch 53, Batch 158 Loss:0.02440956048667431\n",
      "Epoch 53, Batch 159 Loss:0.08057279139757156\n",
      "Epoch 53, Batch 160 Loss:0.06274928897619247\n",
      "Epoch 53, Batch 161 Loss:0.024869343265891075\n",
      "Epoch 53, Batch 162 Loss:0.021548908203840256\n",
      "Epoch 53, Batch 163 Loss:0.019567567855119705\n",
      "Epoch 53, Batch 164 Loss:0.015885258093476295\n",
      "Epoch 53, Batch 165 Loss:0.03266467899084091\n",
      "Epoch 53, Batch 166 Loss:0.01989973708987236\n",
      "Epoch 53, Batch 167 Loss:0.023691775277256966\n",
      "Epoch 53, Batch 168 Loss:0.08120953291654587\n",
      "Epoch 53, Batch 169 Loss:0.03560991212725639\n",
      "Epoch 53, Batch 170 Loss:0.04101792722940445\n",
      "Epoch 53, Batch 171 Loss:0.015041526407003403\n",
      "Epoch 53, Batch 172 Loss:0.023670736700296402\n",
      "Epoch 53, Batch 173 Loss:0.027380233630537987\n",
      "Epoch 53, Batch 174 Loss:0.04024749994277954\n",
      "Epoch 53, Batch 175 Loss:0.08258158713579178\n",
      "Epoch 53, Batch 176 Loss:0.04178722947835922\n",
      "Epoch 53, Batch 177 Loss:0.007908239029347897\n",
      "Epoch 53, Batch 178 Loss:0.03567812591791153\n",
      "Epoch 53, Batch 179 Loss:0.022227639332413673\n",
      "Epoch 53, Batch 180 Loss:0.02234206534922123\n",
      "Epoch 53, Batch 181 Loss:0.06439222395420074\n",
      "Epoch 53, Batch 182 Loss:0.07224778831005096\n",
      "Epoch 53, Batch 183 Loss:0.05568012595176697\n",
      "Epoch 53, Batch 184 Loss:0.036838505417108536\n",
      "Epoch 53, Batch 185 Loss:0.058513231575489044\n",
      "Epoch 53, Batch 186 Loss:0.019200824201107025\n",
      "Epoch 53, Batch 187 Loss:0.024929728358983994\n",
      "Epoch 53, Batch 188 Loss:0.03381948918104172\n",
      "Epoch 53, Batch 189 Loss:0.03960368409752846\n",
      "Epoch 53, Batch 190 Loss:0.01685873232781887\n",
      "Epoch 53, Batch 191 Loss:0.06786266714334488\n",
      "Epoch 53, Batch 192 Loss:0.0229316595941782\n",
      "Epoch 53, Batch 193 Loss:0.07660280913114548\n",
      "Epoch 53, Batch 194 Loss:0.027803946286439896\n",
      "Epoch 53, Batch 195 Loss:0.055963173508644104\n",
      "Epoch 53, Batch 196 Loss:0.029719866812229156\n",
      "Epoch 53, Batch 197 Loss:0.048258841037750244\n",
      "Epoch 53, Batch 198 Loss:0.05092596635222435\n",
      "Epoch 53, Batch 199 Loss:0.03214816004037857\n",
      "Epoch 53, Batch 200 Loss:0.061254605650901794\n",
      "Epoch 53, Batch 201 Loss:0.02261844277381897\n",
      "Epoch 53, Batch 202 Loss:0.08455131202936172\n",
      "Epoch 53, Batch 203 Loss:0.02460387721657753\n",
      "Epoch 53, Batch 204 Loss:0.04481693357229233\n",
      "Epoch 53, Batch 205 Loss:0.042952172458171844\n",
      "Epoch 53, Batch 206 Loss:0.02839665114879608\n",
      "Epoch 53, Batch 207 Loss:0.06588837504386902\n",
      "Epoch 53, Batch 208 Loss:0.06301552057266235\n",
      "Epoch 53, Batch 209 Loss:0.016569556668400764\n",
      "Epoch 53, Batch 210 Loss:0.0126075129956007\n",
      "Epoch 53, Batch 211 Loss:0.029366958886384964\n",
      "Epoch 53, Batch 212 Loss:0.018134307116270065\n",
      "Epoch 53, Batch 213 Loss:0.03505354002118111\n",
      "Epoch 53, Batch 214 Loss:0.018988071009516716\n",
      "Epoch 53, Batch 215 Loss:0.024083834141492844\n",
      "Epoch 53, Batch 216 Loss:0.03297901153564453\n",
      "Epoch 53, Batch 217 Loss:0.015035741031169891\n",
      "Epoch 53, Batch 218 Loss:0.03039349988102913\n",
      "Epoch 53, Batch 219 Loss:0.01920054480433464\n",
      "Epoch 53, Batch 220 Loss:0.02942984737455845\n",
      "Epoch 53, Batch 221 Loss:0.03174665570259094\n",
      "Epoch 53, Batch 222 Loss:0.012962979264557362\n",
      "Epoch 53, Batch 223 Loss:0.011041903868317604\n",
      "Epoch 53, Batch 224 Loss:0.050061676651239395\n",
      "Epoch 53, Batch 225 Loss:0.01919650100171566\n",
      "Epoch 53, Batch 226 Loss:0.028649426996707916\n",
      "Epoch 53, Batch 227 Loss:0.07990910112857819\n",
      "Epoch 53, Batch 228 Loss:0.012522915378212929\n",
      "Epoch 53, Batch 229 Loss:0.030335240066051483\n",
      "Epoch 53, Batch 230 Loss:0.04085340350866318\n",
      "Epoch 53, Batch 231 Loss:0.02212107740342617\n",
      "Epoch 53, Batch 232 Loss:0.01774561032652855\n",
      "Epoch 53, Batch 233 Loss:0.05373381823301315\n",
      "Loss in this Epoch is: 5.3733818233 %\n",
      "Accuracy in this Epoch is: 88.4299993515 %\n",
      "Epoch 54, Batch 0 Loss:0.03810891509056091\n",
      "Epoch 54, Batch 1 Loss:0.010613411664962769\n",
      "Epoch 54, Batch 2 Loss:0.05810106173157692\n",
      "Epoch 54, Batch 3 Loss:0.013162687420845032\n",
      "Epoch 54, Batch 4 Loss:0.05243606120347977\n",
      "Epoch 54, Batch 5 Loss:0.059805892407894135\n",
      "Epoch 54, Batch 6 Loss:0.010711769573390484\n",
      "Epoch 54, Batch 7 Loss:0.049848079681396484\n",
      "Epoch 54, Batch 8 Loss:0.05059705674648285\n",
      "Epoch 54, Batch 9 Loss:0.031130775809288025\n",
      "Epoch 54, Batch 10 Loss:0.038699500262737274\n",
      "Epoch 54, Batch 11 Loss:0.023284101858735085\n",
      "Epoch 54, Batch 12 Loss:0.04057937115430832\n",
      "Epoch 54, Batch 13 Loss:0.04749168083071709\n",
      "Epoch 54, Batch 14 Loss:0.030859632417559624\n",
      "Epoch 54, Batch 15 Loss:0.03017638996243477\n",
      "Epoch 54, Batch 16 Loss:0.040226664394140244\n",
      "Epoch 54, Batch 17 Loss:0.0526871383190155\n",
      "Epoch 54, Batch 18 Loss:0.05334528163075447\n",
      "Epoch 54, Batch 19 Loss:0.02883184514939785\n",
      "Epoch 54, Batch 20 Loss:0.05527699738740921\n",
      "Epoch 54, Batch 21 Loss:0.057541634887456894\n",
      "Epoch 54, Batch 22 Loss:0.012479099445044994\n",
      "Epoch 54, Batch 23 Loss:0.01604102924466133\n",
      "Epoch 54, Batch 24 Loss:0.006849952507764101\n",
      "Epoch 54, Batch 25 Loss:0.016099782660603523\n",
      "Epoch 54, Batch 26 Loss:0.019626889377832413\n",
      "Epoch 54, Batch 27 Loss:0.034964680671691895\n",
      "Epoch 54, Batch 28 Loss:0.022127147763967514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Batch 29 Loss:0.00698586693033576\n",
      "Epoch 54, Batch 30 Loss:0.010618495754897594\n",
      "Epoch 54, Batch 31 Loss:0.05701297149062157\n",
      "Epoch 54, Batch 32 Loss:0.05305979773402214\n",
      "Epoch 54, Batch 33 Loss:0.020755887031555176\n",
      "Epoch 54, Batch 34 Loss:0.015332973562180996\n",
      "Epoch 54, Batch 35 Loss:0.021160930395126343\n",
      "Epoch 54, Batch 36 Loss:0.0326712466776371\n",
      "Epoch 54, Batch 37 Loss:0.016435066238045692\n",
      "Epoch 54, Batch 38 Loss:0.03534052148461342\n",
      "Epoch 54, Batch 39 Loss:0.039937302470207214\n",
      "Epoch 54, Batch 40 Loss:0.028487442061305046\n",
      "Epoch 54, Batch 41 Loss:0.012042148038744926\n",
      "Epoch 54, Batch 42 Loss:0.00877081323415041\n",
      "Epoch 54, Batch 43 Loss:0.010214199312031269\n",
      "Epoch 54, Batch 44 Loss:0.004761833697557449\n",
      "Epoch 54, Batch 45 Loss:0.026931649073958397\n",
      "Epoch 54, Batch 46 Loss:0.021144025027751923\n",
      "Epoch 54, Batch 47 Loss:0.012350727804005146\n",
      "Epoch 54, Batch 48 Loss:0.033120978623628616\n",
      "Epoch 54, Batch 49 Loss:0.005628124810755253\n",
      "Epoch 54, Batch 50 Loss:0.02010890655219555\n",
      "Epoch 54, Batch 51 Loss:0.026672257110476494\n",
      "Epoch 54, Batch 52 Loss:0.026093803346157074\n",
      "Epoch 54, Batch 53 Loss:0.012081572785973549\n",
      "Epoch 54, Batch 54 Loss:0.025428758934140205\n",
      "Epoch 54, Batch 55 Loss:0.050467029213905334\n",
      "Epoch 54, Batch 56 Loss:0.010608780197799206\n",
      "Epoch 54, Batch 57 Loss:0.0351741798222065\n",
      "Epoch 54, Batch 58 Loss:0.017969993874430656\n",
      "Epoch 54, Batch 59 Loss:0.07973776012659073\n",
      "Epoch 54, Batch 60 Loss:0.015498810447752476\n",
      "Epoch 54, Batch 61 Loss:0.05500868707895279\n",
      "Epoch 54, Batch 62 Loss:0.02879266068339348\n",
      "Epoch 54, Batch 63 Loss:0.04201528802514076\n",
      "Epoch 54, Batch 64 Loss:0.020244015380740166\n",
      "Epoch 54, Batch 65 Loss:0.05154019221663475\n",
      "Epoch 54, Batch 66 Loss:0.02166995033621788\n",
      "Epoch 54, Batch 67 Loss:0.025471411645412445\n",
      "Epoch 54, Batch 68 Loss:0.07469180226325989\n",
      "Epoch 54, Batch 69 Loss:0.05676151067018509\n",
      "Epoch 54, Batch 70 Loss:0.03131071478128433\n",
      "Epoch 54, Batch 71 Loss:0.04189212620258331\n",
      "Epoch 54, Batch 72 Loss:0.02751975879073143\n",
      "Epoch 54, Batch 73 Loss:0.0813559964299202\n",
      "Epoch 54, Batch 74 Loss:0.043346062302589417\n",
      "Epoch 54, Batch 75 Loss:0.019560804590582848\n",
      "Epoch 54, Batch 76 Loss:0.028675151988863945\n",
      "Epoch 54, Batch 77 Loss:0.03898712620139122\n",
      "Epoch 54, Batch 78 Loss:0.0816415473818779\n",
      "Epoch 54, Batch 79 Loss:0.053037241101264954\n",
      "Epoch 54, Batch 80 Loss:0.0855967253446579\n",
      "Epoch 54, Batch 81 Loss:0.02308793179690838\n",
      "Epoch 54, Batch 82 Loss:0.06364013254642487\n",
      "Epoch 54, Batch 83 Loss:0.05075082927942276\n",
      "Epoch 54, Batch 84 Loss:0.02219630591571331\n",
      "Epoch 54, Batch 85 Loss:0.022666731849312782\n",
      "Epoch 54, Batch 86 Loss:0.05205724760890007\n",
      "Epoch 54, Batch 87 Loss:0.044369619339704514\n",
      "Epoch 54, Batch 88 Loss:0.03040078654885292\n",
      "Epoch 54, Batch 89 Loss:0.024471238255500793\n",
      "Epoch 54, Batch 90 Loss:0.015135481022298336\n",
      "Epoch 54, Batch 91 Loss:0.015207001939415932\n",
      "Epoch 54, Batch 92 Loss:0.02247905358672142\n",
      "Epoch 54, Batch 93 Loss:0.02214392088353634\n",
      "Epoch 54, Batch 94 Loss:0.028073932975530624\n",
      "Epoch 54, Batch 95 Loss:0.013651072047650814\n",
      "Epoch 54, Batch 96 Loss:0.03263957425951958\n",
      "Epoch 54, Batch 97 Loss:0.030602149665355682\n",
      "Epoch 54, Batch 98 Loss:0.024059604853391647\n",
      "Epoch 54, Batch 99 Loss:0.026434918865561485\n",
      "Epoch 54, Batch 100 Loss:0.021553903818130493\n",
      "Epoch 54, Batch 101 Loss:0.03966795280575752\n",
      "Epoch 54, Batch 102 Loss:0.016062214970588684\n",
      "Epoch 54, Batch 103 Loss:0.020569853484630585\n",
      "Epoch 54, Batch 104 Loss:0.042739689350128174\n",
      "Epoch 54, Batch 105 Loss:0.04251940920948982\n",
      "Epoch 54, Batch 106 Loss:0.0202333964407444\n",
      "Epoch 54, Batch 107 Loss:0.04748077690601349\n",
      "Epoch 54, Batch 108 Loss:0.019210152328014374\n",
      "Epoch 54, Batch 109 Loss:0.011192431673407555\n",
      "Epoch 54, Batch 110 Loss:0.01952018216252327\n",
      "Epoch 54, Batch 111 Loss:0.05954810976982117\n",
      "Epoch 54, Batch 112 Loss:0.04388146102428436\n",
      "Epoch 54, Batch 113 Loss:0.04783868044614792\n",
      "Epoch 54, Batch 114 Loss:0.056089743971824646\n",
      "Epoch 54, Batch 115 Loss:0.021000251173973083\n",
      "Epoch 54, Batch 116 Loss:0.022713439539074898\n",
      "Epoch 54, Batch 117 Loss:0.04723469167947769\n",
      "Epoch 54, Batch 118 Loss:0.07624495029449463\n",
      "Epoch 54, Batch 119 Loss:0.051726795732975006\n",
      "Epoch 54, Batch 120 Loss:0.021269269287586212\n",
      "Epoch 54, Batch 121 Loss:0.05265434831380844\n",
      "Epoch 54, Batch 122 Loss:0.03472139313817024\n",
      "Epoch 54, Batch 123 Loss:0.063349150121212\n",
      "Epoch 54, Batch 124 Loss:0.05203002318739891\n",
      "Epoch 54, Batch 125 Loss:0.039577268064022064\n",
      "Epoch 54, Batch 126 Loss:0.042391806840896606\n",
      "Epoch 54, Batch 127 Loss:0.00966134387999773\n",
      "Epoch 54, Batch 128 Loss:0.016087781637907028\n",
      "Epoch 54, Batch 129 Loss:0.05701553076505661\n",
      "Epoch 54, Batch 130 Loss:0.024339783936738968\n",
      "Epoch 54, Batch 131 Loss:0.021389108151197433\n",
      "Epoch 54, Batch 132 Loss:0.018279213458299637\n",
      "Epoch 54, Batch 133 Loss:0.020565200597047806\n",
      "Epoch 54, Batch 134 Loss:0.051517777144908905\n",
      "Epoch 54, Batch 135 Loss:0.036166414618492126\n",
      "Epoch 54, Batch 136 Loss:0.023537099361419678\n",
      "Epoch 54, Batch 137 Loss:0.04462084174156189\n",
      "Epoch 54, Batch 138 Loss:0.017933614552021027\n",
      "Epoch 54, Batch 139 Loss:0.033331990242004395\n",
      "Epoch 54, Batch 140 Loss:0.03990750014781952\n",
      "Epoch 54, Batch 141 Loss:0.1210823506116867\n",
      "Epoch 54, Batch 142 Loss:0.02618696168065071\n",
      "Epoch 54, Batch 143 Loss:0.02496342733502388\n",
      "Epoch 54, Batch 144 Loss:0.06304685026407242\n",
      "Epoch 54, Batch 145 Loss:0.04866867512464523\n",
      "Epoch 54, Batch 146 Loss:0.053314730525016785\n",
      "Epoch 54, Batch 147 Loss:0.03629153221845627\n",
      "Epoch 54, Batch 148 Loss:0.04318050295114517\n",
      "Epoch 54, Batch 149 Loss:0.024742495268583298\n",
      "Epoch 54, Batch 150 Loss:0.06438187509775162\n",
      "Epoch 54, Batch 151 Loss:0.05800410360097885\n",
      "Epoch 54, Batch 152 Loss:0.02609310857951641\n",
      "Epoch 54, Batch 153 Loss:0.035514019429683685\n",
      "Epoch 54, Batch 154 Loss:0.018081791698932648\n",
      "Epoch 54, Batch 155 Loss:0.1093095988035202\n",
      "Epoch 54, Batch 156 Loss:0.04736047983169556\n",
      "Epoch 54, Batch 157 Loss:0.06810612976551056\n",
      "Epoch 54, Batch 158 Loss:0.07197188585996628\n",
      "Epoch 54, Batch 159 Loss:0.038993604481220245\n",
      "Epoch 54, Batch 160 Loss:0.08463303744792938\n",
      "Epoch 54, Batch 161 Loss:0.047198232263326645\n",
      "Epoch 54, Batch 162 Loss:0.046935562044382095\n",
      "Epoch 54, Batch 163 Loss:0.09568126499652863\n",
      "Epoch 54, Batch 164 Loss:0.05308854952454567\n",
      "Epoch 54, Batch 165 Loss:0.07456771284341812\n",
      "Epoch 54, Batch 166 Loss:0.032574497163295746\n",
      "Epoch 54, Batch 167 Loss:0.023624274879693985\n",
      "Epoch 54, Batch 168 Loss:0.03221116214990616\n",
      "Epoch 54, Batch 169 Loss:0.05499676242470741\n",
      "Epoch 54, Batch 170 Loss:0.04685774818062782\n",
      "Epoch 54, Batch 171 Loss:0.04780946299433708\n",
      "Epoch 54, Batch 172 Loss:0.08143973350524902\n",
      "Epoch 54, Batch 173 Loss:0.03435254842042923\n",
      "Epoch 54, Batch 174 Loss:0.02056720480322838\n",
      "Epoch 54, Batch 175 Loss:0.0178648941218853\n",
      "Epoch 54, Batch 176 Loss:0.033747751265764236\n",
      "Epoch 54, Batch 177 Loss:0.026539646089076996\n",
      "Epoch 54, Batch 178 Loss:0.03759055957198143\n",
      "Epoch 54, Batch 179 Loss:0.01942550577223301\n",
      "Epoch 54, Batch 180 Loss:0.01731567457318306\n",
      "Epoch 54, Batch 181 Loss:0.04540017619729042\n",
      "Epoch 54, Batch 182 Loss:0.031458690762519836\n",
      "Epoch 54, Batch 183 Loss:0.04607953876256943\n",
      "Epoch 54, Batch 184 Loss:0.03817329928278923\n",
      "Epoch 54, Batch 185 Loss:0.03553846478462219\n",
      "Epoch 54, Batch 186 Loss:0.06045377627015114\n",
      "Epoch 54, Batch 187 Loss:0.07894252240657806\n",
      "Epoch 54, Batch 188 Loss:0.01664065755903721\n",
      "Epoch 54, Batch 189 Loss:0.016365403309464455\n",
      "Epoch 54, Batch 190 Loss:0.06383319199085236\n",
      "Epoch 54, Batch 191 Loss:0.012975240126252174\n",
      "Epoch 54, Batch 192 Loss:0.03592666983604431\n",
      "Epoch 54, Batch 193 Loss:0.023981567472219467\n",
      "Epoch 54, Batch 194 Loss:0.012296162545681\n",
      "Epoch 54, Batch 195 Loss:0.020958878099918365\n",
      "Epoch 54, Batch 196 Loss:0.03413170948624611\n",
      "Epoch 54, Batch 197 Loss:0.035524848848581314\n",
      "Epoch 54, Batch 198 Loss:0.06091266870498657\n",
      "Epoch 54, Batch 199 Loss:0.060578085482120514\n",
      "Epoch 54, Batch 200 Loss:0.018212690949440002\n",
      "Epoch 54, Batch 201 Loss:0.03145357221364975\n",
      "Epoch 54, Batch 202 Loss:0.0443992093205452\n",
      "Epoch 54, Batch 203 Loss:0.03141231834888458\n",
      "Epoch 54, Batch 204 Loss:0.04128012806177139\n",
      "Epoch 54, Batch 205 Loss:0.06731611490249634\n",
      "Epoch 54, Batch 206 Loss:0.03151247650384903\n",
      "Epoch 54, Batch 207 Loss:0.04883546009659767\n",
      "Epoch 54, Batch 208 Loss:0.03612999618053436\n",
      "Epoch 54, Batch 209 Loss:0.01802111230790615\n",
      "Epoch 54, Batch 210 Loss:0.007324254140257835\n",
      "Epoch 54, Batch 211 Loss:0.01600227877497673\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54, Batch 212 Loss:0.0787951722741127\n",
      "Epoch 54, Batch 213 Loss:0.021492304280400276\n",
      "Epoch 54, Batch 214 Loss:0.012893155217170715\n",
      "Epoch 54, Batch 215 Loss:0.034787971526384354\n",
      "Epoch 54, Batch 216 Loss:0.03130052983760834\n",
      "Epoch 54, Batch 217 Loss:0.017554841935634613\n",
      "Epoch 54, Batch 218 Loss:0.011527856811881065\n",
      "Epoch 54, Batch 219 Loss:0.025134211406111717\n",
      "Epoch 54, Batch 220 Loss:0.04669267684221268\n",
      "Epoch 54, Batch 221 Loss:0.023900913074612617\n",
      "Epoch 54, Batch 222 Loss:0.04218317195773125\n",
      "Epoch 54, Batch 223 Loss:0.017070412635803223\n",
      "Epoch 54, Batch 224 Loss:0.0172082781791687\n",
      "Epoch 54, Batch 225 Loss:0.02196415141224861\n",
      "Epoch 54, Batch 226 Loss:0.023075563833117485\n",
      "Epoch 54, Batch 227 Loss:0.03022279217839241\n",
      "Epoch 54, Batch 228 Loss:0.030669258907437325\n",
      "Epoch 54, Batch 229 Loss:0.015328009612858295\n",
      "Epoch 54, Batch 230 Loss:0.03667939454317093\n",
      "Epoch 54, Batch 231 Loss:0.021221283823251724\n",
      "Epoch 54, Batch 232 Loss:0.03740774467587471\n",
      "Epoch 54, Batch 233 Loss:0.02915792167186737\n",
      "Loss in this Epoch is: 2.91579216719 %\n",
      "Accuracy in this Epoch is: 88.3199989796 %\n",
      "Epoch 55, Batch 0 Loss:0.013928401283919811\n",
      "Epoch 55, Batch 1 Loss:0.009110317565500736\n",
      "Epoch 55, Batch 2 Loss:0.02330191433429718\n",
      "Epoch 55, Batch 3 Loss:0.018466223031282425\n",
      "Epoch 55, Batch 4 Loss:0.018393008038401604\n",
      "Epoch 55, Batch 5 Loss:0.02663363702595234\n",
      "Epoch 55, Batch 6 Loss:0.01745510846376419\n",
      "Epoch 55, Batch 7 Loss:0.02745233289897442\n",
      "Epoch 55, Batch 8 Loss:0.02935117296874523\n",
      "Epoch 55, Batch 9 Loss:0.024073516950011253\n",
      "Epoch 55, Batch 10 Loss:0.0512494221329689\n",
      "Epoch 55, Batch 11 Loss:0.03161558136343956\n",
      "Epoch 55, Batch 12 Loss:0.022353362292051315\n",
      "Epoch 55, Batch 13 Loss:0.029480379074811935\n",
      "Epoch 55, Batch 14 Loss:0.05860796198248863\n",
      "Epoch 55, Batch 15 Loss:0.02388022467494011\n",
      "Epoch 55, Batch 16 Loss:0.030744465067982674\n",
      "Epoch 55, Batch 17 Loss:0.03375789895653725\n",
      "Epoch 55, Batch 18 Loss:0.07340693473815918\n",
      "Epoch 55, Batch 19 Loss:0.012478108517825603\n",
      "Epoch 55, Batch 20 Loss:0.021261785179376602\n",
      "Epoch 55, Batch 21 Loss:0.021159198135137558\n",
      "Epoch 55, Batch 22 Loss:0.04004707932472229\n",
      "Epoch 55, Batch 23 Loss:0.03267073258757591\n",
      "Epoch 55, Batch 24 Loss:0.0429166741669178\n",
      "Epoch 55, Batch 25 Loss:0.01778321899473667\n",
      "Epoch 55, Batch 26 Loss:0.028584282845258713\n",
      "Epoch 55, Batch 27 Loss:0.012913761660456657\n",
      "Epoch 55, Batch 28 Loss:0.046073973178863525\n",
      "Epoch 55, Batch 29 Loss:0.022113749757409096\n",
      "Epoch 55, Batch 30 Loss:0.009690839797258377\n",
      "Epoch 55, Batch 31 Loss:0.032229095697402954\n",
      "Epoch 55, Batch 32 Loss:0.018934620544314384\n",
      "Epoch 55, Batch 33 Loss:0.03227618336677551\n",
      "Epoch 55, Batch 34 Loss:0.04176625609397888\n",
      "Epoch 55, Batch 35 Loss:0.04933458939194679\n",
      "Epoch 55, Batch 36 Loss:0.02207563817501068\n",
      "Epoch 55, Batch 37 Loss:0.006711490452289581\n",
      "Epoch 55, Batch 38 Loss:0.03597896173596382\n",
      "Epoch 55, Batch 39 Loss:0.013871236704289913\n",
      "Epoch 55, Batch 40 Loss:0.05086096003651619\n",
      "Epoch 55, Batch 41 Loss:0.017725590616464615\n",
      "Epoch 55, Batch 42 Loss:0.02860485389828682\n",
      "Epoch 55, Batch 43 Loss:0.014783252030611038\n",
      "Epoch 55, Batch 44 Loss:0.012450913898646832\n",
      "Epoch 55, Batch 45 Loss:0.03386982902884483\n",
      "Epoch 55, Batch 46 Loss:0.025116194039583206\n",
      "Epoch 55, Batch 47 Loss:0.01903163641691208\n",
      "Epoch 55, Batch 48 Loss:0.014345305971801281\n",
      "Epoch 55, Batch 49 Loss:0.008069303818047047\n",
      "Epoch 55, Batch 50 Loss:0.02508719079196453\n",
      "Epoch 55, Batch 51 Loss:0.05121034011244774\n",
      "Epoch 55, Batch 52 Loss:0.02246289886534214\n",
      "Epoch 55, Batch 53 Loss:0.02008107118308544\n",
      "Epoch 55, Batch 54 Loss:0.01886999048292637\n",
      "Epoch 55, Batch 55 Loss:0.056966304779052734\n",
      "Epoch 55, Batch 56 Loss:0.042443688958883286\n",
      "Epoch 55, Batch 57 Loss:0.018580736592411995\n",
      "Epoch 55, Batch 58 Loss:0.023611344397068024\n",
      "Epoch 55, Batch 59 Loss:0.03620188683271408\n",
      "Epoch 55, Batch 60 Loss:0.02168229967355728\n",
      "Epoch 55, Batch 61 Loss:0.06683938950300217\n",
      "Epoch 55, Batch 62 Loss:0.01744583249092102\n",
      "Epoch 55, Batch 63 Loss:0.0176209919154644\n",
      "Epoch 55, Batch 64 Loss:0.01178703736513853\n",
      "Epoch 55, Batch 65 Loss:0.008424894884228706\n",
      "Epoch 55, Batch 66 Loss:0.04159263148903847\n",
      "Epoch 55, Batch 67 Loss:0.006434604525566101\n",
      "Epoch 55, Batch 68 Loss:0.021611923351883888\n",
      "Epoch 55, Batch 69 Loss:0.0203086007386446\n",
      "Epoch 55, Batch 70 Loss:0.023535406216979027\n",
      "Epoch 55, Batch 71 Loss:0.018471308052539825\n",
      "Epoch 55, Batch 72 Loss:0.05073423311114311\n",
      "Epoch 55, Batch 73 Loss:0.021189464256167412\n",
      "Epoch 55, Batch 74 Loss:0.008016299456357956\n",
      "Epoch 55, Batch 75 Loss:0.0077267857268452644\n",
      "Epoch 55, Batch 76 Loss:0.0431109294295311\n",
      "Epoch 55, Batch 77 Loss:0.011769413948059082\n",
      "Epoch 55, Batch 78 Loss:0.04781799763441086\n",
      "Epoch 55, Batch 79 Loss:0.023523349314928055\n",
      "Epoch 55, Batch 80 Loss:0.022151688113808632\n",
      "Epoch 55, Batch 81 Loss:0.01353444717824459\n",
      "Epoch 55, Batch 82 Loss:0.011932416819036007\n",
      "Epoch 55, Batch 83 Loss:0.03615149110555649\n",
      "Epoch 55, Batch 84 Loss:0.020795488730072975\n",
      "Epoch 55, Batch 85 Loss:0.01797420158982277\n",
      "Epoch 55, Batch 86 Loss:0.01074058748781681\n",
      "Epoch 55, Batch 87 Loss:0.034550607204437256\n",
      "Epoch 55, Batch 88 Loss:0.03694375231862068\n",
      "Epoch 55, Batch 89 Loss:0.0468575581908226\n",
      "Epoch 55, Batch 90 Loss:0.020086530596017838\n",
      "Epoch 55, Batch 91 Loss:0.027622055262327194\n",
      "Epoch 55, Batch 92 Loss:0.01390397921204567\n",
      "Epoch 55, Batch 93 Loss:0.04249083250761032\n",
      "Epoch 55, Batch 94 Loss:0.06125156208872795\n",
      "Epoch 55, Batch 95 Loss:0.0246207807213068\n",
      "Epoch 55, Batch 96 Loss:0.02008771151304245\n",
      "Epoch 55, Batch 97 Loss:0.012041276320815086\n",
      "Epoch 55, Batch 98 Loss:0.023728204891085625\n",
      "Epoch 55, Batch 99 Loss:0.041303619742393494\n",
      "Epoch 55, Batch 100 Loss:0.04745939373970032\n",
      "Epoch 55, Batch 101 Loss:0.034046467393636703\n",
      "Epoch 55, Batch 102 Loss:0.01714414730668068\n",
      "Epoch 55, Batch 103 Loss:0.010875348001718521\n",
      "Epoch 55, Batch 104 Loss:0.05022972822189331\n",
      "Epoch 55, Batch 105 Loss:0.058101676404476166\n",
      "Epoch 55, Batch 106 Loss:0.041655316948890686\n",
      "Epoch 55, Batch 107 Loss:0.027601812034845352\n",
      "Epoch 55, Batch 108 Loss:0.02692897990345955\n",
      "Epoch 55, Batch 109 Loss:0.03936358913779259\n",
      "Epoch 55, Batch 110 Loss:0.008534738793969154\n",
      "Epoch 55, Batch 111 Loss:0.013086517341434956\n",
      "Epoch 55, Batch 112 Loss:0.03380332142114639\n",
      "Epoch 55, Batch 113 Loss:0.045878127217292786\n",
      "Epoch 55, Batch 114 Loss:0.013680394738912582\n",
      "Epoch 55, Batch 115 Loss:0.010830206796526909\n",
      "Epoch 55, Batch 116 Loss:0.014187397435307503\n",
      "Epoch 55, Batch 117 Loss:0.03502596169710159\n",
      "Epoch 55, Batch 118 Loss:0.03772377222776413\n",
      "Epoch 55, Batch 119 Loss:0.0028202268294990063\n",
      "Epoch 55, Batch 120 Loss:0.04226969927549362\n",
      "Epoch 55, Batch 121 Loss:0.018098387867212296\n",
      "Epoch 55, Batch 122 Loss:0.005911773536354303\n",
      "Epoch 55, Batch 123 Loss:0.007600989658385515\n",
      "Epoch 55, Batch 124 Loss:0.023327160626649857\n",
      "Epoch 55, Batch 125 Loss:0.02174844592809677\n",
      "Epoch 55, Batch 126 Loss:0.03495445474982262\n",
      "Epoch 55, Batch 127 Loss:0.047033265233039856\n",
      "Epoch 55, Batch 128 Loss:0.016908694058656693\n",
      "Epoch 55, Batch 129 Loss:0.03289216756820679\n",
      "Epoch 55, Batch 130 Loss:0.01917109824717045\n",
      "Epoch 55, Batch 131 Loss:0.027594106271862984\n",
      "Epoch 55, Batch 132 Loss:0.055846597999334335\n",
      "Epoch 55, Batch 133 Loss:0.02279689721763134\n",
      "Epoch 55, Batch 134 Loss:0.033741213381290436\n",
      "Epoch 55, Batch 135 Loss:0.009549497626721859\n",
      "Epoch 55, Batch 136 Loss:0.007486130576580763\n",
      "Epoch 55, Batch 137 Loss:0.06323052942752838\n",
      "Epoch 55, Batch 138 Loss:0.007298505865037441\n",
      "Epoch 55, Batch 139 Loss:0.020003391429781914\n",
      "Epoch 55, Batch 140 Loss:0.027349023148417473\n",
      "Epoch 55, Batch 141 Loss:0.02120043709874153\n",
      "Epoch 55, Batch 142 Loss:0.009028787724673748\n",
      "Epoch 55, Batch 143 Loss:0.01830870285630226\n",
      "Epoch 55, Batch 144 Loss:0.004382858984172344\n",
      "Epoch 55, Batch 145 Loss:0.019346538931131363\n",
      "Epoch 55, Batch 146 Loss:0.049091920256614685\n",
      "Epoch 55, Batch 147 Loss:0.011747973971068859\n",
      "Epoch 55, Batch 148 Loss:0.02364957332611084\n",
      "Epoch 55, Batch 149 Loss:0.02941753715276718\n",
      "Epoch 55, Batch 150 Loss:0.05675829201936722\n",
      "Epoch 55, Batch 151 Loss:0.017339471727609634\n",
      "Epoch 55, Batch 152 Loss:0.02590828761458397\n",
      "Epoch 55, Batch 153 Loss:0.009802763350307941\n",
      "Epoch 55, Batch 154 Loss:0.025469306856393814\n",
      "Epoch 55, Batch 155 Loss:0.040346041321754456\n",
      "Epoch 55, Batch 156 Loss:0.024603145197033882\n",
      "Epoch 55, Batch 157 Loss:0.019859448075294495\n",
      "Epoch 55, Batch 158 Loss:0.021867893636226654\n",
      "Epoch 55, Batch 159 Loss:0.03174857795238495\n",
      "Epoch 55, Batch 160 Loss:0.04444650560617447\n",
      "Epoch 55, Batch 161 Loss:0.0451381579041481\n",
      "Epoch 55, Batch 162 Loss:0.03428595885634422\n",
      "Epoch 55, Batch 163 Loss:0.05691181495785713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55, Batch 164 Loss:0.021115228533744812\n",
      "Epoch 55, Batch 165 Loss:0.057922277599573135\n",
      "Epoch 55, Batch 166 Loss:0.025609752163290977\n",
      "Epoch 55, Batch 167 Loss:0.05426385626196861\n",
      "Epoch 55, Batch 168 Loss:0.02492855116724968\n",
      "Epoch 55, Batch 169 Loss:0.030494190752506256\n",
      "Epoch 55, Batch 170 Loss:0.01164647564291954\n",
      "Epoch 55, Batch 171 Loss:0.02747480198740959\n",
      "Epoch 55, Batch 172 Loss:0.03183067589998245\n",
      "Epoch 55, Batch 173 Loss:0.0472898855805397\n",
      "Epoch 55, Batch 174 Loss:0.032695963978767395\n",
      "Epoch 55, Batch 175 Loss:0.05881274864077568\n",
      "Epoch 55, Batch 176 Loss:0.03527342528104782\n",
      "Epoch 55, Batch 177 Loss:0.03893817588686943\n",
      "Epoch 55, Batch 178 Loss:0.015140429139137268\n",
      "Epoch 55, Batch 179 Loss:0.08463600277900696\n",
      "Epoch 55, Batch 180 Loss:0.04954364523291588\n",
      "Epoch 55, Batch 181 Loss:0.04396326094865799\n",
      "Epoch 55, Batch 182 Loss:0.019221104681491852\n",
      "Epoch 55, Batch 183 Loss:0.04750324413180351\n",
      "Epoch 55, Batch 184 Loss:0.013006574474275112\n",
      "Epoch 55, Batch 185 Loss:0.037958383560180664\n",
      "Epoch 55, Batch 186 Loss:0.014791232533752918\n",
      "Epoch 55, Batch 187 Loss:0.04217711091041565\n",
      "Epoch 55, Batch 188 Loss:0.013109504245221615\n",
      "Epoch 55, Batch 189 Loss:0.03440755233168602\n",
      "Epoch 55, Batch 190 Loss:0.030335716903209686\n",
      "Epoch 55, Batch 191 Loss:0.031714748591184616\n",
      "Epoch 55, Batch 192 Loss:0.01653529331088066\n",
      "Epoch 55, Batch 193 Loss:0.018151337280869484\n",
      "Epoch 55, Batch 194 Loss:0.03658699244260788\n",
      "Epoch 55, Batch 195 Loss:0.03137664496898651\n",
      "Epoch 55, Batch 196 Loss:0.03693358972668648\n",
      "Epoch 55, Batch 197 Loss:0.03119669482111931\n",
      "Epoch 55, Batch 198 Loss:0.040852926671504974\n",
      "Epoch 55, Batch 199 Loss:0.030223889276385307\n",
      "Epoch 55, Batch 200 Loss:0.04293946921825409\n",
      "Epoch 55, Batch 201 Loss:0.014141622930765152\n",
      "Epoch 55, Batch 202 Loss:0.0294979065656662\n",
      "Epoch 55, Batch 203 Loss:0.03939322009682655\n",
      "Epoch 55, Batch 204 Loss:0.03605657443404198\n",
      "Epoch 55, Batch 205 Loss:0.03891376778483391\n",
      "Epoch 55, Batch 206 Loss:0.02975875698029995\n",
      "Epoch 55, Batch 207 Loss:0.015715252608060837\n",
      "Epoch 55, Batch 208 Loss:0.02141820825636387\n",
      "Epoch 55, Batch 209 Loss:0.016270218417048454\n",
      "Epoch 55, Batch 210 Loss:0.021329812705516815\n",
      "Epoch 55, Batch 211 Loss:0.010512334294617176\n",
      "Epoch 55, Batch 212 Loss:0.04132933169603348\n",
      "Epoch 55, Batch 213 Loss:0.019670378416776657\n",
      "Epoch 55, Batch 214 Loss:0.02153613232076168\n",
      "Epoch 55, Batch 215 Loss:0.013720712624490261\n",
      "Epoch 55, Batch 216 Loss:0.0151014793664217\n",
      "Epoch 55, Batch 217 Loss:0.03662756830453873\n",
      "Epoch 55, Batch 218 Loss:0.036793291568756104\n",
      "Epoch 55, Batch 219 Loss:0.053628042340278625\n",
      "Epoch 55, Batch 220 Loss:0.02807096391916275\n",
      "Epoch 55, Batch 221 Loss:0.03973538801074028\n",
      "Epoch 55, Batch 222 Loss:0.04023955017328262\n",
      "Epoch 55, Batch 223 Loss:0.030818207189440727\n",
      "Epoch 55, Batch 224 Loss:0.05981632322072983\n",
      "Epoch 55, Batch 225 Loss:0.044661011546850204\n",
      "Epoch 55, Batch 226 Loss:0.04745933413505554\n",
      "Epoch 55, Batch 227 Loss:0.013500476256012917\n",
      "Epoch 55, Batch 228 Loss:0.03425214812159538\n",
      "Epoch 55, Batch 229 Loss:0.030787155032157898\n",
      "Epoch 55, Batch 230 Loss:0.014897623099386692\n",
      "Epoch 55, Batch 231 Loss:0.05700342357158661\n",
      "Epoch 55, Batch 232 Loss:0.02469981089234352\n",
      "Epoch 55, Batch 233 Loss:0.03680494427680969\n",
      "Loss in this Epoch is: 3.68049442768 %\n",
      "Accuracy in this Epoch is: 87.7300024033 %\n",
      "Epoch 56, Batch 0 Loss:0.07622426748275757\n",
      "Epoch 56, Batch 1 Loss:0.015907781198620796\n",
      "Epoch 56, Batch 2 Loss:0.06396454572677612\n",
      "Epoch 56, Batch 3 Loss:0.03248896822333336\n",
      "Epoch 56, Batch 4 Loss:0.0887470617890358\n",
      "Epoch 56, Batch 5 Loss:0.07215558737516403\n",
      "Epoch 56, Batch 6 Loss:0.06911400705575943\n",
      "Epoch 56, Batch 7 Loss:0.028053924441337585\n",
      "Epoch 56, Batch 8 Loss:0.023741833865642548\n",
      "Epoch 56, Batch 9 Loss:0.014600470662117004\n",
      "Epoch 56, Batch 10 Loss:0.030685851350426674\n",
      "Epoch 56, Batch 11 Loss:0.02830769121646881\n",
      "Epoch 56, Batch 12 Loss:0.029064303264021873\n",
      "Epoch 56, Batch 13 Loss:0.04955758899450302\n",
      "Epoch 56, Batch 14 Loss:0.018802253529429436\n",
      "Epoch 56, Batch 15 Loss:0.026691682636737823\n",
      "Epoch 56, Batch 16 Loss:0.04551277682185173\n",
      "Epoch 56, Batch 17 Loss:0.030196351930499077\n",
      "Epoch 56, Batch 18 Loss:0.050536904484033585\n",
      "Epoch 56, Batch 19 Loss:0.03889486938714981\n",
      "Epoch 56, Batch 20 Loss:0.02488779090344906\n",
      "Epoch 56, Batch 21 Loss:0.041414808481931686\n",
      "Epoch 56, Batch 22 Loss:0.021629922091960907\n",
      "Epoch 56, Batch 23 Loss:0.024592244997620583\n",
      "Epoch 56, Batch 24 Loss:0.04668941721320152\n",
      "Epoch 56, Batch 25 Loss:0.021289566531777382\n",
      "Epoch 56, Batch 26 Loss:0.012684715911746025\n",
      "Epoch 56, Batch 27 Loss:0.022131964564323425\n",
      "Epoch 56, Batch 28 Loss:0.04006056487560272\n",
      "Epoch 56, Batch 29 Loss:0.06359119713306427\n",
      "Epoch 56, Batch 30 Loss:0.022435922175645828\n",
      "Epoch 56, Batch 31 Loss:0.01953871175646782\n",
      "Epoch 56, Batch 32 Loss:0.03062961995601654\n",
      "Epoch 56, Batch 33 Loss:0.018838029354810715\n",
      "Epoch 56, Batch 34 Loss:0.027970606461167336\n",
      "Epoch 56, Batch 35 Loss:0.029023587703704834\n",
      "Epoch 56, Batch 36 Loss:0.03283082693815231\n",
      "Epoch 56, Batch 37 Loss:0.029138194397091866\n",
      "Epoch 56, Batch 38 Loss:0.03816249594092369\n",
      "Epoch 56, Batch 39 Loss:0.03221555054187775\n",
      "Epoch 56, Batch 40 Loss:0.018839413300156593\n",
      "Epoch 56, Batch 41 Loss:0.05854646861553192\n",
      "Epoch 56, Batch 42 Loss:0.03340037912130356\n",
      "Epoch 56, Batch 43 Loss:0.015208872966468334\n",
      "Epoch 56, Batch 44 Loss:0.015933524817228317\n",
      "Epoch 56, Batch 45 Loss:0.0080947894603014\n",
      "Epoch 56, Batch 46 Loss:0.05639706924557686\n",
      "Epoch 56, Batch 47 Loss:0.020329130813479424\n",
      "Epoch 56, Batch 48 Loss:0.021920498460531235\n",
      "Epoch 56, Batch 49 Loss:0.009022681973874569\n",
      "Epoch 56, Batch 50 Loss:0.006896669510751963\n",
      "Epoch 56, Batch 51 Loss:0.026213280856609344\n",
      "Epoch 56, Batch 52 Loss:0.019546397030353546\n",
      "Epoch 56, Batch 53 Loss:0.01521679013967514\n",
      "Epoch 56, Batch 54 Loss:0.021396685391664505\n",
      "Epoch 56, Batch 55 Loss:0.009080481715500355\n",
      "Epoch 56, Batch 56 Loss:0.012338664382696152\n",
      "Epoch 56, Batch 57 Loss:0.012624491937458515\n",
      "Epoch 56, Batch 58 Loss:0.013576787896454334\n",
      "Epoch 56, Batch 59 Loss:0.029451001435518265\n",
      "Epoch 56, Batch 60 Loss:0.01834726519882679\n",
      "Epoch 56, Batch 61 Loss:0.011615701019763947\n",
      "Epoch 56, Batch 62 Loss:0.0231378935277462\n",
      "Epoch 56, Batch 63 Loss:0.007634170353412628\n",
      "Epoch 56, Batch 64 Loss:0.006120338570326567\n",
      "Epoch 56, Batch 65 Loss:0.015665095299482346\n",
      "Epoch 56, Batch 66 Loss:0.039669979363679886\n",
      "Epoch 56, Batch 67 Loss:0.016530044376850128\n",
      "Epoch 56, Batch 68 Loss:0.011612741276621819\n",
      "Epoch 56, Batch 69 Loss:0.008885453455150127\n",
      "Epoch 56, Batch 70 Loss:0.006781019736081362\n",
      "Epoch 56, Batch 71 Loss:0.0946657657623291\n",
      "Epoch 56, Batch 72 Loss:0.01272105798125267\n",
      "Epoch 56, Batch 73 Loss:0.010518072172999382\n",
      "Epoch 56, Batch 74 Loss:0.010349418967962265\n",
      "Epoch 56, Batch 75 Loss:0.011669998988509178\n",
      "Epoch 56, Batch 76 Loss:0.01978183351457119\n",
      "Epoch 56, Batch 77 Loss:0.03186586871743202\n",
      "Epoch 56, Batch 78 Loss:0.03113422356545925\n",
      "Epoch 56, Batch 79 Loss:0.020265139639377594\n",
      "Epoch 56, Batch 80 Loss:0.012232474982738495\n",
      "Epoch 56, Batch 81 Loss:0.046526484191417694\n",
      "Epoch 56, Batch 82 Loss:0.024002758786082268\n",
      "Epoch 56, Batch 83 Loss:0.021838566288352013\n",
      "Epoch 56, Batch 84 Loss:0.020588530227541924\n",
      "Epoch 56, Batch 85 Loss:0.03216993436217308\n",
      "Epoch 56, Batch 86 Loss:0.02509768307209015\n",
      "Epoch 56, Batch 87 Loss:0.031883757561445236\n",
      "Epoch 56, Batch 88 Loss:0.012382395565509796\n",
      "Epoch 56, Batch 89 Loss:0.057225361466407776\n",
      "Epoch 56, Batch 90 Loss:0.018644392490386963\n",
      "Epoch 56, Batch 91 Loss:0.023797951638698578\n",
      "Epoch 56, Batch 92 Loss:0.017777016386389732\n",
      "Epoch 56, Batch 93 Loss:0.04920237511396408\n",
      "Epoch 56, Batch 94 Loss:0.030789146199822426\n",
      "Epoch 56, Batch 95 Loss:0.04579338803887367\n",
      "Epoch 56, Batch 96 Loss:0.0341213084757328\n",
      "Epoch 56, Batch 97 Loss:0.01928543671965599\n",
      "Epoch 56, Batch 98 Loss:0.04404027387499809\n",
      "Epoch 56, Batch 99 Loss:0.021240517497062683\n",
      "Epoch 56, Batch 100 Loss:0.022112246602773666\n",
      "Epoch 56, Batch 101 Loss:0.013427301310002804\n",
      "Epoch 56, Batch 102 Loss:0.00923475157469511\n",
      "Epoch 56, Batch 103 Loss:0.026291901245713234\n",
      "Epoch 56, Batch 104 Loss:0.01640167087316513\n",
      "Epoch 56, Batch 105 Loss:0.03974850848317146\n",
      "Epoch 56, Batch 106 Loss:0.026276972144842148\n",
      "Epoch 56, Batch 107 Loss:0.025442205369472504\n",
      "Epoch 56, Batch 108 Loss:0.03118540532886982\n",
      "Epoch 56, Batch 109 Loss:0.02815450355410576\n",
      "Epoch 56, Batch 110 Loss:0.03670318424701691\n",
      "Epoch 56, Batch 111 Loss:0.024521995335817337\n",
      "Epoch 56, Batch 112 Loss:0.0049705845303833485\n",
      "Epoch 56, Batch 113 Loss:0.007602696772664785\n",
      "Epoch 56, Batch 114 Loss:0.016903236508369446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56, Batch 115 Loss:0.015936944633722305\n",
      "Epoch 56, Batch 116 Loss:0.010801685974001884\n",
      "Epoch 56, Batch 117 Loss:0.017874231562018394\n",
      "Epoch 56, Batch 118 Loss:0.02100476622581482\n",
      "Epoch 56, Batch 119 Loss:0.01838095858693123\n",
      "Epoch 56, Batch 120 Loss:0.015503453090786934\n",
      "Epoch 56, Batch 121 Loss:0.031083423644304276\n",
      "Epoch 56, Batch 122 Loss:0.015757204964756966\n",
      "Epoch 56, Batch 123 Loss:0.007121667265892029\n",
      "Epoch 56, Batch 124 Loss:0.03337569534778595\n",
      "Epoch 56, Batch 125 Loss:0.011625256389379501\n",
      "Epoch 56, Batch 126 Loss:0.019555669277906418\n",
      "Epoch 56, Batch 127 Loss:0.03742104396224022\n",
      "Epoch 56, Batch 128 Loss:0.015019770711660385\n",
      "Epoch 56, Batch 129 Loss:0.021713731810450554\n",
      "Epoch 56, Batch 130 Loss:0.021424289792776108\n",
      "Epoch 56, Batch 131 Loss:0.034339092671871185\n",
      "Epoch 56, Batch 132 Loss:0.04425770789384842\n",
      "Epoch 56, Batch 133 Loss:0.01343892328441143\n",
      "Epoch 56, Batch 134 Loss:0.03214394301176071\n",
      "Epoch 56, Batch 135 Loss:0.015180615708231926\n",
      "Epoch 56, Batch 136 Loss:0.027624374255537987\n",
      "Epoch 56, Batch 137 Loss:0.03658232092857361\n",
      "Epoch 56, Batch 138 Loss:0.027358267456293106\n",
      "Epoch 56, Batch 139 Loss:0.029697630554437637\n",
      "Epoch 56, Batch 140 Loss:0.012210195884108543\n",
      "Epoch 56, Batch 141 Loss:0.02083747833967209\n",
      "Epoch 56, Batch 142 Loss:0.014995149336755276\n",
      "Epoch 56, Batch 143 Loss:0.04801061004400253\n",
      "Epoch 56, Batch 144 Loss:0.02068510092794895\n",
      "Epoch 56, Batch 145 Loss:0.03400464728474617\n",
      "Epoch 56, Batch 146 Loss:0.024691080674529076\n",
      "Epoch 56, Batch 147 Loss:0.01440112292766571\n",
      "Epoch 56, Batch 148 Loss:0.01803751289844513\n",
      "Epoch 56, Batch 149 Loss:0.051306091248989105\n",
      "Epoch 56, Batch 150 Loss:0.026340998709201813\n",
      "Epoch 56, Batch 151 Loss:0.02050941064953804\n",
      "Epoch 56, Batch 152 Loss:0.043310925364494324\n",
      "Epoch 56, Batch 153 Loss:0.010986430570483208\n",
      "Epoch 56, Batch 154 Loss:0.029337573796510696\n",
      "Epoch 56, Batch 155 Loss:0.02156056836247444\n",
      "Epoch 56, Batch 156 Loss:0.013756685890257359\n",
      "Epoch 56, Batch 157 Loss:0.015976130962371826\n",
      "Epoch 56, Batch 158 Loss:0.012257689610123634\n",
      "Epoch 56, Batch 159 Loss:0.021842118352651596\n",
      "Epoch 56, Batch 160 Loss:0.018208378925919533\n",
      "Epoch 56, Batch 161 Loss:0.012502222321927547\n",
      "Epoch 56, Batch 162 Loss:0.016758030280470848\n",
      "Epoch 56, Batch 163 Loss:0.016223205253481865\n",
      "Epoch 56, Batch 164 Loss:0.018116571009159088\n",
      "Epoch 56, Batch 165 Loss:0.012587041594088078\n",
      "Epoch 56, Batch 166 Loss:0.0282130129635334\n",
      "Epoch 56, Batch 167 Loss:0.045620113611221313\n",
      "Epoch 56, Batch 168 Loss:0.010196592658758163\n",
      "Epoch 56, Batch 169 Loss:0.006758113391697407\n",
      "Epoch 56, Batch 170 Loss:0.010893110185861588\n",
      "Epoch 56, Batch 171 Loss:0.02494429051876068\n",
      "Epoch 56, Batch 172 Loss:0.005279517732560635\n",
      "Epoch 56, Batch 173 Loss:0.02695634216070175\n",
      "Epoch 56, Batch 174 Loss:0.011160326190292835\n",
      "Epoch 56, Batch 175 Loss:0.010717840865254402\n",
      "Epoch 56, Batch 176 Loss:0.020990967750549316\n",
      "Epoch 56, Batch 177 Loss:0.007682940922677517\n",
      "Epoch 56, Batch 178 Loss:0.013207601383328438\n",
      "Epoch 56, Batch 179 Loss:0.055505335330963135\n",
      "Epoch 56, Batch 180 Loss:0.05866048112511635\n",
      "Epoch 56, Batch 181 Loss:0.03936276212334633\n",
      "Epoch 56, Batch 182 Loss:0.028763743117451668\n",
      "Epoch 56, Batch 183 Loss:0.015999596565961838\n",
      "Epoch 56, Batch 184 Loss:0.021252324804663658\n",
      "Epoch 56, Batch 185 Loss:0.0164596289396286\n",
      "Epoch 56, Batch 186 Loss:0.016384175047278404\n",
      "Epoch 56, Batch 187 Loss:0.01952451467514038\n",
      "Epoch 56, Batch 188 Loss:0.011659588664770126\n",
      "Epoch 56, Batch 189 Loss:0.02219383791089058\n",
      "Epoch 56, Batch 190 Loss:0.018504871055483818\n",
      "Epoch 56, Batch 191 Loss:0.011531325057148933\n",
      "Epoch 56, Batch 192 Loss:0.013869719579815865\n",
      "Epoch 56, Batch 193 Loss:0.01286192424595356\n",
      "Epoch 56, Batch 194 Loss:0.012244492769241333\n",
      "Epoch 56, Batch 195 Loss:0.010000149719417095\n",
      "Epoch 56, Batch 196 Loss:0.0759616494178772\n",
      "Epoch 56, Batch 197 Loss:0.08499377965927124\n",
      "Epoch 56, Batch 198 Loss:0.045300647616386414\n",
      "Epoch 56, Batch 199 Loss:0.010233549401164055\n",
      "Epoch 56, Batch 200 Loss:0.010775024071335793\n",
      "Epoch 56, Batch 201 Loss:0.03849712759256363\n",
      "Epoch 56, Batch 202 Loss:0.021632660180330276\n",
      "Epoch 56, Batch 203 Loss:0.03307746723294258\n",
      "Epoch 56, Batch 204 Loss:0.02753971330821514\n",
      "Epoch 56, Batch 205 Loss:0.030261140316724777\n",
      "Epoch 56, Batch 206 Loss:0.018965542316436768\n",
      "Epoch 56, Batch 207 Loss:0.032039083540439606\n",
      "Epoch 56, Batch 208 Loss:0.011322317644953728\n",
      "Epoch 56, Batch 209 Loss:0.02227732539176941\n",
      "Epoch 56, Batch 210 Loss:0.005391811020672321\n",
      "Epoch 56, Batch 211 Loss:0.012495780363678932\n",
      "Epoch 56, Batch 212 Loss:0.02646009437739849\n",
      "Epoch 56, Batch 213 Loss:0.04714035987854004\n",
      "Epoch 56, Batch 214 Loss:0.0086579080671072\n",
      "Epoch 56, Batch 215 Loss:0.029443569481372833\n",
      "Epoch 56, Batch 216 Loss:0.005560949444770813\n",
      "Epoch 56, Batch 217 Loss:0.017396150156855583\n",
      "Epoch 56, Batch 218 Loss:0.013967516832053661\n",
      "Epoch 56, Batch 219 Loss:0.007276359014213085\n",
      "Epoch 56, Batch 220 Loss:0.023186638951301575\n",
      "Epoch 56, Batch 221 Loss:0.01627509482204914\n",
      "Epoch 56, Batch 222 Loss:0.020927419885993004\n",
      "Epoch 56, Batch 223 Loss:0.018184471875429153\n",
      "Epoch 56, Batch 224 Loss:0.012379331514239311\n",
      "Epoch 56, Batch 225 Loss:0.04776480793952942\n",
      "Epoch 56, Batch 226 Loss:0.018074538558721542\n",
      "Epoch 56, Batch 227 Loss:0.0702245905995369\n",
      "Epoch 56, Batch 228 Loss:0.024633292108774185\n",
      "Epoch 56, Batch 229 Loss:0.0178344938904047\n",
      "Epoch 56, Batch 230 Loss:0.03328258544206619\n",
      "Epoch 56, Batch 231 Loss:0.03227030858397484\n",
      "Epoch 56, Batch 232 Loss:0.01973872259259224\n",
      "Epoch 56, Batch 233 Loss:0.015558090060949326\n",
      "Loss in this Epoch is: 1.55580900609 %\n",
      "Accuracy in this Epoch is: 88.6200010777 %\n",
      "Epoch 57, Batch 0 Loss:0.021806500852108\n",
      "Epoch 57, Batch 1 Loss:0.02000676468014717\n",
      "Epoch 57, Batch 2 Loss:0.026509394869208336\n",
      "Epoch 57, Batch 3 Loss:0.029649097472429276\n",
      "Epoch 57, Batch 4 Loss:0.027324385941028595\n",
      "Epoch 57, Batch 5 Loss:0.035790253430604935\n",
      "Epoch 57, Batch 6 Loss:0.025180742144584656\n",
      "Epoch 57, Batch 7 Loss:0.021469194442033768\n",
      "Epoch 57, Batch 8 Loss:0.012123391963541508\n",
      "Epoch 57, Batch 9 Loss:0.011150066740810871\n",
      "Epoch 57, Batch 10 Loss:0.011451547034084797\n",
      "Epoch 57, Batch 11 Loss:0.06759708374738693\n",
      "Epoch 57, Batch 12 Loss:0.011454098857939243\n",
      "Epoch 57, Batch 13 Loss:0.01448169257491827\n",
      "Epoch 57, Batch 14 Loss:0.03995620831847191\n",
      "Epoch 57, Batch 15 Loss:0.014201178215444088\n",
      "Epoch 57, Batch 16 Loss:0.007972534745931625\n",
      "Epoch 57, Batch 17 Loss:0.01868017390370369\n",
      "Epoch 57, Batch 18 Loss:0.007745638955384493\n",
      "Epoch 57, Batch 19 Loss:0.06375355273485184\n",
      "Epoch 57, Batch 20 Loss:0.02641451172530651\n",
      "Epoch 57, Batch 21 Loss:0.021496593952178955\n",
      "Epoch 57, Batch 22 Loss:0.015182018280029297\n",
      "Epoch 57, Batch 23 Loss:0.04227470979094505\n",
      "Epoch 57, Batch 24 Loss:0.024707965552806854\n",
      "Epoch 57, Batch 25 Loss:0.019187714904546738\n",
      "Epoch 57, Batch 26 Loss:0.008487398736178875\n",
      "Epoch 57, Batch 27 Loss:0.016224192455410957\n",
      "Epoch 57, Batch 28 Loss:0.015019961632788181\n",
      "Epoch 57, Batch 29 Loss:0.008483616635203362\n",
      "Epoch 57, Batch 30 Loss:0.0251026451587677\n",
      "Epoch 57, Batch 31 Loss:0.004881946370005608\n",
      "Epoch 57, Batch 32 Loss:0.026083050295710564\n",
      "Epoch 57, Batch 33 Loss:0.007505886256694794\n",
      "Epoch 57, Batch 34 Loss:0.009457506239414215\n",
      "Epoch 57, Batch 35 Loss:0.00825895369052887\n",
      "Epoch 57, Batch 36 Loss:0.008451695553958416\n",
      "Epoch 57, Batch 37 Loss:0.0164489708840847\n",
      "Epoch 57, Batch 38 Loss:0.04906845465302467\n",
      "Epoch 57, Batch 39 Loss:0.02087424509227276\n",
      "Epoch 57, Batch 40 Loss:0.006362553220242262\n",
      "Epoch 57, Batch 41 Loss:0.013758373446762562\n",
      "Epoch 57, Batch 42 Loss:0.02098739519715309\n",
      "Epoch 57, Batch 43 Loss:0.024961160495877266\n",
      "Epoch 57, Batch 44 Loss:0.016512254253029823\n",
      "Epoch 57, Batch 45 Loss:0.024625465273857117\n",
      "Epoch 57, Batch 46 Loss:0.011195671744644642\n",
      "Epoch 57, Batch 47 Loss:0.00674745487049222\n",
      "Epoch 57, Batch 48 Loss:0.007119450252503157\n",
      "Epoch 57, Batch 49 Loss:0.03327355533838272\n",
      "Epoch 57, Batch 50 Loss:0.009919441305100918\n",
      "Epoch 57, Batch 51 Loss:0.014364863745868206\n",
      "Epoch 57, Batch 52 Loss:0.007023303769528866\n",
      "Epoch 57, Batch 53 Loss:0.02581873908638954\n",
      "Epoch 57, Batch 54 Loss:0.032582733780145645\n",
      "Epoch 57, Batch 55 Loss:0.029721539467573166\n",
      "Epoch 57, Batch 56 Loss:0.013822409324347973\n",
      "Epoch 57, Batch 57 Loss:0.02304464392364025\n",
      "Epoch 57, Batch 58 Loss:0.020882606506347656\n",
      "Epoch 57, Batch 59 Loss:0.015305637381970882\n",
      "Epoch 57, Batch 60 Loss:0.016401873901486397\n",
      "Epoch 57, Batch 61 Loss:0.039157986640930176\n",
      "Epoch 57, Batch 62 Loss:0.01506810262799263\n",
      "Epoch 57, Batch 63 Loss:0.039064180105924606\n",
      "Epoch 57, Batch 64 Loss:0.009329215623438358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57, Batch 65 Loss:0.018258320167660713\n",
      "Epoch 57, Batch 66 Loss:0.010294164530932903\n",
      "Epoch 57, Batch 67 Loss:0.0209717508405447\n",
      "Epoch 57, Batch 68 Loss:0.01877768523991108\n",
      "Epoch 57, Batch 69 Loss:0.018921811133623123\n",
      "Epoch 57, Batch 70 Loss:0.014484822750091553\n",
      "Epoch 57, Batch 71 Loss:0.03411637619137764\n",
      "Epoch 57, Batch 72 Loss:0.011285625398159027\n",
      "Epoch 57, Batch 73 Loss:0.034924719482660294\n",
      "Epoch 57, Batch 74 Loss:0.023826273158192635\n",
      "Epoch 57, Batch 75 Loss:0.020005036145448685\n",
      "Epoch 57, Batch 76 Loss:0.029103444889187813\n",
      "Epoch 57, Batch 77 Loss:0.010191118344664574\n",
      "Epoch 57, Batch 78 Loss:0.015125582925975323\n",
      "Epoch 57, Batch 79 Loss:0.02594049833714962\n",
      "Epoch 57, Batch 80 Loss:0.025177303701639175\n",
      "Epoch 57, Batch 81 Loss:0.012892262078821659\n",
      "Epoch 57, Batch 82 Loss:0.04407725855708122\n",
      "Epoch 57, Batch 83 Loss:0.04193677753210068\n",
      "Epoch 57, Batch 84 Loss:0.03144766390323639\n",
      "Epoch 57, Batch 85 Loss:0.006788933649659157\n",
      "Epoch 57, Batch 86 Loss:0.006211430765688419\n",
      "Epoch 57, Batch 87 Loss:0.014515620656311512\n",
      "Epoch 57, Batch 88 Loss:0.015238294377923012\n",
      "Epoch 57, Batch 89 Loss:0.026241689920425415\n",
      "Epoch 57, Batch 90 Loss:0.009792561642825603\n",
      "Epoch 57, Batch 91 Loss:0.03729607164859772\n",
      "Epoch 57, Batch 92 Loss:0.024972395971417427\n",
      "Epoch 57, Batch 93 Loss:0.023577217012643814\n",
      "Epoch 57, Batch 94 Loss:0.009231667034327984\n",
      "Epoch 57, Batch 95 Loss:0.024117985740303993\n",
      "Epoch 57, Batch 96 Loss:0.01946772262454033\n",
      "Epoch 57, Batch 97 Loss:0.014792137779295444\n",
      "Epoch 57, Batch 98 Loss:0.012521165423095226\n",
      "Epoch 57, Batch 99 Loss:0.050815753638744354\n",
      "Epoch 57, Batch 100 Loss:0.0032998030073940754\n",
      "Epoch 57, Batch 101 Loss:0.010058043524622917\n",
      "Epoch 57, Batch 102 Loss:0.028359854593873024\n",
      "Epoch 57, Batch 103 Loss:0.005413769278675318\n",
      "Epoch 57, Batch 104 Loss:0.012485123239457607\n",
      "Epoch 57, Batch 105 Loss:0.020357932895421982\n",
      "Epoch 57, Batch 106 Loss:0.005099799484014511\n",
      "Epoch 57, Batch 107 Loss:0.008349785581231117\n",
      "Epoch 57, Batch 108 Loss:0.007977836765348911\n",
      "Epoch 57, Batch 109 Loss:0.01607527583837509\n",
      "Epoch 57, Batch 110 Loss:0.00869578868150711\n",
      "Epoch 57, Batch 111 Loss:0.05030336603522301\n",
      "Epoch 57, Batch 112 Loss:0.010063007473945618\n",
      "Epoch 57, Batch 113 Loss:0.02653380110859871\n",
      "Epoch 57, Batch 114 Loss:0.03743475675582886\n",
      "Epoch 57, Batch 115 Loss:0.020463895052671432\n",
      "Epoch 57, Batch 116 Loss:0.007930688560009003\n",
      "Epoch 57, Batch 117 Loss:0.008793625980615616\n",
      "Epoch 57, Batch 118 Loss:0.02985914796590805\n",
      "Epoch 57, Batch 119 Loss:0.024236664175987244\n",
      "Epoch 57, Batch 120 Loss:0.017140457406640053\n",
      "Epoch 57, Batch 121 Loss:0.010521288961172104\n",
      "Epoch 57, Batch 122 Loss:0.01870139129459858\n",
      "Epoch 57, Batch 123 Loss:0.019391734153032303\n",
      "Epoch 57, Batch 124 Loss:0.031136691570281982\n",
      "Epoch 57, Batch 125 Loss:0.022658023983240128\n",
      "Epoch 57, Batch 126 Loss:0.025495633482933044\n",
      "Epoch 57, Batch 127 Loss:0.01256564725190401\n",
      "Epoch 57, Batch 128 Loss:0.023558631539344788\n",
      "Epoch 57, Batch 129 Loss:0.004089793190360069\n",
      "Epoch 57, Batch 130 Loss:0.034005120396614075\n",
      "Epoch 57, Batch 131 Loss:0.03540903329849243\n",
      "Epoch 57, Batch 132 Loss:0.015410194173455238\n",
      "Epoch 57, Batch 133 Loss:0.014143655076622963\n",
      "Epoch 57, Batch 134 Loss:0.03218651935458183\n",
      "Epoch 57, Batch 135 Loss:0.06202685460448265\n",
      "Epoch 57, Batch 136 Loss:0.013606661930680275\n",
      "Epoch 57, Batch 137 Loss:0.01495963055640459\n",
      "Epoch 57, Batch 138 Loss:0.013777559623122215\n",
      "Epoch 57, Batch 139 Loss:0.017272967845201492\n",
      "Epoch 57, Batch 140 Loss:0.012332572601735592\n",
      "Epoch 57, Batch 141 Loss:0.013124617747962475\n",
      "Epoch 57, Batch 142 Loss:0.031045973300933838\n",
      "Epoch 57, Batch 143 Loss:0.040621574968099594\n",
      "Epoch 57, Batch 144 Loss:0.008719997480511665\n",
      "Epoch 57, Batch 145 Loss:0.03614076226949692\n",
      "Epoch 57, Batch 146 Loss:0.02763909101486206\n",
      "Epoch 57, Batch 147 Loss:0.019778672605752945\n",
      "Epoch 57, Batch 148 Loss:0.05976162105798721\n",
      "Epoch 57, Batch 149 Loss:0.010208556428551674\n",
      "Epoch 57, Batch 150 Loss:0.01994815655052662\n",
      "Epoch 57, Batch 151 Loss:0.019723691046237946\n",
      "Epoch 57, Batch 152 Loss:0.032969266176223755\n",
      "Epoch 57, Batch 153 Loss:0.018751921132206917\n",
      "Epoch 57, Batch 154 Loss:0.02298971638083458\n",
      "Epoch 57, Batch 155 Loss:0.05449092388153076\n",
      "Epoch 57, Batch 156 Loss:0.03254798799753189\n",
      "Epoch 57, Batch 157 Loss:0.037735868245363235\n",
      "Epoch 57, Batch 158 Loss:0.013229630887508392\n",
      "Epoch 57, Batch 159 Loss:0.029386289417743683\n",
      "Epoch 57, Batch 160 Loss:0.03661753237247467\n",
      "Epoch 57, Batch 161 Loss:0.0313030369579792\n",
      "Epoch 57, Batch 162 Loss:0.021222993731498718\n",
      "Epoch 57, Batch 163 Loss:0.05311301723122597\n",
      "Epoch 57, Batch 164 Loss:0.020896002650260925\n",
      "Epoch 57, Batch 165 Loss:0.035696785897016525\n",
      "Epoch 57, Batch 166 Loss:0.036179572343826294\n",
      "Epoch 57, Batch 167 Loss:0.03670424595475197\n",
      "Epoch 57, Batch 168 Loss:0.03531714528799057\n",
      "Epoch 57, Batch 169 Loss:0.04908256605267525\n",
      "Epoch 57, Batch 170 Loss:0.031373318284749985\n",
      "Epoch 57, Batch 171 Loss:0.0370868481695652\n",
      "Epoch 57, Batch 172 Loss:0.027277100831270218\n",
      "Epoch 57, Batch 173 Loss:0.08699824661016464\n",
      "Epoch 57, Batch 174 Loss:0.016673970967531204\n",
      "Epoch 57, Batch 175 Loss:0.033690981566905975\n",
      "Epoch 57, Batch 176 Loss:0.01696767657995224\n",
      "Epoch 57, Batch 177 Loss:0.05044291913509369\n",
      "Epoch 57, Batch 178 Loss:0.042778950184583664\n",
      "Epoch 57, Batch 179 Loss:0.0727379322052002\n",
      "Epoch 57, Batch 180 Loss:0.06570281088352203\n",
      "Epoch 57, Batch 181 Loss:0.01681814156472683\n",
      "Epoch 57, Batch 182 Loss:0.04102777689695358\n",
      "Epoch 57, Batch 183 Loss:0.048582471907138824\n",
      "Epoch 57, Batch 184 Loss:0.0337553396821022\n",
      "Epoch 57, Batch 185 Loss:0.06270399689674377\n",
      "Epoch 57, Batch 186 Loss:0.021787457168102264\n",
      "Epoch 57, Batch 187 Loss:0.04819464683532715\n",
      "Epoch 57, Batch 188 Loss:0.07273679226636887\n",
      "Epoch 57, Batch 189 Loss:0.018451575189828873\n",
      "Epoch 57, Batch 190 Loss:0.028688915073871613\n",
      "Epoch 57, Batch 191 Loss:0.020162569358944893\n",
      "Epoch 57, Batch 192 Loss:0.026276767253875732\n",
      "Epoch 57, Batch 193 Loss:0.02551579475402832\n",
      "Epoch 57, Batch 194 Loss:0.028150446712970734\n",
      "Epoch 57, Batch 195 Loss:0.030333930626511574\n",
      "Epoch 57, Batch 196 Loss:0.02576114609837532\n",
      "Epoch 57, Batch 197 Loss:0.01790238916873932\n",
      "Epoch 57, Batch 198 Loss:0.017626944929361343\n",
      "Epoch 57, Batch 199 Loss:0.019347840920090675\n",
      "Epoch 57, Batch 200 Loss:0.021567419171333313\n",
      "Epoch 57, Batch 201 Loss:0.03128037601709366\n",
      "Epoch 57, Batch 202 Loss:0.04901769012212753\n",
      "Epoch 57, Batch 203 Loss:0.03473035246133804\n",
      "Epoch 57, Batch 204 Loss:0.03886760026216507\n",
      "Epoch 57, Batch 205 Loss:0.06535017490386963\n",
      "Epoch 57, Batch 206 Loss:0.08237896859645844\n",
      "Epoch 57, Batch 207 Loss:0.045203156769275665\n",
      "Epoch 57, Batch 208 Loss:0.04487204551696777\n",
      "Epoch 57, Batch 209 Loss:0.05589123070240021\n",
      "Epoch 57, Batch 210 Loss:0.06259448081254959\n",
      "Epoch 57, Batch 211 Loss:0.09468112140893936\n",
      "Epoch 57, Batch 212 Loss:0.059777677059173584\n",
      "Epoch 57, Batch 213 Loss:0.05215245857834816\n",
      "Epoch 57, Batch 214 Loss:0.13397672772407532\n",
      "Epoch 57, Batch 215 Loss:0.07455915212631226\n",
      "Epoch 57, Batch 216 Loss:0.09842909127473831\n",
      "Epoch 57, Batch 217 Loss:0.1178072914481163\n",
      "Epoch 57, Batch 218 Loss:0.09873306751251221\n",
      "Epoch 57, Batch 219 Loss:0.0972224622964859\n",
      "Epoch 57, Batch 220 Loss:0.1816273331642151\n",
      "Epoch 57, Batch 221 Loss:0.1454298198223114\n",
      "Epoch 57, Batch 222 Loss:0.0774218887090683\n",
      "Epoch 57, Batch 223 Loss:0.11624699085950851\n",
      "Epoch 57, Batch 224 Loss:0.0742446631193161\n",
      "Epoch 57, Batch 225 Loss:0.06440219283103943\n",
      "Epoch 57, Batch 226 Loss:0.05822022631764412\n",
      "Epoch 57, Batch 227 Loss:0.08842086791992188\n",
      "Epoch 57, Batch 228 Loss:0.1063590794801712\n",
      "Epoch 57, Batch 229 Loss:0.07117277383804321\n",
      "Epoch 57, Batch 230 Loss:0.108699731528759\n",
      "Epoch 57, Batch 231 Loss:0.16926395893096924\n",
      "Epoch 57, Batch 232 Loss:0.18278715014457703\n",
      "Epoch 57, Batch 233 Loss:0.15406255424022675\n",
      "Loss in this Epoch is: 15.406255424 %\n",
      "Accuracy in this Epoch is: 87.190002203 %\n",
      "Epoch 58, Batch 0 Loss:0.07246465981006622\n",
      "Epoch 58, Batch 1 Loss:0.14787286520004272\n",
      "Epoch 58, Batch 2 Loss:0.11557485908269882\n",
      "Epoch 58, Batch 3 Loss:0.0869755670428276\n",
      "Epoch 58, Batch 4 Loss:0.08147311955690384\n",
      "Epoch 58, Batch 5 Loss:0.0864395946264267\n",
      "Epoch 58, Batch 6 Loss:0.07319390773773193\n",
      "Epoch 58, Batch 7 Loss:0.03525225445628166\n",
      "Epoch 58, Batch 8 Loss:0.10094644874334335\n",
      "Epoch 58, Batch 9 Loss:0.11562817543745041\n",
      "Epoch 58, Batch 10 Loss:0.057756517082452774\n",
      "Epoch 58, Batch 11 Loss:0.0633731335401535\n",
      "Epoch 58, Batch 12 Loss:0.09937544912099838\n",
      "Epoch 58, Batch 13 Loss:0.060003165155649185\n",
      "Epoch 58, Batch 14 Loss:0.05514806881546974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Batch 15 Loss:0.03465208038687706\n",
      "Epoch 58, Batch 16 Loss:0.04035155102610588\n",
      "Epoch 58, Batch 17 Loss:0.09464112669229507\n",
      "Epoch 58, Batch 18 Loss:0.06807225942611694\n",
      "Epoch 58, Batch 19 Loss:0.03800497204065323\n",
      "Epoch 58, Batch 20 Loss:0.049921680241823196\n",
      "Epoch 58, Batch 21 Loss:0.03041531890630722\n",
      "Epoch 58, Batch 22 Loss:0.0316496342420578\n",
      "Epoch 58, Batch 23 Loss:0.03520641103386879\n",
      "Epoch 58, Batch 24 Loss:0.0187686700373888\n",
      "Epoch 58, Batch 25 Loss:0.019207540899515152\n",
      "Epoch 58, Batch 26 Loss:0.04120739549398422\n",
      "Epoch 58, Batch 27 Loss:0.03421108052134514\n",
      "Epoch 58, Batch 28 Loss:0.027703452855348587\n",
      "Epoch 58, Batch 29 Loss:0.02399570122361183\n",
      "Epoch 58, Batch 30 Loss:0.08719488233327866\n",
      "Epoch 58, Batch 31 Loss:0.05534635856747627\n",
      "Epoch 58, Batch 32 Loss:0.025458432734012604\n",
      "Epoch 58, Batch 33 Loss:0.014499275013804436\n",
      "Epoch 58, Batch 34 Loss:0.04145565629005432\n",
      "Epoch 58, Batch 35 Loss:0.04160132631659508\n",
      "Epoch 58, Batch 36 Loss:0.03492166846990585\n",
      "Epoch 58, Batch 37 Loss:0.022010335698723793\n",
      "Epoch 58, Batch 38 Loss:0.06139535829424858\n",
      "Epoch 58, Batch 39 Loss:0.025056123733520508\n",
      "Epoch 58, Batch 40 Loss:0.012582561932504177\n",
      "Epoch 58, Batch 41 Loss:0.018189551308751106\n",
      "Epoch 58, Batch 42 Loss:0.02099350467324257\n",
      "Epoch 58, Batch 43 Loss:0.021562060341238976\n",
      "Epoch 58, Batch 44 Loss:0.03721241280436516\n",
      "Epoch 58, Batch 45 Loss:0.036394909024238586\n",
      "Epoch 58, Batch 46 Loss:0.07612398266792297\n",
      "Epoch 58, Batch 47 Loss:0.033416420221328735\n",
      "Epoch 58, Batch 48 Loss:0.02339501492679119\n",
      "Epoch 58, Batch 49 Loss:0.040060654282569885\n",
      "Epoch 58, Batch 50 Loss:0.045889947563409805\n",
      "Epoch 58, Batch 51 Loss:0.04266487434506416\n",
      "Epoch 58, Batch 52 Loss:0.014062996953725815\n",
      "Epoch 58, Batch 53 Loss:0.024833740666508675\n",
      "Epoch 58, Batch 54 Loss:0.016073426231741905\n",
      "Epoch 58, Batch 55 Loss:0.022837653756141663\n",
      "Epoch 58, Batch 56 Loss:0.03974379226565361\n",
      "Epoch 58, Batch 57 Loss:0.0211067833006382\n",
      "Epoch 58, Batch 58 Loss:0.05208675563335419\n",
      "Epoch 58, Batch 59 Loss:0.00999221671372652\n",
      "Epoch 58, Batch 60 Loss:0.013629015535116196\n",
      "Epoch 58, Batch 61 Loss:0.025964489206671715\n",
      "Epoch 58, Batch 62 Loss:0.07225193083286285\n",
      "Epoch 58, Batch 63 Loss:0.0203776303678751\n",
      "Epoch 58, Batch 64 Loss:0.05762917920947075\n",
      "Epoch 58, Batch 65 Loss:0.04338220879435539\n",
      "Epoch 58, Batch 66 Loss:0.013198860920965672\n",
      "Epoch 58, Batch 67 Loss:0.03179984167218208\n",
      "Epoch 58, Batch 68 Loss:0.0880512222647667\n",
      "Epoch 58, Batch 69 Loss:0.04669860005378723\n",
      "Epoch 58, Batch 70 Loss:0.015390455722808838\n",
      "Epoch 58, Batch 71 Loss:0.0322420634329319\n",
      "Epoch 58, Batch 72 Loss:0.013246548362076283\n",
      "Epoch 58, Batch 73 Loss:0.01836147904396057\n",
      "Epoch 58, Batch 74 Loss:0.02009613625705242\n",
      "Epoch 58, Batch 75 Loss:0.09864684194326401\n",
      "Epoch 58, Batch 76 Loss:0.04509619250893593\n",
      "Epoch 58, Batch 77 Loss:0.10645999014377594\n",
      "Epoch 58, Batch 78 Loss:0.034089937806129456\n",
      "Epoch 58, Batch 79 Loss:0.03490864112973213\n",
      "Epoch 58, Batch 80 Loss:0.025324959307909012\n",
      "Epoch 58, Batch 81 Loss:0.044507864862680435\n",
      "Epoch 58, Batch 82 Loss:0.016344644129276276\n",
      "Epoch 58, Batch 83 Loss:0.023523719981312752\n",
      "Epoch 58, Batch 84 Loss:0.05267605558037758\n",
      "Epoch 58, Batch 85 Loss:0.02205570973455906\n",
      "Epoch 58, Batch 86 Loss:0.04950472339987755\n",
      "Epoch 58, Batch 87 Loss:0.029178522527217865\n",
      "Epoch 58, Batch 88 Loss:0.06472256779670715\n",
      "Epoch 58, Batch 89 Loss:0.03261730074882507\n",
      "Epoch 58, Batch 90 Loss:0.03982236236333847\n",
      "Epoch 58, Batch 91 Loss:0.034274522215127945\n",
      "Epoch 58, Batch 92 Loss:0.0757225975394249\n",
      "Epoch 58, Batch 93 Loss:0.03294573351740837\n",
      "Epoch 58, Batch 94 Loss:0.03706057369709015\n",
      "Epoch 58, Batch 95 Loss:0.04102860763669014\n",
      "Epoch 58, Batch 96 Loss:0.038833245635032654\n",
      "Epoch 58, Batch 97 Loss:0.021955706179142\n",
      "Epoch 58, Batch 98 Loss:0.022899411618709564\n",
      "Epoch 58, Batch 99 Loss:0.026692967861890793\n",
      "Epoch 58, Batch 100 Loss:0.029043449088931084\n",
      "Epoch 58, Batch 101 Loss:0.04220438003540039\n",
      "Epoch 58, Batch 102 Loss:0.04512815922498703\n",
      "Epoch 58, Batch 103 Loss:0.03337189927697182\n",
      "Epoch 58, Batch 104 Loss:0.018461190164089203\n",
      "Epoch 58, Batch 105 Loss:0.013705299235880375\n",
      "Epoch 58, Batch 106 Loss:0.025445856153964996\n",
      "Epoch 58, Batch 107 Loss:0.008629721589386463\n",
      "Epoch 58, Batch 108 Loss:0.05073695629835129\n",
      "Epoch 58, Batch 109 Loss:0.021742157638072968\n",
      "Epoch 58, Batch 110 Loss:0.022399436682462692\n",
      "Epoch 58, Batch 111 Loss:0.04486193135380745\n",
      "Epoch 58, Batch 112 Loss:0.027743320912122726\n",
      "Epoch 58, Batch 113 Loss:0.015977729111909866\n",
      "Epoch 58, Batch 114 Loss:0.041000980883836746\n",
      "Epoch 58, Batch 115 Loss:0.05500780791044235\n",
      "Epoch 58, Batch 116 Loss:0.04448840022087097\n",
      "Epoch 58, Batch 117 Loss:0.029978567734360695\n",
      "Epoch 58, Batch 118 Loss:0.06375663727521896\n",
      "Epoch 58, Batch 119 Loss:0.023671399801969528\n",
      "Epoch 58, Batch 120 Loss:0.02970089204609394\n",
      "Epoch 58, Batch 121 Loss:0.039934590458869934\n",
      "Epoch 58, Batch 122 Loss:0.02777908183634281\n",
      "Epoch 58, Batch 123 Loss:0.008115985430777073\n",
      "Epoch 58, Batch 124 Loss:0.02378866635262966\n",
      "Epoch 58, Batch 125 Loss:0.009169123135507107\n",
      "Epoch 58, Batch 126 Loss:0.007177891209721565\n",
      "Epoch 58, Batch 127 Loss:0.024183083325624466\n",
      "Epoch 58, Batch 128 Loss:0.03759974241256714\n",
      "Epoch 58, Batch 129 Loss:0.01681600511074066\n",
      "Epoch 58, Batch 130 Loss:0.03920091688632965\n",
      "Epoch 58, Batch 131 Loss:0.015115353278815746\n",
      "Epoch 58, Batch 132 Loss:0.01606813445687294\n",
      "Epoch 58, Batch 133 Loss:0.025323977693915367\n",
      "Epoch 58, Batch 134 Loss:0.09355290979146957\n",
      "Epoch 58, Batch 135 Loss:0.04346014931797981\n",
      "Epoch 58, Batch 136 Loss:0.01521073468029499\n",
      "Epoch 58, Batch 137 Loss:0.056502148509025574\n",
      "Epoch 58, Batch 138 Loss:0.03632569685578346\n",
      "Epoch 58, Batch 139 Loss:0.025102075189352036\n",
      "Epoch 58, Batch 140 Loss:0.013547072187066078\n",
      "Epoch 58, Batch 141 Loss:0.012597508728504181\n",
      "Epoch 58, Batch 142 Loss:0.035999663174152374\n",
      "Epoch 58, Batch 143 Loss:0.05030303820967674\n",
      "Epoch 58, Batch 144 Loss:0.03287925198674202\n",
      "Epoch 58, Batch 145 Loss:0.036507125943899155\n",
      "Epoch 58, Batch 146 Loss:0.01849542185664177\n",
      "Epoch 58, Batch 147 Loss:0.007912838831543922\n",
      "Epoch 58, Batch 148 Loss:0.03184034302830696\n",
      "Epoch 58, Batch 149 Loss:0.018396656960248947\n",
      "Epoch 58, Batch 150 Loss:0.02131943218410015\n",
      "Epoch 58, Batch 151 Loss:0.02990512177348137\n",
      "Epoch 58, Batch 152 Loss:0.05820230394601822\n",
      "Epoch 58, Batch 153 Loss:0.03681857883930206\n",
      "Epoch 58, Batch 154 Loss:0.03702875226736069\n",
      "Epoch 58, Batch 155 Loss:0.042055170983076096\n",
      "Epoch 58, Batch 156 Loss:0.01201925240457058\n",
      "Epoch 58, Batch 157 Loss:0.029849188402295113\n",
      "Epoch 58, Batch 158 Loss:0.017421968281269073\n",
      "Epoch 58, Batch 159 Loss:0.048947833478450775\n",
      "Epoch 58, Batch 160 Loss:0.07488856464624405\n",
      "Epoch 58, Batch 161 Loss:0.08194977045059204\n",
      "Epoch 58, Batch 162 Loss:0.018473442643880844\n",
      "Epoch 58, Batch 163 Loss:0.032862529158592224\n",
      "Epoch 58, Batch 164 Loss:0.006631158757954836\n",
      "Epoch 58, Batch 165 Loss:0.02271171659231186\n",
      "Epoch 58, Batch 166 Loss:0.025981862097978592\n",
      "Epoch 58, Batch 167 Loss:0.03674877807497978\n",
      "Epoch 58, Batch 168 Loss:0.042359933257102966\n",
      "Epoch 58, Batch 169 Loss:0.03745311498641968\n",
      "Epoch 58, Batch 170 Loss:0.026470525190234184\n",
      "Epoch 58, Batch 171 Loss:0.01599223166704178\n",
      "Epoch 58, Batch 172 Loss:0.04154875501990318\n",
      "Epoch 58, Batch 173 Loss:0.048314522951841354\n",
      "Epoch 58, Batch 174 Loss:0.014437422156333923\n",
      "Epoch 58, Batch 175 Loss:0.019568070769309998\n",
      "Epoch 58, Batch 176 Loss:0.017247989773750305\n",
      "Epoch 58, Batch 177 Loss:0.01871540956199169\n",
      "Epoch 58, Batch 178 Loss:0.06622658669948578\n",
      "Epoch 58, Batch 179 Loss:0.03347686678171158\n",
      "Epoch 58, Batch 180 Loss:0.016144998371601105\n",
      "Epoch 58, Batch 181 Loss:0.03363009914755821\n",
      "Epoch 58, Batch 182 Loss:0.04341704025864601\n",
      "Epoch 58, Batch 183 Loss:0.03585343435406685\n",
      "Epoch 58, Batch 184 Loss:0.017944928258657455\n",
      "Epoch 58, Batch 185 Loss:0.028012868016958237\n",
      "Epoch 58, Batch 186 Loss:0.008575662970542908\n",
      "Epoch 58, Batch 187 Loss:0.036277297884225845\n",
      "Epoch 58, Batch 188 Loss:0.041756048798561096\n",
      "Epoch 58, Batch 189 Loss:0.03101152926683426\n",
      "Epoch 58, Batch 190 Loss:0.03695301711559296\n",
      "Epoch 58, Batch 191 Loss:0.07601973414421082\n",
      "Epoch 58, Batch 192 Loss:0.017709750682115555\n",
      "Epoch 58, Batch 193 Loss:0.04281852766871452\n",
      "Epoch 58, Batch 194 Loss:0.01573362573981285\n",
      "Epoch 58, Batch 195 Loss:0.03779594600200653\n",
      "Epoch 58, Batch 196 Loss:0.025377076119184494\n",
      "Epoch 58, Batch 197 Loss:0.026491861790418625\n",
      "Epoch 58, Batch 198 Loss:0.024663833901286125\n",
      "Epoch 58, Batch 199 Loss:0.05317764729261398\n",
      "Epoch 58, Batch 200 Loss:0.012630211189389229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58, Batch 201 Loss:0.02381027489900589\n",
      "Epoch 58, Batch 202 Loss:0.024784501641988754\n",
      "Epoch 58, Batch 203 Loss:0.014624889940023422\n",
      "Epoch 58, Batch 204 Loss:0.016353312879800797\n",
      "Epoch 58, Batch 205 Loss:0.04449402540922165\n",
      "Epoch 58, Batch 206 Loss:0.04686877503991127\n",
      "Epoch 58, Batch 207 Loss:0.03562066704034805\n",
      "Epoch 58, Batch 208 Loss:0.01650005578994751\n",
      "Epoch 58, Batch 209 Loss:0.020579371601343155\n",
      "Epoch 58, Batch 210 Loss:0.0332590788602829\n",
      "Epoch 58, Batch 211 Loss:0.062134191393852234\n",
      "Epoch 58, Batch 212 Loss:0.05165107175707817\n",
      "Epoch 58, Batch 213 Loss:0.019470853731036186\n",
      "Epoch 58, Batch 214 Loss:0.08587998896837234\n",
      "Epoch 58, Batch 215 Loss:0.04011587053537369\n",
      "Epoch 58, Batch 216 Loss:0.05459596589207649\n",
      "Epoch 58, Batch 217 Loss:0.02719699963927269\n",
      "Epoch 58, Batch 218 Loss:0.029135091230273247\n",
      "Epoch 58, Batch 219 Loss:0.07980772852897644\n",
      "Epoch 58, Batch 220 Loss:0.07669948786497116\n",
      "Epoch 58, Batch 221 Loss:0.07711488008499146\n",
      "Epoch 58, Batch 222 Loss:0.10201004892587662\n",
      "Epoch 58, Batch 223 Loss:0.06605825573205948\n",
      "Epoch 58, Batch 224 Loss:0.06148572638630867\n",
      "Epoch 58, Batch 225 Loss:0.044723354279994965\n",
      "Epoch 58, Batch 226 Loss:0.16198690235614777\n",
      "Epoch 58, Batch 227 Loss:0.06301137059926987\n",
      "Epoch 58, Batch 228 Loss:0.07996982336044312\n",
      "Epoch 58, Batch 229 Loss:0.08142898976802826\n",
      "Epoch 58, Batch 230 Loss:0.04682260379195213\n",
      "Epoch 58, Batch 231 Loss:0.11742757260799408\n",
      "Epoch 58, Batch 232 Loss:0.05676925554871559\n",
      "Epoch 58, Batch 233 Loss:0.1065751165151596\n",
      "Loss in this Epoch is: 10.6575116515 %\n",
      "Accuracy in this Epoch is: 87.9000008106 %\n",
      "Epoch 59, Batch 0 Loss:0.14823323488235474\n",
      "Epoch 59, Batch 1 Loss:0.051127053797245026\n",
      "Epoch 59, Batch 2 Loss:0.02119067870080471\n",
      "Epoch 59, Batch 3 Loss:0.058965053409338\n",
      "Epoch 59, Batch 4 Loss:0.07081346213817596\n",
      "Epoch 59, Batch 5 Loss:0.1066928505897522\n",
      "Epoch 59, Batch 6 Loss:0.040544603019952774\n",
      "Epoch 59, Batch 7 Loss:0.026084519922733307\n",
      "Epoch 59, Batch 8 Loss:0.10410494357347488\n",
      "Epoch 59, Batch 9 Loss:0.05554177612066269\n",
      "Epoch 59, Batch 10 Loss:0.05836193263530731\n",
      "Epoch 59, Batch 11 Loss:0.06245797127485275\n",
      "Epoch 59, Batch 12 Loss:0.063228540122509\n",
      "Epoch 59, Batch 13 Loss:0.04711436480283737\n",
      "Epoch 59, Batch 14 Loss:0.06600009649991989\n",
      "Epoch 59, Batch 15 Loss:0.05643244460225105\n",
      "Epoch 59, Batch 16 Loss:0.057242538779973984\n",
      "Epoch 59, Batch 17 Loss:0.05409405753016472\n",
      "Epoch 59, Batch 18 Loss:0.06660271435976028\n",
      "Epoch 59, Batch 19 Loss:0.0607052743434906\n",
      "Epoch 59, Batch 20 Loss:0.0185340978205204\n",
      "Epoch 59, Batch 21 Loss:0.03133631870150566\n",
      "Epoch 59, Batch 22 Loss:0.04118523746728897\n",
      "Epoch 59, Batch 23 Loss:0.0154817970469594\n",
      "Epoch 59, Batch 24 Loss:0.04722937196493149\n",
      "Epoch 59, Batch 25 Loss:0.056851815432310104\n",
      "Epoch 59, Batch 26 Loss:0.03262421116232872\n",
      "Epoch 59, Batch 27 Loss:0.07106003165245056\n",
      "Epoch 59, Batch 28 Loss:0.025441717356443405\n",
      "Epoch 59, Batch 29 Loss:0.03872322663664818\n",
      "Epoch 59, Batch 30 Loss:0.008941524662077427\n",
      "Epoch 59, Batch 31 Loss:0.05731261149048805\n",
      "Epoch 59, Batch 32 Loss:0.0374380499124527\n",
      "Epoch 59, Batch 33 Loss:0.036944419145584106\n",
      "Epoch 59, Batch 34 Loss:0.04831879213452339\n",
      "Epoch 59, Batch 35 Loss:0.06049384921789169\n",
      "Epoch 59, Batch 36 Loss:0.05995829030871391\n",
      "Epoch 59, Batch 37 Loss:0.031094832345843315\n",
      "Epoch 59, Batch 38 Loss:0.05070143938064575\n",
      "Epoch 59, Batch 39 Loss:0.057594623416662216\n",
      "Epoch 59, Batch 40 Loss:0.03385269641876221\n",
      "Epoch 59, Batch 41 Loss:0.02911994792521\n",
      "Epoch 59, Batch 42 Loss:0.04533983767032623\n",
      "Epoch 59, Batch 43 Loss:0.0398540161550045\n",
      "Epoch 59, Batch 44 Loss:0.056463878601789474\n",
      "Epoch 59, Batch 45 Loss:0.06400646269321442\n",
      "Epoch 59, Batch 46 Loss:0.02029595524072647\n",
      "Epoch 59, Batch 47 Loss:0.022324616089463234\n",
      "Epoch 59, Batch 48 Loss:0.01808820851147175\n",
      "Epoch 59, Batch 49 Loss:0.07979295402765274\n",
      "Epoch 59, Batch 50 Loss:0.023811014369130135\n",
      "Epoch 59, Batch 51 Loss:0.04642004892230034\n",
      "Epoch 59, Batch 52 Loss:0.02842399664223194\n",
      "Epoch 59, Batch 53 Loss:0.041003379970788956\n",
      "Epoch 59, Batch 54 Loss:0.04489241912961006\n",
      "Epoch 59, Batch 55 Loss:0.041482701897621155\n",
      "Epoch 59, Batch 56 Loss:0.008490984328091145\n",
      "Epoch 59, Batch 57 Loss:0.04223236069083214\n",
      "Epoch 59, Batch 58 Loss:0.06918162852525711\n",
      "Epoch 59, Batch 59 Loss:0.030539022758603096\n",
      "Epoch 59, Batch 60 Loss:0.06329619884490967\n",
      "Epoch 59, Batch 61 Loss:0.047012463212013245\n",
      "Epoch 59, Batch 62 Loss:0.026269296184182167\n",
      "Epoch 59, Batch 63 Loss:0.035881076008081436\n",
      "Epoch 59, Batch 64 Loss:0.06011383980512619\n",
      "Epoch 59, Batch 65 Loss:0.03154598921537399\n",
      "Epoch 59, Batch 66 Loss:0.05604603514075279\n",
      "Epoch 59, Batch 67 Loss:0.07354546338319778\n",
      "Epoch 59, Batch 68 Loss:0.03042570687830448\n",
      "Epoch 59, Batch 69 Loss:0.016913168132305145\n",
      "Epoch 59, Batch 70 Loss:0.044663600623607635\n",
      "Epoch 59, Batch 71 Loss:0.027241317555308342\n",
      "Epoch 59, Batch 72 Loss:0.045352768152952194\n",
      "Epoch 59, Batch 73 Loss:0.03901553526520729\n",
      "Epoch 59, Batch 74 Loss:0.043139901012182236\n",
      "Epoch 59, Batch 75 Loss:0.05690860003232956\n",
      "Epoch 59, Batch 76 Loss:0.01913672871887684\n",
      "Epoch 59, Batch 77 Loss:0.026185547932982445\n",
      "Epoch 59, Batch 78 Loss:0.04250722378492355\n",
      "Epoch 59, Batch 79 Loss:0.07094425708055496\n",
      "Epoch 59, Batch 80 Loss:0.025350283831357956\n",
      "Epoch 59, Batch 81 Loss:0.024546943604946136\n",
      "Epoch 59, Batch 82 Loss:0.05884895473718643\n",
      "Epoch 59, Batch 83 Loss:0.01482468657195568\n",
      "Epoch 59, Batch 84 Loss:0.038118790835142136\n",
      "Epoch 59, Batch 85 Loss:0.027287159115076065\n",
      "Epoch 59, Batch 86 Loss:0.021178781986236572\n",
      "Epoch 59, Batch 87 Loss:0.024252142757177353\n",
      "Epoch 59, Batch 88 Loss:0.04612785205245018\n",
      "Epoch 59, Batch 89 Loss:0.0800122544169426\n",
      "Epoch 59, Batch 90 Loss:0.038658078759908676\n",
      "Epoch 59, Batch 91 Loss:0.03328387811779976\n",
      "Epoch 59, Batch 92 Loss:0.03561987727880478\n",
      "Epoch 59, Batch 93 Loss:0.03568412363529205\n",
      "Epoch 59, Batch 94 Loss:0.055028002709150314\n",
      "Epoch 59, Batch 95 Loss:0.03134600818157196\n",
      "Epoch 59, Batch 96 Loss:0.026903148740530014\n",
      "Epoch 59, Batch 97 Loss:0.021316321566700935\n",
      "Epoch 59, Batch 98 Loss:0.053649406880140305\n",
      "Epoch 59, Batch 99 Loss:0.0304249320179224\n",
      "Epoch 59, Batch 100 Loss:0.07468443363904953\n",
      "Epoch 59, Batch 101 Loss:0.018682917580008507\n",
      "Epoch 59, Batch 102 Loss:0.017934149131178856\n",
      "Epoch 59, Batch 103 Loss:0.0349951796233654\n",
      "Epoch 59, Batch 104 Loss:0.050947487354278564\n",
      "Epoch 59, Batch 105 Loss:0.0315815694630146\n",
      "Epoch 59, Batch 106 Loss:0.03500525280833244\n",
      "Epoch 59, Batch 107 Loss:0.04003026336431503\n",
      "Epoch 59, Batch 108 Loss:0.026326097548007965\n",
      "Epoch 59, Batch 109 Loss:0.02855755016207695\n",
      "Epoch 59, Batch 110 Loss:0.049566663801670074\n",
      "Epoch 59, Batch 111 Loss:0.038720108568668365\n",
      "Epoch 59, Batch 112 Loss:0.03570634126663208\n",
      "Epoch 59, Batch 113 Loss:0.02421010285615921\n",
      "Epoch 59, Batch 114 Loss:0.0467449314892292\n",
      "Epoch 59, Batch 115 Loss:0.033976685255765915\n",
      "Epoch 59, Batch 116 Loss:0.04378855600953102\n",
      "Epoch 59, Batch 117 Loss:0.033565931022167206\n",
      "Epoch 59, Batch 118 Loss:0.03629684820771217\n",
      "Epoch 59, Batch 119 Loss:0.026073742657899857\n",
      "Epoch 59, Batch 120 Loss:0.0469929538667202\n",
      "Epoch 59, Batch 121 Loss:0.03932573273777962\n",
      "Epoch 59, Batch 122 Loss:0.04263090342283249\n",
      "Epoch 59, Batch 123 Loss:0.03339496999979019\n",
      "Epoch 59, Batch 124 Loss:0.030612152069807053\n",
      "Epoch 59, Batch 125 Loss:0.030481964349746704\n",
      "Epoch 59, Batch 126 Loss:0.04307970032095909\n",
      "Epoch 59, Batch 127 Loss:0.02439371682703495\n",
      "Epoch 59, Batch 128 Loss:0.05336158350110054\n",
      "Epoch 59, Batch 129 Loss:0.027213439345359802\n",
      "Epoch 59, Batch 130 Loss:0.030703753232955933\n",
      "Epoch 59, Batch 131 Loss:0.028925396502017975\n",
      "Epoch 59, Batch 132 Loss:0.03854798898100853\n",
      "Epoch 59, Batch 133 Loss:0.030925964936614037\n",
      "Epoch 59, Batch 134 Loss:0.014554186724126339\n",
      "Epoch 59, Batch 135 Loss:0.035457588732242584\n",
      "Epoch 59, Batch 136 Loss:0.0424930602312088\n",
      "Epoch 59, Batch 137 Loss:0.025889620184898376\n",
      "Epoch 59, Batch 138 Loss:0.022120170295238495\n",
      "Epoch 59, Batch 139 Loss:0.009477086365222931\n",
      "Epoch 59, Batch 140 Loss:0.01278386265039444\n",
      "Epoch 59, Batch 141 Loss:0.015229477547109127\n",
      "Epoch 59, Batch 142 Loss:0.026730945333838463\n",
      "Epoch 59, Batch 143 Loss:0.025003887712955475\n",
      "Epoch 59, Batch 144 Loss:0.07369358092546463\n",
      "Epoch 59, Batch 145 Loss:0.0188498143106699\n",
      "Epoch 59, Batch 146 Loss:0.02018916979432106\n",
      "Epoch 59, Batch 147 Loss:0.006379648111760616\n",
      "Epoch 59, Batch 148 Loss:0.01828683540225029\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59, Batch 149 Loss:0.061499107629060745\n",
      "Epoch 59, Batch 150 Loss:0.01769336313009262\n",
      "Epoch 59, Batch 151 Loss:0.01571987383067608\n",
      "Epoch 59, Batch 152 Loss:0.032913483679294586\n",
      "Epoch 59, Batch 153 Loss:0.013492433354258537\n",
      "Epoch 59, Batch 154 Loss:0.018313158303499222\n",
      "Epoch 59, Batch 155 Loss:0.018430456519126892\n",
      "Epoch 59, Batch 156 Loss:0.01961507834494114\n",
      "Epoch 59, Batch 157 Loss:0.014565471559762955\n",
      "Epoch 59, Batch 158 Loss:0.0212479829788208\n",
      "Epoch 59, Batch 159 Loss:0.060408227145671844\n",
      "Epoch 59, Batch 160 Loss:0.01165994256734848\n",
      "Epoch 59, Batch 161 Loss:0.040648944675922394\n",
      "Epoch 59, Batch 162 Loss:0.019161760807037354\n",
      "Epoch 59, Batch 163 Loss:0.03452431410551071\n",
      "Epoch 59, Batch 164 Loss:0.023306213319301605\n",
      "Epoch 59, Batch 165 Loss:0.06258635222911835\n",
      "Epoch 59, Batch 166 Loss:0.01959427073597908\n",
      "Epoch 59, Batch 167 Loss:0.009468434378504753\n",
      "Epoch 59, Batch 168 Loss:0.019600406289100647\n",
      "Epoch 59, Batch 169 Loss:0.013130378909409046\n",
      "Epoch 59, Batch 170 Loss:0.03665138781070709\n",
      "Epoch 59, Batch 171 Loss:0.032715536653995514\n",
      "Epoch 59, Batch 172 Loss:0.010416902601718903\n",
      "Epoch 59, Batch 173 Loss:0.029061617329716682\n",
      "Epoch 59, Batch 174 Loss:0.028069186955690384\n",
      "Epoch 59, Batch 175 Loss:0.022762563079595566\n",
      "Epoch 59, Batch 176 Loss:0.026858650147914886\n",
      "Epoch 59, Batch 177 Loss:0.03509439527988434\n",
      "Epoch 59, Batch 178 Loss:0.03626605123281479\n",
      "Epoch 59, Batch 179 Loss:0.026263652369379997\n",
      "Epoch 59, Batch 180 Loss:0.03726368397474289\n",
      "Epoch 59, Batch 181 Loss:0.03755345568060875\n",
      "Epoch 59, Batch 182 Loss:0.054120149463415146\n",
      "Epoch 59, Batch 183 Loss:0.028859715908765793\n",
      "Epoch 59, Batch 184 Loss:0.04461244121193886\n",
      "Epoch 59, Batch 185 Loss:0.03153054416179657\n",
      "Epoch 59, Batch 186 Loss:0.06784965097904205\n",
      "Epoch 59, Batch 187 Loss:0.012727027758955956\n",
      "Epoch 59, Batch 188 Loss:0.06241311505436897\n",
      "Epoch 59, Batch 189 Loss:0.04766056314110756\n",
      "Epoch 59, Batch 190 Loss:0.02970147132873535\n",
      "Epoch 59, Batch 191 Loss:0.07567860931158066\n",
      "Epoch 59, Batch 192 Loss:0.01650797575712204\n",
      "Epoch 59, Batch 193 Loss:0.012145431712269783\n",
      "Epoch 59, Batch 194 Loss:0.022587411105632782\n",
      "Epoch 59, Batch 195 Loss:0.014944850467145443\n",
      "Epoch 59, Batch 196 Loss:0.027159273624420166\n",
      "Epoch 59, Batch 197 Loss:0.03367149084806442\n",
      "Epoch 59, Batch 198 Loss:0.03344564139842987\n",
      "Epoch 59, Batch 199 Loss:0.02071302756667137\n",
      "Epoch 59, Batch 200 Loss:0.056585244834423065\n",
      "Epoch 59, Batch 201 Loss:0.06934179365634918\n",
      "Epoch 59, Batch 202 Loss:0.03613784909248352\n",
      "Epoch 59, Batch 203 Loss:0.02793455682694912\n",
      "Epoch 59, Batch 204 Loss:0.03919399529695511\n",
      "Epoch 59, Batch 205 Loss:0.03080269694328308\n",
      "Epoch 59, Batch 206 Loss:0.02124069444835186\n",
      "Epoch 59, Batch 207 Loss:0.014824957586824894\n",
      "Epoch 59, Batch 208 Loss:0.009418166242539883\n",
      "Epoch 59, Batch 209 Loss:0.01716497540473938\n",
      "Epoch 59, Batch 210 Loss:0.03385576605796814\n",
      "Epoch 59, Batch 211 Loss:0.0771331638097763\n",
      "Epoch 59, Batch 212 Loss:0.019073020666837692\n",
      "Epoch 59, Batch 213 Loss:0.016989169642329216\n",
      "Epoch 59, Batch 214 Loss:0.016491342335939407\n",
      "Epoch 59, Batch 215 Loss:0.024000659584999084\n",
      "Epoch 59, Batch 216 Loss:0.012744138948619366\n",
      "Epoch 59, Batch 217 Loss:0.02245105244219303\n",
      "Epoch 59, Batch 218 Loss:0.0573054701089859\n",
      "Epoch 59, Batch 219 Loss:0.09879744052886963\n",
      "Epoch 59, Batch 220 Loss:0.03976762294769287\n",
      "Epoch 59, Batch 221 Loss:0.037811003625392914\n",
      "Epoch 59, Batch 222 Loss:0.02530396357178688\n",
      "Epoch 59, Batch 223 Loss:0.037172675132751465\n",
      "Epoch 59, Batch 224 Loss:0.02009798213839531\n",
      "Epoch 59, Batch 225 Loss:0.06336641311645508\n",
      "Epoch 59, Batch 226 Loss:0.03734303265810013\n",
      "Epoch 59, Batch 227 Loss:0.01868164725601673\n",
      "Epoch 59, Batch 228 Loss:0.0224527046084404\n",
      "Epoch 59, Batch 229 Loss:0.01316886581480503\n",
      "Epoch 59, Batch 230 Loss:0.026248596608638763\n",
      "Epoch 59, Batch 231 Loss:0.04802071303129196\n",
      "Epoch 59, Batch 232 Loss:0.03760107606649399\n",
      "Epoch 59, Batch 233 Loss:0.020778726786375046\n",
      "Loss in this Epoch is: 2.07787267864 %\n",
      "Accuracy in this Epoch is: 88.0100011826 %\n",
      "Epoch 60, Batch 0 Loss:0.0155424103140831\n",
      "Epoch 60, Batch 1 Loss:0.045860469341278076\n",
      "Epoch 60, Batch 2 Loss:0.030018072575330734\n",
      "Epoch 60, Batch 3 Loss:0.07716651260852814\n",
      "Epoch 60, Batch 4 Loss:0.03282316401600838\n",
      "Epoch 60, Batch 5 Loss:0.05508929863572121\n",
      "Epoch 60, Batch 6 Loss:0.03799162805080414\n",
      "Epoch 60, Batch 7 Loss:0.03381625935435295\n",
      "Epoch 60, Batch 8 Loss:0.025726357474923134\n",
      "Epoch 60, Batch 9 Loss:0.04992659017443657\n",
      "Epoch 60, Batch 10 Loss:0.022654587402939796\n",
      "Epoch 60, Batch 11 Loss:0.04668289050459862\n",
      "Epoch 60, Batch 12 Loss:0.014883974567055702\n",
      "Epoch 60, Batch 13 Loss:0.02061331458389759\n",
      "Epoch 60, Batch 14 Loss:0.017779866233468056\n",
      "Epoch 60, Batch 15 Loss:0.02564297989010811\n",
      "Epoch 60, Batch 16 Loss:0.05405862256884575\n",
      "Epoch 60, Batch 17 Loss:0.02385026030242443\n",
      "Epoch 60, Batch 18 Loss:0.04985557869076729\n",
      "Epoch 60, Batch 19 Loss:0.014521803706884384\n",
      "Epoch 60, Batch 20 Loss:0.02019849605858326\n",
      "Epoch 60, Batch 21 Loss:0.02163926139473915\n",
      "Epoch 60, Batch 22 Loss:0.04507236182689667\n",
      "Epoch 60, Batch 23 Loss:0.02136269025504589\n",
      "Epoch 60, Batch 24 Loss:0.0076553053222596645\n",
      "Epoch 60, Batch 25 Loss:0.05767219886183739\n",
      "Epoch 60, Batch 26 Loss:0.0356312096118927\n",
      "Epoch 60, Batch 27 Loss:0.024180514737963676\n",
      "Epoch 60, Batch 28 Loss:0.010977081023156643\n",
      "Epoch 60, Batch 29 Loss:0.02314617671072483\n",
      "Epoch 60, Batch 30 Loss:0.059743404388427734\n",
      "Epoch 60, Batch 31 Loss:0.014646628871560097\n",
      "Epoch 60, Batch 32 Loss:0.01330855768173933\n",
      "Epoch 60, Batch 33 Loss:0.019415460526943207\n",
      "Epoch 60, Batch 34 Loss:0.035601481795310974\n",
      "Epoch 60, Batch 35 Loss:0.016240179538726807\n",
      "Epoch 60, Batch 36 Loss:0.01776529662311077\n",
      "Epoch 60, Batch 37 Loss:0.021104862913489342\n",
      "Epoch 60, Batch 38 Loss:0.026993023231625557\n",
      "Epoch 60, Batch 39 Loss:0.03312767297029495\n",
      "Epoch 60, Batch 40 Loss:0.03966841101646423\n",
      "Epoch 60, Batch 41 Loss:0.08884838223457336\n",
      "Epoch 60, Batch 42 Loss:0.1120072528719902\n",
      "Epoch 60, Batch 43 Loss:0.08793973922729492\n",
      "Epoch 60, Batch 44 Loss:0.15504345297813416\n",
      "Epoch 60, Batch 45 Loss:0.12090326845645905\n",
      "Epoch 60, Batch 46 Loss:0.18687206506729126\n",
      "Epoch 60, Batch 47 Loss:0.07457597553730011\n",
      "Epoch 60, Batch 48 Loss:0.090298593044281\n",
      "Epoch 60, Batch 49 Loss:0.06649433076381683\n",
      "Epoch 60, Batch 50 Loss:0.10881016403436661\n",
      "Epoch 60, Batch 51 Loss:0.06717049330472946\n",
      "Epoch 60, Batch 52 Loss:0.0569358691573143\n",
      "Epoch 60, Batch 53 Loss:0.08494693040847778\n",
      "Epoch 60, Batch 54 Loss:0.07371336966753006\n",
      "Epoch 60, Batch 55 Loss:0.12954415380954742\n",
      "Epoch 60, Batch 56 Loss:0.09770053625106812\n",
      "Epoch 60, Batch 57 Loss:0.08845566213130951\n",
      "Epoch 60, Batch 58 Loss:0.08099032193422318\n",
      "Epoch 60, Batch 59 Loss:0.0766959860920906\n",
      "Epoch 60, Batch 60 Loss:0.12092950195074081\n",
      "Epoch 60, Batch 61 Loss:0.16745764017105103\n",
      "Epoch 60, Batch 62 Loss:0.12139800935983658\n",
      "Epoch 60, Batch 63 Loss:0.08506837487220764\n",
      "Epoch 60, Batch 64 Loss:0.14636005461215973\n",
      "Epoch 60, Batch 65 Loss:0.1028263121843338\n",
      "Epoch 60, Batch 66 Loss:0.09919358789920807\n",
      "Epoch 60, Batch 67 Loss:0.11953889578580856\n",
      "Epoch 60, Batch 68 Loss:0.06738945096731186\n",
      "Epoch 60, Batch 69 Loss:0.11767987906932831\n",
      "Epoch 60, Batch 70 Loss:0.09758369624614716\n",
      "Epoch 60, Batch 71 Loss:0.052865974605083466\n",
      "Epoch 60, Batch 72 Loss:0.06147345155477524\n",
      "Epoch 60, Batch 73 Loss:0.08057508617639542\n",
      "Epoch 60, Batch 74 Loss:0.07231301814317703\n",
      "Epoch 60, Batch 75 Loss:0.08661699295043945\n",
      "Epoch 60, Batch 76 Loss:0.05507904663681984\n",
      "Epoch 60, Batch 77 Loss:0.04925905913114548\n",
      "Epoch 60, Batch 78 Loss:0.06476978212594986\n",
      "Epoch 60, Batch 79 Loss:0.09751053154468536\n",
      "Epoch 60, Batch 80 Loss:0.06680713593959808\n",
      "Epoch 60, Batch 81 Loss:0.09336134046316147\n",
      "Epoch 60, Batch 82 Loss:0.04756191372871399\n",
      "Epoch 60, Batch 83 Loss:0.12429007142782211\n",
      "Epoch 60, Batch 84 Loss:0.052936967462301254\n",
      "Epoch 60, Batch 85 Loss:0.08507241308689117\n",
      "Epoch 60, Batch 86 Loss:0.09963665157556534\n",
      "Epoch 60, Batch 87 Loss:0.032412946224212646\n",
      "Epoch 60, Batch 88 Loss:0.06401070952415466\n",
      "Epoch 60, Batch 89 Loss:0.045591458678245544\n",
      "Epoch 60, Batch 90 Loss:0.04433966055512428\n",
      "Epoch 60, Batch 91 Loss:0.07278928905725479\n",
      "Epoch 60, Batch 92 Loss:0.05918896570801735\n",
      "Epoch 60, Batch 93 Loss:0.05561325326561928\n",
      "Epoch 60, Batch 94 Loss:0.06130516156554222\n",
      "Epoch 60, Batch 95 Loss:0.030457744374871254\n",
      "Epoch 60, Batch 96 Loss:0.03155266493558884\n",
      "Epoch 60, Batch 97 Loss:0.031586725264787674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60, Batch 98 Loss:0.01674826443195343\n",
      "Epoch 60, Batch 99 Loss:0.06868666410446167\n",
      "Epoch 60, Batch 100 Loss:0.04446122422814369\n",
      "Epoch 60, Batch 101 Loss:0.03406502678990364\n",
      "Epoch 60, Batch 102 Loss:0.10803021490573883\n",
      "Epoch 60, Batch 103 Loss:0.03381917625665665\n",
      "Epoch 60, Batch 104 Loss:0.053404416888952255\n",
      "Epoch 60, Batch 105 Loss:0.02326781488955021\n",
      "Epoch 60, Batch 106 Loss:0.10659144818782806\n",
      "Epoch 60, Batch 107 Loss:0.05667351558804512\n",
      "Epoch 60, Batch 108 Loss:0.016856089234352112\n",
      "Epoch 60, Batch 109 Loss:0.020868411287665367\n",
      "Epoch 60, Batch 110 Loss:0.043828725814819336\n",
      "Epoch 60, Batch 111 Loss:0.033175524324178696\n",
      "Epoch 60, Batch 112 Loss:0.03821156919002533\n",
      "Epoch 60, Batch 113 Loss:0.052556272596120834\n",
      "Epoch 60, Batch 114 Loss:0.0369674488902092\n",
      "Epoch 60, Batch 115 Loss:0.027329500764608383\n",
      "Epoch 60, Batch 116 Loss:0.0279233418405056\n",
      "Epoch 60, Batch 117 Loss:0.011401689611375332\n",
      "Epoch 60, Batch 118 Loss:0.03359944000840187\n",
      "Epoch 60, Batch 119 Loss:0.03256608545780182\n",
      "Epoch 60, Batch 120 Loss:0.017328500747680664\n",
      "Epoch 60, Batch 121 Loss:0.011230409145355225\n",
      "Epoch 60, Batch 122 Loss:0.02361411228775978\n",
      "Epoch 60, Batch 123 Loss:0.02868940308690071\n",
      "Epoch 60, Batch 124 Loss:0.019864168018102646\n",
      "Epoch 60, Batch 125 Loss:0.02602727711200714\n",
      "Epoch 60, Batch 126 Loss:0.016258252784609795\n",
      "Epoch 60, Batch 127 Loss:0.03071986883878708\n",
      "Epoch 60, Batch 128 Loss:0.01898213103413582\n",
      "Epoch 60, Batch 129 Loss:0.012795930728316307\n",
      "Epoch 60, Batch 130 Loss:0.01210545189678669\n",
      "Epoch 60, Batch 131 Loss:0.028086325153708458\n",
      "Epoch 60, Batch 132 Loss:0.06840623915195465\n",
      "Epoch 60, Batch 133 Loss:0.06546083837747574\n",
      "Epoch 60, Batch 134 Loss:0.06430327892303467\n",
      "Epoch 60, Batch 135 Loss:0.049810148775577545\n",
      "Epoch 60, Batch 136 Loss:0.010950982570648193\n",
      "Epoch 60, Batch 137 Loss:0.058922335505485535\n",
      "Epoch 60, Batch 138 Loss:0.05869866907596588\n",
      "Epoch 60, Batch 139 Loss:0.03728921711444855\n",
      "Epoch 60, Batch 140 Loss:0.06959563493728638\n",
      "Epoch 60, Batch 141 Loss:0.007037327624857426\n",
      "Epoch 60, Batch 142 Loss:0.06199197843670845\n",
      "Epoch 60, Batch 143 Loss:0.04893359914422035\n",
      "Epoch 60, Batch 144 Loss:0.03138957917690277\n",
      "Epoch 60, Batch 145 Loss:0.03834560513496399\n",
      "Epoch 60, Batch 146 Loss:0.056524671614170074\n",
      "Epoch 60, Batch 147 Loss:0.04019425809383392\n",
      "Epoch 60, Batch 148 Loss:0.050502583384513855\n",
      "Epoch 60, Batch 149 Loss:0.05333516374230385\n",
      "Epoch 60, Batch 150 Loss:0.067543163895607\n",
      "Epoch 60, Batch 151 Loss:0.055116020143032074\n",
      "Epoch 60, Batch 152 Loss:0.04838547855615616\n",
      "Epoch 60, Batch 153 Loss:0.03953898698091507\n",
      "Epoch 60, Batch 154 Loss:0.01905602589249611\n",
      "Epoch 60, Batch 155 Loss:0.05315353721380234\n",
      "Epoch 60, Batch 156 Loss:0.048755235970020294\n",
      "Epoch 60, Batch 157 Loss:0.062429241836071014\n",
      "Epoch 60, Batch 158 Loss:0.03817920759320259\n",
      "Epoch 60, Batch 159 Loss:0.02024112641811371\n",
      "Epoch 60, Batch 160 Loss:0.05015868693590164\n",
      "Epoch 60, Batch 161 Loss:0.054140619933605194\n",
      "Epoch 60, Batch 162 Loss:0.01825004070997238\n",
      "Epoch 60, Batch 163 Loss:0.04430272430181503\n",
      "Epoch 60, Batch 164 Loss:0.04666917026042938\n",
      "Epoch 60, Batch 165 Loss:0.06028014421463013\n",
      "Epoch 60, Batch 166 Loss:0.021846795454621315\n",
      "Epoch 60, Batch 167 Loss:0.039634838700294495\n",
      "Epoch 60, Batch 168 Loss:0.07683029770851135\n",
      "Epoch 60, Batch 169 Loss:0.02685053087770939\n",
      "Epoch 60, Batch 170 Loss:0.06296355277299881\n",
      "Epoch 60, Batch 171 Loss:0.05217859148979187\n",
      "Epoch 60, Batch 172 Loss:0.023913446813821793\n",
      "Epoch 60, Batch 173 Loss:0.014085715636610985\n",
      "Epoch 60, Batch 174 Loss:0.02098838798701763\n",
      "Epoch 60, Batch 175 Loss:0.011430740356445312\n",
      "Epoch 60, Batch 176 Loss:0.019119743257761\n",
      "Epoch 60, Batch 177 Loss:0.030487708747386932\n",
      "Epoch 60, Batch 178 Loss:0.022785821929574013\n",
      "Epoch 60, Batch 179 Loss:0.05192048102617264\n",
      "Epoch 60, Batch 180 Loss:0.026935450732707977\n",
      "Epoch 60, Batch 181 Loss:0.04379083588719368\n",
      "Epoch 60, Batch 182 Loss:0.01782151684165001\n",
      "Epoch 60, Batch 183 Loss:0.011946383863687515\n",
      "Epoch 60, Batch 184 Loss:0.025452295318245888\n",
      "Epoch 60, Batch 185 Loss:0.024331752210855484\n",
      "Epoch 60, Batch 186 Loss:0.007863452658057213\n",
      "Epoch 60, Batch 187 Loss:0.013664325699210167\n",
      "Epoch 60, Batch 188 Loss:0.011899331584572792\n",
      "Epoch 60, Batch 189 Loss:0.018838612362742424\n",
      "Epoch 60, Batch 190 Loss:0.012250492349267006\n",
      "Epoch 60, Batch 191 Loss:0.012784013524651527\n",
      "Epoch 60, Batch 192 Loss:0.02533991076052189\n",
      "Epoch 60, Batch 193 Loss:0.04958055168390274\n",
      "Epoch 60, Batch 194 Loss:0.02286062017083168\n",
      "Epoch 60, Batch 195 Loss:0.011690603569149971\n",
      "Epoch 60, Batch 196 Loss:0.010709915310144424\n",
      "Epoch 60, Batch 197 Loss:0.009839864447712898\n",
      "Epoch 60, Batch 198 Loss:0.047414492815732956\n",
      "Epoch 60, Batch 199 Loss:0.01355205848813057\n",
      "Epoch 60, Batch 200 Loss:0.02611835114657879\n",
      "Epoch 60, Batch 201 Loss:0.04430818185210228\n",
      "Epoch 60, Batch 202 Loss:0.02845725044608116\n",
      "Epoch 60, Batch 203 Loss:0.03119257092475891\n",
      "Epoch 60, Batch 204 Loss:0.03244966268539429\n",
      "Epoch 60, Batch 205 Loss:0.060246214270591736\n",
      "Epoch 60, Batch 206 Loss:0.016034197062253952\n",
      "Epoch 60, Batch 207 Loss:0.020214369520545006\n",
      "Epoch 60, Batch 208 Loss:0.07719311118125916\n",
      "Epoch 60, Batch 209 Loss:0.01570112258195877\n",
      "Epoch 60, Batch 210 Loss:0.05780567601323128\n",
      "Epoch 60, Batch 211 Loss:0.023844193667173386\n",
      "Epoch 60, Batch 212 Loss:0.024303637444972992\n",
      "Epoch 60, Batch 213 Loss:0.014934314414858818\n",
      "Epoch 60, Batch 214 Loss:0.01724487915635109\n",
      "Epoch 60, Batch 215 Loss:0.023894421756267548\n",
      "Epoch 60, Batch 216 Loss:0.01821134425699711\n",
      "Epoch 60, Batch 217 Loss:0.02364901825785637\n",
      "Epoch 60, Batch 218 Loss:0.030315395444631577\n",
      "Epoch 60, Batch 219 Loss:0.016691874712705612\n",
      "Epoch 60, Batch 220 Loss:0.03298867866396904\n",
      "Epoch 60, Batch 221 Loss:0.03339812904596329\n",
      "Epoch 60, Batch 222 Loss:0.033392928540706635\n",
      "Epoch 60, Batch 223 Loss:0.013004302978515625\n",
      "Epoch 60, Batch 224 Loss:0.025017790496349335\n",
      "Epoch 60, Batch 225 Loss:0.010151605121791363\n",
      "Epoch 60, Batch 226 Loss:0.027842683717608452\n",
      "Epoch 60, Batch 227 Loss:0.012734453193843365\n",
      "Epoch 60, Batch 228 Loss:0.018900342285633087\n",
      "Epoch 60, Batch 229 Loss:0.012964782305061817\n",
      "Epoch 60, Batch 230 Loss:0.021005960181355476\n",
      "Epoch 60, Batch 231 Loss:0.021944982931017876\n",
      "Epoch 60, Batch 232 Loss:0.0389242097735405\n",
      "Epoch 60, Batch 233 Loss:0.038792043924331665\n",
      "Loss in this Epoch is: 3.87920439243 %\n",
      "Accuracy in this Epoch is: 88.5800004005 %\n",
      "Epoch 61, Batch 0 Loss:0.004742950201034546\n",
      "Epoch 61, Batch 1 Loss:0.016956752166152\n",
      "Epoch 61, Batch 2 Loss:0.01229751668870449\n",
      "Epoch 61, Batch 3 Loss:0.02898920699954033\n",
      "Epoch 61, Batch 4 Loss:0.03941389545798302\n",
      "Epoch 61, Batch 5 Loss:0.008376127108931541\n",
      "Epoch 61, Batch 6 Loss:0.04380844905972481\n",
      "Epoch 61, Batch 7 Loss:0.02166113816201687\n",
      "Epoch 61, Batch 8 Loss:0.03587402403354645\n",
      "Epoch 61, Batch 9 Loss:0.012645972892642021\n",
      "Epoch 61, Batch 10 Loss:0.01919419877231121\n",
      "Epoch 61, Batch 11 Loss:0.020247196778655052\n",
      "Epoch 61, Batch 12 Loss:0.018367132171988487\n",
      "Epoch 61, Batch 13 Loss:0.05260128527879715\n",
      "Epoch 61, Batch 14 Loss:0.010195630602538586\n",
      "Epoch 61, Batch 15 Loss:0.02035929635167122\n",
      "Epoch 61, Batch 16 Loss:0.03376471623778343\n",
      "Epoch 61, Batch 17 Loss:0.01319942157715559\n",
      "Epoch 61, Batch 18 Loss:0.012437814846634865\n",
      "Epoch 61, Batch 19 Loss:0.014563341625034809\n",
      "Epoch 61, Batch 20 Loss:0.0161063764244318\n",
      "Epoch 61, Batch 21 Loss:0.01009749062359333\n",
      "Epoch 61, Batch 22 Loss:0.013384065590798855\n",
      "Epoch 61, Batch 23 Loss:0.03530425578355789\n",
      "Epoch 61, Batch 24 Loss:0.038577850908041\n",
      "Epoch 61, Batch 25 Loss:0.017809927463531494\n",
      "Epoch 61, Batch 26 Loss:0.03066609427332878\n",
      "Epoch 61, Batch 27 Loss:0.01752164587378502\n",
      "Epoch 61, Batch 28 Loss:0.03240170329809189\n",
      "Epoch 61, Batch 29 Loss:0.021756576374173164\n",
      "Epoch 61, Batch 30 Loss:0.007587352301925421\n",
      "Epoch 61, Batch 31 Loss:0.04491135850548744\n",
      "Epoch 61, Batch 32 Loss:0.024298958480358124\n",
      "Epoch 61, Batch 33 Loss:0.018094560131430626\n",
      "Epoch 61, Batch 34 Loss:0.0038585106376558542\n",
      "Epoch 61, Batch 35 Loss:0.011073077097535133\n",
      "Epoch 61, Batch 36 Loss:0.01844555139541626\n",
      "Epoch 61, Batch 37 Loss:0.011724542826414108\n",
      "Epoch 61, Batch 38 Loss:0.021442685276269913\n",
      "Epoch 61, Batch 39 Loss:0.05799039825797081\n",
      "Epoch 61, Batch 40 Loss:0.014857761561870575\n",
      "Epoch 61, Batch 41 Loss:0.010143658146262169\n",
      "Epoch 61, Batch 42 Loss:0.04785042628645897\n",
      "Epoch 61, Batch 43 Loss:0.008364619687199593\n",
      "Epoch 61, Batch 44 Loss:0.009611413814127445\n",
      "Epoch 61, Batch 45 Loss:0.019801635295152664\n",
      "Epoch 61, Batch 46 Loss:0.02443518117070198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Batch 47 Loss:0.007548746652901173\n",
      "Epoch 61, Batch 48 Loss:0.0063087292946875095\n",
      "Epoch 61, Batch 49 Loss:0.06678248941898346\n",
      "Epoch 61, Batch 50 Loss:0.015777073800563812\n",
      "Epoch 61, Batch 51 Loss:0.01570925861597061\n",
      "Epoch 61, Batch 52 Loss:0.03157610446214676\n",
      "Epoch 61, Batch 53 Loss:0.02320430427789688\n",
      "Epoch 61, Batch 54 Loss:0.01084955409169197\n",
      "Epoch 61, Batch 55 Loss:0.017803337424993515\n",
      "Epoch 61, Batch 56 Loss:0.004939274862408638\n",
      "Epoch 61, Batch 57 Loss:0.05678751692175865\n",
      "Epoch 61, Batch 58 Loss:0.022959869354963303\n",
      "Epoch 61, Batch 59 Loss:0.02204784005880356\n",
      "Epoch 61, Batch 60 Loss:0.06576132774353027\n",
      "Epoch 61, Batch 61 Loss:0.0065908716060221195\n",
      "Epoch 61, Batch 62 Loss:0.02097943238914013\n",
      "Epoch 61, Batch 63 Loss:0.0109037384390831\n",
      "Epoch 61, Batch 64 Loss:0.014689337462186813\n",
      "Epoch 61, Batch 65 Loss:0.007453138008713722\n",
      "Epoch 61, Batch 66 Loss:0.0069621047005057335\n",
      "Epoch 61, Batch 67 Loss:0.008067670278251171\n",
      "Epoch 61, Batch 68 Loss:0.01155322976410389\n",
      "Epoch 61, Batch 69 Loss:0.029613114893436432\n",
      "Epoch 61, Batch 70 Loss:0.012000090442597866\n",
      "Epoch 61, Batch 71 Loss:0.020322458818554878\n",
      "Epoch 61, Batch 72 Loss:0.019364651292562485\n",
      "Epoch 61, Batch 73 Loss:0.012183653190732002\n",
      "Epoch 61, Batch 74 Loss:0.004227680619806051\n",
      "Epoch 61, Batch 75 Loss:0.01738019473850727\n",
      "Epoch 61, Batch 76 Loss:0.01291245874017477\n",
      "Epoch 61, Batch 77 Loss:0.02844560705125332\n",
      "Epoch 61, Batch 78 Loss:0.012985734269022942\n",
      "Epoch 61, Batch 79 Loss:0.013621184974908829\n",
      "Epoch 61, Batch 80 Loss:0.009992211125791073\n",
      "Epoch 61, Batch 81 Loss:0.0049250246956944466\n",
      "Epoch 61, Batch 82 Loss:0.004437417723238468\n",
      "Epoch 61, Batch 83 Loss:0.01994856633245945\n",
      "Epoch 61, Batch 84 Loss:0.06824461370706558\n",
      "Epoch 61, Batch 85 Loss:0.019023042172193527\n",
      "Epoch 61, Batch 86 Loss:0.012292755767703056\n",
      "Epoch 61, Batch 87 Loss:0.006290413439273834\n",
      "Epoch 61, Batch 88 Loss:0.02719690650701523\n",
      "Epoch 61, Batch 89 Loss:0.023624666035175323\n",
      "Epoch 61, Batch 90 Loss:0.035005319863557816\n",
      "Epoch 61, Batch 91 Loss:0.039069946855306625\n",
      "Epoch 61, Batch 92 Loss:0.012851669453084469\n",
      "Epoch 61, Batch 93 Loss:0.022974584251642227\n",
      "Epoch 61, Batch 94 Loss:0.012956853955984116\n",
      "Epoch 61, Batch 95 Loss:0.010390541516244411\n",
      "Epoch 61, Batch 96 Loss:0.06769488006830215\n",
      "Epoch 61, Batch 97 Loss:0.010515030473470688\n",
      "Epoch 61, Batch 98 Loss:0.0454053059220314\n",
      "Epoch 61, Batch 99 Loss:0.020639169961214066\n",
      "Epoch 61, Batch 100 Loss:0.0049108825623989105\n",
      "Epoch 61, Batch 101 Loss:0.008240671828389168\n",
      "Epoch 61, Batch 102 Loss:0.013980075716972351\n",
      "Epoch 61, Batch 103 Loss:0.0191572867333889\n",
      "Epoch 61, Batch 104 Loss:0.014332219958305359\n",
      "Epoch 61, Batch 105 Loss:0.008068902418017387\n",
      "Epoch 61, Batch 106 Loss:0.007363538723438978\n",
      "Epoch 61, Batch 107 Loss:0.008056030608713627\n",
      "Epoch 61, Batch 108 Loss:0.01597633585333824\n",
      "Epoch 61, Batch 109 Loss:0.007440789602696896\n",
      "Epoch 61, Batch 110 Loss:0.007180248387157917\n",
      "Epoch 61, Batch 111 Loss:0.025380190461874008\n",
      "Epoch 61, Batch 112 Loss:0.031158193945884705\n",
      "Epoch 61, Batch 113 Loss:0.02150224708020687\n",
      "Epoch 61, Batch 114 Loss:0.011944614350795746\n",
      "Epoch 61, Batch 115 Loss:0.00911501981317997\n",
      "Epoch 61, Batch 116 Loss:0.010581597685813904\n",
      "Epoch 61, Batch 117 Loss:0.01107441820204258\n",
      "Epoch 61, Batch 118 Loss:0.01686302199959755\n",
      "Epoch 61, Batch 119 Loss:0.011393697932362556\n",
      "Epoch 61, Batch 120 Loss:0.010776840150356293\n",
      "Epoch 61, Batch 121 Loss:0.006647628732025623\n",
      "Epoch 61, Batch 122 Loss:0.017018485814332962\n",
      "Epoch 61, Batch 123 Loss:0.007549377158284187\n",
      "Epoch 61, Batch 124 Loss:0.012657052837312222\n",
      "Epoch 61, Batch 125 Loss:0.04422205686569214\n",
      "Epoch 61, Batch 126 Loss:0.023804161697626114\n",
      "Epoch 61, Batch 127 Loss:0.030570928007364273\n",
      "Epoch 61, Batch 128 Loss:0.0658467635512352\n",
      "Epoch 61, Batch 129 Loss:0.007558648940175772\n",
      "Epoch 61, Batch 130 Loss:0.0041747973300516605\n",
      "Epoch 61, Batch 131 Loss:0.01587865687906742\n",
      "Epoch 61, Batch 132 Loss:0.02566554956138134\n",
      "Epoch 61, Batch 133 Loss:0.00884980522096157\n",
      "Epoch 61, Batch 134 Loss:0.006877407431602478\n",
      "Epoch 61, Batch 135 Loss:0.017438745126128197\n",
      "Epoch 61, Batch 136 Loss:0.011576663702726364\n",
      "Epoch 61, Batch 137 Loss:0.015597403049468994\n",
      "Epoch 61, Batch 138 Loss:0.015361777506768703\n",
      "Epoch 61, Batch 139 Loss:0.016917556524276733\n",
      "Epoch 61, Batch 140 Loss:0.006428414490073919\n",
      "Epoch 61, Batch 141 Loss:0.02897484600543976\n",
      "Epoch 61, Batch 142 Loss:0.007287823129445314\n",
      "Epoch 61, Batch 143 Loss:0.043828099966049194\n",
      "Epoch 61, Batch 144 Loss:0.013135896995663643\n",
      "Epoch 61, Batch 145 Loss:0.02353169023990631\n",
      "Epoch 61, Batch 146 Loss:0.0068662394769489765\n",
      "Epoch 61, Batch 147 Loss:0.025477606803178787\n",
      "Epoch 61, Batch 148 Loss:0.026128169149160385\n",
      "Epoch 61, Batch 149 Loss:0.005899449810385704\n",
      "Epoch 61, Batch 150 Loss:0.011823007836937904\n",
      "Epoch 61, Batch 151 Loss:0.02593470737338066\n",
      "Epoch 61, Batch 152 Loss:0.02946866676211357\n",
      "Epoch 61, Batch 153 Loss:0.012074947357177734\n",
      "Epoch 61, Batch 154 Loss:0.022774234414100647\n",
      "Epoch 61, Batch 155 Loss:0.04935641586780548\n",
      "Epoch 61, Batch 156 Loss:0.03349188715219498\n",
      "Epoch 61, Batch 157 Loss:0.019979439675807953\n",
      "Epoch 61, Batch 158 Loss:0.008565067313611507\n",
      "Epoch 61, Batch 159 Loss:0.010707447305321693\n",
      "Epoch 61, Batch 160 Loss:0.025357190519571304\n",
      "Epoch 61, Batch 161 Loss:0.037308983504772186\n",
      "Epoch 61, Batch 162 Loss:0.021605394780635834\n",
      "Epoch 61, Batch 163 Loss:0.014796379022300243\n",
      "Epoch 61, Batch 164 Loss:0.012014608830213547\n",
      "Epoch 61, Batch 165 Loss:0.025493314489722252\n",
      "Epoch 61, Batch 166 Loss:0.0132218599319458\n",
      "Epoch 61, Batch 167 Loss:0.030116207897663116\n",
      "Epoch 61, Batch 168 Loss:0.010046360082924366\n",
      "Epoch 61, Batch 169 Loss:0.039203256368637085\n",
      "Epoch 61, Batch 170 Loss:0.009198091924190521\n",
      "Epoch 61, Batch 171 Loss:0.01247821282595396\n",
      "Epoch 61, Batch 172 Loss:0.020736804232001305\n",
      "Epoch 61, Batch 173 Loss:0.02298194169998169\n",
      "Epoch 61, Batch 174 Loss:0.011586758308112621\n",
      "Epoch 61, Batch 175 Loss:0.01104659028351307\n",
      "Epoch 61, Batch 176 Loss:0.026914497837424278\n",
      "Epoch 61, Batch 177 Loss:0.027497325092554092\n",
      "Epoch 61, Batch 178 Loss:0.017797866836190224\n",
      "Epoch 61, Batch 179 Loss:0.009682638570666313\n",
      "Epoch 61, Batch 180 Loss:0.019580578431487083\n",
      "Epoch 61, Batch 181 Loss:0.005635027773678303\n",
      "Epoch 61, Batch 182 Loss:0.018471941351890564\n",
      "Epoch 61, Batch 183 Loss:0.008627684786915779\n",
      "Epoch 61, Batch 184 Loss:0.02420545369386673\n",
      "Epoch 61, Batch 185 Loss:0.011331237852573395\n",
      "Epoch 61, Batch 186 Loss:0.008517660200595856\n",
      "Epoch 61, Batch 187 Loss:0.02229943312704563\n",
      "Epoch 61, Batch 188 Loss:0.025264980271458626\n",
      "Epoch 61, Batch 189 Loss:0.030901866033673286\n",
      "Epoch 61, Batch 190 Loss:0.02074996568262577\n",
      "Epoch 61, Batch 191 Loss:0.008108535781502724\n",
      "Epoch 61, Batch 192 Loss:0.025210926309227943\n",
      "Epoch 61, Batch 193 Loss:0.015944253653287888\n",
      "Epoch 61, Batch 194 Loss:0.015563592314720154\n",
      "Epoch 61, Batch 195 Loss:0.022626811638474464\n",
      "Epoch 61, Batch 196 Loss:0.01709527149796486\n",
      "Epoch 61, Batch 197 Loss:0.028002358973026276\n",
      "Epoch 61, Batch 198 Loss:0.02272471971809864\n",
      "Epoch 61, Batch 199 Loss:0.011972326785326004\n",
      "Epoch 61, Batch 200 Loss:0.041554827243089676\n",
      "Epoch 61, Batch 201 Loss:0.0054800016805529594\n",
      "Epoch 61, Batch 202 Loss:0.07598190754652023\n",
      "Epoch 61, Batch 203 Loss:0.010179195553064346\n",
      "Epoch 61, Batch 204 Loss:0.013305733911693096\n",
      "Epoch 61, Batch 205 Loss:0.019960153847932816\n",
      "Epoch 61, Batch 206 Loss:0.01615174487233162\n",
      "Epoch 61, Batch 207 Loss:0.0082025658339262\n",
      "Epoch 61, Batch 208 Loss:0.010016268119215965\n",
      "Epoch 61, Batch 209 Loss:0.008766834624111652\n",
      "Epoch 61, Batch 210 Loss:0.010108714923262596\n",
      "Epoch 61, Batch 211 Loss:0.01707342080771923\n",
      "Epoch 61, Batch 212 Loss:0.005222685169428587\n",
      "Epoch 61, Batch 213 Loss:0.011742591857910156\n",
      "Epoch 61, Batch 214 Loss:0.044330816715955734\n",
      "Epoch 61, Batch 215 Loss:0.014341107569634914\n",
      "Epoch 61, Batch 216 Loss:0.030958302319049835\n",
      "Epoch 61, Batch 217 Loss:0.00884833000600338\n",
      "Epoch 61, Batch 218 Loss:0.02033662050962448\n",
      "Epoch 61, Batch 219 Loss:0.007905563339591026\n",
      "Epoch 61, Batch 220 Loss:0.0018877475522458553\n",
      "Epoch 61, Batch 221 Loss:0.02708488516509533\n",
      "Epoch 61, Batch 222 Loss:0.015340534970164299\n",
      "Epoch 61, Batch 223 Loss:0.02450667694211006\n",
      "Epoch 61, Batch 224 Loss:0.021541977301239967\n",
      "Epoch 61, Batch 225 Loss:0.019089408218860626\n",
      "Epoch 61, Batch 226 Loss:0.006010729819536209\n",
      "Epoch 61, Batch 227 Loss:0.006630973424762487\n",
      "Epoch 61, Batch 228 Loss:0.0059685553424060345\n",
      "Epoch 61, Batch 229 Loss:0.030924491584300995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61, Batch 230 Loss:0.048631202429533005\n",
      "Epoch 61, Batch 231 Loss:0.04974622279405594\n",
      "Epoch 61, Batch 232 Loss:0.01875467412173748\n",
      "Epoch 61, Batch 233 Loss:0.019355107098817825\n",
      "Loss in this Epoch is: 1.93551070988 %\n",
      "Accuracy in this Epoch is: 88.7000024319 %\n",
      "Epoch 62, Batch 0 Loss:0.011532089672982693\n",
      "Epoch 62, Batch 1 Loss:0.01339627057313919\n",
      "Epoch 62, Batch 2 Loss:0.010877668857574463\n",
      "Epoch 62, Batch 3 Loss:0.007326135411858559\n",
      "Epoch 62, Batch 4 Loss:0.006179426796734333\n",
      "Epoch 62, Batch 5 Loss:0.023400595411658287\n",
      "Epoch 62, Batch 6 Loss:0.005804890301078558\n",
      "Epoch 62, Batch 7 Loss:0.00820026732981205\n",
      "Epoch 62, Batch 8 Loss:0.007175016682595015\n",
      "Epoch 62, Batch 9 Loss:0.01396934688091278\n",
      "Epoch 62, Batch 10 Loss:0.010042018257081509\n",
      "Epoch 62, Batch 11 Loss:0.013027631677687168\n",
      "Epoch 62, Batch 12 Loss:0.016281992197036743\n",
      "Epoch 62, Batch 13 Loss:0.020315002650022507\n",
      "Epoch 62, Batch 14 Loss:0.01846795715391636\n",
      "Epoch 62, Batch 15 Loss:0.02383125200867653\n",
      "Epoch 62, Batch 16 Loss:0.012225653976202011\n",
      "Epoch 62, Batch 17 Loss:0.006091839633882046\n",
      "Epoch 62, Batch 18 Loss:0.020602846518158913\n",
      "Epoch 62, Batch 19 Loss:0.0018594636349007487\n",
      "Epoch 62, Batch 20 Loss:0.0158761627972126\n",
      "Epoch 62, Batch 21 Loss:0.02628723531961441\n",
      "Epoch 62, Batch 22 Loss:0.02291545458137989\n",
      "Epoch 62, Batch 23 Loss:0.010947590693831444\n",
      "Epoch 62, Batch 24 Loss:0.014728806912899017\n",
      "Epoch 62, Batch 25 Loss:0.029820898547768593\n",
      "Epoch 62, Batch 26 Loss:0.014309005811810493\n",
      "Epoch 62, Batch 27 Loss:0.029866818338632584\n",
      "Epoch 62, Batch 28 Loss:0.011299536563456059\n",
      "Epoch 62, Batch 29 Loss:0.024281194433569908\n",
      "Epoch 62, Batch 30 Loss:0.01546443346887827\n",
      "Epoch 62, Batch 31 Loss:0.0063919308595359325\n",
      "Epoch 62, Batch 32 Loss:0.017170557752251625\n",
      "Epoch 62, Batch 33 Loss:0.026607230305671692\n",
      "Epoch 62, Batch 34 Loss:0.013718508183956146\n",
      "Epoch 62, Batch 35 Loss:0.03242827579379082\n",
      "Epoch 62, Batch 36 Loss:0.0033489465713500977\n",
      "Epoch 62, Batch 37 Loss:0.03726153448224068\n",
      "Epoch 62, Batch 38 Loss:0.011697357520461082\n",
      "Epoch 62, Batch 39 Loss:0.0037811920046806335\n",
      "Epoch 62, Batch 40 Loss:0.012110915035009384\n",
      "Epoch 62, Batch 41 Loss:0.00656158896163106\n",
      "Epoch 62, Batch 42 Loss:0.01245154533535242\n",
      "Epoch 62, Batch 43 Loss:0.04276704788208008\n",
      "Epoch 62, Batch 44 Loss:0.005074309650808573\n",
      "Epoch 62, Batch 45 Loss:0.013113820925354958\n",
      "Epoch 62, Batch 46 Loss:0.020094962790608406\n",
      "Epoch 62, Batch 47 Loss:0.0061248792335391045\n",
      "Epoch 62, Batch 48 Loss:0.012310376390814781\n",
      "Epoch 62, Batch 49 Loss:0.007060460280627012\n",
      "Epoch 62, Batch 50 Loss:0.0527883917093277\n",
      "Epoch 62, Batch 51 Loss:0.02609473653137684\n",
      "Epoch 62, Batch 52 Loss:0.03277254104614258\n",
      "Epoch 62, Batch 53 Loss:0.016413016244769096\n",
      "Epoch 62, Batch 54 Loss:0.015401320531964302\n",
      "Epoch 62, Batch 55 Loss:0.022090474143624306\n",
      "Epoch 62, Batch 56 Loss:0.008219495415687561\n",
      "Epoch 62, Batch 57 Loss:0.007912784814834595\n",
      "Epoch 62, Batch 58 Loss:0.013634616509079933\n",
      "Epoch 62, Batch 59 Loss:0.012463473714888096\n",
      "Epoch 62, Batch 60 Loss:0.11118366569280624\n",
      "Epoch 62, Batch 61 Loss:0.04944295063614845\n",
      "Epoch 62, Batch 62 Loss:0.02836833894252777\n",
      "Epoch 62, Batch 63 Loss:0.011879733763635159\n",
      "Epoch 62, Batch 64 Loss:0.023879459127783775\n",
      "Epoch 62, Batch 65 Loss:0.01894025132060051\n",
      "Epoch 62, Batch 66 Loss:0.02253328077495098\n",
      "Epoch 62, Batch 67 Loss:0.02920585684478283\n",
      "Epoch 62, Batch 68 Loss:0.1872413605451584\n",
      "Epoch 62, Batch 69 Loss:0.033406056463718414\n",
      "Epoch 62, Batch 70 Loss:0.14501667022705078\n",
      "Epoch 62, Batch 71 Loss:0.059683896601200104\n",
      "Epoch 62, Batch 72 Loss:0.08781441301107407\n",
      "Epoch 62, Batch 73 Loss:0.161545991897583\n",
      "Epoch 62, Batch 74 Loss:0.057255372405052185\n",
      "Epoch 62, Batch 75 Loss:0.14910441637039185\n",
      "Epoch 62, Batch 76 Loss:0.09499147534370422\n",
      "Epoch 62, Batch 77 Loss:0.06170368939638138\n",
      "Epoch 62, Batch 78 Loss:0.11515644937753677\n",
      "Epoch 62, Batch 79 Loss:0.15027546882629395\n",
      "Epoch 62, Batch 80 Loss:0.10065997391939163\n",
      "Epoch 62, Batch 81 Loss:0.06726738810539246\n",
      "Epoch 62, Batch 82 Loss:0.05809113010764122\n",
      "Epoch 62, Batch 83 Loss:0.06693168729543686\n",
      "Epoch 62, Batch 84 Loss:0.09191776067018509\n",
      "Epoch 62, Batch 85 Loss:0.06430886685848236\n",
      "Epoch 62, Batch 86 Loss:0.02873314917087555\n",
      "Epoch 62, Batch 87 Loss:0.028215957805514336\n",
      "Epoch 62, Batch 88 Loss:0.05904371663928032\n",
      "Epoch 62, Batch 89 Loss:0.04969942569732666\n",
      "Epoch 62, Batch 90 Loss:0.03817983344197273\n",
      "Epoch 62, Batch 91 Loss:0.04743222892284393\n",
      "Epoch 62, Batch 92 Loss:0.03485846891999245\n",
      "Epoch 62, Batch 93 Loss:0.028857680037617683\n",
      "Epoch 62, Batch 94 Loss:0.05216101184487343\n",
      "Epoch 62, Batch 95 Loss:0.06275981664657593\n",
      "Epoch 62, Batch 96 Loss:0.049168698489665985\n",
      "Epoch 62, Batch 97 Loss:0.020641133189201355\n",
      "Epoch 62, Batch 98 Loss:0.06696562469005585\n",
      "Epoch 62, Batch 99 Loss:0.05319686233997345\n",
      "Epoch 62, Batch 100 Loss:0.04632018506526947\n",
      "Epoch 62, Batch 101 Loss:0.019926032051444054\n",
      "Epoch 62, Batch 102 Loss:0.04900806397199631\n",
      "Epoch 62, Batch 103 Loss:0.05849379673600197\n",
      "Epoch 62, Batch 104 Loss:0.03170713409781456\n",
      "Epoch 62, Batch 105 Loss:0.02798205241560936\n",
      "Epoch 62, Batch 106 Loss:0.04250465705990791\n",
      "Epoch 62, Batch 107 Loss:0.04607540741562843\n",
      "Epoch 62, Batch 108 Loss:0.020786430686712265\n",
      "Epoch 62, Batch 109 Loss:0.0424165241420269\n",
      "Epoch 62, Batch 110 Loss:0.027828022837638855\n",
      "Epoch 62, Batch 111 Loss:0.06605420261621475\n",
      "Epoch 62, Batch 112 Loss:0.06268593668937683\n",
      "Epoch 62, Batch 113 Loss:0.02985859289765358\n",
      "Epoch 62, Batch 114 Loss:0.017904072999954224\n",
      "Epoch 62, Batch 115 Loss:0.030611034482717514\n",
      "Epoch 62, Batch 116 Loss:0.05432906001806259\n",
      "Epoch 62, Batch 117 Loss:0.07834063470363617\n",
      "Epoch 62, Batch 118 Loss:0.019012097269296646\n",
      "Epoch 62, Batch 119 Loss:0.07428613305091858\n",
      "Epoch 62, Batch 120 Loss:0.04888491332530975\n",
      "Epoch 62, Batch 121 Loss:0.020459916442632675\n",
      "Epoch 62, Batch 122 Loss:0.049844808876514435\n",
      "Epoch 62, Batch 123 Loss:0.028329916298389435\n",
      "Epoch 62, Batch 124 Loss:0.01216788962483406\n",
      "Epoch 62, Batch 125 Loss:0.038181815296411514\n",
      "Epoch 62, Batch 126 Loss:0.03770621120929718\n",
      "Epoch 62, Batch 127 Loss:0.062388695776462555\n",
      "Epoch 62, Batch 128 Loss:0.059226326644420624\n",
      "Epoch 62, Batch 129 Loss:0.02783418819308281\n",
      "Epoch 62, Batch 130 Loss:0.01940954476594925\n",
      "Epoch 62, Batch 131 Loss:0.030016876757144928\n",
      "Epoch 62, Batch 132 Loss:0.030759543180465698\n",
      "Epoch 62, Batch 133 Loss:0.014415984973311424\n",
      "Epoch 62, Batch 134 Loss:0.025737760588526726\n",
      "Epoch 62, Batch 135 Loss:0.027551326900720596\n",
      "Epoch 62, Batch 136 Loss:0.042343392968177795\n",
      "Epoch 62, Batch 137 Loss:0.011167557910084724\n",
      "Epoch 62, Batch 138 Loss:0.025211401283740997\n",
      "Epoch 62, Batch 139 Loss:0.0292208269238472\n",
      "Epoch 62, Batch 140 Loss:0.03511315956711769\n",
      "Epoch 62, Batch 141 Loss:0.012379368767142296\n",
      "Epoch 62, Batch 142 Loss:0.014990715309977531\n",
      "Epoch 62, Batch 143 Loss:0.029752496629953384\n",
      "Epoch 62, Batch 144 Loss:0.0087448013946414\n",
      "Epoch 62, Batch 145 Loss:0.061550240963697433\n",
      "Epoch 62, Batch 146 Loss:0.015427656471729279\n",
      "Epoch 62, Batch 147 Loss:0.01615171693265438\n",
      "Epoch 62, Batch 148 Loss:0.020777367055416107\n",
      "Epoch 62, Batch 149 Loss:0.009820313192903996\n",
      "Epoch 62, Batch 150 Loss:0.020009245723485947\n",
      "Epoch 62, Batch 151 Loss:0.021962955594062805\n",
      "Epoch 62, Batch 152 Loss:0.022210869938135147\n",
      "Epoch 62, Batch 153 Loss:0.01943780481815338\n",
      "Epoch 62, Batch 154 Loss:0.01540747843682766\n",
      "Epoch 62, Batch 155 Loss:0.007631930988281965\n",
      "Epoch 62, Batch 156 Loss:0.0194981899112463\n",
      "Epoch 62, Batch 157 Loss:0.037141770124435425\n",
      "Epoch 62, Batch 158 Loss:0.029697580263018608\n",
      "Epoch 62, Batch 159 Loss:0.00749331247061491\n",
      "Epoch 62, Batch 160 Loss:0.04556325078010559\n",
      "Epoch 62, Batch 161 Loss:0.02882341295480728\n",
      "Epoch 62, Batch 162 Loss:0.027592984959483147\n",
      "Epoch 62, Batch 163 Loss:0.01746533066034317\n",
      "Epoch 62, Batch 164 Loss:0.0304657444357872\n",
      "Epoch 62, Batch 165 Loss:0.025638964027166367\n",
      "Epoch 62, Batch 166 Loss:0.019867807626724243\n",
      "Epoch 62, Batch 167 Loss:0.00995720736682415\n",
      "Epoch 62, Batch 168 Loss:0.020295308902859688\n",
      "Epoch 62, Batch 169 Loss:0.027675163000822067\n",
      "Epoch 62, Batch 170 Loss:0.03530192747712135\n",
      "Epoch 62, Batch 171 Loss:0.012670880183577538\n",
      "Epoch 62, Batch 172 Loss:0.012030333280563354\n",
      "Epoch 62, Batch 173 Loss:0.03366079926490784\n",
      "Epoch 62, Batch 174 Loss:0.01716754585504532\n",
      "Epoch 62, Batch 175 Loss:0.006197230890393257\n",
      "Epoch 62, Batch 176 Loss:0.009699770249426365\n",
      "Epoch 62, Batch 177 Loss:0.008139552548527718\n",
      "Epoch 62, Batch 178 Loss:0.01042480580508709\n",
      "Epoch 62, Batch 179 Loss:0.004118073731660843\n",
      "Epoch 62, Batch 180 Loss:0.005214307457208633\n",
      "Epoch 62, Batch 181 Loss:0.005638844333589077\n",
      "Epoch 62, Batch 182 Loss:0.023254290223121643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62, Batch 183 Loss:0.021785501390695572\n",
      "Epoch 62, Batch 184 Loss:0.010336537845432758\n",
      "Epoch 62, Batch 185 Loss:0.018097426742315292\n",
      "Epoch 62, Batch 186 Loss:0.05513441935181618\n",
      "Epoch 62, Batch 187 Loss:0.043827369809150696\n",
      "Epoch 62, Batch 188 Loss:0.017147967591881752\n",
      "Epoch 62, Batch 189 Loss:0.013722235336899757\n",
      "Epoch 62, Batch 190 Loss:0.014977787621319294\n",
      "Epoch 62, Batch 191 Loss:0.041176937520504\n",
      "Epoch 62, Batch 192 Loss:0.02967885136604309\n",
      "Epoch 62, Batch 193 Loss:0.01706254854798317\n",
      "Epoch 62, Batch 194 Loss:0.030541561543941498\n",
      "Epoch 62, Batch 195 Loss:0.028920425102114677\n",
      "Epoch 62, Batch 196 Loss:0.047683075070381165\n",
      "Epoch 62, Batch 197 Loss:0.02881721407175064\n",
      "Epoch 62, Batch 198 Loss:0.01312774233520031\n",
      "Epoch 62, Batch 199 Loss:0.04513649642467499\n",
      "Epoch 62, Batch 200 Loss:0.01620645448565483\n",
      "Epoch 62, Batch 201 Loss:0.011368636041879654\n",
      "Epoch 62, Batch 202 Loss:0.014167054556310177\n",
      "Epoch 62, Batch 203 Loss:0.032419972121715546\n",
      "Epoch 62, Batch 204 Loss:0.014650413766503334\n",
      "Epoch 62, Batch 205 Loss:0.017593877390027046\n",
      "Epoch 62, Batch 206 Loss:0.025975611060857773\n",
      "Epoch 62, Batch 207 Loss:0.05017451196908951\n",
      "Epoch 62, Batch 208 Loss:0.01983325555920601\n",
      "Epoch 62, Batch 209 Loss:0.021279094740748405\n",
      "Epoch 62, Batch 210 Loss:0.003783721709623933\n",
      "Epoch 62, Batch 211 Loss:0.027999883517622948\n",
      "Epoch 62, Batch 212 Loss:0.07524660229682922\n",
      "Epoch 62, Batch 213 Loss:0.03404945135116577\n",
      "Epoch 62, Batch 214 Loss:0.029254566878080368\n",
      "Epoch 62, Batch 215 Loss:0.05692164599895477\n",
      "Epoch 62, Batch 216 Loss:0.09043153375387192\n",
      "Epoch 62, Batch 217 Loss:0.02886446937918663\n",
      "Epoch 62, Batch 218 Loss:0.06194251775741577\n",
      "Epoch 62, Batch 219 Loss:0.030602071434259415\n",
      "Epoch 62, Batch 220 Loss:0.018541695550084114\n",
      "Epoch 62, Batch 221 Loss:0.022302910685539246\n",
      "Epoch 62, Batch 222 Loss:0.018655765801668167\n",
      "Epoch 62, Batch 223 Loss:0.02307836525142193\n",
      "Epoch 62, Batch 224 Loss:0.04218427836894989\n",
      "Epoch 62, Batch 225 Loss:0.04835173860192299\n",
      "Epoch 62, Batch 226 Loss:0.03253651782870293\n",
      "Epoch 62, Batch 227 Loss:0.0341462567448616\n",
      "Epoch 62, Batch 228 Loss:0.01953008398413658\n",
      "Epoch 62, Batch 229 Loss:0.018961166962981224\n",
      "Epoch 62, Batch 230 Loss:0.022885637357831\n",
      "Epoch 62, Batch 231 Loss:0.014205447398126125\n",
      "Epoch 62, Batch 232 Loss:0.01024145632982254\n",
      "Epoch 62, Batch 233 Loss:0.016556400805711746\n",
      "Loss in this Epoch is: 1.65564008057 %\n",
      "Accuracy in this Epoch is: 88.4700000286 %\n",
      "Epoch 63, Batch 0 Loss:0.018016330897808075\n",
      "Epoch 63, Batch 1 Loss:0.005893722642213106\n",
      "Epoch 63, Batch 2 Loss:0.004274718463420868\n",
      "Epoch 63, Batch 3 Loss:0.04520954191684723\n",
      "Epoch 63, Batch 4 Loss:0.007051513530313969\n",
      "Epoch 63, Batch 5 Loss:0.013048353604972363\n",
      "Epoch 63, Batch 6 Loss:0.013507057912647724\n",
      "Epoch 63, Batch 7 Loss:0.03734302893280983\n",
      "Epoch 63, Batch 8 Loss:0.03746664896607399\n",
      "Epoch 63, Batch 9 Loss:0.030324405059218407\n",
      "Epoch 63, Batch 10 Loss:0.009576773270964622\n",
      "Epoch 63, Batch 11 Loss:0.010696915909647942\n",
      "Epoch 63, Batch 12 Loss:0.03416615352034569\n",
      "Epoch 63, Batch 13 Loss:0.015416846610605717\n",
      "Epoch 63, Batch 14 Loss:0.024867162108421326\n",
      "Epoch 63, Batch 15 Loss:0.026530342176556587\n",
      "Epoch 63, Batch 16 Loss:0.018250733613967896\n",
      "Epoch 63, Batch 17 Loss:0.04303083196282387\n",
      "Epoch 63, Batch 18 Loss:0.01836976781487465\n",
      "Epoch 63, Batch 19 Loss:0.05542462319135666\n",
      "Epoch 63, Batch 20 Loss:0.041518453508615494\n",
      "Epoch 63, Batch 21 Loss:0.02864835411310196\n",
      "Epoch 63, Batch 22 Loss:0.09206105023622513\n",
      "Epoch 63, Batch 23 Loss:0.017162004485726357\n",
      "Epoch 63, Batch 24 Loss:0.03097439557313919\n",
      "Epoch 63, Batch 25 Loss:0.03425070643424988\n",
      "Epoch 63, Batch 26 Loss:0.013937127776443958\n",
      "Epoch 63, Batch 27 Loss:0.06468670815229416\n",
      "Epoch 63, Batch 28 Loss:0.01717553660273552\n",
      "Epoch 63, Batch 29 Loss:0.05280518904328346\n",
      "Epoch 63, Batch 30 Loss:0.019263185560703278\n",
      "Epoch 63, Batch 31 Loss:0.022500211372971535\n",
      "Epoch 63, Batch 32 Loss:0.02785695157945156\n",
      "Epoch 63, Batch 33 Loss:0.03469666466116905\n",
      "Epoch 63, Batch 34 Loss:0.01961163990199566\n",
      "Epoch 63, Batch 35 Loss:0.03300466388463974\n",
      "Epoch 63, Batch 36 Loss:0.0229380801320076\n",
      "Epoch 63, Batch 37 Loss:0.02597191371023655\n",
      "Epoch 63, Batch 38 Loss:0.012662035413086414\n",
      "Epoch 63, Batch 39 Loss:0.008340842090547085\n",
      "Epoch 63, Batch 40 Loss:0.040270157158374786\n",
      "Epoch 63, Batch 41 Loss:0.019664844498038292\n",
      "Epoch 63, Batch 42 Loss:0.059389833360910416\n",
      "Epoch 63, Batch 43 Loss:0.024699890986084938\n",
      "Epoch 63, Batch 44 Loss:0.013596600852906704\n",
      "Epoch 63, Batch 45 Loss:0.02195739932358265\n",
      "Epoch 63, Batch 46 Loss:0.03419749066233635\n",
      "Epoch 63, Batch 47 Loss:0.013806217350065708\n",
      "Epoch 63, Batch 48 Loss:0.006647458765655756\n",
      "Epoch 63, Batch 49 Loss:0.026208100840449333\n",
      "Epoch 63, Batch 50 Loss:0.011931903660297394\n",
      "Epoch 63, Batch 51 Loss:0.014759487472474575\n",
      "Epoch 63, Batch 52 Loss:0.05086607486009598\n",
      "Epoch 63, Batch 53 Loss:0.01589101180434227\n",
      "Epoch 63, Batch 54 Loss:0.03629159927368164\n",
      "Epoch 63, Batch 55 Loss:0.00702793151140213\n",
      "Epoch 63, Batch 56 Loss:0.05447316914796829\n",
      "Epoch 63, Batch 57 Loss:0.02731986716389656\n",
      "Epoch 63, Batch 58 Loss:0.040325943380594254\n",
      "Epoch 63, Batch 59 Loss:0.017746102064847946\n",
      "Epoch 63, Batch 60 Loss:0.02348872274160385\n",
      "Epoch 63, Batch 61 Loss:0.01979988068342209\n",
      "Epoch 63, Batch 62 Loss:0.02464120462536812\n",
      "Epoch 63, Batch 63 Loss:0.031549401581287384\n",
      "Epoch 63, Batch 64 Loss:0.06937400251626968\n",
      "Epoch 63, Batch 65 Loss:0.008328904397785664\n",
      "Epoch 63, Batch 66 Loss:0.01901601254940033\n",
      "Epoch 63, Batch 67 Loss:0.025764157995581627\n",
      "Epoch 63, Batch 68 Loss:0.05457175895571709\n",
      "Epoch 63, Batch 69 Loss:0.009951812215149403\n",
      "Epoch 63, Batch 70 Loss:0.015398654155433178\n",
      "Epoch 63, Batch 71 Loss:0.04377051070332527\n",
      "Epoch 63, Batch 72 Loss:0.017693104222416878\n",
      "Epoch 63, Batch 73 Loss:0.038389261811971664\n",
      "Epoch 63, Batch 74 Loss:0.01479297410696745\n",
      "Epoch 63, Batch 75 Loss:0.022429203614592552\n",
      "Epoch 63, Batch 76 Loss:0.015906071290373802\n",
      "Epoch 63, Batch 77 Loss:0.012710238806903362\n",
      "Epoch 63, Batch 78 Loss:0.027601147070527077\n",
      "Epoch 63, Batch 79 Loss:0.022062575444579124\n",
      "Epoch 63, Batch 80 Loss:0.006909578572958708\n",
      "Epoch 63, Batch 81 Loss:0.009373173117637634\n",
      "Epoch 63, Batch 82 Loss:0.03198688477277756\n",
      "Epoch 63, Batch 83 Loss:0.014121769927442074\n",
      "Epoch 63, Batch 84 Loss:0.00824491772800684\n",
      "Epoch 63, Batch 85 Loss:0.0791616439819336\n",
      "Epoch 63, Batch 86 Loss:0.04017304256558418\n",
      "Epoch 63, Batch 87 Loss:0.009970385581254959\n",
      "Epoch 63, Batch 88 Loss:0.007098843343555927\n",
      "Epoch 63, Batch 89 Loss:0.030277283862233162\n",
      "Epoch 63, Batch 90 Loss:0.09131575375795364\n",
      "Epoch 63, Batch 91 Loss:0.014222025871276855\n",
      "Epoch 63, Batch 92 Loss:0.010287933051586151\n",
      "Epoch 63, Batch 93 Loss:0.009471580386161804\n",
      "Epoch 63, Batch 94 Loss:0.018265534192323685\n",
      "Epoch 63, Batch 95 Loss:0.007535919547080994\n",
      "Epoch 63, Batch 96 Loss:0.007857412099838257\n",
      "Epoch 63, Batch 97 Loss:0.031232822686433792\n",
      "Epoch 63, Batch 98 Loss:0.011804620735347271\n",
      "Epoch 63, Batch 99 Loss:0.02120117098093033\n",
      "Epoch 63, Batch 100 Loss:0.012130809016525745\n",
      "Epoch 63, Batch 101 Loss:0.010824840515851974\n",
      "Epoch 63, Batch 102 Loss:0.014407161623239517\n",
      "Epoch 63, Batch 103 Loss:0.013511067256331444\n",
      "Epoch 63, Batch 104 Loss:0.006812677253037691\n",
      "Epoch 63, Batch 105 Loss:0.010351493023335934\n",
      "Epoch 63, Batch 106 Loss:0.028879258781671524\n",
      "Epoch 63, Batch 107 Loss:0.026985786855220795\n",
      "Epoch 63, Batch 108 Loss:0.02524339221417904\n",
      "Epoch 63, Batch 109 Loss:0.0163984727114439\n",
      "Epoch 63, Batch 110 Loss:0.03899506479501724\n",
      "Epoch 63, Batch 111 Loss:0.012979378923773766\n",
      "Epoch 63, Batch 112 Loss:0.009327525272965431\n",
      "Epoch 63, Batch 113 Loss:0.023639103397727013\n",
      "Epoch 63, Batch 114 Loss:0.012218480929732323\n",
      "Epoch 63, Batch 115 Loss:0.016768211498856544\n",
      "Epoch 63, Batch 116 Loss:0.051268287003040314\n",
      "Epoch 63, Batch 117 Loss:0.03967816382646561\n",
      "Epoch 63, Batch 118 Loss:0.09589408338069916\n",
      "Epoch 63, Batch 119 Loss:0.09911124408245087\n",
      "Epoch 63, Batch 120 Loss:0.013203727081418037\n",
      "Epoch 63, Batch 121 Loss:0.028921622782945633\n",
      "Epoch 63, Batch 122 Loss:0.023191779851913452\n",
      "Epoch 63, Batch 123 Loss:0.015629779547452927\n",
      "Epoch 63, Batch 124 Loss:0.04220760241150856\n",
      "Epoch 63, Batch 125 Loss:0.02859000489115715\n",
      "Epoch 63, Batch 126 Loss:0.02446216158568859\n",
      "Epoch 63, Batch 127 Loss:0.05300205573439598\n",
      "Epoch 63, Batch 128 Loss:0.029724370688199997\n",
      "Epoch 63, Batch 129 Loss:0.013756953179836273\n",
      "Epoch 63, Batch 130 Loss:0.046087175607681274\n",
      "Epoch 63, Batch 131 Loss:0.03835894539952278\n",
      "Epoch 63, Batch 132 Loss:0.02841959521174431\n",
      "Epoch 63, Batch 133 Loss:0.04626160115003586\n",
      "Epoch 63, Batch 134 Loss:0.00984668917953968\n",
      "Epoch 63, Batch 135 Loss:0.034621670842170715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63, Batch 136 Loss:0.021137522533535957\n",
      "Epoch 63, Batch 137 Loss:0.014890328049659729\n",
      "Epoch 63, Batch 138 Loss:0.04707960784435272\n",
      "Epoch 63, Batch 139 Loss:0.011596577242016792\n",
      "Epoch 63, Batch 140 Loss:0.008300412446260452\n",
      "Epoch 63, Batch 141 Loss:0.030569156631827354\n",
      "Epoch 63, Batch 142 Loss:0.022701479494571686\n",
      "Epoch 63, Batch 143 Loss:0.03231395408511162\n",
      "Epoch 63, Batch 144 Loss:0.019106827676296234\n",
      "Epoch 63, Batch 145 Loss:0.014526608400046825\n",
      "Epoch 63, Batch 146 Loss:0.03244407847523689\n",
      "Epoch 63, Batch 147 Loss:0.030786391347646713\n",
      "Epoch 63, Batch 148 Loss:0.017771515995264053\n",
      "Epoch 63, Batch 149 Loss:0.07391194254159927\n",
      "Epoch 63, Batch 150 Loss:0.03264043852686882\n",
      "Epoch 63, Batch 151 Loss:0.007057631853967905\n",
      "Epoch 63, Batch 152 Loss:0.01727469451725483\n",
      "Epoch 63, Batch 153 Loss:0.0243915356695652\n",
      "Epoch 63, Batch 154 Loss:0.014694252982735634\n",
      "Epoch 63, Batch 155 Loss:0.02380291372537613\n",
      "Epoch 63, Batch 156 Loss:0.011055433191359043\n",
      "Epoch 63, Batch 157 Loss:0.03213569521903992\n",
      "Epoch 63, Batch 158 Loss:0.009111420251429081\n",
      "Epoch 63, Batch 159 Loss:0.027734855189919472\n",
      "Epoch 63, Batch 160 Loss:0.010870683938264847\n",
      "Epoch 63, Batch 161 Loss:0.0033953599631786346\n",
      "Epoch 63, Batch 162 Loss:0.06687836349010468\n",
      "Epoch 63, Batch 163 Loss:0.017163237556815147\n",
      "Epoch 63, Batch 164 Loss:0.01621418446302414\n",
      "Epoch 63, Batch 165 Loss:0.0062909238040447235\n",
      "Epoch 63, Batch 166 Loss:0.008656182326376438\n",
      "Epoch 63, Batch 167 Loss:0.04584749788045883\n",
      "Epoch 63, Batch 168 Loss:0.007406146265566349\n",
      "Epoch 63, Batch 169 Loss:0.015565520152449608\n",
      "Epoch 63, Batch 170 Loss:0.01519008632749319\n",
      "Epoch 63, Batch 171 Loss:0.0059634060598909855\n",
      "Epoch 63, Batch 172 Loss:0.0037061660550534725\n",
      "Epoch 63, Batch 173 Loss:0.04163311421871185\n",
      "Epoch 63, Batch 174 Loss:0.02889622189104557\n",
      "Epoch 63, Batch 175 Loss:0.037457093596458435\n",
      "Epoch 63, Batch 176 Loss:0.04368242993950844\n",
      "Epoch 63, Batch 177 Loss:0.012298347428441048\n",
      "Epoch 63, Batch 178 Loss:0.009597891010344028\n",
      "Epoch 63, Batch 179 Loss:0.007347242906689644\n",
      "Epoch 63, Batch 180 Loss:0.018338192254304886\n",
      "Epoch 63, Batch 181 Loss:0.05228715389966965\n",
      "Epoch 63, Batch 182 Loss:0.010401934385299683\n",
      "Epoch 63, Batch 183 Loss:0.008616119623184204\n",
      "Epoch 63, Batch 184 Loss:0.029343176633119583\n",
      "Epoch 63, Batch 185 Loss:0.015873178839683533\n",
      "Epoch 63, Batch 186 Loss:0.005477364175021648\n",
      "Epoch 63, Batch 187 Loss:0.04496283456683159\n",
      "Epoch 63, Batch 188 Loss:0.0112533550709486\n",
      "Epoch 63, Batch 189 Loss:0.014653783291578293\n",
      "Epoch 63, Batch 190 Loss:0.043118659406900406\n",
      "Epoch 63, Batch 191 Loss:0.010242076590657234\n",
      "Epoch 63, Batch 192 Loss:0.006553051061928272\n",
      "Epoch 63, Batch 193 Loss:0.02548903040587902\n",
      "Epoch 63, Batch 194 Loss:0.017485160380601883\n",
      "Epoch 63, Batch 195 Loss:0.011046288534998894\n",
      "Epoch 63, Batch 196 Loss:0.020093360915780067\n",
      "Epoch 63, Batch 197 Loss:0.015889432281255722\n",
      "Epoch 63, Batch 198 Loss:0.06457231938838959\n",
      "Epoch 63, Batch 199 Loss:0.009546788409352303\n",
      "Epoch 63, Batch 200 Loss:0.01203069742769003\n",
      "Epoch 63, Batch 201 Loss:0.02994353324174881\n",
      "Epoch 63, Batch 202 Loss:0.01790982112288475\n",
      "Epoch 63, Batch 203 Loss:0.016026483848690987\n",
      "Epoch 63, Batch 204 Loss:0.028650358319282532\n",
      "Epoch 63, Batch 205 Loss:0.05209481716156006\n",
      "Epoch 63, Batch 206 Loss:0.028146158903837204\n",
      "Epoch 63, Batch 207 Loss:0.014726221561431885\n",
      "Epoch 63, Batch 208 Loss:0.01763007417321205\n",
      "Epoch 63, Batch 209 Loss:0.016075709834694862\n",
      "Epoch 63, Batch 210 Loss:0.00842379778623581\n",
      "Epoch 63, Batch 211 Loss:0.011409245431423187\n",
      "Epoch 63, Batch 212 Loss:0.04739513248205185\n",
      "Epoch 63, Batch 213 Loss:0.02263244055211544\n",
      "Epoch 63, Batch 214 Loss:0.01235363632440567\n",
      "Epoch 63, Batch 215 Loss:0.01313771866261959\n",
      "Epoch 63, Batch 216 Loss:0.04208967834711075\n",
      "Epoch 63, Batch 217 Loss:0.022705398499965668\n",
      "Epoch 63, Batch 218 Loss:0.013041913509368896\n",
      "Epoch 63, Batch 219 Loss:0.035378675907850266\n",
      "Epoch 63, Batch 220 Loss:0.005724290385842323\n",
      "Epoch 63, Batch 221 Loss:0.024263232946395874\n",
      "Epoch 63, Batch 222 Loss:0.014222580939531326\n",
      "Epoch 63, Batch 223 Loss:0.004300709813833237\n",
      "Epoch 63, Batch 224 Loss:0.02283846214413643\n",
      "Epoch 63, Batch 225 Loss:0.0187231358140707\n",
      "Epoch 63, Batch 226 Loss:0.02634686976671219\n",
      "Epoch 63, Batch 227 Loss:0.02170579321682453\n",
      "Epoch 63, Batch 228 Loss:0.02010582759976387\n",
      "Epoch 63, Batch 229 Loss:0.008308225311338902\n",
      "Epoch 63, Batch 230 Loss:0.00731021910905838\n",
      "Epoch 63, Batch 231 Loss:0.01519538089632988\n",
      "Epoch 63, Batch 232 Loss:0.013636748306453228\n",
      "Epoch 63, Batch 233 Loss:0.011591595597565174\n",
      "Loss in this Epoch is: 1.15915955976 %\n",
      "Accuracy in this Epoch is: 88.5500013828 %\n",
      "Epoch 64, Batch 0 Loss:0.010525730438530445\n",
      "Epoch 64, Batch 1 Loss:0.007797869388014078\n",
      "Epoch 64, Batch 2 Loss:0.01082519069314003\n",
      "Epoch 64, Batch 3 Loss:0.010126681998372078\n",
      "Epoch 64, Batch 4 Loss:0.0032021331135183573\n",
      "Epoch 64, Batch 5 Loss:0.007910787127912045\n",
      "Epoch 64, Batch 6 Loss:0.010799791663885117\n",
      "Epoch 64, Batch 7 Loss:0.005197676829993725\n",
      "Epoch 64, Batch 8 Loss:0.02069469541311264\n",
      "Epoch 64, Batch 9 Loss:0.007126715034246445\n",
      "Epoch 64, Batch 10 Loss:0.002889141673222184\n",
      "Epoch 64, Batch 11 Loss:0.00365202478133142\n",
      "Epoch 64, Batch 12 Loss:0.0064934492111206055\n",
      "Epoch 64, Batch 13 Loss:0.0013148232828825712\n",
      "Epoch 64, Batch 14 Loss:0.014183898456394672\n",
      "Epoch 64, Batch 15 Loss:0.008766813203692436\n",
      "Epoch 64, Batch 16 Loss:0.013934716582298279\n",
      "Epoch 64, Batch 17 Loss:0.015321868471801281\n",
      "Epoch 64, Batch 18 Loss:0.010159105062484741\n",
      "Epoch 64, Batch 19 Loss:0.007389023434370756\n",
      "Epoch 64, Batch 20 Loss:0.006212519481778145\n",
      "Epoch 64, Batch 21 Loss:0.006679909769445658\n",
      "Epoch 64, Batch 22 Loss:0.004205843433737755\n",
      "Epoch 64, Batch 23 Loss:0.035564955323934555\n",
      "Epoch 64, Batch 24 Loss:0.0044556488282978535\n",
      "Epoch 64, Batch 25 Loss:0.003420873312279582\n",
      "Epoch 64, Batch 26 Loss:0.006082944106310606\n",
      "Epoch 64, Batch 27 Loss:0.016947923228144646\n",
      "Epoch 64, Batch 28 Loss:0.021167736500501633\n",
      "Epoch 64, Batch 29 Loss:0.004925570450723171\n",
      "Epoch 64, Batch 30 Loss:0.0021243183873593807\n",
      "Epoch 64, Batch 31 Loss:0.0009616606403142214\n",
      "Epoch 64, Batch 32 Loss:0.022970467805862427\n",
      "Epoch 64, Batch 33 Loss:0.008798305876553059\n",
      "Epoch 64, Batch 34 Loss:0.006201668176800013\n",
      "Epoch 64, Batch 35 Loss:0.00814899429678917\n",
      "Epoch 64, Batch 36 Loss:0.006975078489631414\n",
      "Epoch 64, Batch 37 Loss:0.021574368700385094\n",
      "Epoch 64, Batch 38 Loss:0.013328096829354763\n",
      "Epoch 64, Batch 39 Loss:0.0020237809512764215\n",
      "Epoch 64, Batch 40 Loss:0.003404392395168543\n",
      "Epoch 64, Batch 41 Loss:0.007444918155670166\n",
      "Epoch 64, Batch 42 Loss:0.002400581492111087\n",
      "Epoch 64, Batch 43 Loss:0.014958261512219906\n",
      "Epoch 64, Batch 44 Loss:0.01641293615102768\n",
      "Epoch 64, Batch 45 Loss:0.0031595940236002207\n",
      "Epoch 64, Batch 46 Loss:0.02695252001285553\n",
      "Epoch 64, Batch 47 Loss:0.02207908220589161\n",
      "Epoch 64, Batch 48 Loss:0.009052609093487263\n",
      "Epoch 64, Batch 49 Loss:0.004958640784025192\n",
      "Epoch 64, Batch 50 Loss:0.0040088011883199215\n",
      "Epoch 64, Batch 51 Loss:0.01323690265417099\n",
      "Epoch 64, Batch 52 Loss:0.014106720685958862\n",
      "Epoch 64, Batch 53 Loss:0.00811333954334259\n",
      "Epoch 64, Batch 54 Loss:0.03605588525533676\n",
      "Epoch 64, Batch 55 Loss:0.004878453444689512\n",
      "Epoch 64, Batch 56 Loss:0.012813589535653591\n",
      "Epoch 64, Batch 57 Loss:0.021169792860746384\n",
      "Epoch 64, Batch 58 Loss:0.010289064608514309\n",
      "Epoch 64, Batch 59 Loss:0.015242946334183216\n",
      "Epoch 64, Batch 60 Loss:0.026579927653074265\n",
      "Epoch 64, Batch 61 Loss:0.01810844987630844\n",
      "Epoch 64, Batch 62 Loss:0.016547882929444313\n",
      "Epoch 64, Batch 63 Loss:0.02416813373565674\n",
      "Epoch 64, Batch 64 Loss:0.035687509924173355\n",
      "Epoch 64, Batch 65 Loss:0.024000579491257668\n",
      "Epoch 64, Batch 66 Loss:0.011755283921957016\n",
      "Epoch 64, Batch 67 Loss:0.006046298425644636\n",
      "Epoch 64, Batch 68 Loss:0.019058234989643097\n",
      "Epoch 64, Batch 69 Loss:0.013657310977578163\n",
      "Epoch 64, Batch 70 Loss:0.010467038489878178\n",
      "Epoch 64, Batch 71 Loss:0.00918713491410017\n",
      "Epoch 64, Batch 72 Loss:0.01881895773112774\n",
      "Epoch 64, Batch 73 Loss:0.05578096583485603\n",
      "Epoch 64, Batch 74 Loss:0.006764847319573164\n",
      "Epoch 64, Batch 75 Loss:0.009285314939916134\n",
      "Epoch 64, Batch 76 Loss:0.04826261103153229\n",
      "Epoch 64, Batch 77 Loss:0.0054172552190721035\n",
      "Epoch 64, Batch 78 Loss:0.014266306534409523\n",
      "Epoch 64, Batch 79 Loss:0.012877151370048523\n",
      "Epoch 64, Batch 80 Loss:0.03248622268438339\n",
      "Epoch 64, Batch 81 Loss:0.06540938466787338\n",
      "Epoch 64, Batch 82 Loss:0.021792536601424217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64, Batch 83 Loss:0.03732564300298691\n",
      "Epoch 64, Batch 84 Loss:0.010309457778930664\n",
      "Epoch 64, Batch 85 Loss:0.056935835629701614\n",
      "Epoch 64, Batch 86 Loss:0.011800795793533325\n",
      "Epoch 64, Batch 87 Loss:0.031297579407691956\n",
      "Epoch 64, Batch 88 Loss:0.01184485387057066\n",
      "Epoch 64, Batch 89 Loss:0.02023322507739067\n",
      "Epoch 64, Batch 90 Loss:0.01927761174738407\n",
      "Epoch 64, Batch 91 Loss:0.010217827744781971\n",
      "Epoch 64, Batch 92 Loss:0.007169513031840324\n",
      "Epoch 64, Batch 93 Loss:0.007759101688861847\n",
      "Epoch 64, Batch 94 Loss:0.018896693363785744\n",
      "Epoch 64, Batch 95 Loss:0.004805188626050949\n",
      "Epoch 64, Batch 96 Loss:0.01054731011390686\n",
      "Epoch 64, Batch 97 Loss:0.006505608092993498\n",
      "Epoch 64, Batch 98 Loss:0.008562998846173286\n",
      "Epoch 64, Batch 99 Loss:0.00891050137579441\n",
      "Epoch 64, Batch 100 Loss:0.01487359032034874\n",
      "Epoch 64, Batch 101 Loss:0.010975041426718235\n",
      "Epoch 64, Batch 102 Loss:0.009929414838552475\n",
      "Epoch 64, Batch 103 Loss:0.013541318476200104\n",
      "Epoch 64, Batch 104 Loss:0.006373680662363768\n",
      "Epoch 64, Batch 105 Loss:0.02553955465555191\n",
      "Epoch 64, Batch 106 Loss:0.003936754539608955\n",
      "Epoch 64, Batch 107 Loss:0.007415366359055042\n",
      "Epoch 64, Batch 108 Loss:0.021707376465201378\n",
      "Epoch 64, Batch 109 Loss:0.006817647721618414\n",
      "Epoch 64, Batch 110 Loss:0.005263477563858032\n",
      "Epoch 64, Batch 111 Loss:0.022745979949831963\n",
      "Epoch 64, Batch 112 Loss:0.01501290500164032\n",
      "Epoch 64, Batch 113 Loss:0.018070437014102936\n",
      "Epoch 64, Batch 114 Loss:0.008337199687957764\n",
      "Epoch 64, Batch 115 Loss:0.03548169508576393\n",
      "Epoch 64, Batch 116 Loss:0.029763057827949524\n",
      "Epoch 64, Batch 117 Loss:0.02676858752965927\n",
      "Epoch 64, Batch 118 Loss:0.02446748875081539\n",
      "Epoch 64, Batch 119 Loss:0.0591546930372715\n",
      "Epoch 64, Batch 120 Loss:0.011188618838787079\n",
      "Epoch 64, Batch 121 Loss:0.0120343416929245\n",
      "Epoch 64, Batch 122 Loss:0.012471046298742294\n",
      "Epoch 64, Batch 123 Loss:0.011550897732377052\n",
      "Epoch 64, Batch 124 Loss:0.018418695777654648\n",
      "Epoch 64, Batch 125 Loss:0.030208032578229904\n",
      "Epoch 64, Batch 126 Loss:0.01653536967933178\n",
      "Epoch 64, Batch 127 Loss:0.006720029748976231\n",
      "Epoch 64, Batch 128 Loss:0.036276064813137054\n",
      "Epoch 64, Batch 129 Loss:0.028793515637516975\n",
      "Epoch 64, Batch 130 Loss:0.01382673904299736\n",
      "Epoch 64, Batch 131 Loss:0.10946778953075409\n",
      "Epoch 64, Batch 132 Loss:0.036169104278087616\n",
      "Epoch 64, Batch 133 Loss:0.007172433193773031\n",
      "Epoch 64, Batch 134 Loss:0.025084838271141052\n",
      "Epoch 64, Batch 135 Loss:0.016603974625468254\n",
      "Epoch 64, Batch 136 Loss:0.02749239280819893\n",
      "Epoch 64, Batch 137 Loss:0.046784430742263794\n",
      "Epoch 64, Batch 138 Loss:0.04910459369421005\n",
      "Epoch 64, Batch 139 Loss:0.02310297265648842\n",
      "Epoch 64, Batch 140 Loss:0.041017819195985794\n",
      "Epoch 64, Batch 141 Loss:0.047122761607170105\n",
      "Epoch 64, Batch 142 Loss:0.01555393636226654\n",
      "Epoch 64, Batch 143 Loss:0.013732670806348324\n",
      "Epoch 64, Batch 144 Loss:0.007691400125622749\n",
      "Epoch 64, Batch 145 Loss:0.02973281778395176\n",
      "Epoch 64, Batch 146 Loss:0.01212106179445982\n",
      "Epoch 64, Batch 147 Loss:0.06744444370269775\n",
      "Epoch 64, Batch 148 Loss:0.02075503021478653\n",
      "Epoch 64, Batch 149 Loss:0.019488804042339325\n",
      "Epoch 64, Batch 150 Loss:0.003082502167671919\n",
      "Epoch 64, Batch 151 Loss:0.010590419173240662\n",
      "Epoch 64, Batch 152 Loss:0.018445078283548355\n",
      "Epoch 64, Batch 153 Loss:0.0218831654638052\n",
      "Epoch 64, Batch 154 Loss:0.02359785884618759\n",
      "Epoch 64, Batch 155 Loss:0.047364380210638046\n",
      "Epoch 64, Batch 156 Loss:0.016941679641604424\n",
      "Epoch 64, Batch 157 Loss:0.03153210133314133\n",
      "Epoch 64, Batch 158 Loss:0.03462517261505127\n",
      "Epoch 64, Batch 159 Loss:0.008735348470509052\n",
      "Epoch 64, Batch 160 Loss:0.016471462324261665\n",
      "Epoch 64, Batch 161 Loss:0.07612816989421844\n",
      "Epoch 64, Batch 162 Loss:0.016934869810938835\n",
      "Epoch 64, Batch 163 Loss:0.03848418965935707\n",
      "Epoch 64, Batch 164 Loss:0.03196895867586136\n",
      "Epoch 64, Batch 165 Loss:0.02729426883161068\n",
      "Epoch 64, Batch 166 Loss:0.005562986247241497\n",
      "Epoch 64, Batch 167 Loss:0.03705605864524841\n",
      "Epoch 64, Batch 168 Loss:0.022740289568901062\n",
      "Epoch 64, Batch 169 Loss:0.02590477280318737\n",
      "Epoch 64, Batch 170 Loss:0.004588904324918985\n",
      "Epoch 64, Batch 171 Loss:0.01124498713761568\n",
      "Epoch 64, Batch 172 Loss:0.03288673236966133\n",
      "Epoch 64, Batch 173 Loss:0.028098754584789276\n",
      "Epoch 64, Batch 174 Loss:0.018276318907737732\n",
      "Epoch 64, Batch 175 Loss:0.027121465653181076\n",
      "Epoch 64, Batch 176 Loss:0.03222715109586716\n",
      "Epoch 64, Batch 177 Loss:0.06669054180383682\n",
      "Epoch 64, Batch 178 Loss:0.010172992944717407\n",
      "Epoch 64, Batch 179 Loss:0.01372411847114563\n",
      "Epoch 64, Batch 180 Loss:0.010750334709882736\n",
      "Epoch 64, Batch 181 Loss:0.024053752422332764\n",
      "Epoch 64, Batch 182 Loss:0.05321962386369705\n",
      "Epoch 64, Batch 183 Loss:0.021999642252922058\n",
      "Epoch 64, Batch 184 Loss:0.014284634962677956\n",
      "Epoch 64, Batch 185 Loss:0.03899047523736954\n",
      "Epoch 64, Batch 186 Loss:0.03149223327636719\n",
      "Epoch 64, Batch 187 Loss:0.034169793128967285\n",
      "Epoch 64, Batch 188 Loss:0.03560219705104828\n",
      "Epoch 64, Batch 189 Loss:0.03965972736477852\n",
      "Epoch 64, Batch 190 Loss:0.03255309909582138\n",
      "Epoch 64, Batch 191 Loss:0.04279312491416931\n",
      "Epoch 64, Batch 192 Loss:0.015053966082632542\n",
      "Epoch 64, Batch 193 Loss:0.033865757286548615\n",
      "Epoch 64, Batch 194 Loss:0.022498678416013718\n",
      "Epoch 64, Batch 195 Loss:0.020635738968849182\n",
      "Epoch 64, Batch 196 Loss:0.008802669122815132\n",
      "Epoch 64, Batch 197 Loss:0.025091854855418205\n",
      "Epoch 64, Batch 198 Loss:0.0467381551861763\n",
      "Epoch 64, Batch 199 Loss:0.04870568960905075\n",
      "Epoch 64, Batch 200 Loss:0.027557112276554108\n",
      "Epoch 64, Batch 201 Loss:0.009043678641319275\n",
      "Epoch 64, Batch 202 Loss:0.017156127840280533\n",
      "Epoch 64, Batch 203 Loss:0.013416965492069721\n",
      "Epoch 64, Batch 204 Loss:0.01304297149181366\n",
      "Epoch 64, Batch 205 Loss:0.023984335362911224\n",
      "Epoch 64, Batch 206 Loss:0.011906463652849197\n",
      "Epoch 64, Batch 207 Loss:0.037094783037900925\n",
      "Epoch 64, Batch 208 Loss:0.015289556235074997\n",
      "Epoch 64, Batch 209 Loss:0.023850107565522194\n",
      "Epoch 64, Batch 210 Loss:0.013205991126596928\n",
      "Epoch 64, Batch 211 Loss:0.016152899712324142\n",
      "Epoch 64, Batch 212 Loss:0.013867532834410667\n",
      "Epoch 64, Batch 213 Loss:0.009483912959694862\n",
      "Epoch 64, Batch 214 Loss:0.03062605671584606\n",
      "Epoch 64, Batch 215 Loss:0.013347757048904896\n",
      "Epoch 64, Batch 216 Loss:0.015069318935275078\n",
      "Epoch 64, Batch 217 Loss:0.015559770166873932\n",
      "Epoch 64, Batch 218 Loss:0.011972475796937943\n",
      "Epoch 64, Batch 219 Loss:0.035480253398418427\n",
      "Epoch 64, Batch 220 Loss:0.016544874757528305\n",
      "Epoch 64, Batch 221 Loss:0.007214388810098171\n",
      "Epoch 64, Batch 222 Loss:0.011655046604573727\n",
      "Epoch 64, Batch 223 Loss:0.015344803221523762\n",
      "Epoch 64, Batch 224 Loss:0.03582146018743515\n",
      "Epoch 64, Batch 225 Loss:0.01548860501497984\n",
      "Epoch 64, Batch 226 Loss:0.016455519944429398\n",
      "Epoch 64, Batch 227 Loss:0.05498518422245979\n",
      "Epoch 64, Batch 228 Loss:0.030304130166769028\n",
      "Epoch 64, Batch 229 Loss:0.13302917778491974\n",
      "Epoch 64, Batch 230 Loss:0.05943659693002701\n",
      "Epoch 64, Batch 231 Loss:0.01789039932191372\n",
      "Epoch 64, Batch 232 Loss:0.0032820096239447594\n",
      "Epoch 64, Batch 233 Loss:0.01924380287528038\n",
      "Loss in this Epoch is: 1.92438028753 %\n",
      "Accuracy in this Epoch is: 88.6600017548 %\n",
      "Epoch 65, Batch 0 Loss:0.04375660419464111\n",
      "Epoch 65, Batch 1 Loss:0.03700016811490059\n",
      "Epoch 65, Batch 2 Loss:0.015658000484108925\n",
      "Epoch 65, Batch 3 Loss:0.029498545452952385\n",
      "Epoch 65, Batch 4 Loss:0.01634872332215309\n",
      "Epoch 65, Batch 5 Loss:0.011925779283046722\n",
      "Epoch 65, Batch 6 Loss:0.030841974541544914\n",
      "Epoch 65, Batch 7 Loss:0.018076464533805847\n",
      "Epoch 65, Batch 8 Loss:0.007212665397673845\n",
      "Epoch 65, Batch 9 Loss:0.011453003622591496\n",
      "Epoch 65, Batch 10 Loss:0.016438735648989677\n",
      "Epoch 65, Batch 11 Loss:0.016484444960951805\n",
      "Epoch 65, Batch 12 Loss:0.012286784127354622\n",
      "Epoch 65, Batch 13 Loss:0.025019679218530655\n",
      "Epoch 65, Batch 14 Loss:0.021388724446296692\n",
      "Epoch 65, Batch 15 Loss:0.020509883761405945\n",
      "Epoch 65, Batch 16 Loss:0.016807956621050835\n",
      "Epoch 65, Batch 17 Loss:0.007569633889943361\n",
      "Epoch 65, Batch 18 Loss:0.014642423950135708\n",
      "Epoch 65, Batch 19 Loss:0.00530927674844861\n",
      "Epoch 65, Batch 20 Loss:0.010093126446008682\n",
      "Epoch 65, Batch 21 Loss:0.02076628990471363\n",
      "Epoch 65, Batch 22 Loss:0.002741631818935275\n",
      "Epoch 65, Batch 23 Loss:0.007521393243223429\n",
      "Epoch 65, Batch 24 Loss:0.006364838685840368\n",
      "Epoch 65, Batch 25 Loss:0.020725000649690628\n",
      "Epoch 65, Batch 26 Loss:0.01815870590507984\n",
      "Epoch 65, Batch 27 Loss:0.0071174381300807\n",
      "Epoch 65, Batch 28 Loss:0.006685434374958277\n",
      "Epoch 65, Batch 29 Loss:0.012734153307974339\n",
      "Epoch 65, Batch 30 Loss:0.0037339222617447376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Batch 31 Loss:0.0010107002453878522\n",
      "Epoch 65, Batch 32 Loss:0.02581372670829296\n",
      "Epoch 65, Batch 33 Loss:0.00366761046461761\n",
      "Epoch 65, Batch 34 Loss:0.011138983070850372\n",
      "Epoch 65, Batch 35 Loss:0.029405055567622185\n",
      "Epoch 65, Batch 36 Loss:0.006152007728815079\n",
      "Epoch 65, Batch 37 Loss:0.011709483340382576\n",
      "Epoch 65, Batch 38 Loss:0.0036651878617703915\n",
      "Epoch 65, Batch 39 Loss:0.026537572965025902\n",
      "Epoch 65, Batch 40 Loss:0.022242482751607895\n",
      "Epoch 65, Batch 41 Loss:0.03423885256052017\n",
      "Epoch 65, Batch 42 Loss:0.015912994742393494\n",
      "Epoch 65, Batch 43 Loss:0.02931370958685875\n",
      "Epoch 65, Batch 44 Loss:0.017963260412216187\n",
      "Epoch 65, Batch 45 Loss:0.013754516839981079\n",
      "Epoch 65, Batch 46 Loss:0.010145680047571659\n",
      "Epoch 65, Batch 47 Loss:0.0038534512277692556\n",
      "Epoch 65, Batch 48 Loss:0.007910113781690598\n",
      "Epoch 65, Batch 49 Loss:0.010414340533316135\n",
      "Epoch 65, Batch 50 Loss:0.029947349801659584\n",
      "Epoch 65, Batch 51 Loss:0.014277798123657703\n",
      "Epoch 65, Batch 52 Loss:0.026789693161845207\n",
      "Epoch 65, Batch 53 Loss:0.013797956518828869\n",
      "Epoch 65, Batch 54 Loss:0.01062057539820671\n",
      "Epoch 65, Batch 55 Loss:0.0065832980908453465\n",
      "Epoch 65, Batch 56 Loss:0.0075201615691185\n",
      "Epoch 65, Batch 57 Loss:0.011576423421502113\n",
      "Epoch 65, Batch 58 Loss:0.02488759160041809\n",
      "Epoch 65, Batch 59 Loss:0.007459194399416447\n",
      "Epoch 65, Batch 60 Loss:0.06003279983997345\n",
      "Epoch 65, Batch 61 Loss:0.013082855381071568\n",
      "Epoch 65, Batch 62 Loss:0.005546711850911379\n",
      "Epoch 65, Batch 63 Loss:0.017771897837519646\n",
      "Epoch 65, Batch 64 Loss:0.01878838799893856\n",
      "Epoch 65, Batch 65 Loss:0.007923618890345097\n",
      "Epoch 65, Batch 66 Loss:0.026710575446486473\n",
      "Epoch 65, Batch 67 Loss:0.032539911568164825\n",
      "Epoch 65, Batch 68 Loss:0.02406720072031021\n",
      "Epoch 65, Batch 69 Loss:0.02035408653318882\n",
      "Epoch 65, Batch 70 Loss:0.011487841606140137\n",
      "Epoch 65, Batch 71 Loss:0.03812532499432564\n",
      "Epoch 65, Batch 72 Loss:0.019470185041427612\n",
      "Epoch 65, Batch 73 Loss:0.00817903596907854\n",
      "Epoch 65, Batch 74 Loss:0.01830892078578472\n",
      "Epoch 65, Batch 75 Loss:0.005184525158256292\n",
      "Epoch 65, Batch 76 Loss:0.02051597274839878\n",
      "Epoch 65, Batch 77 Loss:0.02394896186888218\n",
      "Epoch 65, Batch 78 Loss:0.020129479467868805\n",
      "Epoch 65, Batch 79 Loss:0.04862703010439873\n",
      "Epoch 65, Batch 80 Loss:0.02456856705248356\n",
      "Epoch 65, Batch 81 Loss:0.053487733006477356\n",
      "Epoch 65, Batch 82 Loss:0.02080938220024109\n",
      "Epoch 65, Batch 83 Loss:0.031065387651324272\n",
      "Epoch 65, Batch 84 Loss:0.04502014070749283\n",
      "Epoch 65, Batch 85 Loss:0.07539965212345123\n",
      "Epoch 65, Batch 86 Loss:0.06929732114076614\n",
      "Epoch 65, Batch 87 Loss:0.015134337358176708\n",
      "Epoch 65, Batch 88 Loss:0.01838531158864498\n",
      "Epoch 65, Batch 89 Loss:0.027449429035186768\n",
      "Epoch 65, Batch 90 Loss:0.027066446840763092\n",
      "Epoch 65, Batch 91 Loss:0.02430383488535881\n",
      "Epoch 65, Batch 92 Loss:0.020839395001530647\n",
      "Epoch 65, Batch 93 Loss:0.01970689743757248\n",
      "Epoch 65, Batch 94 Loss:0.04449420049786568\n",
      "Epoch 65, Batch 95 Loss:0.01608971692621708\n",
      "Epoch 65, Batch 96 Loss:0.012588758952915668\n",
      "Epoch 65, Batch 97 Loss:0.016164926812052727\n",
      "Epoch 65, Batch 98 Loss:0.06258756667375565\n",
      "Epoch 65, Batch 99 Loss:0.024927858263254166\n",
      "Epoch 65, Batch 100 Loss:0.023177120834589005\n",
      "Epoch 65, Batch 101 Loss:0.013263868167996407\n",
      "Epoch 65, Batch 102 Loss:0.04079139605164528\n",
      "Epoch 65, Batch 103 Loss:0.015139268711209297\n",
      "Epoch 65, Batch 104 Loss:0.04331145063042641\n",
      "Epoch 65, Batch 105 Loss:0.02225271239876747\n",
      "Epoch 65, Batch 106 Loss:0.015933925285935402\n",
      "Epoch 65, Batch 107 Loss:0.0733562633395195\n",
      "Epoch 65, Batch 108 Loss:0.03608867526054382\n",
      "Epoch 65, Batch 109 Loss:0.031584031879901886\n",
      "Epoch 65, Batch 110 Loss:0.04873913154006004\n",
      "Epoch 65, Batch 111 Loss:0.04130435734987259\n",
      "Epoch 65, Batch 112 Loss:0.022288508713245392\n",
      "Epoch 65, Batch 113 Loss:0.009105629287660122\n",
      "Epoch 65, Batch 114 Loss:0.030563361942768097\n",
      "Epoch 65, Batch 115 Loss:0.04008803144097328\n",
      "Epoch 65, Batch 116 Loss:0.025350209325551987\n",
      "Epoch 65, Batch 117 Loss:0.02253349870443344\n",
      "Epoch 65, Batch 118 Loss:0.03649549186229706\n",
      "Epoch 65, Batch 119 Loss:0.032627981156110764\n",
      "Epoch 65, Batch 120 Loss:0.02710220403969288\n",
      "Epoch 65, Batch 121 Loss:0.0105553288012743\n",
      "Epoch 65, Batch 122 Loss:0.04980187118053436\n",
      "Epoch 65, Batch 123 Loss:0.03820674121379852\n",
      "Epoch 65, Batch 124 Loss:0.05050300061702728\n",
      "Epoch 65, Batch 125 Loss:0.04413139075040817\n",
      "Epoch 65, Batch 126 Loss:0.028569741174578667\n",
      "Epoch 65, Batch 127 Loss:0.030116811394691467\n",
      "Epoch 65, Batch 128 Loss:0.019557956606149673\n",
      "Epoch 65, Batch 129 Loss:0.043813131749629974\n",
      "Epoch 65, Batch 130 Loss:0.017116239294409752\n",
      "Epoch 65, Batch 131 Loss:0.04800562188029289\n",
      "Epoch 65, Batch 132 Loss:0.009264186955988407\n",
      "Epoch 65, Batch 133 Loss:0.04279819503426552\n",
      "Epoch 65, Batch 134 Loss:0.04214823246002197\n",
      "Epoch 65, Batch 135 Loss:0.014408216811716557\n",
      "Epoch 65, Batch 136 Loss:0.009871171787381172\n",
      "Epoch 65, Batch 137 Loss:0.008856384083628654\n",
      "Epoch 65, Batch 138 Loss:0.05527806282043457\n",
      "Epoch 65, Batch 139 Loss:0.021599993109703064\n",
      "Epoch 65, Batch 140 Loss:0.030428804457187653\n",
      "Epoch 65, Batch 141 Loss:0.0654347836971283\n",
      "Epoch 65, Batch 142 Loss:0.022416159510612488\n",
      "Epoch 65, Batch 143 Loss:0.039889171719551086\n",
      "Epoch 65, Batch 144 Loss:0.04477260261774063\n",
      "Epoch 65, Batch 145 Loss:0.07145743817090988\n",
      "Epoch 65, Batch 146 Loss:0.007752338424324989\n",
      "Epoch 65, Batch 147 Loss:0.005057028494775295\n",
      "Epoch 65, Batch 148 Loss:0.05143752694129944\n",
      "Epoch 65, Batch 149 Loss:0.0082479203119874\n",
      "Epoch 65, Batch 150 Loss:0.04824262857437134\n",
      "Epoch 65, Batch 151 Loss:0.018854565918445587\n",
      "Epoch 65, Batch 152 Loss:0.04032926261425018\n",
      "Epoch 65, Batch 153 Loss:0.03247480094432831\n",
      "Epoch 65, Batch 154 Loss:0.022103603929281235\n",
      "Epoch 65, Batch 155 Loss:0.029559962451457977\n",
      "Epoch 65, Batch 156 Loss:0.06794935464859009\n",
      "Epoch 65, Batch 157 Loss:0.021024825051426888\n",
      "Epoch 65, Batch 158 Loss:0.022857578471302986\n",
      "Epoch 65, Batch 159 Loss:0.044644374400377274\n",
      "Epoch 65, Batch 160 Loss:0.06162172555923462\n",
      "Epoch 65, Batch 161 Loss:0.0468527153134346\n",
      "Epoch 65, Batch 162 Loss:0.05059826001524925\n",
      "Epoch 65, Batch 163 Loss:0.0312650203704834\n",
      "Epoch 65, Batch 164 Loss:0.042510610073804855\n",
      "Epoch 65, Batch 165 Loss:0.058987993746995926\n",
      "Epoch 65, Batch 166 Loss:0.05958522856235504\n",
      "Epoch 65, Batch 167 Loss:0.06509946286678314\n",
      "Epoch 65, Batch 168 Loss:0.02400795929133892\n",
      "Epoch 65, Batch 169 Loss:0.047072045505046844\n",
      "Epoch 65, Batch 170 Loss:0.050657957792282104\n",
      "Epoch 65, Batch 171 Loss:0.064275823533535\n",
      "Epoch 65, Batch 172 Loss:0.09852400422096252\n",
      "Epoch 65, Batch 173 Loss:0.08961424231529236\n",
      "Epoch 65, Batch 174 Loss:0.07535810768604279\n",
      "Epoch 65, Batch 175 Loss:0.0741141140460968\n",
      "Epoch 65, Batch 176 Loss:0.04336198419332504\n",
      "Epoch 65, Batch 177 Loss:0.058183860033750534\n",
      "Epoch 65, Batch 178 Loss:0.042129792273044586\n",
      "Epoch 65, Batch 179 Loss:0.042766883969306946\n",
      "Epoch 65, Batch 180 Loss:0.07153356075286865\n",
      "Epoch 65, Batch 181 Loss:0.027048900723457336\n",
      "Epoch 65, Batch 182 Loss:0.028656385838985443\n",
      "Epoch 65, Batch 183 Loss:0.0489509254693985\n",
      "Epoch 65, Batch 184 Loss:0.08573692291975021\n",
      "Epoch 65, Batch 185 Loss:0.04608367756009102\n",
      "Epoch 65, Batch 186 Loss:0.05345204472541809\n",
      "Epoch 65, Batch 187 Loss:0.05642054229974747\n",
      "Epoch 65, Batch 188 Loss:0.03650610148906708\n",
      "Epoch 65, Batch 189 Loss:0.022970406338572502\n",
      "Epoch 65, Batch 190 Loss:0.04429613798856735\n",
      "Epoch 65, Batch 191 Loss:0.018655220046639442\n",
      "Epoch 65, Batch 192 Loss:0.02931213565170765\n",
      "Epoch 65, Batch 193 Loss:0.04063810408115387\n",
      "Epoch 65, Batch 194 Loss:0.050045762211084366\n",
      "Epoch 65, Batch 195 Loss:0.017433077096939087\n",
      "Epoch 65, Batch 196 Loss:0.022413531318306923\n",
      "Epoch 65, Batch 197 Loss:0.046469900757074356\n",
      "Epoch 65, Batch 198 Loss:0.023008018732070923\n",
      "Epoch 65, Batch 199 Loss:0.04504614695906639\n",
      "Epoch 65, Batch 200 Loss:0.019779732450842857\n",
      "Epoch 65, Batch 201 Loss:0.03658881410956383\n",
      "Epoch 65, Batch 202 Loss:0.04475131258368492\n",
      "Epoch 65, Batch 203 Loss:0.01980607584118843\n",
      "Epoch 65, Batch 204 Loss:0.050799544900655746\n",
      "Epoch 65, Batch 205 Loss:0.031679198145866394\n",
      "Epoch 65, Batch 206 Loss:0.037350550293922424\n",
      "Epoch 65, Batch 207 Loss:0.030364861711859703\n",
      "Epoch 65, Batch 208 Loss:0.026324117556214333\n",
      "Epoch 65, Batch 209 Loss:0.037347979843616486\n",
      "Epoch 65, Batch 210 Loss:0.01500997506082058\n",
      "Epoch 65, Batch 211 Loss:0.018095266073942184\n",
      "Epoch 65, Batch 212 Loss:0.03043452650308609\n",
      "Epoch 65, Batch 213 Loss:0.010271089151501656\n",
      "Epoch 65, Batch 214 Loss:0.01725924387574196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65, Batch 215 Loss:0.030467694625258446\n",
      "Epoch 65, Batch 216 Loss:0.018391892313957214\n",
      "Epoch 65, Batch 217 Loss:0.03957470506429672\n",
      "Epoch 65, Batch 218 Loss:0.04275544732809067\n",
      "Epoch 65, Batch 219 Loss:0.009507065638899803\n",
      "Epoch 65, Batch 220 Loss:0.04202977195382118\n",
      "Epoch 65, Batch 221 Loss:0.037022970616817474\n",
      "Epoch 65, Batch 222 Loss:0.027976002544164658\n",
      "Epoch 65, Batch 223 Loss:0.019133444875478745\n",
      "Epoch 65, Batch 224 Loss:0.053512848913669586\n",
      "Epoch 65, Batch 225 Loss:0.027521509677171707\n",
      "Epoch 65, Batch 226 Loss:0.06622552126646042\n",
      "Epoch 65, Batch 227 Loss:0.02275942638516426\n",
      "Epoch 65, Batch 228 Loss:0.021823905408382416\n",
      "Epoch 65, Batch 229 Loss:0.0332643985748291\n",
      "Epoch 65, Batch 230 Loss:0.04646166414022446\n",
      "Epoch 65, Batch 231 Loss:0.024944566190242767\n",
      "Epoch 65, Batch 232 Loss:0.024914909154176712\n",
      "Epoch 65, Batch 233 Loss:0.01803022064268589\n",
      "Loss in this Epoch is: 1.80302206427 %\n",
      "Accuracy in this Epoch is: 88.4899973869 %\n",
      "Epoch 66, Batch 0 Loss:0.025946995243430138\n",
      "Epoch 66, Batch 1 Loss:0.009280627593398094\n",
      "Epoch 66, Batch 2 Loss:0.019800379872322083\n",
      "Epoch 66, Batch 3 Loss:0.0467166006565094\n",
      "Epoch 66, Batch 4 Loss:0.04019326716661453\n",
      "Epoch 66, Batch 5 Loss:0.05811231955885887\n",
      "Epoch 66, Batch 6 Loss:0.03416615352034569\n",
      "Epoch 66, Batch 7 Loss:0.0443720780313015\n",
      "Epoch 66, Batch 8 Loss:0.019718701019883156\n",
      "Epoch 66, Batch 9 Loss:0.023410614579916\n",
      "Epoch 66, Batch 10 Loss:0.01696823351085186\n",
      "Epoch 66, Batch 11 Loss:0.034332726150751114\n",
      "Epoch 66, Batch 12 Loss:0.054317668080329895\n",
      "Epoch 66, Batch 13 Loss:0.02718893438577652\n",
      "Epoch 66, Batch 14 Loss:0.03256627917289734\n",
      "Epoch 66, Batch 15 Loss:0.02662350982427597\n",
      "Epoch 66, Batch 16 Loss:0.029403993859887123\n",
      "Epoch 66, Batch 17 Loss:0.036070410162210464\n",
      "Epoch 66, Batch 18 Loss:0.023300444707274437\n",
      "Epoch 66, Batch 19 Loss:0.027987506240606308\n",
      "Epoch 66, Batch 20 Loss:0.026815084740519524\n",
      "Epoch 66, Batch 21 Loss:0.023302165791392326\n",
      "Epoch 66, Batch 22 Loss:0.02707676962018013\n",
      "Epoch 66, Batch 23 Loss:0.017667625099420547\n",
      "Epoch 66, Batch 24 Loss:0.006774156354367733\n",
      "Epoch 66, Batch 25 Loss:0.0076068309135735035\n",
      "Epoch 66, Batch 26 Loss:0.0072877840138971806\n",
      "Epoch 66, Batch 27 Loss:0.014599931426346302\n",
      "Epoch 66, Batch 28 Loss:0.013800185173749924\n",
      "Epoch 66, Batch 29 Loss:0.01318783313035965\n",
      "Epoch 66, Batch 30 Loss:0.01650656759738922\n",
      "Epoch 66, Batch 31 Loss:0.02548770233988762\n",
      "Epoch 66, Batch 32 Loss:0.013529998250305653\n",
      "Epoch 66, Batch 33 Loss:0.024547407403588295\n",
      "Epoch 66, Batch 34 Loss:0.020666295662522316\n",
      "Epoch 66, Batch 35 Loss:0.008595592342317104\n",
      "Epoch 66, Batch 36 Loss:0.01567975990474224\n",
      "Epoch 66, Batch 37 Loss:0.023906933143734932\n",
      "Epoch 66, Batch 38 Loss:0.020797528326511383\n",
      "Epoch 66, Batch 39 Loss:0.016176927834749222\n",
      "Epoch 66, Batch 40 Loss:0.017881125211715698\n",
      "Epoch 66, Batch 41 Loss:0.013290785253047943\n",
      "Epoch 66, Batch 42 Loss:0.006580408196896315\n",
      "Epoch 66, Batch 43 Loss:0.00642836419865489\n",
      "Epoch 66, Batch 44 Loss:0.016795769333839417\n",
      "Epoch 66, Batch 45 Loss:0.009156693704426289\n",
      "Epoch 66, Batch 46 Loss:0.006458353251218796\n",
      "Epoch 66, Batch 47 Loss:0.01731416955590248\n",
      "Epoch 66, Batch 48 Loss:0.0072907619178295135\n",
      "Epoch 66, Batch 49 Loss:0.0396440252661705\n",
      "Epoch 66, Batch 50 Loss:0.007365527097135782\n",
      "Epoch 66, Batch 51 Loss:0.020962078124284744\n",
      "Epoch 66, Batch 52 Loss:0.010941407643258572\n",
      "Epoch 66, Batch 53 Loss:0.0166742242872715\n",
      "Epoch 66, Batch 54 Loss:0.005818895064294338\n",
      "Epoch 66, Batch 55 Loss:0.009545168839395046\n",
      "Epoch 66, Batch 56 Loss:0.008248653262853622\n",
      "Epoch 66, Batch 57 Loss:0.03423652797937393\n",
      "Epoch 66, Batch 58 Loss:0.00729616591706872\n",
      "Epoch 66, Batch 59 Loss:0.013389432802796364\n",
      "Epoch 66, Batch 60 Loss:0.015106008388102055\n",
      "Epoch 66, Batch 61 Loss:0.0204386655241251\n",
      "Epoch 66, Batch 62 Loss:0.005663508549332619\n",
      "Epoch 66, Batch 63 Loss:0.017168715596199036\n",
      "Epoch 66, Batch 64 Loss:0.03476579487323761\n",
      "Epoch 66, Batch 65 Loss:0.018172044306993484\n",
      "Epoch 66, Batch 66 Loss:0.02017531916499138\n",
      "Epoch 66, Batch 67 Loss:0.024941464886069298\n",
      "Epoch 66, Batch 68 Loss:0.019285982474684715\n",
      "Epoch 66, Batch 69 Loss:0.019889624789357185\n",
      "Epoch 66, Batch 70 Loss:0.01553400233387947\n",
      "Epoch 66, Batch 71 Loss:0.04354684799909592\n",
      "Epoch 66, Batch 72 Loss:0.01046532392501831\n",
      "Epoch 66, Batch 73 Loss:0.007104320917278528\n",
      "Epoch 66, Batch 74 Loss:0.03313346579670906\n",
      "Epoch 66, Batch 75 Loss:0.010540788061916828\n",
      "Epoch 66, Batch 76 Loss:0.0466146282851696\n",
      "Epoch 66, Batch 77 Loss:0.0045995768159627914\n",
      "Epoch 66, Batch 78 Loss:0.01503523625433445\n",
      "Epoch 66, Batch 79 Loss:0.006881375331431627\n",
      "Epoch 66, Batch 80 Loss:0.009117172099649906\n",
      "Epoch 66, Batch 81 Loss:0.033333830535411835\n",
      "Epoch 66, Batch 82 Loss:0.010285192169249058\n",
      "Epoch 66, Batch 83 Loss:0.00833673495799303\n",
      "Epoch 66, Batch 84 Loss:0.019685856997966766\n",
      "Epoch 66, Batch 85 Loss:0.07271621376276016\n",
      "Epoch 66, Batch 86 Loss:0.007230960298329592\n",
      "Epoch 66, Batch 87 Loss:0.01096996571868658\n",
      "Epoch 66, Batch 88 Loss:0.011424951255321503\n",
      "Epoch 66, Batch 89 Loss:0.006314883474260569\n",
      "Epoch 66, Batch 90 Loss:0.023022620007395744\n",
      "Epoch 66, Batch 91 Loss:0.04154665395617485\n",
      "Epoch 66, Batch 92 Loss:0.012998959049582481\n",
      "Epoch 66, Batch 93 Loss:0.010984494350850582\n",
      "Epoch 66, Batch 94 Loss:0.007635877467691898\n",
      "Epoch 66, Batch 95 Loss:0.0056448085233569145\n",
      "Epoch 66, Batch 96 Loss:0.004887471906840801\n",
      "Epoch 66, Batch 97 Loss:0.024597369134426117\n",
      "Epoch 66, Batch 98 Loss:0.033060602843761444\n",
      "Epoch 66, Batch 99 Loss:0.009357472881674767\n",
      "Epoch 66, Batch 100 Loss:0.012681268155574799\n",
      "Epoch 66, Batch 101 Loss:0.03634672611951828\n",
      "Epoch 66, Batch 102 Loss:0.0213942714035511\n",
      "Epoch 66, Batch 103 Loss:0.019770577549934387\n",
      "Epoch 66, Batch 104 Loss:0.030072715133428574\n",
      "Epoch 66, Batch 105 Loss:0.012219767086207867\n",
      "Epoch 66, Batch 106 Loss:0.011795184575021267\n",
      "Epoch 66, Batch 107 Loss:0.03475908190011978\n",
      "Epoch 66, Batch 108 Loss:0.023258168250322342\n",
      "Epoch 66, Batch 109 Loss:0.007443165872246027\n",
      "Epoch 66, Batch 110 Loss:0.0094898771494627\n",
      "Epoch 66, Batch 111 Loss:0.016925474628806114\n",
      "Epoch 66, Batch 112 Loss:0.011058489792048931\n",
      "Epoch 66, Batch 113 Loss:0.02163209393620491\n",
      "Epoch 66, Batch 114 Loss:0.02048073336482048\n",
      "Epoch 66, Batch 115 Loss:0.01099525485187769\n",
      "Epoch 66, Batch 116 Loss:0.022268157452344894\n",
      "Epoch 66, Batch 117 Loss:0.024313846603035927\n",
      "Epoch 66, Batch 118 Loss:0.059812940657138824\n",
      "Epoch 66, Batch 119 Loss:0.009004033170640469\n",
      "Epoch 66, Batch 120 Loss:0.019491814076900482\n",
      "Epoch 66, Batch 121 Loss:0.005289450287818909\n",
      "Epoch 66, Batch 122 Loss:0.011050394736230373\n",
      "Epoch 66, Batch 123 Loss:0.013187959790229797\n",
      "Epoch 66, Batch 124 Loss:0.022700710222125053\n",
      "Epoch 66, Batch 125 Loss:0.02444598451256752\n",
      "Epoch 66, Batch 126 Loss:0.013269998133182526\n",
      "Epoch 66, Batch 127 Loss:0.02018747106194496\n",
      "Epoch 66, Batch 128 Loss:0.018381159752607346\n",
      "Epoch 66, Batch 129 Loss:0.013176752254366875\n",
      "Epoch 66, Batch 130 Loss:0.022369718179106712\n",
      "Epoch 66, Batch 131 Loss:0.023226309567689896\n",
      "Epoch 66, Batch 132 Loss:0.029398657381534576\n",
      "Epoch 66, Batch 133 Loss:0.024528736248612404\n",
      "Epoch 66, Batch 134 Loss:0.006709979847073555\n",
      "Epoch 66, Batch 135 Loss:0.01336481049656868\n",
      "Epoch 66, Batch 136 Loss:0.03722384572029114\n",
      "Epoch 66, Batch 137 Loss:0.016955964267253876\n",
      "Epoch 66, Batch 138 Loss:0.004861569497734308\n",
      "Epoch 66, Batch 139 Loss:0.017730161547660828\n",
      "Epoch 66, Batch 140 Loss:0.01771734468638897\n",
      "Epoch 66, Batch 141 Loss:0.011487510055303574\n",
      "Epoch 66, Batch 142 Loss:0.008504054509103298\n",
      "Epoch 66, Batch 143 Loss:0.016620134934782982\n",
      "Epoch 66, Batch 144 Loss:0.048838965594768524\n",
      "Epoch 66, Batch 145 Loss:0.014718852937221527\n",
      "Epoch 66, Batch 146 Loss:0.02979111671447754\n",
      "Epoch 66, Batch 147 Loss:0.013686268590390682\n",
      "Epoch 66, Batch 148 Loss:0.019211607053875923\n",
      "Epoch 66, Batch 149 Loss:0.05364418774843216\n",
      "Epoch 66, Batch 150 Loss:0.03305945545434952\n",
      "Epoch 66, Batch 151 Loss:0.008046002127230167\n",
      "Epoch 66, Batch 152 Loss:0.027347784489393234\n",
      "Epoch 66, Batch 153 Loss:0.015331320464611053\n",
      "Epoch 66, Batch 154 Loss:0.010655042715370655\n",
      "Epoch 66, Batch 155 Loss:0.0193166583776474\n",
      "Epoch 66, Batch 156 Loss:0.01044352725148201\n",
      "Epoch 66, Batch 157 Loss:0.009533101692795753\n",
      "Epoch 66, Batch 158 Loss:0.019098125398159027\n",
      "Epoch 66, Batch 159 Loss:0.02049897611141205\n",
      "Epoch 66, Batch 160 Loss:0.015074098482728004\n",
      "Epoch 66, Batch 161 Loss:0.023985248059034348\n",
      "Epoch 66, Batch 162 Loss:0.03464332967996597\n",
      "Epoch 66, Batch 163 Loss:0.07086332142353058\n",
      "Epoch 66, Batch 164 Loss:0.011558223515748978\n",
      "Epoch 66, Batch 165 Loss:0.03319064527750015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66, Batch 166 Loss:0.043014876544475555\n",
      "Epoch 66, Batch 167 Loss:0.019413094967603683\n",
      "Epoch 66, Batch 168 Loss:0.0225721076130867\n",
      "Epoch 66, Batch 169 Loss:0.014389578253030777\n",
      "Epoch 66, Batch 170 Loss:0.016086116433143616\n",
      "Epoch 66, Batch 171 Loss:0.024816984310746193\n",
      "Epoch 66, Batch 172 Loss:0.021889179944992065\n",
      "Epoch 66, Batch 173 Loss:0.018449701368808746\n",
      "Epoch 66, Batch 174 Loss:0.017667343840003014\n",
      "Epoch 66, Batch 175 Loss:0.018551895394921303\n",
      "Epoch 66, Batch 176 Loss:0.028229033574461937\n",
      "Epoch 66, Batch 177 Loss:0.038554657250642776\n",
      "Epoch 66, Batch 178 Loss:0.05736026540398598\n",
      "Epoch 66, Batch 179 Loss:0.019094320014119148\n",
      "Epoch 66, Batch 180 Loss:0.06089737266302109\n",
      "Epoch 66, Batch 181 Loss:0.02289881743490696\n",
      "Epoch 66, Batch 182 Loss:0.01879141293466091\n",
      "Epoch 66, Batch 183 Loss:0.005998593755066395\n",
      "Epoch 66, Batch 184 Loss:0.020487194880843163\n",
      "Epoch 66, Batch 185 Loss:0.0263186264783144\n",
      "Epoch 66, Batch 186 Loss:0.01207309402525425\n",
      "Epoch 66, Batch 187 Loss:0.057594653218984604\n",
      "Epoch 66, Batch 188 Loss:0.014881914481520653\n",
      "Epoch 66, Batch 189 Loss:0.025943778455257416\n",
      "Epoch 66, Batch 190 Loss:0.04200837016105652\n",
      "Epoch 66, Batch 191 Loss:0.009280744008719921\n",
      "Epoch 66, Batch 192 Loss:0.03262358903884888\n",
      "Epoch 66, Batch 193 Loss:0.04202042892575264\n",
      "Epoch 66, Batch 194 Loss:0.03021540679037571\n",
      "Epoch 66, Batch 195 Loss:0.025286713615059853\n",
      "Epoch 66, Batch 196 Loss:0.009371447376906872\n",
      "Epoch 66, Batch 197 Loss:0.028555991128087044\n",
      "Epoch 66, Batch 198 Loss:0.0566420704126358\n",
      "Epoch 66, Batch 199 Loss:0.022746890783309937\n",
      "Epoch 66, Batch 200 Loss:0.01690780743956566\n",
      "Epoch 66, Batch 201 Loss:0.03629056736826897\n",
      "Epoch 66, Batch 202 Loss:0.015468022786080837\n",
      "Epoch 66, Batch 203 Loss:0.0225362591445446\n",
      "Epoch 66, Batch 204 Loss:0.016316723078489304\n",
      "Epoch 66, Batch 205 Loss:0.01412786915898323\n",
      "Epoch 66, Batch 206 Loss:0.012716427445411682\n",
      "Epoch 66, Batch 207 Loss:0.00753417331725359\n",
      "Epoch 66, Batch 208 Loss:0.08324837684631348\n",
      "Epoch 66, Batch 209 Loss:0.014360272325575352\n",
      "Epoch 66, Batch 210 Loss:0.031057219952344894\n",
      "Epoch 66, Batch 211 Loss:0.032332245260477066\n",
      "Epoch 66, Batch 212 Loss:0.05648272857069969\n",
      "Epoch 66, Batch 213 Loss:0.012675726786255836\n",
      "Epoch 66, Batch 214 Loss:0.019096987321972847\n",
      "Epoch 66, Batch 215 Loss:0.01060856319963932\n",
      "Epoch 66, Batch 216 Loss:0.034873537719249725\n",
      "Epoch 66, Batch 217 Loss:0.06246088445186615\n",
      "Epoch 66, Batch 218 Loss:0.008976660668849945\n",
      "Epoch 66, Batch 219 Loss:0.023586034774780273\n",
      "Epoch 66, Batch 220 Loss:0.013982253149151802\n",
      "Epoch 66, Batch 221 Loss:0.06743110716342926\n",
      "Epoch 66, Batch 222 Loss:0.0643027275800705\n",
      "Epoch 66, Batch 223 Loss:0.03218759596347809\n",
      "Epoch 66, Batch 224 Loss:0.00538256298750639\n",
      "Epoch 66, Batch 225 Loss:0.0405622273683548\n",
      "Epoch 66, Batch 226 Loss:0.07193418592214584\n",
      "Epoch 66, Batch 227 Loss:0.05649702250957489\n",
      "Epoch 66, Batch 228 Loss:0.04804176092147827\n",
      "Epoch 66, Batch 229 Loss:0.042624332010746\n",
      "Epoch 66, Batch 230 Loss:0.022588592022657394\n",
      "Epoch 66, Batch 231 Loss:0.03541720286011696\n",
      "Epoch 66, Batch 232 Loss:0.05921599268913269\n",
      "Epoch 66, Batch 233 Loss:0.027695462107658386\n",
      "Loss in this Epoch is: 2.76954621077 %\n",
      "Accuracy in this Epoch is: 88.8499975204 %\n",
      "Epoch 67, Batch 0 Loss:0.04136249050498009\n",
      "Epoch 67, Batch 1 Loss:0.01661308854818344\n",
      "Epoch 67, Batch 2 Loss:0.014875375665724277\n",
      "Epoch 67, Batch 3 Loss:0.014392868615686893\n",
      "Epoch 67, Batch 4 Loss:0.022396747022867203\n",
      "Epoch 67, Batch 5 Loss:0.017593329772353172\n",
      "Epoch 67, Batch 6 Loss:0.031683921813964844\n",
      "Epoch 67, Batch 7 Loss:0.011401291005313396\n",
      "Epoch 67, Batch 8 Loss:0.01084770355373621\n",
      "Epoch 67, Batch 9 Loss:0.01824217103421688\n",
      "Epoch 67, Batch 10 Loss:0.005614778492599726\n",
      "Epoch 67, Batch 11 Loss:0.031170299276709557\n",
      "Epoch 67, Batch 12 Loss:0.023598290979862213\n",
      "Epoch 67, Batch 13 Loss:0.003940739203244448\n",
      "Epoch 67, Batch 14 Loss:0.008087079972028732\n",
      "Epoch 67, Batch 15 Loss:0.02462010085582733\n",
      "Epoch 67, Batch 16 Loss:0.015160085633397102\n",
      "Epoch 67, Batch 17 Loss:0.01457415521144867\n",
      "Epoch 67, Batch 18 Loss:0.044141702353954315\n",
      "Epoch 67, Batch 19 Loss:0.008151885122060776\n",
      "Epoch 67, Batch 20 Loss:0.0058126128278672695\n",
      "Epoch 67, Batch 21 Loss:0.005134467501193285\n",
      "Epoch 67, Batch 22 Loss:0.010479921475052834\n",
      "Epoch 67, Batch 23 Loss:0.028386453166604042\n",
      "Epoch 67, Batch 24 Loss:0.01889856718480587\n",
      "Epoch 67, Batch 25 Loss:0.0064012096263468266\n",
      "Epoch 67, Batch 26 Loss:0.007409739773720503\n",
      "Epoch 67, Batch 27 Loss:0.030453044921159744\n",
      "Epoch 67, Batch 28 Loss:0.017700862139463425\n",
      "Epoch 67, Batch 29 Loss:0.024218430742621422\n",
      "Epoch 67, Batch 30 Loss:0.006565696559846401\n",
      "Epoch 67, Batch 31 Loss:0.004742732271552086\n",
      "Epoch 67, Batch 32 Loss:0.015789447352290154\n",
      "Epoch 67, Batch 33 Loss:0.018119340762495995\n",
      "Epoch 67, Batch 34 Loss:0.0031407764181494713\n",
      "Epoch 67, Batch 35 Loss:0.004583613947033882\n",
      "Epoch 67, Batch 36 Loss:0.0022047238890081644\n",
      "Epoch 67, Batch 37 Loss:0.0033977662678807974\n",
      "Epoch 67, Batch 38 Loss:0.0063138618133962154\n",
      "Epoch 67, Batch 39 Loss:0.020095860585570335\n",
      "Epoch 67, Batch 40 Loss:0.0022742983419448137\n",
      "Epoch 67, Batch 41 Loss:0.0030797834042459726\n",
      "Epoch 67, Batch 42 Loss:0.013829867355525494\n",
      "Epoch 67, Batch 43 Loss:0.01901027373969555\n",
      "Epoch 67, Batch 44 Loss:0.006155780050903559\n",
      "Epoch 67, Batch 45 Loss:0.00616129394620657\n",
      "Epoch 67, Batch 46 Loss:0.006198412273079157\n",
      "Epoch 67, Batch 47 Loss:0.003061471274122596\n",
      "Epoch 67, Batch 48 Loss:0.013119214214384556\n",
      "Epoch 67, Batch 49 Loss:0.022097529843449593\n",
      "Epoch 67, Batch 50 Loss:0.0034615120384842157\n",
      "Epoch 67, Batch 51 Loss:0.02032035030424595\n",
      "Epoch 67, Batch 52 Loss:0.003084598807618022\n",
      "Epoch 67, Batch 53 Loss:0.008584295399487019\n",
      "Epoch 67, Batch 54 Loss:0.003164571011438966\n",
      "Epoch 67, Batch 55 Loss:0.006517852656543255\n",
      "Epoch 67, Batch 56 Loss:0.006473139394074678\n",
      "Epoch 67, Batch 57 Loss:0.008283148519694805\n",
      "Epoch 67, Batch 58 Loss:0.002328897127881646\n",
      "Epoch 67, Batch 59 Loss:0.006251840852200985\n",
      "Epoch 67, Batch 60 Loss:0.003967914264649153\n",
      "Epoch 67, Batch 61 Loss:0.012277689762413502\n",
      "Epoch 67, Batch 62 Loss:0.0021971408277750015\n",
      "Epoch 67, Batch 63 Loss:0.003154260106384754\n",
      "Epoch 67, Batch 64 Loss:0.010590326972305775\n",
      "Epoch 67, Batch 65 Loss:0.0019107948755845428\n",
      "Epoch 67, Batch 66 Loss:0.003882685210555792\n",
      "Epoch 67, Batch 67 Loss:0.010449190624058247\n",
      "Epoch 67, Batch 68 Loss:0.004249498248100281\n",
      "Epoch 67, Batch 69 Loss:0.005748793017119169\n",
      "Epoch 67, Batch 70 Loss:0.010442813858389854\n",
      "Epoch 67, Batch 71 Loss:0.0017960523255169392\n",
      "Epoch 67, Batch 72 Loss:0.004274641163647175\n",
      "Epoch 67, Batch 73 Loss:0.009090573526918888\n",
      "Epoch 67, Batch 74 Loss:0.02974516525864601\n",
      "Epoch 67, Batch 75 Loss:0.002718622563406825\n",
      "Epoch 67, Batch 76 Loss:0.0019314809469506145\n",
      "Epoch 67, Batch 77 Loss:0.003051905892789364\n",
      "Epoch 67, Batch 78 Loss:0.0013221806148067117\n",
      "Epoch 67, Batch 79 Loss:0.007876078598201275\n",
      "Epoch 67, Batch 80 Loss:0.006629836279898882\n",
      "Epoch 67, Batch 81 Loss:0.03133524954319\n",
      "Epoch 67, Batch 82 Loss:0.025746481493115425\n",
      "Epoch 67, Batch 83 Loss:0.0015597267774865031\n",
      "Epoch 67, Batch 84 Loss:0.012066567316651344\n",
      "Epoch 67, Batch 85 Loss:0.023691099137067795\n",
      "Epoch 67, Batch 86 Loss:0.0024819958489388227\n",
      "Epoch 67, Batch 87 Loss:0.027061378583312035\n",
      "Epoch 67, Batch 88 Loss:0.005373011808842421\n",
      "Epoch 67, Batch 89 Loss:0.005529978312551975\n",
      "Epoch 67, Batch 90 Loss:0.01807597652077675\n",
      "Epoch 67, Batch 91 Loss:0.015253949910402298\n",
      "Epoch 67, Batch 92 Loss:0.04242706298828125\n",
      "Epoch 67, Batch 93 Loss:0.006511456333100796\n",
      "Epoch 67, Batch 94 Loss:0.01807304099202156\n",
      "Epoch 67, Batch 95 Loss:0.028273800387978554\n",
      "Epoch 67, Batch 96 Loss:0.027240809053182602\n",
      "Epoch 67, Batch 97 Loss:0.012780715711414814\n",
      "Epoch 67, Batch 98 Loss:0.017476258799433708\n",
      "Epoch 67, Batch 99 Loss:0.004402355756610632\n",
      "Epoch 67, Batch 100 Loss:0.006973398849368095\n",
      "Epoch 67, Batch 101 Loss:0.00430756900459528\n",
      "Epoch 67, Batch 102 Loss:0.01564199849963188\n",
      "Epoch 67, Batch 103 Loss:0.006121447775512934\n",
      "Epoch 67, Batch 104 Loss:0.01395797822624445\n",
      "Epoch 67, Batch 105 Loss:0.007497514132410288\n",
      "Epoch 67, Batch 106 Loss:0.02372961863875389\n",
      "Epoch 67, Batch 107 Loss:0.005288089159876108\n",
      "Epoch 67, Batch 108 Loss:0.01855216734111309\n",
      "Epoch 67, Batch 109 Loss:0.04987575486302376\n",
      "Epoch 67, Batch 110 Loss:0.04013459011912346\n",
      "Epoch 67, Batch 111 Loss:0.012383111752569675\n",
      "Epoch 67, Batch 112 Loss:0.012073708698153496\n",
      "Epoch 67, Batch 113 Loss:0.010761089622974396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67, Batch 114 Loss:0.017507169395685196\n",
      "Epoch 67, Batch 115 Loss:0.010837424546480179\n",
      "Epoch 67, Batch 116 Loss:0.029159747064113617\n",
      "Epoch 67, Batch 117 Loss:0.013728826306760311\n",
      "Epoch 67, Batch 118 Loss:0.020342819392681122\n",
      "Epoch 67, Batch 119 Loss:0.014542439952492714\n",
      "Epoch 67, Batch 120 Loss:0.02222593128681183\n",
      "Epoch 67, Batch 121 Loss:0.010559791699051857\n",
      "Epoch 67, Batch 122 Loss:0.00874207355082035\n",
      "Epoch 67, Batch 123 Loss:0.012019535526633263\n",
      "Epoch 67, Batch 124 Loss:0.08088778704404831\n",
      "Epoch 67, Batch 125 Loss:0.02189832553267479\n",
      "Epoch 67, Batch 126 Loss:0.015222758986055851\n",
      "Epoch 67, Batch 127 Loss:0.008821379393339157\n",
      "Epoch 67, Batch 128 Loss:0.015612123534083366\n",
      "Epoch 67, Batch 129 Loss:0.003726561786606908\n",
      "Epoch 67, Batch 130 Loss:0.0014845296973362565\n",
      "Epoch 67, Batch 131 Loss:0.0042649381794035435\n",
      "Epoch 67, Batch 132 Loss:0.012606238946318626\n",
      "Epoch 67, Batch 133 Loss:0.019774917513132095\n",
      "Epoch 67, Batch 134 Loss:0.006265322677791119\n",
      "Epoch 67, Batch 135 Loss:0.004315108060836792\n",
      "Epoch 67, Batch 136 Loss:0.00409572571516037\n",
      "Epoch 67, Batch 137 Loss:0.008116273209452629\n",
      "Epoch 67, Batch 138 Loss:0.0016983050154522061\n",
      "Epoch 67, Batch 139 Loss:0.0021749362349510193\n",
      "Epoch 67, Batch 140 Loss:0.006788839586079121\n",
      "Epoch 67, Batch 141 Loss:0.0013377309078350663\n",
      "Epoch 67, Batch 142 Loss:0.017709849402308464\n",
      "Epoch 67, Batch 143 Loss:0.008288626559078693\n",
      "Epoch 67, Batch 144 Loss:0.023759078234434128\n",
      "Epoch 67, Batch 145 Loss:0.004726286977529526\n",
      "Epoch 67, Batch 146 Loss:0.005073657725006342\n",
      "Epoch 67, Batch 147 Loss:0.0030818097293376923\n",
      "Epoch 67, Batch 148 Loss:0.013073133304715157\n",
      "Epoch 67, Batch 149 Loss:0.03835121914744377\n",
      "Epoch 67, Batch 150 Loss:0.008923896588385105\n",
      "Epoch 67, Batch 151 Loss:0.00526815839111805\n",
      "Epoch 67, Batch 152 Loss:0.007859126664698124\n",
      "Epoch 67, Batch 153 Loss:0.015417102724313736\n",
      "Epoch 67, Batch 154 Loss:0.008769985288381577\n",
      "Epoch 67, Batch 155 Loss:0.0033495051320642233\n",
      "Epoch 67, Batch 156 Loss:0.002476765774190426\n",
      "Epoch 67, Batch 157 Loss:0.02957305684685707\n",
      "Epoch 67, Batch 158 Loss:0.012356840074062347\n",
      "Epoch 67, Batch 159 Loss:0.004819017369300127\n",
      "Epoch 67, Batch 160 Loss:0.028279650956392288\n",
      "Epoch 67, Batch 161 Loss:0.007566134911030531\n",
      "Epoch 67, Batch 162 Loss:0.026340302079916\n",
      "Epoch 67, Batch 163 Loss:0.018069803714752197\n",
      "Epoch 67, Batch 164 Loss:0.008491573855280876\n",
      "Epoch 67, Batch 165 Loss:0.03182174265384674\n",
      "Epoch 67, Batch 166 Loss:0.005331503227353096\n",
      "Epoch 67, Batch 167 Loss:0.010264147073030472\n",
      "Epoch 67, Batch 168 Loss:0.006328871473670006\n",
      "Epoch 67, Batch 169 Loss:0.015132366679608822\n",
      "Epoch 67, Batch 170 Loss:0.003232814371585846\n",
      "Epoch 67, Batch 171 Loss:0.010737337172031403\n",
      "Epoch 67, Batch 172 Loss:0.01825600676238537\n",
      "Epoch 67, Batch 173 Loss:0.010344913229346275\n",
      "Epoch 67, Batch 174 Loss:0.011667076498270035\n",
      "Epoch 67, Batch 175 Loss:0.017607076093554497\n",
      "Epoch 67, Batch 176 Loss:0.023147953674197197\n",
      "Epoch 67, Batch 177 Loss:0.03197880834341049\n",
      "Epoch 67, Batch 178 Loss:0.007378808688372374\n",
      "Epoch 67, Batch 179 Loss:0.024516239762306213\n",
      "Epoch 67, Batch 180 Loss:0.0076386164873838425\n",
      "Epoch 67, Batch 181 Loss:0.005916195455938578\n",
      "Epoch 67, Batch 182 Loss:0.006013456732034683\n",
      "Epoch 67, Batch 183 Loss:0.007254192139953375\n",
      "Epoch 67, Batch 184 Loss:0.012615522369742393\n",
      "Epoch 67, Batch 185 Loss:0.005090557970106602\n",
      "Epoch 67, Batch 186 Loss:0.009172062389552593\n",
      "Epoch 67, Batch 187 Loss:0.007538508623838425\n",
      "Epoch 67, Batch 188 Loss:0.028932983055710793\n",
      "Epoch 67, Batch 189 Loss:0.01165132038295269\n",
      "Epoch 67, Batch 190 Loss:0.020849209278821945\n",
      "Epoch 67, Batch 191 Loss:0.013296574354171753\n",
      "Epoch 67, Batch 192 Loss:0.007403891999274492\n",
      "Epoch 67, Batch 193 Loss:0.015001422725617886\n",
      "Epoch 67, Batch 194 Loss:0.005110306665301323\n",
      "Epoch 67, Batch 195 Loss:0.006410924252122641\n",
      "Epoch 67, Batch 196 Loss:0.027728941291570663\n",
      "Epoch 67, Batch 197 Loss:0.007193346973508596\n",
      "Epoch 67, Batch 198 Loss:0.008859062567353249\n",
      "Epoch 67, Batch 199 Loss:0.00603655818849802\n",
      "Epoch 67, Batch 200 Loss:0.058688364923000336\n",
      "Epoch 67, Batch 201 Loss:0.004912512376904488\n",
      "Epoch 67, Batch 202 Loss:0.004213847219944\n",
      "Epoch 67, Batch 203 Loss:0.005619117058813572\n",
      "Epoch 67, Batch 204 Loss:0.007731348276138306\n",
      "Epoch 67, Batch 205 Loss:0.003864120692014694\n",
      "Epoch 67, Batch 206 Loss:0.007981576956808567\n",
      "Epoch 67, Batch 207 Loss:0.021563291549682617\n",
      "Epoch 67, Batch 208 Loss:0.025569360703229904\n",
      "Epoch 67, Batch 209 Loss:0.006927263457328081\n",
      "Epoch 67, Batch 210 Loss:0.012426703236997128\n",
      "Epoch 67, Batch 211 Loss:0.006313340738415718\n",
      "Epoch 67, Batch 212 Loss:0.0027365610003471375\n",
      "Epoch 67, Batch 213 Loss:0.014709573239088058\n",
      "Epoch 67, Batch 214 Loss:0.00981997326016426\n",
      "Epoch 67, Batch 215 Loss:0.02298957109451294\n",
      "Epoch 67, Batch 216 Loss:0.009550103917717934\n",
      "Epoch 67, Batch 217 Loss:0.006114392541348934\n",
      "Epoch 67, Batch 218 Loss:0.08808520436286926\n",
      "Epoch 67, Batch 219 Loss:0.02447962760925293\n",
      "Epoch 67, Batch 220 Loss:0.03969358652830124\n",
      "Epoch 67, Batch 221 Loss:0.012163490988314152\n",
      "Epoch 67, Batch 222 Loss:0.013559824787080288\n",
      "Epoch 67, Batch 223 Loss:0.005747547373175621\n",
      "Epoch 67, Batch 224 Loss:0.00956378597766161\n",
      "Epoch 67, Batch 225 Loss:0.016684936359524727\n",
      "Epoch 67, Batch 226 Loss:0.003860875265672803\n",
      "Epoch 67, Batch 227 Loss:0.02121363952755928\n",
      "Epoch 67, Batch 228 Loss:0.016270451247692108\n",
      "Epoch 67, Batch 229 Loss:0.013201747089624405\n",
      "Epoch 67, Batch 230 Loss:0.005232471507042646\n",
      "Epoch 67, Batch 231 Loss:0.01250182930380106\n",
      "Epoch 67, Batch 232 Loss:0.02026338130235672\n",
      "Epoch 67, Batch 233 Loss:0.00816895067691803\n",
      "Loss in this Epoch is: 0.816895067692 %\n",
      "Accuracy in this Epoch is: 88.4400010109 %\n",
      "Epoch 68, Batch 0 Loss:0.006250383798032999\n",
      "Epoch 68, Batch 1 Loss:0.013455948792397976\n",
      "Epoch 68, Batch 2 Loss:0.011074359528720379\n",
      "Epoch 68, Batch 3 Loss:0.01591147668659687\n",
      "Epoch 68, Batch 4 Loss:0.004932372830808163\n",
      "Epoch 68, Batch 5 Loss:0.009089764207601547\n",
      "Epoch 68, Batch 6 Loss:0.0068983519449830055\n",
      "Epoch 68, Batch 7 Loss:0.0031384543981403112\n",
      "Epoch 68, Batch 8 Loss:0.01751447282731533\n",
      "Epoch 68, Batch 9 Loss:0.008289933204650879\n",
      "Epoch 68, Batch 10 Loss:0.020205439999699593\n",
      "Epoch 68, Batch 11 Loss:0.010957577265799046\n",
      "Epoch 68, Batch 12 Loss:0.008899146690964699\n",
      "Epoch 68, Batch 13 Loss:0.013628670014441013\n",
      "Epoch 68, Batch 14 Loss:0.021372804418206215\n",
      "Epoch 68, Batch 15 Loss:0.022483469918370247\n",
      "Epoch 68, Batch 16 Loss:0.016954978927969933\n",
      "Epoch 68, Batch 17 Loss:0.0052226935513317585\n",
      "Epoch 68, Batch 18 Loss:0.009132263250648975\n",
      "Epoch 68, Batch 19 Loss:0.03014637529850006\n",
      "Epoch 68, Batch 20 Loss:0.008182782679796219\n",
      "Epoch 68, Batch 21 Loss:0.008154671639204025\n",
      "Epoch 68, Batch 22 Loss:0.010277843102812767\n",
      "Epoch 68, Batch 23 Loss:0.004961728118360043\n",
      "Epoch 68, Batch 24 Loss:0.020510289818048477\n",
      "Epoch 68, Batch 25 Loss:0.004154737573117018\n",
      "Epoch 68, Batch 26 Loss:0.00998002104461193\n",
      "Epoch 68, Batch 27 Loss:0.007075782399624586\n",
      "Epoch 68, Batch 28 Loss:0.01387971080839634\n",
      "Epoch 68, Batch 29 Loss:0.015739906579256058\n",
      "Epoch 68, Batch 30 Loss:0.016713406890630722\n",
      "Epoch 68, Batch 31 Loss:0.011342965997755527\n",
      "Epoch 68, Batch 32 Loss:0.005390615202486515\n",
      "Epoch 68, Batch 33 Loss:0.018382210284471512\n",
      "Epoch 68, Batch 34 Loss:0.014236543327569962\n",
      "Epoch 68, Batch 35 Loss:0.009616479277610779\n",
      "Epoch 68, Batch 36 Loss:0.012395737692713737\n",
      "Epoch 68, Batch 37 Loss:0.011051436886191368\n",
      "Epoch 68, Batch 38 Loss:0.00737832160666585\n",
      "Epoch 68, Batch 39 Loss:0.0075114150531589985\n",
      "Epoch 68, Batch 40 Loss:0.008861048147082329\n",
      "Epoch 68, Batch 41 Loss:0.016763685271143913\n",
      "Epoch 68, Batch 42 Loss:0.006877438630908728\n",
      "Epoch 68, Batch 43 Loss:0.002467908663675189\n",
      "Epoch 68, Batch 44 Loss:0.011112265288829803\n",
      "Epoch 68, Batch 45 Loss:0.007288879714906216\n",
      "Epoch 68, Batch 46 Loss:0.01811448112130165\n",
      "Epoch 68, Batch 47 Loss:0.011174610815942287\n",
      "Epoch 68, Batch 48 Loss:0.006016647443175316\n",
      "Epoch 68, Batch 49 Loss:0.015972860157489777\n",
      "Epoch 68, Batch 50 Loss:0.007572837173938751\n",
      "Epoch 68, Batch 51 Loss:0.027235986664891243\n",
      "Epoch 68, Batch 52 Loss:0.004368955735117197\n",
      "Epoch 68, Batch 53 Loss:0.00898846797645092\n",
      "Epoch 68, Batch 54 Loss:0.010568365454673767\n",
      "Epoch 68, Batch 55 Loss:0.014088531956076622\n",
      "Epoch 68, Batch 56 Loss:0.015489855781197548\n",
      "Epoch 68, Batch 57 Loss:0.007531177252531052\n",
      "Epoch 68, Batch 58 Loss:0.0044058519415557384\n",
      "Epoch 68, Batch 59 Loss:0.01019264105707407\n",
      "Epoch 68, Batch 60 Loss:0.010965070687234402\n",
      "Epoch 68, Batch 61 Loss:0.003788543865084648\n",
      "Epoch 68, Batch 62 Loss:0.014538172632455826\n",
      "Epoch 68, Batch 63 Loss:0.012797673232853413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68, Batch 64 Loss:0.021750202402472496\n",
      "Epoch 68, Batch 65 Loss:0.01590588688850403\n",
      "Epoch 68, Batch 66 Loss:0.07232891023159027\n",
      "Epoch 68, Batch 67 Loss:0.013585123233497143\n",
      "Epoch 68, Batch 68 Loss:0.01176520437002182\n",
      "Epoch 68, Batch 69 Loss:0.03330746665596962\n",
      "Epoch 68, Batch 70 Loss:0.009582922793924809\n",
      "Epoch 68, Batch 71 Loss:0.037265099585056305\n",
      "Epoch 68, Batch 72 Loss:0.04675499349832535\n",
      "Epoch 68, Batch 73 Loss:0.03472099080681801\n",
      "Epoch 68, Batch 74 Loss:0.05363521724939346\n",
      "Epoch 68, Batch 75 Loss:0.013994504697620869\n",
      "Epoch 68, Batch 76 Loss:0.037127990275621414\n",
      "Epoch 68, Batch 77 Loss:0.02483583241701126\n",
      "Epoch 68, Batch 78 Loss:0.012127699330449104\n",
      "Epoch 68, Batch 79 Loss:0.013230884447693825\n",
      "Epoch 68, Batch 80 Loss:0.006724738981574774\n",
      "Epoch 68, Batch 81 Loss:0.02565012313425541\n",
      "Epoch 68, Batch 82 Loss:0.01582535356283188\n",
      "Epoch 68, Batch 83 Loss:0.03005935624241829\n",
      "Epoch 68, Batch 84 Loss:0.022136935964226723\n",
      "Epoch 68, Batch 85 Loss:0.02131737768650055\n",
      "Epoch 68, Batch 86 Loss:0.051030199974775314\n",
      "Epoch 68, Batch 87 Loss:0.032747771590948105\n",
      "Epoch 68, Batch 88 Loss:0.008734832517802715\n",
      "Epoch 68, Batch 89 Loss:0.01850210130214691\n",
      "Epoch 68, Batch 90 Loss:0.016256771981716156\n",
      "Epoch 68, Batch 91 Loss:0.038019128143787384\n",
      "Epoch 68, Batch 92 Loss:0.011046004481613636\n",
      "Epoch 68, Batch 93 Loss:0.010518093593418598\n",
      "Epoch 68, Batch 94 Loss:0.010532565414905548\n",
      "Epoch 68, Batch 95 Loss:0.016688531264662743\n",
      "Epoch 68, Batch 96 Loss:0.03076273202896118\n",
      "Epoch 68, Batch 97 Loss:0.05149437487125397\n",
      "Epoch 68, Batch 98 Loss:0.02094597928225994\n",
      "Epoch 68, Batch 99 Loss:0.02105916477739811\n",
      "Epoch 68, Batch 100 Loss:0.04093066230416298\n",
      "Epoch 68, Batch 101 Loss:0.02666282281279564\n",
      "Epoch 68, Batch 102 Loss:0.009126126766204834\n",
      "Epoch 68, Batch 103 Loss:0.003393888007849455\n",
      "Epoch 68, Batch 104 Loss:0.013480424880981445\n",
      "Epoch 68, Batch 105 Loss:0.06662362068891525\n",
      "Epoch 68, Batch 106 Loss:0.02723785862326622\n",
      "Epoch 68, Batch 107 Loss:0.017386194318532944\n",
      "Epoch 68, Batch 108 Loss:0.011836986057460308\n",
      "Epoch 68, Batch 109 Loss:0.021422188729047775\n",
      "Epoch 68, Batch 110 Loss:0.007830662652850151\n",
      "Epoch 68, Batch 111 Loss:0.03626347705721855\n",
      "Epoch 68, Batch 112 Loss:0.050932712852954865\n",
      "Epoch 68, Batch 113 Loss:0.004331548698246479\n",
      "Epoch 68, Batch 114 Loss:0.04063374921679497\n",
      "Epoch 68, Batch 115 Loss:0.027365032583475113\n",
      "Epoch 68, Batch 116 Loss:0.03990572690963745\n",
      "Epoch 68, Batch 117 Loss:0.007461874280124903\n",
      "Epoch 68, Batch 118 Loss:0.017937734723091125\n",
      "Epoch 68, Batch 119 Loss:0.07859843969345093\n",
      "Epoch 68, Batch 120 Loss:0.019929666072130203\n",
      "Epoch 68, Batch 121 Loss:0.013387942686676979\n",
      "Epoch 68, Batch 122 Loss:0.023407580330967903\n",
      "Epoch 68, Batch 123 Loss:0.011157221160829067\n",
      "Epoch 68, Batch 124 Loss:0.015073378570377827\n",
      "Epoch 68, Batch 125 Loss:0.03086668998003006\n",
      "Epoch 68, Batch 126 Loss:0.04113788157701492\n",
      "Epoch 68, Batch 127 Loss:0.03973960503935814\n",
      "Epoch 68, Batch 128 Loss:0.09301915019750595\n",
      "Epoch 68, Batch 129 Loss:0.008046125061810017\n",
      "Epoch 68, Batch 130 Loss:0.03195933625102043\n",
      "Epoch 68, Batch 131 Loss:0.014948053285479546\n",
      "Epoch 68, Batch 132 Loss:0.016742205247282982\n",
      "Epoch 68, Batch 133 Loss:0.03452429920434952\n",
      "Epoch 68, Batch 134 Loss:0.011091386899352074\n",
      "Epoch 68, Batch 135 Loss:0.0448700487613678\n",
      "Epoch 68, Batch 136 Loss:0.012334326282143593\n",
      "Epoch 68, Batch 137 Loss:0.03038247860968113\n",
      "Epoch 68, Batch 138 Loss:0.04165682941675186\n",
      "Epoch 68, Batch 139 Loss:0.02541327103972435\n",
      "Epoch 68, Batch 140 Loss:0.016090363264083862\n",
      "Epoch 68, Batch 141 Loss:0.013100619427859783\n",
      "Epoch 68, Batch 142 Loss:0.05764845013618469\n",
      "Epoch 68, Batch 143 Loss:0.04733588546514511\n",
      "Epoch 68, Batch 144 Loss:0.02732345461845398\n",
      "Epoch 68, Batch 145 Loss:0.018825434148311615\n",
      "Epoch 68, Batch 146 Loss:0.014085975475609303\n",
      "Epoch 68, Batch 147 Loss:0.07447005063295364\n",
      "Epoch 68, Batch 148 Loss:0.007087116129696369\n",
      "Epoch 68, Batch 149 Loss:0.01286572590470314\n",
      "Epoch 68, Batch 150 Loss:0.06245551258325577\n",
      "Epoch 68, Batch 151 Loss:0.03413083404302597\n",
      "Epoch 68, Batch 152 Loss:0.005324703641235828\n",
      "Epoch 68, Batch 153 Loss:0.02714967355132103\n",
      "Epoch 68, Batch 154 Loss:0.013209477998316288\n",
      "Epoch 68, Batch 155 Loss:0.03897184133529663\n",
      "Epoch 68, Batch 156 Loss:0.05766823887825012\n",
      "Epoch 68, Batch 157 Loss:0.018779223784804344\n",
      "Epoch 68, Batch 158 Loss:0.014315877109766006\n",
      "Epoch 68, Batch 159 Loss:0.04276624321937561\n",
      "Epoch 68, Batch 160 Loss:0.011620874516665936\n",
      "Epoch 68, Batch 161 Loss:0.019774582237005234\n",
      "Epoch 68, Batch 162 Loss:0.039911944419145584\n",
      "Epoch 68, Batch 163 Loss:0.1062244325876236\n",
      "Epoch 68, Batch 164 Loss:0.09843811392784119\n",
      "Epoch 68, Batch 165 Loss:0.0667591392993927\n",
      "Epoch 68, Batch 166 Loss:0.05912761390209198\n",
      "Epoch 68, Batch 167 Loss:0.05532245710492134\n",
      "Epoch 68, Batch 168 Loss:0.10412953048944473\n",
      "Epoch 68, Batch 169 Loss:0.03309084102511406\n",
      "Epoch 68, Batch 170 Loss:0.052153702825307846\n",
      "Epoch 68, Batch 171 Loss:0.018640153110027313\n",
      "Epoch 68, Batch 172 Loss:0.06091897189617157\n",
      "Epoch 68, Batch 173 Loss:0.014068537391722202\n",
      "Epoch 68, Batch 174 Loss:0.03604176640510559\n",
      "Epoch 68, Batch 175 Loss:0.0805538073182106\n",
      "Epoch 68, Batch 176 Loss:0.0537104457616806\n",
      "Epoch 68, Batch 177 Loss:0.02023562602698803\n",
      "Epoch 68, Batch 178 Loss:0.020932944491505623\n",
      "Epoch 68, Batch 179 Loss:0.018741436302661896\n",
      "Epoch 68, Batch 180 Loss:0.03889739513397217\n",
      "Epoch 68, Batch 181 Loss:0.03147803619503975\n",
      "Epoch 68, Batch 182 Loss:0.024032320827245712\n",
      "Epoch 68, Batch 183 Loss:0.05100030452013016\n",
      "Epoch 68, Batch 184 Loss:0.0178469717502594\n",
      "Epoch 68, Batch 185 Loss:0.03350609913468361\n",
      "Epoch 68, Batch 186 Loss:0.0594029501080513\n",
      "Epoch 68, Batch 187 Loss:0.05015280470252037\n",
      "Epoch 68, Batch 188 Loss:0.01149541512131691\n",
      "Epoch 68, Batch 189 Loss:0.017146511003375053\n",
      "Epoch 68, Batch 190 Loss:0.025281092151999474\n",
      "Epoch 68, Batch 191 Loss:0.0270814411342144\n",
      "Epoch 68, Batch 192 Loss:0.026873985305428505\n",
      "Epoch 68, Batch 193 Loss:0.01543149258941412\n",
      "Epoch 68, Batch 194 Loss:0.020372003316879272\n",
      "Epoch 68, Batch 195 Loss:0.009169311262667179\n",
      "Epoch 68, Batch 196 Loss:0.017616145312786102\n",
      "Epoch 68, Batch 197 Loss:0.017855558544397354\n",
      "Epoch 68, Batch 198 Loss:0.024086475372314453\n",
      "Epoch 68, Batch 199 Loss:0.012378603219985962\n",
      "Epoch 68, Batch 200 Loss:0.013573200441896915\n",
      "Epoch 68, Batch 201 Loss:0.017245952039957047\n",
      "Epoch 68, Batch 202 Loss:0.0598408579826355\n",
      "Epoch 68, Batch 203 Loss:0.023463238030672073\n",
      "Epoch 68, Batch 204 Loss:0.05293701961636543\n",
      "Epoch 68, Batch 205 Loss:0.045671142637729645\n",
      "Epoch 68, Batch 206 Loss:0.014899098314344883\n",
      "Epoch 68, Batch 207 Loss:0.012655152007937431\n",
      "Epoch 68, Batch 208 Loss:0.0050753019750118256\n",
      "Epoch 68, Batch 209 Loss:0.02912582829594612\n",
      "Epoch 68, Batch 210 Loss:0.02030392736196518\n",
      "Epoch 68, Batch 211 Loss:0.010892383754253387\n",
      "Epoch 68, Batch 212 Loss:0.021696284413337708\n",
      "Epoch 68, Batch 213 Loss:0.014905251562595367\n",
      "Epoch 68, Batch 214 Loss:0.045383624732494354\n",
      "Epoch 68, Batch 215 Loss:0.022240277379751205\n",
      "Epoch 68, Batch 216 Loss:0.012063801288604736\n",
      "Epoch 68, Batch 217 Loss:0.05060561001300812\n",
      "Epoch 68, Batch 218 Loss:0.00998363271355629\n",
      "Epoch 68, Batch 219 Loss:0.019414128735661507\n",
      "Epoch 68, Batch 220 Loss:0.024790644645690918\n",
      "Epoch 68, Batch 221 Loss:0.05080020800232887\n",
      "Epoch 68, Batch 222 Loss:0.051557570695877075\n",
      "Epoch 68, Batch 223 Loss:0.01703255996108055\n",
      "Epoch 68, Batch 224 Loss:0.015238695777952671\n",
      "Epoch 68, Batch 225 Loss:0.010031381621956825\n",
      "Epoch 68, Batch 226 Loss:0.020737245678901672\n",
      "Epoch 68, Batch 227 Loss:0.09538105130195618\n",
      "Epoch 68, Batch 228 Loss:0.01739739254117012\n",
      "Epoch 68, Batch 229 Loss:0.012567282654345036\n",
      "Epoch 68, Batch 230 Loss:0.016279416158795357\n",
      "Epoch 68, Batch 231 Loss:0.025490131229162216\n",
      "Epoch 68, Batch 232 Loss:0.024455659091472626\n",
      "Epoch 68, Batch 233 Loss:0.08017254620790482\n",
      "Loss in this Epoch is: 8.01725462079 %\n",
      "Accuracy in this Epoch is: 88.0500018597 %\n",
      "Epoch 69, Batch 0 Loss:0.046996768563985825\n",
      "Epoch 69, Batch 1 Loss:0.03165063634514809\n",
      "Epoch 69, Batch 2 Loss:0.02480790577828884\n",
      "Epoch 69, Batch 3 Loss:0.017508281394839287\n",
      "Epoch 69, Batch 4 Loss:0.016812361776828766\n",
      "Epoch 69, Batch 5 Loss:0.04669830575585365\n",
      "Epoch 69, Batch 6 Loss:0.015953538939356804\n",
      "Epoch 69, Batch 7 Loss:0.011148126795887947\n",
      "Epoch 69, Batch 8 Loss:0.02850707247853279\n",
      "Epoch 69, Batch 9 Loss:0.014724436216056347\n",
      "Epoch 69, Batch 10 Loss:0.04747586324810982\n",
      "Epoch 69, Batch 11 Loss:0.08503708243370056\n",
      "Epoch 69, Batch 12 Loss:0.02238616906106472\n",
      "Epoch 69, Batch 13 Loss:0.023945162072777748\n",
      "Epoch 69, Batch 14 Loss:0.019860565662384033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Batch 15 Loss:0.0074838013388216496\n",
      "Epoch 69, Batch 16 Loss:0.022081004455685616\n",
      "Epoch 69, Batch 17 Loss:0.008121692575514317\n",
      "Epoch 69, Batch 18 Loss:0.01103624701499939\n",
      "Epoch 69, Batch 19 Loss:0.002861840883269906\n",
      "Epoch 69, Batch 20 Loss:0.022339649498462677\n",
      "Epoch 69, Batch 21 Loss:0.027734961360692978\n",
      "Epoch 69, Batch 22 Loss:0.052158646285533905\n",
      "Epoch 69, Batch 23 Loss:0.006140253506600857\n",
      "Epoch 69, Batch 24 Loss:0.015311026945710182\n",
      "Epoch 69, Batch 25 Loss:0.010575930587947369\n",
      "Epoch 69, Batch 26 Loss:0.006450180895626545\n",
      "Epoch 69, Batch 27 Loss:0.01620686799287796\n",
      "Epoch 69, Batch 28 Loss:0.02070397511124611\n",
      "Epoch 69, Batch 29 Loss:0.009759676642715931\n",
      "Epoch 69, Batch 30 Loss:0.037482600659132004\n",
      "Epoch 69, Batch 31 Loss:0.019131777808070183\n",
      "Epoch 69, Batch 32 Loss:0.011011084541678429\n",
      "Epoch 69, Batch 33 Loss:0.014026925899088383\n",
      "Epoch 69, Batch 34 Loss:0.019918292760849\n",
      "Epoch 69, Batch 35 Loss:0.0036648211535066366\n",
      "Epoch 69, Batch 36 Loss:0.0042316908948123455\n",
      "Epoch 69, Batch 37 Loss:0.01791379041969776\n",
      "Epoch 69, Batch 38 Loss:0.019513631239533424\n",
      "Epoch 69, Batch 39 Loss:0.025793610140681267\n",
      "Epoch 69, Batch 40 Loss:0.006991127505898476\n",
      "Epoch 69, Batch 41 Loss:0.03578141704201698\n",
      "Epoch 69, Batch 42 Loss:0.03557275980710983\n",
      "Epoch 69, Batch 43 Loss:0.05881962552666664\n",
      "Epoch 69, Batch 44 Loss:0.007593291345983744\n",
      "Epoch 69, Batch 45 Loss:0.021435648202896118\n",
      "Epoch 69, Batch 46 Loss:0.005724782589823008\n",
      "Epoch 69, Batch 47 Loss:0.010707870125770569\n",
      "Epoch 69, Batch 48 Loss:0.0449352040886879\n",
      "Epoch 69, Batch 49 Loss:0.07381460815668106\n",
      "Epoch 69, Batch 50 Loss:0.008871018886566162\n",
      "Epoch 69, Batch 51 Loss:0.0063477326184511185\n",
      "Epoch 69, Batch 52 Loss:0.0069738044403493404\n",
      "Epoch 69, Batch 53 Loss:0.014163315296173096\n",
      "Epoch 69, Batch 54 Loss:0.00292128580622375\n",
      "Epoch 69, Batch 55 Loss:0.03257262334227562\n",
      "Epoch 69, Batch 56 Loss:0.030711382627487183\n",
      "Epoch 69, Batch 57 Loss:0.014927472919225693\n",
      "Epoch 69, Batch 58 Loss:0.010621728375554085\n",
      "Epoch 69, Batch 59 Loss:0.03566093370318413\n",
      "Epoch 69, Batch 60 Loss:0.050672031939029694\n",
      "Epoch 69, Batch 61 Loss:0.019495584070682526\n",
      "Epoch 69, Batch 62 Loss:0.01123660895973444\n",
      "Epoch 69, Batch 63 Loss:0.018041318282485008\n",
      "Epoch 69, Batch 64 Loss:0.01443856954574585\n",
      "Epoch 69, Batch 65 Loss:0.013350971043109894\n",
      "Epoch 69, Batch 66 Loss:0.03700688108801842\n",
      "Epoch 69, Batch 67 Loss:0.016870195046067238\n",
      "Epoch 69, Batch 68 Loss:0.013911951333284378\n",
      "Epoch 69, Batch 69 Loss:0.008341220207512379\n",
      "Epoch 69, Batch 70 Loss:0.012382694520056248\n",
      "Epoch 69, Batch 71 Loss:0.011444657109677792\n",
      "Epoch 69, Batch 72 Loss:0.03073708899319172\n",
      "Epoch 69, Batch 73 Loss:0.021573510020971298\n",
      "Epoch 69, Batch 74 Loss:0.03459785506129265\n",
      "Epoch 69, Batch 75 Loss:0.016697464510798454\n",
      "Epoch 69, Batch 76 Loss:0.03287166357040405\n",
      "Epoch 69, Batch 77 Loss:0.010809586383402348\n",
      "Epoch 69, Batch 78 Loss:0.06368272751569748\n",
      "Epoch 69, Batch 79 Loss:0.06109800562262535\n",
      "Epoch 69, Batch 80 Loss:0.014106919057667255\n",
      "Epoch 69, Batch 81 Loss:0.03179457038640976\n",
      "Epoch 69, Batch 82 Loss:0.06001785397529602\n",
      "Epoch 69, Batch 83 Loss:0.013154140673577785\n",
      "Epoch 69, Batch 84 Loss:0.03005175106227398\n",
      "Epoch 69, Batch 85 Loss:0.03760883957147598\n",
      "Epoch 69, Batch 86 Loss:0.03809226676821709\n",
      "Epoch 69, Batch 87 Loss:0.036657240241765976\n",
      "Epoch 69, Batch 88 Loss:0.012285567820072174\n",
      "Epoch 69, Batch 89 Loss:0.013895408250391483\n",
      "Epoch 69, Batch 90 Loss:0.01843179389834404\n",
      "Epoch 69, Batch 91 Loss:0.016928672790527344\n",
      "Epoch 69, Batch 92 Loss:0.02387341484427452\n",
      "Epoch 69, Batch 93 Loss:0.01022045686841011\n",
      "Epoch 69, Batch 94 Loss:0.031523726880550385\n",
      "Epoch 69, Batch 95 Loss:0.028728285804390907\n",
      "Epoch 69, Batch 96 Loss:0.03860347718000412\n",
      "Epoch 69, Batch 97 Loss:0.01704750955104828\n",
      "Epoch 69, Batch 98 Loss:0.0876038670539856\n",
      "Epoch 69, Batch 99 Loss:0.012010990642011166\n",
      "Epoch 69, Batch 100 Loss:0.019655980169773102\n",
      "Epoch 69, Batch 101 Loss:0.014113271608948708\n",
      "Epoch 69, Batch 102 Loss:0.020143359899520874\n",
      "Epoch 69, Batch 103 Loss:0.012208591215312481\n",
      "Epoch 69, Batch 104 Loss:0.005416523665189743\n",
      "Epoch 69, Batch 105 Loss:0.04141506925225258\n",
      "Epoch 69, Batch 106 Loss:0.035170525312423706\n",
      "Epoch 69, Batch 107 Loss:0.042048968374729156\n",
      "Epoch 69, Batch 108 Loss:0.018948687240481377\n",
      "Epoch 69, Batch 109 Loss:0.03384535387158394\n",
      "Epoch 69, Batch 110 Loss:0.03474381938576698\n",
      "Epoch 69, Batch 111 Loss:0.03382838889956474\n",
      "Epoch 69, Batch 112 Loss:0.06651961803436279\n",
      "Epoch 69, Batch 113 Loss:0.046131156384944916\n",
      "Epoch 69, Batch 114 Loss:0.02171587385237217\n",
      "Epoch 69, Batch 115 Loss:0.01009623147547245\n",
      "Epoch 69, Batch 116 Loss:0.01213239785283804\n",
      "Epoch 69, Batch 117 Loss:0.05536225810647011\n",
      "Epoch 69, Batch 118 Loss:0.04218588024377823\n",
      "Epoch 69, Batch 119 Loss:0.018389057368040085\n",
      "Epoch 69, Batch 120 Loss:0.03348160535097122\n",
      "Epoch 69, Batch 121 Loss:0.040506668388843536\n",
      "Epoch 69, Batch 122 Loss:0.008582583628594875\n",
      "Epoch 69, Batch 123 Loss:0.0368049293756485\n",
      "Epoch 69, Batch 124 Loss:0.019437342882156372\n",
      "Epoch 69, Batch 125 Loss:0.022309523075819016\n",
      "Epoch 69, Batch 126 Loss:0.018086757510900497\n",
      "Epoch 69, Batch 127 Loss:0.013356051407754421\n",
      "Epoch 69, Batch 128 Loss:0.03730734810233116\n",
      "Epoch 69, Batch 129 Loss:0.02867714874446392\n",
      "Epoch 69, Batch 130 Loss:0.05836773291230202\n",
      "Epoch 69, Batch 131 Loss:0.014530282467603683\n",
      "Epoch 69, Batch 132 Loss:0.016904547810554504\n",
      "Epoch 69, Batch 133 Loss:0.050360340625047684\n",
      "Epoch 69, Batch 134 Loss:0.022192444652318954\n",
      "Epoch 69, Batch 135 Loss:0.03956286236643791\n",
      "Epoch 69, Batch 136 Loss:0.0939498245716095\n",
      "Epoch 69, Batch 137 Loss:0.006281566806137562\n",
      "Epoch 69, Batch 138 Loss:0.038766324520111084\n",
      "Epoch 69, Batch 139 Loss:0.018646007403731346\n",
      "Epoch 69, Batch 140 Loss:0.013713818974792957\n",
      "Epoch 69, Batch 141 Loss:0.025015519931912422\n",
      "Epoch 69, Batch 142 Loss:0.013201914727687836\n",
      "Epoch 69, Batch 143 Loss:0.03192853182554245\n",
      "Epoch 69, Batch 144 Loss:0.013015097007155418\n",
      "Epoch 69, Batch 145 Loss:0.021200280636548996\n",
      "Epoch 69, Batch 146 Loss:0.028108768165111542\n",
      "Epoch 69, Batch 147 Loss:0.02027512714266777\n",
      "Epoch 69, Batch 148 Loss:0.052082423120737076\n",
      "Epoch 69, Batch 149 Loss:0.10655977576971054\n",
      "Epoch 69, Batch 150 Loss:0.04670838266611099\n",
      "Epoch 69, Batch 151 Loss:0.023481691256165504\n",
      "Epoch 69, Batch 152 Loss:0.03595016524195671\n",
      "Epoch 69, Batch 153 Loss:0.01086059957742691\n",
      "Epoch 69, Batch 154 Loss:0.07918360829353333\n",
      "Epoch 69, Batch 155 Loss:0.06862548738718033\n",
      "Epoch 69, Batch 156 Loss:0.03634894639253616\n",
      "Epoch 69, Batch 157 Loss:0.03292575851082802\n",
      "Epoch 69, Batch 158 Loss:0.03252822905778885\n",
      "Epoch 69, Batch 159 Loss:0.008151484653353691\n",
      "Epoch 69, Batch 160 Loss:0.03053225763142109\n",
      "Epoch 69, Batch 161 Loss:0.05337972566485405\n",
      "Epoch 69, Batch 162 Loss:0.056663643568754196\n",
      "Epoch 69, Batch 163 Loss:0.04495788365602493\n",
      "Epoch 69, Batch 164 Loss:0.03362888842821121\n",
      "Epoch 69, Batch 165 Loss:0.10082387924194336\n",
      "Epoch 69, Batch 166 Loss:0.03937704861164093\n",
      "Epoch 69, Batch 167 Loss:0.02356364205479622\n",
      "Epoch 69, Batch 168 Loss:0.03485456481575966\n",
      "Epoch 69, Batch 169 Loss:0.018981417641043663\n",
      "Epoch 69, Batch 170 Loss:0.029201827943325043\n",
      "Epoch 69, Batch 171 Loss:0.024055995047092438\n",
      "Epoch 69, Batch 172 Loss:0.01557387225329876\n",
      "Epoch 69, Batch 173 Loss:0.06840495765209198\n",
      "Epoch 69, Batch 174 Loss:0.011705044656991959\n",
      "Epoch 69, Batch 175 Loss:0.053182944655418396\n",
      "Epoch 69, Batch 176 Loss:0.18525665998458862\n",
      "Epoch 69, Batch 177 Loss:0.009135644882917404\n",
      "Epoch 69, Batch 178 Loss:0.014475606381893158\n",
      "Epoch 69, Batch 179 Loss:0.01991811767220497\n",
      "Epoch 69, Batch 180 Loss:0.06609918922185898\n",
      "Epoch 69, Batch 181 Loss:0.021412745118141174\n",
      "Epoch 69, Batch 182 Loss:0.039722658693790436\n",
      "Epoch 69, Batch 183 Loss:0.038087911903858185\n",
      "Epoch 69, Batch 184 Loss:0.03678954020142555\n",
      "Epoch 69, Batch 185 Loss:0.04097232595086098\n",
      "Epoch 69, Batch 186 Loss:0.0466727614402771\n",
      "Epoch 69, Batch 187 Loss:0.04357057437300682\n",
      "Epoch 69, Batch 188 Loss:0.04723334312438965\n",
      "Epoch 69, Batch 189 Loss:0.05988985300064087\n",
      "Epoch 69, Batch 190 Loss:0.029327351599931717\n",
      "Epoch 69, Batch 191 Loss:0.07866575568914413\n",
      "Epoch 69, Batch 192 Loss:0.024806031957268715\n",
      "Epoch 69, Batch 193 Loss:0.05800372362136841\n",
      "Epoch 69, Batch 194 Loss:0.03514363616704941\n",
      "Epoch 69, Batch 195 Loss:0.04842495545744896\n",
      "Epoch 69, Batch 196 Loss:0.03821183741092682\n",
      "Epoch 69, Batch 197 Loss:0.04102594405412674\n",
      "Epoch 69, Batch 198 Loss:0.0640619695186615\n",
      "Epoch 69, Batch 199 Loss:0.015662681311368942\n",
      "Epoch 69, Batch 200 Loss:0.03896266594529152\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69, Batch 201 Loss:0.03459098935127258\n",
      "Epoch 69, Batch 202 Loss:0.04673830047249794\n",
      "Epoch 69, Batch 203 Loss:0.031982868909835815\n",
      "Epoch 69, Batch 204 Loss:0.01696794480085373\n",
      "Epoch 69, Batch 205 Loss:0.019053850322961807\n",
      "Epoch 69, Batch 206 Loss:0.014544768258929253\n",
      "Epoch 69, Batch 207 Loss:0.01072937436401844\n",
      "Epoch 69, Batch 208 Loss:0.02303115651011467\n",
      "Epoch 69, Batch 209 Loss:0.0204632468521595\n",
      "Epoch 69, Batch 210 Loss:0.007039662450551987\n",
      "Epoch 69, Batch 211 Loss:0.05012882128357887\n",
      "Epoch 69, Batch 212 Loss:0.015368549153208733\n",
      "Epoch 69, Batch 213 Loss:0.0183375496417284\n",
      "Epoch 69, Batch 214 Loss:0.05856987461447716\n",
      "Epoch 69, Batch 215 Loss:0.03366699814796448\n",
      "Epoch 69, Batch 216 Loss:0.029195472598075867\n",
      "Epoch 69, Batch 217 Loss:0.02044021338224411\n",
      "Epoch 69, Batch 218 Loss:0.06127624213695526\n",
      "Epoch 69, Batch 219 Loss:0.035417910665273666\n",
      "Epoch 69, Batch 220 Loss:0.02033422514796257\n",
      "Epoch 69, Batch 221 Loss:0.02903999760746956\n",
      "Epoch 69, Batch 222 Loss:0.03223003074526787\n",
      "Epoch 69, Batch 223 Loss:0.01486416719853878\n",
      "Epoch 69, Batch 224 Loss:0.05230119824409485\n",
      "Epoch 69, Batch 225 Loss:0.028947927057743073\n",
      "Epoch 69, Batch 226 Loss:0.015087992884218693\n",
      "Epoch 69, Batch 227 Loss:0.019849367439746857\n",
      "Epoch 69, Batch 228 Loss:0.022434473037719727\n",
      "Epoch 69, Batch 229 Loss:0.03834269195795059\n",
      "Epoch 69, Batch 230 Loss:0.09459114074707031\n",
      "Epoch 69, Batch 231 Loss:0.07009207457304001\n",
      "Epoch 69, Batch 232 Loss:0.020402174443006516\n",
      "Epoch 69, Batch 233 Loss:0.013522826135158539\n",
      "Loss in this Epoch is: 1.35228261352 %\n",
      "Accuracy in this Epoch is: 88.4100019932 %\n",
      "Epoch 70, Batch 0 Loss:0.03017813339829445\n",
      "Epoch 70, Batch 1 Loss:0.03437861427664757\n",
      "Epoch 70, Batch 2 Loss:0.015047164633870125\n",
      "Epoch 70, Batch 3 Loss:0.01793387532234192\n",
      "Epoch 70, Batch 4 Loss:0.01010026317089796\n",
      "Epoch 70, Batch 5 Loss:0.03864481672644615\n",
      "Epoch 70, Batch 6 Loss:0.009319432079792023\n",
      "Epoch 70, Batch 7 Loss:0.019650869071483612\n",
      "Epoch 70, Batch 8 Loss:0.021924231201410294\n",
      "Epoch 70, Batch 9 Loss:0.008455339819192886\n",
      "Epoch 70, Batch 10 Loss:0.012312859296798706\n",
      "Epoch 70, Batch 11 Loss:0.021523743867874146\n",
      "Epoch 70, Batch 12 Loss:0.034901831299066544\n",
      "Epoch 70, Batch 13 Loss:0.01869536004960537\n",
      "Epoch 70, Batch 14 Loss:0.021375538781285286\n",
      "Epoch 70, Batch 15 Loss:0.027604201808571815\n",
      "Epoch 70, Batch 16 Loss:0.017474494874477386\n",
      "Epoch 70, Batch 17 Loss:0.008016451261937618\n",
      "Epoch 70, Batch 18 Loss:0.00833752378821373\n",
      "Epoch 70, Batch 19 Loss:0.010689640417695045\n",
      "Epoch 70, Batch 20 Loss:0.020467152819037437\n",
      "Epoch 70, Batch 21 Loss:0.014392402954399586\n",
      "Epoch 70, Batch 22 Loss:0.013080844655632973\n",
      "Epoch 70, Batch 23 Loss:0.043708231300115585\n",
      "Epoch 70, Batch 24 Loss:0.060199517756700516\n",
      "Epoch 70, Batch 25 Loss:0.03143315017223358\n",
      "Epoch 70, Batch 26 Loss:0.02214759588241577\n",
      "Epoch 70, Batch 27 Loss:0.01142469048500061\n",
      "Epoch 70, Batch 28 Loss:0.02929784171283245\n",
      "Epoch 70, Batch 29 Loss:0.020877540111541748\n",
      "Epoch 70, Batch 30 Loss:0.021793747320771217\n",
      "Epoch 70, Batch 31 Loss:0.00573387835174799\n",
      "Epoch 70, Batch 32 Loss:0.004633802454918623\n",
      "Epoch 70, Batch 33 Loss:0.01597519777715206\n",
      "Epoch 70, Batch 34 Loss:0.005639370530843735\n",
      "Epoch 70, Batch 35 Loss:0.0050913491286337376\n",
      "Epoch 70, Batch 36 Loss:0.007512205746024847\n",
      "Epoch 70, Batch 37 Loss:0.012551327235996723\n",
      "Epoch 70, Batch 38 Loss:0.006488882936537266\n",
      "Epoch 70, Batch 39 Loss:0.008478245697915554\n",
      "Epoch 70, Batch 40 Loss:0.0028311614878475666\n",
      "Epoch 70, Batch 41 Loss:0.0097675621509552\n",
      "Epoch 70, Batch 42 Loss:0.03283611685037613\n",
      "Epoch 70, Batch 43 Loss:0.005506719928234816\n",
      "Epoch 70, Batch 44 Loss:0.010644109919667244\n",
      "Epoch 70, Batch 45 Loss:0.007017352618277073\n",
      "Epoch 70, Batch 46 Loss:0.007836388424038887\n",
      "Epoch 70, Batch 47 Loss:0.009510966017842293\n",
      "Epoch 70, Batch 48 Loss:0.01910870335996151\n",
      "Epoch 70, Batch 49 Loss:0.011506532318890095\n",
      "Epoch 70, Batch 50 Loss:0.020789504051208496\n",
      "Epoch 70, Batch 51 Loss:0.0501326322555542\n",
      "Epoch 70, Batch 52 Loss:0.009967652149498463\n",
      "Epoch 70, Batch 53 Loss:0.06368514150381088\n",
      "Epoch 70, Batch 54 Loss:0.03465026617050171\n",
      "Epoch 70, Batch 55 Loss:0.008458231575787067\n",
      "Epoch 70, Batch 56 Loss:0.03964146599173546\n",
      "Epoch 70, Batch 57 Loss:0.015346321277320385\n",
      "Epoch 70, Batch 58 Loss:0.011674565263092518\n",
      "Epoch 70, Batch 59 Loss:0.03880677744746208\n",
      "Epoch 70, Batch 60 Loss:0.03903292864561081\n",
      "Epoch 70, Batch 61 Loss:0.008337191306054592\n",
      "Epoch 70, Batch 62 Loss:0.03849956393241882\n",
      "Epoch 70, Batch 63 Loss:0.03896048292517662\n",
      "Epoch 70, Batch 64 Loss:0.03896462917327881\n",
      "Epoch 70, Batch 65 Loss:0.02948646992444992\n",
      "Epoch 70, Batch 66 Loss:0.03362276405096054\n",
      "Epoch 70, Batch 67 Loss:0.05855057016015053\n",
      "Epoch 70, Batch 68 Loss:0.03794311732053757\n",
      "Epoch 70, Batch 69 Loss:0.009402315132319927\n",
      "Epoch 70, Batch 70 Loss:0.005061481613665819\n",
      "Epoch 70, Batch 71 Loss:0.009997035376727581\n",
      "Epoch 70, Batch 72 Loss:0.01851019822061062\n",
      "Epoch 70, Batch 73 Loss:0.05056135356426239\n",
      "Epoch 70, Batch 74 Loss:0.06146008148789406\n",
      "Epoch 70, Batch 75 Loss:0.03824208676815033\n",
      "Epoch 70, Batch 76 Loss:0.017442286014556885\n",
      "Epoch 70, Batch 77 Loss:0.02125977724790573\n",
      "Epoch 70, Batch 78 Loss:0.07560864835977554\n",
      "Epoch 70, Batch 79 Loss:0.0156619343906641\n",
      "Epoch 70, Batch 80 Loss:0.05716810002923012\n",
      "Epoch 70, Batch 81 Loss:0.09322237223386765\n",
      "Epoch 70, Batch 82 Loss:0.03651188686490059\n",
      "Epoch 70, Batch 83 Loss:0.018830280750989914\n",
      "Epoch 70, Batch 84 Loss:0.023833373561501503\n",
      "Epoch 70, Batch 85 Loss:0.04614017531275749\n",
      "Epoch 70, Batch 86 Loss:0.06700416654348373\n",
      "Epoch 70, Batch 87 Loss:0.06681109964847565\n",
      "Epoch 70, Batch 88 Loss:0.04983094334602356\n",
      "Epoch 70, Batch 89 Loss:0.05095871165394783\n",
      "Epoch 70, Batch 90 Loss:0.024085626006126404\n",
      "Epoch 70, Batch 91 Loss:0.027506094425916672\n",
      "Epoch 70, Batch 92 Loss:0.028154972940683365\n",
      "Epoch 70, Batch 93 Loss:0.0436520017683506\n",
      "Epoch 70, Batch 94 Loss:0.03948776423931122\n",
      "Epoch 70, Batch 95 Loss:0.015590420924127102\n",
      "Epoch 70, Batch 96 Loss:0.02840631827712059\n",
      "Epoch 70, Batch 97 Loss:0.041288748383522034\n",
      "Epoch 70, Batch 98 Loss:0.01942216046154499\n",
      "Epoch 70, Batch 99 Loss:0.025246210396289825\n",
      "Epoch 70, Batch 100 Loss:0.007320687174797058\n",
      "Epoch 70, Batch 101 Loss:0.014404618181288242\n",
      "Epoch 70, Batch 102 Loss:0.017570802941918373\n",
      "Epoch 70, Batch 103 Loss:0.05180700868368149\n",
      "Epoch 70, Batch 104 Loss:0.05432005226612091\n",
      "Epoch 70, Batch 105 Loss:0.009690842591226101\n",
      "Epoch 70, Batch 106 Loss:0.022507844492793083\n",
      "Epoch 70, Batch 107 Loss:0.01404106430709362\n",
      "Epoch 70, Batch 108 Loss:0.02022497169673443\n",
      "Epoch 70, Batch 109 Loss:0.006587175652384758\n",
      "Epoch 70, Batch 110 Loss:0.04025993496179581\n",
      "Epoch 70, Batch 111 Loss:0.013285921886563301\n",
      "Epoch 70, Batch 112 Loss:0.04649785906076431\n",
      "Epoch 70, Batch 113 Loss:0.0258930753916502\n",
      "Epoch 70, Batch 114 Loss:0.026722393929958344\n",
      "Epoch 70, Batch 115 Loss:0.02606379985809326\n",
      "Epoch 70, Batch 116 Loss:0.027632545679807663\n",
      "Epoch 70, Batch 117 Loss:0.030143050476908684\n",
      "Epoch 70, Batch 118 Loss:0.052499376237392426\n",
      "Epoch 70, Batch 119 Loss:0.03710511699318886\n",
      "Epoch 70, Batch 120 Loss:0.01466994360089302\n",
      "Epoch 70, Batch 121 Loss:0.06930705159902573\n",
      "Epoch 70, Batch 122 Loss:0.03520941361784935\n",
      "Epoch 70, Batch 123 Loss:0.012382463552057743\n",
      "Epoch 70, Batch 124 Loss:0.024462871253490448\n",
      "Epoch 70, Batch 125 Loss:0.04760179668664932\n",
      "Epoch 70, Batch 126 Loss:0.018981005996465683\n",
      "Epoch 70, Batch 127 Loss:0.025096716359257698\n",
      "Epoch 70, Batch 128 Loss:0.04891952499747276\n",
      "Epoch 70, Batch 129 Loss:0.013184506446123123\n",
      "Epoch 70, Batch 130 Loss:0.0388544499874115\n",
      "Epoch 70, Batch 131 Loss:0.007374204229563475\n",
      "Epoch 70, Batch 132 Loss:0.05547935143113136\n",
      "Epoch 70, Batch 133 Loss:0.05039060488343239\n",
      "Epoch 70, Batch 134 Loss:0.04685362055897713\n",
      "Epoch 70, Batch 135 Loss:0.007567624561488628\n",
      "Epoch 70, Batch 136 Loss:0.012321481481194496\n",
      "Epoch 70, Batch 137 Loss:0.0326698012650013\n",
      "Epoch 70, Batch 138 Loss:0.01191739458590746\n",
      "Epoch 70, Batch 139 Loss:0.024446897208690643\n",
      "Epoch 70, Batch 140 Loss:0.018140509724617004\n",
      "Epoch 70, Batch 141 Loss:0.02456759288907051\n",
      "Epoch 70, Batch 142 Loss:0.05190206319093704\n",
      "Epoch 70, Batch 143 Loss:0.04784863442182541\n",
      "Epoch 70, Batch 144 Loss:0.04378773272037506\n",
      "Epoch 70, Batch 145 Loss:0.008752193301916122\n",
      "Epoch 70, Batch 146 Loss:0.017137646675109863\n",
      "Epoch 70, Batch 147 Loss:0.009066753089427948\n",
      "Epoch 70, Batch 148 Loss:0.05254722759127617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, Batch 149 Loss:0.05214282497763634\n",
      "Epoch 70, Batch 150 Loss:0.10081839561462402\n",
      "Epoch 70, Batch 151 Loss:0.012676021084189415\n",
      "Epoch 70, Batch 152 Loss:0.03834918886423111\n",
      "Epoch 70, Batch 153 Loss:0.022251732647418976\n",
      "Epoch 70, Batch 154 Loss:0.02015816792845726\n",
      "Epoch 70, Batch 155 Loss:0.00955837219953537\n",
      "Epoch 70, Batch 156 Loss:0.010571606457233429\n",
      "Epoch 70, Batch 157 Loss:0.01307189092040062\n",
      "Epoch 70, Batch 158 Loss:0.016110211610794067\n",
      "Epoch 70, Batch 159 Loss:0.014169951900839806\n",
      "Epoch 70, Batch 160 Loss:0.02235414832830429\n",
      "Epoch 70, Batch 161 Loss:0.026205262169241905\n",
      "Epoch 70, Batch 162 Loss:0.04432308301329613\n",
      "Epoch 70, Batch 163 Loss:0.028365954756736755\n",
      "Epoch 70, Batch 164 Loss:0.0872141420841217\n",
      "Epoch 70, Batch 165 Loss:0.05038739740848541\n",
      "Epoch 70, Batch 166 Loss:0.06019896641373634\n",
      "Epoch 70, Batch 167 Loss:0.01645534485578537\n",
      "Epoch 70, Batch 168 Loss:0.01351085677742958\n",
      "Epoch 70, Batch 169 Loss:0.0472521036863327\n",
      "Epoch 70, Batch 170 Loss:0.02253127470612526\n",
      "Epoch 70, Batch 171 Loss:0.010642086155712605\n",
      "Epoch 70, Batch 172 Loss:0.02045471780002117\n",
      "Epoch 70, Batch 173 Loss:0.037424203008413315\n",
      "Epoch 70, Batch 174 Loss:0.031102865934371948\n",
      "Epoch 70, Batch 175 Loss:0.03713173046708107\n",
      "Epoch 70, Batch 176 Loss:0.017748378217220306\n",
      "Epoch 70, Batch 177 Loss:0.01676909253001213\n",
      "Epoch 70, Batch 178 Loss:0.01402425765991211\n",
      "Epoch 70, Batch 179 Loss:0.011958269402384758\n",
      "Epoch 70, Batch 180 Loss:0.00696195662021637\n",
      "Epoch 70, Batch 181 Loss:0.021336112171411514\n",
      "Epoch 70, Batch 182 Loss:0.013932028785347939\n",
      "Epoch 70, Batch 183 Loss:0.02100256457924843\n",
      "Epoch 70, Batch 184 Loss:0.018347494304180145\n",
      "Epoch 70, Batch 185 Loss:0.016139674931764603\n",
      "Epoch 70, Batch 186 Loss:0.012163117527961731\n",
      "Epoch 70, Batch 187 Loss:0.013189943507313728\n",
      "Epoch 70, Batch 188 Loss:0.018983500078320503\n",
      "Epoch 70, Batch 189 Loss:0.013481796719133854\n",
      "Epoch 70, Batch 190 Loss:0.010302452370524406\n",
      "Epoch 70, Batch 191 Loss:0.013148638419806957\n",
      "Epoch 70, Batch 192 Loss:0.03595106303691864\n",
      "Epoch 70, Batch 193 Loss:0.021766437217593193\n",
      "Epoch 70, Batch 194 Loss:0.020616278052330017\n",
      "Epoch 70, Batch 195 Loss:0.035587769001722336\n",
      "Epoch 70, Batch 196 Loss:0.013245798647403717\n",
      "Epoch 70, Batch 197 Loss:0.004837268032133579\n",
      "Epoch 70, Batch 198 Loss:0.0062478650361299515\n",
      "Epoch 70, Batch 199 Loss:0.025590894743800163\n",
      "Epoch 70, Batch 200 Loss:0.0369449108839035\n",
      "Epoch 70, Batch 201 Loss:0.02777511067688465\n",
      "Epoch 70, Batch 202 Loss:0.04426358640193939\n",
      "Epoch 70, Batch 203 Loss:0.026074450463056564\n",
      "Epoch 70, Batch 204 Loss:0.00873345322906971\n",
      "Epoch 70, Batch 205 Loss:0.013469116762280464\n",
      "Epoch 70, Batch 206 Loss:0.008119582198560238\n",
      "Epoch 70, Batch 207 Loss:0.019287211820483208\n",
      "Epoch 70, Batch 208 Loss:0.008544376119971275\n",
      "Epoch 70, Batch 209 Loss:0.011091494001448154\n",
      "Epoch 70, Batch 210 Loss:0.010334616526961327\n",
      "Epoch 70, Batch 211 Loss:0.03314321115612984\n",
      "Epoch 70, Batch 212 Loss:0.02257489040493965\n",
      "Epoch 70, Batch 213 Loss:0.042507246136665344\n",
      "Epoch 70, Batch 214 Loss:0.03806249797344208\n",
      "Epoch 70, Batch 215 Loss:0.010072624310851097\n",
      "Epoch 70, Batch 216 Loss:0.021388329565525055\n",
      "Epoch 70, Batch 217 Loss:0.016678983345627785\n",
      "Epoch 70, Batch 218 Loss:0.03265043720602989\n",
      "Epoch 70, Batch 219 Loss:0.019755668938159943\n",
      "Epoch 70, Batch 220 Loss:0.002978913951665163\n",
      "Epoch 70, Batch 221 Loss:0.012371236458420753\n",
      "Epoch 70, Batch 222 Loss:0.009301668033003807\n",
      "Epoch 70, Batch 223 Loss:0.023667968809604645\n",
      "Epoch 70, Batch 224 Loss:0.0062016816809773445\n",
      "Epoch 70, Batch 225 Loss:0.007313636597245932\n",
      "Epoch 70, Batch 226 Loss:0.006301797926425934\n",
      "Epoch 70, Batch 227 Loss:0.039384033530950546\n",
      "Epoch 70, Batch 228 Loss:0.012659888714551926\n",
      "Epoch 70, Batch 229 Loss:0.004135114140808582\n",
      "Epoch 70, Batch 230 Loss:0.016083654016256332\n",
      "Epoch 70, Batch 231 Loss:0.023909853771328926\n",
      "Epoch 70, Batch 232 Loss:0.00931159034371376\n",
      "Epoch 70, Batch 233 Loss:0.008464807644486427\n",
      "Loss in this Epoch is: 0.846480764449 %\n",
      "Accuracy in this Epoch is: 88.2600009441 %\n",
      "Epoch 71, Batch 0 Loss:0.03671443834900856\n",
      "Epoch 71, Batch 1 Loss:0.004995780065655708\n",
      "Epoch 71, Batch 2 Loss:0.005817827768623829\n",
      "Epoch 71, Batch 3 Loss:0.019602343440055847\n",
      "Epoch 71, Batch 4 Loss:0.005118147470057011\n",
      "Epoch 71, Batch 5 Loss:0.008172902278602123\n",
      "Epoch 71, Batch 6 Loss:0.04566989094018936\n",
      "Epoch 71, Batch 7 Loss:0.01635458692908287\n",
      "Epoch 71, Batch 8 Loss:0.037268076092004776\n",
      "Epoch 71, Batch 9 Loss:0.006702949292957783\n",
      "Epoch 71, Batch 10 Loss:0.0032154247164726257\n",
      "Epoch 71, Batch 11 Loss:0.012468887493014336\n",
      "Epoch 71, Batch 12 Loss:0.02613172121345997\n",
      "Epoch 71, Batch 13 Loss:0.004320456180721521\n",
      "Epoch 71, Batch 14 Loss:0.009092357009649277\n",
      "Epoch 71, Batch 15 Loss:0.010646937415003777\n",
      "Epoch 71, Batch 16 Loss:0.009507833980023861\n",
      "Epoch 71, Batch 17 Loss:0.012629062868654728\n",
      "Epoch 71, Batch 18 Loss:0.026780830696225166\n",
      "Epoch 71, Batch 19 Loss:0.03386494144797325\n",
      "Epoch 71, Batch 20 Loss:0.009567702189087868\n",
      "Epoch 71, Batch 21 Loss:0.015995869413018227\n",
      "Epoch 71, Batch 22 Loss:0.0230072233825922\n",
      "Epoch 71, Batch 23 Loss:0.020417602732777596\n",
      "Epoch 71, Batch 24 Loss:0.022185523062944412\n",
      "Epoch 71, Batch 25 Loss:0.021998902782797813\n",
      "Epoch 71, Batch 26 Loss:0.015571175143122673\n",
      "Epoch 71, Batch 27 Loss:0.00888676568865776\n",
      "Epoch 71, Batch 28 Loss:0.011335620656609535\n",
      "Epoch 71, Batch 29 Loss:0.04187397286295891\n",
      "Epoch 71, Batch 30 Loss:0.012844661250710487\n",
      "Epoch 71, Batch 31 Loss:0.004815993364900351\n",
      "Epoch 71, Batch 32 Loss:0.010643571615219116\n",
      "Epoch 71, Batch 33 Loss:0.025897666811943054\n",
      "Epoch 71, Batch 34 Loss:0.02149341255426407\n",
      "Epoch 71, Batch 35 Loss:0.007091109175235033\n",
      "Epoch 71, Batch 36 Loss:0.031426802277565\n",
      "Epoch 71, Batch 37 Loss:0.03052656352519989\n",
      "Epoch 71, Batch 38 Loss:0.035710085183382034\n",
      "Epoch 71, Batch 39 Loss:0.021455468609929085\n",
      "Epoch 71, Batch 40 Loss:0.04545094445347786\n",
      "Epoch 71, Batch 41 Loss:0.013827566988766193\n",
      "Epoch 71, Batch 42 Loss:0.014422695152461529\n",
      "Epoch 71, Batch 43 Loss:0.008213194087147713\n",
      "Epoch 71, Batch 44 Loss:0.01013140007853508\n",
      "Epoch 71, Batch 45 Loss:0.013338344171643257\n",
      "Epoch 71, Batch 46 Loss:0.008093256503343582\n",
      "Epoch 71, Batch 47 Loss:0.0057618385180830956\n",
      "Epoch 71, Batch 48 Loss:0.013595697470009327\n",
      "Epoch 71, Batch 49 Loss:0.004712401423603296\n",
      "Epoch 71, Batch 50 Loss:0.0441209115087986\n",
      "Epoch 71, Batch 51 Loss:0.04121465981006622\n",
      "Epoch 71, Batch 52 Loss:0.009966391138732433\n",
      "Epoch 71, Batch 53 Loss:0.0038651989307254553\n",
      "Epoch 71, Batch 54 Loss:0.01687333919107914\n",
      "Epoch 71, Batch 55 Loss:0.032097235321998596\n",
      "Epoch 71, Batch 56 Loss:0.004248418379575014\n",
      "Epoch 71, Batch 57 Loss:0.047325022518634796\n",
      "Epoch 71, Batch 58 Loss:0.010036814026534557\n",
      "Epoch 71, Batch 59 Loss:0.015616283752024174\n",
      "Epoch 71, Batch 60 Loss:0.003341040341183543\n",
      "Epoch 71, Batch 61 Loss:0.009736405685544014\n",
      "Epoch 71, Batch 62 Loss:0.0176609568297863\n",
      "Epoch 71, Batch 63 Loss:0.004850107245147228\n",
      "Epoch 71, Batch 64 Loss:0.025961564853787422\n",
      "Epoch 71, Batch 65 Loss:0.006518248468637466\n",
      "Epoch 71, Batch 66 Loss:0.006464894395321608\n",
      "Epoch 71, Batch 67 Loss:0.009467088617384434\n",
      "Epoch 71, Batch 68 Loss:0.009611072018742561\n",
      "Epoch 71, Batch 69 Loss:0.010132432915270329\n",
      "Epoch 71, Batch 70 Loss:0.016316106542944908\n",
      "Epoch 71, Batch 71 Loss:0.004898985382169485\n",
      "Epoch 71, Batch 72 Loss:0.01787439174950123\n",
      "Epoch 71, Batch 73 Loss:0.007159994449466467\n",
      "Epoch 71, Batch 74 Loss:0.026159066706895828\n",
      "Epoch 71, Batch 75 Loss:0.023418862372636795\n",
      "Epoch 71, Batch 76 Loss:0.005016556475311518\n",
      "Epoch 71, Batch 77 Loss:0.003052392043173313\n",
      "Epoch 71, Batch 78 Loss:0.0064143771305680275\n",
      "Epoch 71, Batch 79 Loss:0.009911024942994118\n",
      "Epoch 71, Batch 80 Loss:0.012101689353585243\n",
      "Epoch 71, Batch 81 Loss:0.002584965666756034\n",
      "Epoch 71, Batch 82 Loss:0.003791387192904949\n",
      "Epoch 71, Batch 83 Loss:0.005029995925724506\n",
      "Epoch 71, Batch 84 Loss:0.014836103655397892\n",
      "Epoch 71, Batch 85 Loss:0.012231067754328251\n",
      "Epoch 71, Batch 86 Loss:0.011509427800774574\n",
      "Epoch 71, Batch 87 Loss:0.010585502721369267\n",
      "Epoch 71, Batch 88 Loss:0.007943504489958286\n",
      "Epoch 71, Batch 89 Loss:0.002195866545662284\n",
      "Epoch 71, Batch 90 Loss:0.03098779357969761\n",
      "Epoch 71, Batch 91 Loss:0.014604950323700905\n",
      "Epoch 71, Batch 92 Loss:0.011823061853647232\n",
      "Epoch 71, Batch 93 Loss:0.005763838533312082\n",
      "Epoch 71, Batch 94 Loss:0.004638269543647766\n",
      "Epoch 71, Batch 95 Loss:0.024270832538604736\n",
      "Epoch 71, Batch 96 Loss:0.03260623291134834\n",
      "Epoch 71, Batch 97 Loss:0.021656649187207222\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71, Batch 98 Loss:0.016180872917175293\n",
      "Epoch 71, Batch 99 Loss:0.022948168218135834\n",
      "Epoch 71, Batch 100 Loss:0.03287043049931526\n",
      "Epoch 71, Batch 101 Loss:0.00352301262319088\n",
      "Epoch 71, Batch 102 Loss:0.022532787173986435\n",
      "Epoch 71, Batch 103 Loss:0.014195776544511318\n",
      "Epoch 71, Batch 104 Loss:0.0180827546864748\n",
      "Epoch 71, Batch 105 Loss:0.0062263342551887035\n",
      "Epoch 71, Batch 106 Loss:0.02267131581902504\n",
      "Epoch 71, Batch 107 Loss:0.012667994946241379\n",
      "Epoch 71, Batch 108 Loss:0.0261659175157547\n",
      "Epoch 71, Batch 109 Loss:0.013277837075293064\n",
      "Epoch 71, Batch 110 Loss:0.02307397499680519\n",
      "Epoch 71, Batch 111 Loss:0.004451942630112171\n",
      "Epoch 71, Batch 112 Loss:0.009567649103701115\n",
      "Epoch 71, Batch 113 Loss:0.04818406701087952\n",
      "Epoch 71, Batch 114 Loss:0.008763106539845467\n",
      "Epoch 71, Batch 115 Loss:0.0499001182615757\n",
      "Epoch 71, Batch 116 Loss:0.0288730226457119\n",
      "Epoch 71, Batch 117 Loss:0.009759318083524704\n",
      "Epoch 71, Batch 118 Loss:0.04306943714618683\n",
      "Epoch 71, Batch 119 Loss:0.02139478549361229\n",
      "Epoch 71, Batch 120 Loss:0.015858130529522896\n",
      "Epoch 71, Batch 121 Loss:0.024558983743190765\n",
      "Epoch 71, Batch 122 Loss:0.009394112974405289\n",
      "Epoch 71, Batch 123 Loss:0.007273108698427677\n",
      "Epoch 71, Batch 124 Loss:0.0044450219720602036\n",
      "Epoch 71, Batch 125 Loss:0.014573893509805202\n",
      "Epoch 71, Batch 126 Loss:0.015160232782363892\n",
      "Epoch 71, Batch 127 Loss:0.026785846799612045\n",
      "Epoch 71, Batch 128 Loss:0.03818567842245102\n",
      "Epoch 71, Batch 129 Loss:0.03459663689136505\n",
      "Epoch 71, Batch 130 Loss:0.006148045416921377\n",
      "Epoch 71, Batch 131 Loss:0.030815577134490013\n",
      "Epoch 71, Batch 132 Loss:0.01893710531294346\n",
      "Epoch 71, Batch 133 Loss:0.018866173923015594\n",
      "Epoch 71, Batch 134 Loss:0.013745005242526531\n",
      "Epoch 71, Batch 135 Loss:0.02806386537849903\n",
      "Epoch 71, Batch 136 Loss:0.025218568742275238\n",
      "Epoch 71, Batch 137 Loss:0.03855105862021446\n",
      "Epoch 71, Batch 138 Loss:0.019351322203874588\n",
      "Epoch 71, Batch 139 Loss:0.016580436378717422\n",
      "Epoch 71, Batch 140 Loss:0.03829537704586983\n",
      "Epoch 71, Batch 141 Loss:0.036015190184116364\n",
      "Epoch 71, Batch 142 Loss:0.04954636096954346\n",
      "Epoch 71, Batch 143 Loss:0.011812175624072552\n",
      "Epoch 71, Batch 144 Loss:0.07504867017269135\n",
      "Epoch 71, Batch 145 Loss:0.0215131975710392\n",
      "Epoch 71, Batch 146 Loss:0.07168947905302048\n",
      "Epoch 71, Batch 147 Loss:0.038403935730457306\n",
      "Epoch 71, Batch 148 Loss:0.016622574999928474\n",
      "Epoch 71, Batch 149 Loss:0.022284064441919327\n",
      "Epoch 71, Batch 150 Loss:0.017740583047270775\n",
      "Epoch 71, Batch 151 Loss:0.02712499350309372\n",
      "Epoch 71, Batch 152 Loss:0.015797462314367294\n",
      "Epoch 71, Batch 153 Loss:0.02695424109697342\n",
      "Epoch 71, Batch 154 Loss:0.016419576480984688\n",
      "Epoch 71, Batch 155 Loss:0.029187943786382675\n",
      "Epoch 71, Batch 156 Loss:0.029349638149142265\n",
      "Epoch 71, Batch 157 Loss:0.01878621056675911\n",
      "Epoch 71, Batch 158 Loss:0.038754381239414215\n",
      "Epoch 71, Batch 159 Loss:0.0193667933344841\n",
      "Epoch 71, Batch 160 Loss:0.03109639137983322\n",
      "Epoch 71, Batch 161 Loss:0.008492609485983849\n",
      "Epoch 71, Batch 162 Loss:0.02310410514473915\n",
      "Epoch 71, Batch 163 Loss:0.03975857049226761\n",
      "Epoch 71, Batch 164 Loss:0.02126670628786087\n",
      "Epoch 71, Batch 165 Loss:0.011891944333910942\n",
      "Epoch 71, Batch 166 Loss:0.024345602840185165\n",
      "Epoch 71, Batch 167 Loss:0.04154640808701515\n",
      "Epoch 71, Batch 168 Loss:0.01947750337421894\n",
      "Epoch 71, Batch 169 Loss:0.04010855406522751\n",
      "Epoch 71, Batch 170 Loss:0.03285211697220802\n",
      "Epoch 71, Batch 171 Loss:0.0186742153018713\n",
      "Epoch 71, Batch 172 Loss:0.01865215413272381\n",
      "Epoch 71, Batch 173 Loss:0.015707872807979584\n",
      "Epoch 71, Batch 174 Loss:0.015926647931337357\n",
      "Epoch 71, Batch 175 Loss:0.010275089181959629\n",
      "Epoch 71, Batch 176 Loss:0.0250408798456192\n",
      "Epoch 71, Batch 177 Loss:0.04793941229581833\n",
      "Epoch 71, Batch 178 Loss:0.052724339067935944\n",
      "Epoch 71, Batch 179 Loss:0.007281862664967775\n",
      "Epoch 71, Batch 180 Loss:0.01598593220114708\n",
      "Epoch 71, Batch 181 Loss:0.00964635144919157\n",
      "Epoch 71, Batch 182 Loss:0.02818823978304863\n",
      "Epoch 71, Batch 183 Loss:0.03013620153069496\n",
      "Epoch 71, Batch 184 Loss:0.026077790185809135\n",
      "Epoch 71, Batch 185 Loss:0.02165982313454151\n",
      "Epoch 71, Batch 186 Loss:0.009356889873743057\n",
      "Epoch 71, Batch 187 Loss:0.027502533048391342\n",
      "Epoch 71, Batch 188 Loss:0.08816538006067276\n",
      "Epoch 71, Batch 189 Loss:0.08055415004491806\n",
      "Epoch 71, Batch 190 Loss:0.008106301538646221\n",
      "Epoch 71, Batch 191 Loss:0.02718072384595871\n",
      "Epoch 71, Batch 192 Loss:0.029810093343257904\n",
      "Epoch 71, Batch 193 Loss:0.027651721611618996\n",
      "Epoch 71, Batch 194 Loss:0.03121386095881462\n",
      "Epoch 71, Batch 195 Loss:0.03268003091216087\n",
      "Epoch 71, Batch 196 Loss:0.026625871658325195\n",
      "Epoch 71, Batch 197 Loss:0.015996970236301422\n",
      "Epoch 71, Batch 198 Loss:0.010992972180247307\n",
      "Epoch 71, Batch 199 Loss:0.019061746075749397\n",
      "Epoch 71, Batch 200 Loss:0.012764351442456245\n",
      "Epoch 71, Batch 201 Loss:0.007145965471863747\n",
      "Epoch 71, Batch 202 Loss:0.028423305600881577\n",
      "Epoch 71, Batch 203 Loss:0.03260539099574089\n",
      "Epoch 71, Batch 204 Loss:0.025259315967559814\n",
      "Epoch 71, Batch 205 Loss:0.030734507367014885\n",
      "Epoch 71, Batch 206 Loss:0.03028297796845436\n",
      "Epoch 71, Batch 207 Loss:0.02021358720958233\n",
      "Epoch 71, Batch 208 Loss:0.0382254496216774\n",
      "Epoch 71, Batch 209 Loss:0.005354555789381266\n",
      "Epoch 71, Batch 210 Loss:0.016227083280682564\n",
      "Epoch 71, Batch 211 Loss:0.021642105653882027\n",
      "Epoch 71, Batch 212 Loss:0.010598380118608475\n",
      "Epoch 71, Batch 213 Loss:0.01050590444356203\n",
      "Epoch 71, Batch 214 Loss:0.03530857712030411\n",
      "Epoch 71, Batch 215 Loss:0.022033708170056343\n",
      "Epoch 71, Batch 216 Loss:0.010207723826169968\n",
      "Epoch 71, Batch 217 Loss:0.016994617879390717\n",
      "Epoch 71, Batch 218 Loss:0.02487136609852314\n",
      "Epoch 71, Batch 219 Loss:0.04616985470056534\n",
      "Epoch 71, Batch 220 Loss:0.01598101109266281\n",
      "Epoch 71, Batch 221 Loss:0.009221084415912628\n",
      "Epoch 71, Batch 222 Loss:0.03890444338321686\n",
      "Epoch 71, Batch 223 Loss:0.011816749349236488\n",
      "Epoch 71, Batch 224 Loss:0.022324396297335625\n",
      "Epoch 71, Batch 225 Loss:0.024432513862848282\n",
      "Epoch 71, Batch 226 Loss:0.018403153866529465\n",
      "Epoch 71, Batch 227 Loss:0.0313345268368721\n",
      "Epoch 71, Batch 228 Loss:0.02785194292664528\n",
      "Epoch 71, Batch 229 Loss:0.01077937800437212\n",
      "Epoch 71, Batch 230 Loss:0.05385410040616989\n",
      "Epoch 71, Batch 231 Loss:0.014432834461331367\n",
      "Epoch 71, Batch 232 Loss:0.04155493155121803\n",
      "Epoch 71, Batch 233 Loss:0.028592705726623535\n",
      "Loss in this Epoch is: 2.85927057266 %\n",
      "Accuracy in this Epoch is: 88.1699979305 %\n",
      "Epoch 72, Batch 0 Loss:0.019311146810650826\n",
      "Epoch 72, Batch 1 Loss:0.011978778056800365\n",
      "Epoch 72, Batch 2 Loss:0.01343404408544302\n",
      "Epoch 72, Batch 3 Loss:0.02093580737709999\n",
      "Epoch 72, Batch 4 Loss:0.03429685905575752\n",
      "Epoch 72, Batch 5 Loss:0.012179014272987843\n",
      "Epoch 72, Batch 6 Loss:0.015105744823813438\n",
      "Epoch 72, Batch 7 Loss:0.02632412500679493\n",
      "Epoch 72, Batch 8 Loss:0.0033953944221138954\n",
      "Epoch 72, Batch 9 Loss:0.004574045538902283\n",
      "Epoch 72, Batch 10 Loss:0.021582791581749916\n",
      "Epoch 72, Batch 11 Loss:0.06568434089422226\n",
      "Epoch 72, Batch 12 Loss:0.04275015369057655\n",
      "Epoch 72, Batch 13 Loss:0.018345924094319344\n",
      "Epoch 72, Batch 14 Loss:0.012029541656374931\n",
      "Epoch 72, Batch 15 Loss:0.0027604280039668083\n",
      "Epoch 72, Batch 16 Loss:0.01539328508079052\n",
      "Epoch 72, Batch 17 Loss:0.02086886391043663\n",
      "Epoch 72, Batch 18 Loss:0.009422674775123596\n",
      "Epoch 72, Batch 19 Loss:0.011454318650066853\n",
      "Epoch 72, Batch 20 Loss:0.021049864590168\n",
      "Epoch 72, Batch 21 Loss:0.01708308421075344\n",
      "Epoch 72, Batch 22 Loss:0.03431684151291847\n",
      "Epoch 72, Batch 23 Loss:0.017048388719558716\n",
      "Epoch 72, Batch 24 Loss:0.01231013797223568\n",
      "Epoch 72, Batch 25 Loss:0.011802422814071178\n",
      "Epoch 72, Batch 26 Loss:0.03238186240196228\n",
      "Epoch 72, Batch 27 Loss:0.014948935247957706\n",
      "Epoch 72, Batch 28 Loss:0.024957796558737755\n",
      "Epoch 72, Batch 29 Loss:0.03613613173365593\n",
      "Epoch 72, Batch 30 Loss:0.016004346311092377\n",
      "Epoch 72, Batch 31 Loss:0.01676154136657715\n",
      "Epoch 72, Batch 32 Loss:0.018498780205845833\n",
      "Epoch 72, Batch 33 Loss:0.02017793245613575\n",
      "Epoch 72, Batch 34 Loss:0.010011370293796062\n",
      "Epoch 72, Batch 35 Loss:0.010508449748158455\n",
      "Epoch 72, Batch 36 Loss:0.039351936429739\n",
      "Epoch 72, Batch 37 Loss:0.03541126474738121\n",
      "Epoch 72, Batch 38 Loss:0.04411237686872482\n",
      "Epoch 72, Batch 39 Loss:0.03301791101694107\n",
      "Epoch 72, Batch 40 Loss:0.01886574923992157\n",
      "Epoch 72, Batch 41 Loss:0.023420577868819237\n",
      "Epoch 72, Batch 42 Loss:0.018073834478855133\n",
      "Epoch 72, Batch 43 Loss:0.06232767924666405\n",
      "Epoch 72, Batch 44 Loss:0.019201911985874176\n",
      "Epoch 72, Batch 45 Loss:0.01589718647301197\n",
      "Epoch 72, Batch 46 Loss:0.022818703204393387\n",
      "Epoch 72, Batch 47 Loss:0.01147784199565649\n",
      "Epoch 72, Batch 48 Loss:0.029312951490283012\n",
      "Epoch 72, Batch 49 Loss:0.009876331314444542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Batch 50 Loss:0.036495741456747055\n",
      "Epoch 72, Batch 51 Loss:0.030972495675086975\n",
      "Epoch 72, Batch 52 Loss:0.023014070466160774\n",
      "Epoch 72, Batch 53 Loss:0.027707571163773537\n",
      "Epoch 72, Batch 54 Loss:0.013506206683814526\n",
      "Epoch 72, Batch 55 Loss:0.04644249379634857\n",
      "Epoch 72, Batch 56 Loss:0.006665525957942009\n",
      "Epoch 72, Batch 57 Loss:0.03356385976076126\n",
      "Epoch 72, Batch 58 Loss:0.019769102334976196\n",
      "Epoch 72, Batch 59 Loss:0.03851494565606117\n",
      "Epoch 72, Batch 60 Loss:0.027628425508737564\n",
      "Epoch 72, Batch 61 Loss:0.015417568385601044\n",
      "Epoch 72, Batch 62 Loss:0.034888286143541336\n",
      "Epoch 72, Batch 63 Loss:0.016443468630313873\n",
      "Epoch 72, Batch 64 Loss:0.06299366801977158\n",
      "Epoch 72, Batch 65 Loss:0.022951679304242134\n",
      "Epoch 72, Batch 66 Loss:0.02904505655169487\n",
      "Epoch 72, Batch 67 Loss:0.026258988305926323\n",
      "Epoch 72, Batch 68 Loss:0.02979554608464241\n",
      "Epoch 72, Batch 69 Loss:0.03604227304458618\n",
      "Epoch 72, Batch 70 Loss:0.0690246969461441\n",
      "Epoch 72, Batch 71 Loss:0.0120591064915061\n",
      "Epoch 72, Batch 72 Loss:0.0418754480779171\n",
      "Epoch 72, Batch 73 Loss:0.036415550857782364\n",
      "Epoch 72, Batch 74 Loss:0.0077201067470014095\n",
      "Epoch 72, Batch 75 Loss:0.035304054617881775\n",
      "Epoch 72, Batch 76 Loss:0.01037599891424179\n",
      "Epoch 72, Batch 77 Loss:0.0406343936920166\n",
      "Epoch 72, Batch 78 Loss:0.03474312648177147\n",
      "Epoch 72, Batch 79 Loss:0.026371028274297714\n",
      "Epoch 72, Batch 80 Loss:0.026542892679572105\n",
      "Epoch 72, Batch 81 Loss:0.014475354924798012\n",
      "Epoch 72, Batch 82 Loss:0.016346676275134087\n",
      "Epoch 72, Batch 83 Loss:0.028202008455991745\n",
      "Epoch 72, Batch 84 Loss:0.04180460423231125\n",
      "Epoch 72, Batch 85 Loss:0.008227049373090267\n",
      "Epoch 72, Batch 86 Loss:0.020376291126012802\n",
      "Epoch 72, Batch 87 Loss:0.007581672631204128\n",
      "Epoch 72, Batch 88 Loss:0.024108506739139557\n",
      "Epoch 72, Batch 89 Loss:0.013118864968419075\n",
      "Epoch 72, Batch 90 Loss:0.027689563110470772\n",
      "Epoch 72, Batch 91 Loss:0.00882207602262497\n",
      "Epoch 72, Batch 92 Loss:0.010581145994365215\n",
      "Epoch 72, Batch 93 Loss:0.022960936650633812\n",
      "Epoch 72, Batch 94 Loss:0.006887921132147312\n",
      "Epoch 72, Batch 95 Loss:0.02995746023952961\n",
      "Epoch 72, Batch 96 Loss:0.014390349388122559\n",
      "Epoch 72, Batch 97 Loss:0.017850201576948166\n",
      "Epoch 72, Batch 98 Loss:0.007384679280221462\n",
      "Epoch 72, Batch 99 Loss:0.02962864749133587\n",
      "Epoch 72, Batch 100 Loss:0.031753432005643845\n",
      "Epoch 72, Batch 101 Loss:0.021858852356672287\n",
      "Epoch 72, Batch 102 Loss:0.012749745510518551\n",
      "Epoch 72, Batch 103 Loss:0.014892198145389557\n",
      "Epoch 72, Batch 104 Loss:0.012868506833910942\n",
      "Epoch 72, Batch 105 Loss:0.00998332817107439\n",
      "Epoch 72, Batch 106 Loss:0.012777199037373066\n",
      "Epoch 72, Batch 107 Loss:0.008422031067311764\n",
      "Epoch 72, Batch 108 Loss:0.7358232140541077\n",
      "Epoch 72, Batch 109 Loss:0.021831786260008812\n",
      "Epoch 72, Batch 110 Loss:0.024088535457849503\n",
      "Epoch 72, Batch 111 Loss:0.04500613361597061\n",
      "Epoch 72, Batch 112 Loss:0.019282067194581032\n",
      "Epoch 72, Batch 113 Loss:0.03065885789692402\n",
      "Epoch 72, Batch 114 Loss:0.028120335191488266\n",
      "Epoch 72, Batch 115 Loss:0.019567502662539482\n",
      "Epoch 72, Batch 116 Loss:0.05930781364440918\n",
      "Epoch 72, Batch 117 Loss:0.03711177408695221\n",
      "Epoch 72, Batch 118 Loss:0.019887899979948997\n",
      "Epoch 72, Batch 119 Loss:0.02734566479921341\n",
      "Epoch 72, Batch 120 Loss:0.027353715151548386\n",
      "Epoch 72, Batch 121 Loss:0.04583356902003288\n",
      "Epoch 72, Batch 122 Loss:0.03582053631544113\n",
      "Epoch 72, Batch 123 Loss:0.039719704538583755\n",
      "Epoch 72, Batch 124 Loss:0.029225096106529236\n",
      "Epoch 72, Batch 125 Loss:0.022302022203803062\n",
      "Epoch 72, Batch 126 Loss:0.020028751343488693\n",
      "Epoch 72, Batch 127 Loss:0.026005694642663002\n",
      "Epoch 72, Batch 128 Loss:0.04452822357416153\n",
      "Epoch 72, Batch 129 Loss:0.058363426476716995\n",
      "Epoch 72, Batch 130 Loss:0.01687883958220482\n",
      "Epoch 72, Batch 131 Loss:0.030189484357833862\n",
      "Epoch 72, Batch 132 Loss:0.024448970332741737\n",
      "Epoch 72, Batch 133 Loss:0.02587738446891308\n",
      "Epoch 72, Batch 134 Loss:0.026970406994223595\n",
      "Epoch 72, Batch 135 Loss:0.043204165995121\n",
      "Epoch 72, Batch 136 Loss:0.03751208260655403\n",
      "Epoch 72, Batch 137 Loss:0.03944416716694832\n",
      "Epoch 72, Batch 138 Loss:0.022630255669355392\n",
      "Epoch 72, Batch 139 Loss:0.015942255035042763\n",
      "Epoch 72, Batch 140 Loss:0.025759972631931305\n",
      "Epoch 72, Batch 141 Loss:0.05119655281305313\n",
      "Epoch 72, Batch 142 Loss:0.009551883675158024\n",
      "Epoch 72, Batch 143 Loss:0.09785507619380951\n",
      "Epoch 72, Batch 144 Loss:0.015277789905667305\n",
      "Epoch 72, Batch 145 Loss:0.019299741834402084\n",
      "Epoch 72, Batch 146 Loss:0.014170300215482712\n",
      "Epoch 72, Batch 147 Loss:0.04270777851343155\n",
      "Epoch 72, Batch 148 Loss:0.029463518410921097\n",
      "Epoch 72, Batch 149 Loss:0.032823801040649414\n",
      "Epoch 72, Batch 150 Loss:0.07186651229858398\n",
      "Epoch 72, Batch 151 Loss:0.009112006053328514\n",
      "Epoch 72, Batch 152 Loss:0.016678612679243088\n",
      "Epoch 72, Batch 153 Loss:0.024402810260653496\n",
      "Epoch 72, Batch 154 Loss:0.02718607895076275\n",
      "Epoch 72, Batch 155 Loss:0.030475199222564697\n",
      "Epoch 72, Batch 156 Loss:0.020954972133040428\n",
      "Epoch 72, Batch 157 Loss:0.02861529216170311\n",
      "Epoch 72, Batch 158 Loss:0.027157042175531387\n",
      "Epoch 72, Batch 159 Loss:0.024584351107478142\n",
      "Epoch 72, Batch 160 Loss:0.02092863991856575\n",
      "Epoch 72, Batch 161 Loss:0.02865801751613617\n",
      "Epoch 72, Batch 162 Loss:0.029517360031604767\n",
      "Epoch 72, Batch 163 Loss:0.004771457053720951\n",
      "Epoch 72, Batch 164 Loss:0.06803679466247559\n",
      "Epoch 72, Batch 165 Loss:0.0189052727073431\n",
      "Epoch 72, Batch 166 Loss:0.022075604647397995\n",
      "Epoch 72, Batch 167 Loss:0.016044137999415398\n",
      "Epoch 72, Batch 168 Loss:0.02336626686155796\n",
      "Epoch 72, Batch 169 Loss:0.04586505517363548\n",
      "Epoch 72, Batch 170 Loss:0.0057084206491708755\n",
      "Epoch 72, Batch 171 Loss:0.02694648504257202\n",
      "Epoch 72, Batch 172 Loss:0.006437127478420734\n",
      "Epoch 72, Batch 173 Loss:0.036701373755931854\n",
      "Epoch 72, Batch 174 Loss:0.013301841914653778\n",
      "Epoch 72, Batch 175 Loss:0.03673854470252991\n",
      "Epoch 72, Batch 176 Loss:0.07869421690702438\n",
      "Epoch 72, Batch 177 Loss:0.004808544181287289\n",
      "Epoch 72, Batch 178 Loss:0.051632605493068695\n",
      "Epoch 72, Batch 179 Loss:0.019250351935625076\n",
      "Epoch 72, Batch 180 Loss:0.04598110914230347\n",
      "Epoch 72, Batch 181 Loss:0.01619335263967514\n",
      "Epoch 72, Batch 182 Loss:0.018381386995315552\n",
      "Epoch 72, Batch 183 Loss:0.014398012310266495\n",
      "Epoch 72, Batch 184 Loss:0.007905806414783001\n",
      "Epoch 72, Batch 185 Loss:0.01422053761780262\n",
      "Epoch 72, Batch 186 Loss:0.03476032242178917\n",
      "Epoch 72, Batch 187 Loss:0.024839766323566437\n",
      "Epoch 72, Batch 188 Loss:0.01783229224383831\n",
      "Epoch 72, Batch 189 Loss:0.050096798688173294\n",
      "Epoch 72, Batch 190 Loss:0.037559591233730316\n",
      "Epoch 72, Batch 191 Loss:0.034719318151474\n",
      "Epoch 72, Batch 192 Loss:0.011344251222908497\n",
      "Epoch 72, Batch 193 Loss:0.014787167310714722\n",
      "Epoch 72, Batch 194 Loss:0.015638645738363266\n",
      "Epoch 72, Batch 195 Loss:0.0854131206870079\n",
      "Epoch 72, Batch 196 Loss:0.009999646805226803\n",
      "Epoch 72, Batch 197 Loss:0.020666860044002533\n",
      "Epoch 72, Batch 198 Loss:0.012298766523599625\n",
      "Epoch 72, Batch 199 Loss:0.015161160379648209\n",
      "Epoch 72, Batch 200 Loss:0.029737526550889015\n",
      "Epoch 72, Batch 201 Loss:0.03457004204392433\n",
      "Epoch 72, Batch 202 Loss:0.03145633637905121\n",
      "Epoch 72, Batch 203 Loss:0.0305684432387352\n",
      "Epoch 72, Batch 204 Loss:0.006761595141142607\n",
      "Epoch 72, Batch 205 Loss:0.030942970886826515\n",
      "Epoch 72, Batch 206 Loss:0.03835185989737511\n",
      "Epoch 72, Batch 207 Loss:0.01198638416826725\n",
      "Epoch 72, Batch 208 Loss:0.04292876273393631\n",
      "Epoch 72, Batch 209 Loss:0.011798926629126072\n",
      "Epoch 72, Batch 210 Loss:0.059522345662117004\n",
      "Epoch 72, Batch 211 Loss:0.021281691268086433\n",
      "Epoch 72, Batch 212 Loss:0.006273780949413776\n",
      "Epoch 72, Batch 213 Loss:0.043352145701646805\n",
      "Epoch 72, Batch 214 Loss:0.03630063682794571\n",
      "Epoch 72, Batch 215 Loss:0.02040756680071354\n",
      "Epoch 72, Batch 216 Loss:0.0446840301156044\n",
      "Epoch 72, Batch 217 Loss:0.06764397025108337\n",
      "Epoch 72, Batch 218 Loss:0.018905766308307648\n",
      "Epoch 72, Batch 219 Loss:0.03705858439207077\n",
      "Epoch 72, Batch 220 Loss:0.026346879079937935\n",
      "Epoch 72, Batch 221 Loss:0.012892188504338264\n",
      "Epoch 72, Batch 222 Loss:0.027402415871620178\n",
      "Epoch 72, Batch 223 Loss:0.021400537341833115\n",
      "Epoch 72, Batch 224 Loss:0.04223872348666191\n",
      "Epoch 72, Batch 225 Loss:0.03319597616791725\n",
      "Epoch 72, Batch 226 Loss:0.033660873770713806\n",
      "Epoch 72, Batch 227 Loss:0.0072798291221261024\n",
      "Epoch 72, Batch 228 Loss:0.016361724585294724\n",
      "Epoch 72, Batch 229 Loss:0.025213021785020828\n",
      "Epoch 72, Batch 230 Loss:0.01115486305207014\n",
      "Epoch 72, Batch 231 Loss:0.019683629274368286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72, Batch 232 Loss:0.008353663608431816\n",
      "Epoch 72, Batch 233 Loss:0.021068096160888672\n",
      "Loss in this Epoch is: 2.10680961609 %\n",
      "Accuracy in this Epoch is: 88.7000024319 %\n",
      "Epoch 73, Batch 0 Loss:0.011636590585112572\n",
      "Epoch 73, Batch 1 Loss:0.011567484587430954\n",
      "Epoch 73, Batch 2 Loss:0.00896013155579567\n",
      "Epoch 73, Batch 3 Loss:0.03441649675369263\n",
      "Epoch 73, Batch 4 Loss:0.01935928873717785\n",
      "Epoch 73, Batch 5 Loss:0.0060040014795959\n",
      "Epoch 73, Batch 6 Loss:0.014061051420867443\n",
      "Epoch 73, Batch 7 Loss:0.015363400802016258\n",
      "Epoch 73, Batch 8 Loss:0.015627864748239517\n",
      "Epoch 73, Batch 9 Loss:0.01765056699514389\n",
      "Epoch 73, Batch 10 Loss:0.010460786521434784\n",
      "Epoch 73, Batch 11 Loss:0.024330105632543564\n",
      "Epoch 73, Batch 12 Loss:0.006044056732207537\n",
      "Epoch 73, Batch 13 Loss:0.03528410568833351\n",
      "Epoch 73, Batch 14 Loss:0.012446774169802666\n",
      "Epoch 73, Batch 15 Loss:0.00626486586406827\n",
      "Epoch 73, Batch 16 Loss:0.008245313540101051\n",
      "Epoch 73, Batch 17 Loss:0.005839408840984106\n",
      "Epoch 73, Batch 18 Loss:0.027892421931028366\n",
      "Epoch 73, Batch 19 Loss:0.018808024004101753\n",
      "Epoch 73, Batch 20 Loss:0.0178165715187788\n",
      "Epoch 73, Batch 21 Loss:0.026316257193684578\n",
      "Epoch 73, Batch 22 Loss:0.005940267350524664\n",
      "Epoch 73, Batch 23 Loss:0.01069725677371025\n",
      "Epoch 73, Batch 24 Loss:0.0022734026424586773\n",
      "Epoch 73, Batch 25 Loss:0.0060677295550704\n",
      "Epoch 73, Batch 26 Loss:0.011776301078498363\n",
      "Epoch 73, Batch 27 Loss:0.0056336065754294395\n",
      "Epoch 73, Batch 28 Loss:0.04089905321598053\n",
      "Epoch 73, Batch 29 Loss:0.007154547143727541\n",
      "Epoch 73, Batch 30 Loss:0.0047059678472578526\n",
      "Epoch 73, Batch 31 Loss:0.010430851951241493\n",
      "Epoch 73, Batch 32 Loss:0.0058522471226751804\n",
      "Epoch 73, Batch 33 Loss:0.022633196786046028\n",
      "Epoch 73, Batch 34 Loss:0.020292464643716812\n",
      "Epoch 73, Batch 35 Loss:0.02653856948018074\n",
      "Epoch 73, Batch 36 Loss:0.01623062789440155\n",
      "Epoch 73, Batch 37 Loss:0.024518176913261414\n",
      "Epoch 73, Batch 38 Loss:0.008049535565078259\n",
      "Epoch 73, Batch 39 Loss:0.01954568177461624\n",
      "Epoch 73, Batch 40 Loss:0.0066657462157309055\n",
      "Epoch 73, Batch 41 Loss:0.015102406963706017\n",
      "Epoch 73, Batch 42 Loss:0.03142290934920311\n",
      "Epoch 73, Batch 43 Loss:0.018229972571134567\n",
      "Epoch 73, Batch 44 Loss:0.012095295824110508\n",
      "Epoch 73, Batch 45 Loss:0.0033768946304917336\n",
      "Epoch 73, Batch 46 Loss:0.017962509766221046\n",
      "Epoch 73, Batch 47 Loss:0.026142442598938942\n",
      "Epoch 73, Batch 48 Loss:0.013764487579464912\n",
      "Epoch 73, Batch 49 Loss:0.009829203598201275\n",
      "Epoch 73, Batch 50 Loss:0.03158462047576904\n",
      "Epoch 73, Batch 51 Loss:0.0262541975826025\n",
      "Epoch 73, Batch 52 Loss:0.008571641519665718\n",
      "Epoch 73, Batch 53 Loss:0.019549660384655\n",
      "Epoch 73, Batch 54 Loss:0.013389860279858112\n",
      "Epoch 73, Batch 55 Loss:0.009149221703410149\n",
      "Epoch 73, Batch 56 Loss:0.016592098399996758\n",
      "Epoch 73, Batch 57 Loss:0.02839929796755314\n",
      "Epoch 73, Batch 58 Loss:0.0044655343517661095\n",
      "Epoch 73, Batch 59 Loss:0.003495625453069806\n",
      "Epoch 73, Batch 60 Loss:0.023566002026200294\n",
      "Epoch 73, Batch 61 Loss:0.06614623963832855\n",
      "Epoch 73, Batch 62 Loss:0.025797691196203232\n",
      "Epoch 73, Batch 63 Loss:0.011277766898274422\n",
      "Epoch 73, Batch 64 Loss:0.015611314214766026\n",
      "Epoch 73, Batch 65 Loss:0.023547282442450523\n",
      "Epoch 73, Batch 66 Loss:0.0052017453126609325\n",
      "Epoch 73, Batch 67 Loss:0.009498151950538158\n",
      "Epoch 73, Batch 68 Loss:0.045965611934661865\n",
      "Epoch 73, Batch 69 Loss:0.03773925453424454\n",
      "Epoch 73, Batch 70 Loss:0.013618058525025845\n",
      "Epoch 73, Batch 71 Loss:0.024708617478609085\n",
      "Epoch 73, Batch 72 Loss:0.028426771983504295\n",
      "Epoch 73, Batch 73 Loss:0.029020873829722404\n",
      "Epoch 73, Batch 74 Loss:0.024524608626961708\n",
      "Epoch 73, Batch 75 Loss:0.01120639219880104\n",
      "Epoch 73, Batch 76 Loss:0.013239484280347824\n",
      "Epoch 73, Batch 77 Loss:0.004678242839872837\n",
      "Epoch 73, Batch 78 Loss:0.01568036712706089\n",
      "Epoch 73, Batch 79 Loss:0.02083168737590313\n",
      "Epoch 73, Batch 80 Loss:0.010638128966093063\n",
      "Epoch 73, Batch 81 Loss:0.0327925905585289\n",
      "Epoch 73, Batch 82 Loss:0.03003532625734806\n",
      "Epoch 73, Batch 83 Loss:0.03313261270523071\n",
      "Epoch 73, Batch 84 Loss:0.008025175891816616\n",
      "Epoch 73, Batch 85 Loss:0.011366338469088078\n",
      "Epoch 73, Batch 86 Loss:0.008925599977374077\n",
      "Epoch 73, Batch 87 Loss:0.032005637884140015\n",
      "Epoch 73, Batch 88 Loss:0.05469683185219765\n",
      "Epoch 73, Batch 89 Loss:0.040137067437171936\n",
      "Epoch 73, Batch 90 Loss:0.030462849885225296\n",
      "Epoch 73, Batch 91 Loss:0.01045714970678091\n",
      "Epoch 73, Batch 92 Loss:0.013830955140292645\n",
      "Epoch 73, Batch 93 Loss:0.030943503603339195\n",
      "Epoch 73, Batch 94 Loss:0.017422925680875778\n",
      "Epoch 73, Batch 95 Loss:0.014499744400382042\n",
      "Epoch 73, Batch 96 Loss:0.00786531064659357\n",
      "Epoch 73, Batch 97 Loss:0.03399164229631424\n",
      "Epoch 73, Batch 98 Loss:0.005857936106622219\n",
      "Epoch 73, Batch 99 Loss:0.023754684254527092\n",
      "Epoch 73, Batch 100 Loss:0.007009580265730619\n",
      "Epoch 73, Batch 101 Loss:0.019788051024079323\n",
      "Epoch 73, Batch 102 Loss:0.027696959674358368\n",
      "Epoch 73, Batch 103 Loss:0.015074251219630241\n",
      "Epoch 73, Batch 104 Loss:0.020593566820025444\n",
      "Epoch 73, Batch 105 Loss:0.012475862167775631\n",
      "Epoch 73, Batch 106 Loss:0.016583530232310295\n",
      "Epoch 73, Batch 107 Loss:0.009534532204270363\n",
      "Epoch 73, Batch 108 Loss:0.01882622018456459\n",
      "Epoch 73, Batch 109 Loss:0.04596955329179764\n",
      "Epoch 73, Batch 110 Loss:0.03834085911512375\n",
      "Epoch 73, Batch 111 Loss:0.03804474323987961\n",
      "Epoch 73, Batch 112 Loss:0.029419604688882828\n",
      "Epoch 73, Batch 113 Loss:0.020076418295502663\n",
      "Epoch 73, Batch 114 Loss:0.027070308104157448\n",
      "Epoch 73, Batch 115 Loss:0.06794281303882599\n",
      "Epoch 73, Batch 116 Loss:0.01294105313718319\n",
      "Epoch 73, Batch 117 Loss:0.008347643539309502\n",
      "Epoch 73, Batch 118 Loss:0.0035874941386282444\n",
      "Epoch 73, Batch 119 Loss:0.005526616238057613\n",
      "Epoch 73, Batch 120 Loss:0.006423029117286205\n",
      "Epoch 73, Batch 121 Loss:0.018562085926532745\n",
      "Epoch 73, Batch 122 Loss:0.014442614279687405\n",
      "Epoch 73, Batch 123 Loss:0.017769446596503258\n",
      "Epoch 73, Batch 124 Loss:0.05891609191894531\n",
      "Epoch 73, Batch 125 Loss:0.06312747299671173\n",
      "Epoch 73, Batch 126 Loss:0.020449010655283928\n",
      "Epoch 73, Batch 127 Loss:0.03608933836221695\n",
      "Epoch 73, Batch 128 Loss:0.017426876351237297\n",
      "Epoch 73, Batch 129 Loss:0.013282829895615578\n",
      "Epoch 73, Batch 130 Loss:0.016961267217993736\n",
      "Epoch 73, Batch 131 Loss:0.03221955522894859\n",
      "Epoch 73, Batch 132 Loss:0.014311502687633038\n",
      "Epoch 73, Batch 133 Loss:0.026955582201480865\n",
      "Epoch 73, Batch 134 Loss:0.017412830144166946\n",
      "Epoch 73, Batch 135 Loss:0.011791709810495377\n",
      "Epoch 73, Batch 136 Loss:0.028783591464161873\n",
      "Epoch 73, Batch 137 Loss:0.017823627218604088\n",
      "Epoch 73, Batch 138 Loss:0.01657835766673088\n",
      "Epoch 73, Batch 139 Loss:0.015188482590019703\n",
      "Epoch 73, Batch 140 Loss:0.01691364496946335\n",
      "Epoch 73, Batch 141 Loss:0.04185306280851364\n",
      "Epoch 73, Batch 142 Loss:0.02970844879746437\n",
      "Epoch 73, Batch 143 Loss:0.01144400704652071\n",
      "Epoch 73, Batch 144 Loss:0.03705645352602005\n",
      "Epoch 73, Batch 145 Loss:0.03233541548252106\n",
      "Epoch 73, Batch 146 Loss:0.020521076396107674\n",
      "Epoch 73, Batch 147 Loss:0.026518702507019043\n",
      "Epoch 73, Batch 148 Loss:0.009828471578657627\n",
      "Epoch 73, Batch 149 Loss:0.06472714990377426\n",
      "Epoch 73, Batch 150 Loss:0.05854681134223938\n",
      "Epoch 73, Batch 151 Loss:0.03988596424460411\n",
      "Epoch 73, Batch 152 Loss:0.05898226425051689\n",
      "Epoch 73, Batch 153 Loss:0.012200809083878994\n",
      "Epoch 73, Batch 154 Loss:0.06340335309505463\n",
      "Epoch 73, Batch 155 Loss:0.07929717749357224\n",
      "Epoch 73, Batch 156 Loss:0.02780798077583313\n",
      "Epoch 73, Batch 157 Loss:0.04912801831960678\n",
      "Epoch 73, Batch 158 Loss:0.03562018647789955\n",
      "Epoch 73, Batch 159 Loss:0.020038414746522903\n",
      "Epoch 73, Batch 160 Loss:0.027796126902103424\n",
      "Epoch 73, Batch 161 Loss:0.028648115694522858\n",
      "Epoch 73, Batch 162 Loss:0.0614451989531517\n",
      "Epoch 73, Batch 163 Loss:0.03568185865879059\n",
      "Epoch 73, Batch 164 Loss:0.05724554508924484\n",
      "Epoch 73, Batch 165 Loss:0.027426082640886307\n",
      "Epoch 73, Batch 166 Loss:0.026974260807037354\n",
      "Epoch 73, Batch 167 Loss:0.028212372213602066\n",
      "Epoch 73, Batch 168 Loss:0.0334787517786026\n",
      "Epoch 73, Batch 169 Loss:0.028309399262070656\n",
      "Epoch 73, Batch 170 Loss:0.02078499272465706\n",
      "Epoch 73, Batch 171 Loss:0.04619982838630676\n",
      "Epoch 73, Batch 172 Loss:0.03886256366968155\n",
      "Epoch 73, Batch 173 Loss:0.027517981827259064\n",
      "Epoch 73, Batch 174 Loss:0.07960943132638931\n",
      "Epoch 73, Batch 175 Loss:0.04010655730962753\n",
      "Epoch 73, Batch 176 Loss:0.007873105816543102\n",
      "Epoch 73, Batch 177 Loss:0.04341891407966614\n",
      "Epoch 73, Batch 178 Loss:0.02042311802506447\n",
      "Epoch 73, Batch 179 Loss:0.031213562935590744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73, Batch 180 Loss:0.03317386656999588\n",
      "Epoch 73, Batch 181 Loss:0.04521925002336502\n",
      "Epoch 73, Batch 182 Loss:0.03939937800168991\n",
      "Epoch 73, Batch 183 Loss:0.01355274859815836\n",
      "Epoch 73, Batch 184 Loss:0.015256861224770546\n",
      "Epoch 73, Batch 185 Loss:0.011804521083831787\n",
      "Epoch 73, Batch 186 Loss:0.010373149067163467\n",
      "Epoch 73, Batch 187 Loss:0.021833576261997223\n",
      "Epoch 73, Batch 188 Loss:0.016320565715432167\n",
      "Epoch 73, Batch 189 Loss:0.014809101819992065\n",
      "Epoch 73, Batch 190 Loss:0.02291956916451454\n",
      "Epoch 73, Batch 191 Loss:0.022754106670618057\n",
      "Epoch 73, Batch 192 Loss:0.026353903114795685\n",
      "Epoch 73, Batch 193 Loss:0.04709317535161972\n",
      "Epoch 73, Batch 194 Loss:0.016423238441348076\n",
      "Epoch 73, Batch 195 Loss:0.08971476554870605\n",
      "Epoch 73, Batch 196 Loss:0.00831994041800499\n",
      "Epoch 73, Batch 197 Loss:0.05582990497350693\n",
      "Epoch 73, Batch 198 Loss:0.021324843168258667\n",
      "Epoch 73, Batch 199 Loss:0.043952781707048416\n",
      "Epoch 73, Batch 200 Loss:0.01124431099742651\n",
      "Epoch 73, Batch 201 Loss:0.008126572705805302\n",
      "Epoch 73, Batch 202 Loss:0.01859181560575962\n",
      "Epoch 73, Batch 203 Loss:0.0344276987016201\n",
      "Epoch 73, Batch 204 Loss:0.04074700176715851\n",
      "Epoch 73, Batch 205 Loss:0.037308067083358765\n",
      "Epoch 73, Batch 206 Loss:0.019196346402168274\n",
      "Epoch 73, Batch 207 Loss:0.010232258588075638\n",
      "Epoch 73, Batch 208 Loss:0.008228419348597527\n",
      "Epoch 73, Batch 209 Loss:0.023076243698596954\n",
      "Epoch 73, Batch 210 Loss:0.01974746398627758\n",
      "Epoch 73, Batch 211 Loss:0.053275737911462784\n",
      "Epoch 73, Batch 212 Loss:0.011220887303352356\n",
      "Epoch 73, Batch 213 Loss:0.010793285444378853\n",
      "Epoch 73, Batch 214 Loss:0.03165991231799126\n",
      "Epoch 73, Batch 215 Loss:0.04161067679524422\n",
      "Epoch 73, Batch 216 Loss:0.03940336033701897\n",
      "Epoch 73, Batch 217 Loss:0.06175222620368004\n",
      "Epoch 73, Batch 218 Loss:0.03322964161634445\n",
      "Epoch 73, Batch 219 Loss:0.05493183434009552\n",
      "Epoch 73, Batch 220 Loss:0.018252158537507057\n",
      "Epoch 73, Batch 221 Loss:0.041047826409339905\n",
      "Epoch 73, Batch 222 Loss:0.03804657980799675\n",
      "Epoch 73, Batch 223 Loss:0.030098140239715576\n",
      "Epoch 73, Batch 224 Loss:0.051604680716991425\n",
      "Epoch 73, Batch 225 Loss:0.05368643254041672\n",
      "Epoch 73, Batch 226 Loss:0.052515462040901184\n",
      "Epoch 73, Batch 227 Loss:0.01861298829317093\n",
      "Epoch 73, Batch 228 Loss:0.010570281185209751\n",
      "Epoch 73, Batch 229 Loss:0.027300016954541206\n",
      "Epoch 73, Batch 230 Loss:0.007330526597797871\n",
      "Epoch 73, Batch 231 Loss:0.07633333653211594\n",
      "Epoch 73, Batch 232 Loss:0.021754875779151917\n",
      "Epoch 73, Batch 233 Loss:0.008274799212813377\n",
      "Loss in this Epoch is: 0.827479921281 %\n",
      "Accuracy in this Epoch is: 88.8599991798 %\n",
      "Epoch 74, Batch 0 Loss:0.02984602190554142\n",
      "Epoch 74, Batch 1 Loss:0.009706422686576843\n",
      "Epoch 74, Batch 2 Loss:0.02515062503516674\n",
      "Epoch 74, Batch 3 Loss:0.031155070289969444\n",
      "Epoch 74, Batch 4 Loss:0.006667146924883127\n",
      "Epoch 74, Batch 5 Loss:0.017945023253560066\n",
      "Epoch 74, Batch 6 Loss:0.009816929697990417\n",
      "Epoch 74, Batch 7 Loss:0.035602860152721405\n",
      "Epoch 74, Batch 8 Loss:0.010947075672447681\n",
      "Epoch 74, Batch 9 Loss:0.020115457475185394\n",
      "Epoch 74, Batch 10 Loss:0.008930538780987263\n",
      "Epoch 74, Batch 11 Loss:0.007296124007552862\n",
      "Epoch 74, Batch 12 Loss:0.03748023137450218\n",
      "Epoch 74, Batch 13 Loss:0.006241241004317999\n",
      "Epoch 74, Batch 14 Loss:0.014741966500878334\n",
      "Epoch 74, Batch 15 Loss:0.006042781751602888\n",
      "Epoch 74, Batch 16 Loss:0.011096504516899586\n",
      "Epoch 74, Batch 17 Loss:0.06002078205347061\n",
      "Epoch 74, Batch 18 Loss:0.00579330138862133\n",
      "Epoch 74, Batch 19 Loss:0.005182789638638496\n",
      "Epoch 74, Batch 20 Loss:0.014931616373360157\n",
      "Epoch 74, Batch 21 Loss:0.00853078905493021\n",
      "Epoch 74, Batch 22 Loss:0.0032136051449924707\n",
      "Epoch 74, Batch 23 Loss:0.008004127070307732\n",
      "Epoch 74, Batch 24 Loss:0.036186523735523224\n",
      "Epoch 74, Batch 25 Loss:0.019277852028608322\n",
      "Epoch 74, Batch 26 Loss:0.008031721226871014\n",
      "Epoch 74, Batch 27 Loss:0.012491550296545029\n",
      "Epoch 74, Batch 28 Loss:0.01198788732290268\n",
      "Epoch 74, Batch 29 Loss:0.017396191135048866\n",
      "Epoch 74, Batch 30 Loss:0.02077113278210163\n",
      "Epoch 74, Batch 31 Loss:0.02037864737212658\n",
      "Epoch 74, Batch 32 Loss:0.01287129893898964\n",
      "Epoch 74, Batch 33 Loss:0.04191357642412186\n",
      "Epoch 74, Batch 34 Loss:0.016598017886281013\n",
      "Epoch 74, Batch 35 Loss:0.04111597314476967\n",
      "Epoch 74, Batch 36 Loss:0.008452451787889004\n",
      "Epoch 74, Batch 37 Loss:0.005260087084025145\n",
      "Epoch 74, Batch 38 Loss:0.01499099936336279\n",
      "Epoch 74, Batch 39 Loss:0.01894572377204895\n",
      "Epoch 74, Batch 40 Loss:0.04336235299706459\n",
      "Epoch 74, Batch 41 Loss:0.015021475963294506\n",
      "Epoch 74, Batch 42 Loss:0.021308105438947678\n",
      "Epoch 74, Batch 43 Loss:0.01695593073964119\n",
      "Epoch 74, Batch 44 Loss:0.0401022769510746\n",
      "Epoch 74, Batch 45 Loss:0.04196029156446457\n",
      "Epoch 74, Batch 46 Loss:0.02516772225499153\n",
      "Epoch 74, Batch 47 Loss:0.021863264963030815\n",
      "Epoch 74, Batch 48 Loss:0.02388644590973854\n",
      "Epoch 74, Batch 49 Loss:0.009693457745015621\n",
      "Epoch 74, Batch 50 Loss:0.05495258420705795\n",
      "Epoch 74, Batch 51 Loss:0.04013892635703087\n",
      "Epoch 74, Batch 52 Loss:0.0246861781924963\n",
      "Epoch 74, Batch 53 Loss:0.0353667289018631\n",
      "Epoch 74, Batch 54 Loss:0.029345277696847916\n",
      "Epoch 74, Batch 55 Loss:0.022419944405555725\n",
      "Epoch 74, Batch 56 Loss:0.036331307142972946\n",
      "Epoch 74, Batch 57 Loss:0.017878813669085503\n",
      "Epoch 74, Batch 58 Loss:0.015836674720048904\n",
      "Epoch 74, Batch 59 Loss:0.0061114015989005566\n",
      "Epoch 74, Batch 60 Loss:0.02541501261293888\n",
      "Epoch 74, Batch 61 Loss:0.005059806164354086\n",
      "Epoch 74, Batch 62 Loss:0.018108759075403214\n",
      "Epoch 74, Batch 63 Loss:0.03253461793065071\n",
      "Epoch 74, Batch 64 Loss:0.011117540299892426\n",
      "Epoch 74, Batch 65 Loss:0.013528297655284405\n",
      "Epoch 74, Batch 66 Loss:0.019948216155171394\n",
      "Epoch 74, Batch 67 Loss:0.015033891424536705\n",
      "Epoch 74, Batch 68 Loss:0.04208454489707947\n",
      "Epoch 74, Batch 69 Loss:0.04321978613734245\n",
      "Epoch 74, Batch 70 Loss:0.019816294312477112\n",
      "Epoch 74, Batch 71 Loss:0.044670626521110535\n",
      "Epoch 74, Batch 72 Loss:0.012365621514618397\n",
      "Epoch 74, Batch 73 Loss:0.05536212399601936\n",
      "Epoch 74, Batch 74 Loss:0.01081028301268816\n",
      "Epoch 74, Batch 75 Loss:0.014234522357583046\n",
      "Epoch 74, Batch 76 Loss:0.010991192422807217\n",
      "Epoch 74, Batch 77 Loss:0.018195299431681633\n",
      "Epoch 74, Batch 78 Loss:0.011794790625572205\n",
      "Epoch 74, Batch 79 Loss:0.03403417021036148\n",
      "Epoch 74, Batch 80 Loss:0.024453753605484962\n",
      "Epoch 74, Batch 81 Loss:0.008893164806067944\n",
      "Epoch 74, Batch 82 Loss:0.0113509027287364\n",
      "Epoch 74, Batch 83 Loss:0.009641689248383045\n",
      "Epoch 74, Batch 84 Loss:0.019298547878861427\n",
      "Epoch 74, Batch 85 Loss:0.02027195133268833\n",
      "Epoch 74, Batch 86 Loss:0.01778082177042961\n",
      "Epoch 74, Batch 87 Loss:0.028840070590376854\n",
      "Epoch 74, Batch 88 Loss:0.021850548684597015\n",
      "Epoch 74, Batch 89 Loss:0.003962491173297167\n",
      "Epoch 74, Batch 90 Loss:0.023068711161613464\n",
      "Epoch 74, Batch 91 Loss:0.020970208570361137\n",
      "Epoch 74, Batch 92 Loss:0.029304832220077515\n",
      "Epoch 74, Batch 93 Loss:0.024904770776629448\n",
      "Epoch 74, Batch 94 Loss:0.06116175651550293\n",
      "Epoch 74, Batch 95 Loss:0.020771238952875137\n",
      "Epoch 74, Batch 96 Loss:0.04229622334241867\n",
      "Epoch 74, Batch 97 Loss:0.026820600032806396\n",
      "Epoch 74, Batch 98 Loss:0.020956899970769882\n",
      "Epoch 74, Batch 99 Loss:0.024727676063776016\n",
      "Epoch 74, Batch 100 Loss:0.06085996329784393\n",
      "Epoch 74, Batch 101 Loss:0.015074923634529114\n",
      "Epoch 74, Batch 102 Loss:0.023817725479602814\n",
      "Epoch 74, Batch 103 Loss:0.012506230734288692\n",
      "Epoch 74, Batch 104 Loss:0.011060848832130432\n",
      "Epoch 74, Batch 105 Loss:0.03740643337368965\n",
      "Epoch 74, Batch 106 Loss:0.04060210660099983\n",
      "Epoch 74, Batch 107 Loss:0.023090127855539322\n",
      "Epoch 74, Batch 108 Loss:0.01675625890493393\n",
      "Epoch 74, Batch 109 Loss:0.025007188320159912\n",
      "Epoch 74, Batch 110 Loss:0.006410099565982819\n",
      "Epoch 74, Batch 111 Loss:0.08024028688669205\n",
      "Epoch 74, Batch 112 Loss:0.031125538051128387\n",
      "Epoch 74, Batch 113 Loss:0.03131029009819031\n",
      "Epoch 74, Batch 114 Loss:0.01542050950229168\n",
      "Epoch 74, Batch 115 Loss:0.04914144054055214\n",
      "Epoch 74, Batch 116 Loss:0.03340306133031845\n",
      "Epoch 74, Batch 117 Loss:0.020353034138679504\n",
      "Epoch 74, Batch 118 Loss:0.016393650323152542\n",
      "Epoch 74, Batch 119 Loss:0.008124135434627533\n",
      "Epoch 74, Batch 120 Loss:0.01911354437470436\n",
      "Epoch 74, Batch 121 Loss:0.02105521224439144\n",
      "Epoch 74, Batch 122 Loss:0.01605774462223053\n",
      "Epoch 74, Batch 123 Loss:0.005604507867246866\n",
      "Epoch 74, Batch 124 Loss:0.03255297988653183\n",
      "Epoch 74, Batch 125 Loss:0.04157819598913193\n",
      "Epoch 74, Batch 126 Loss:0.020667489618062973\n",
      "Epoch 74, Batch 127 Loss:0.013850784860551357\n",
      "Epoch 74, Batch 128 Loss:0.017945049330592155\n",
      "Epoch 74, Batch 129 Loss:0.042560212314128876\n",
      "Epoch 74, Batch 130 Loss:0.01808466762304306\n",
      "Epoch 74, Batch 131 Loss:0.015629470348358154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74, Batch 132 Loss:0.007720619905740023\n",
      "Epoch 74, Batch 133 Loss:0.043038833886384964\n",
      "Epoch 74, Batch 134 Loss:0.011727659031748772\n",
      "Epoch 74, Batch 135 Loss:0.03865363076329231\n",
      "Epoch 74, Batch 136 Loss:0.013729804195463657\n",
      "Epoch 74, Batch 137 Loss:0.04800413176417351\n",
      "Epoch 74, Batch 138 Loss:0.02758936770260334\n",
      "Epoch 74, Batch 139 Loss:0.016714977100491524\n",
      "Epoch 74, Batch 140 Loss:0.011910021305084229\n",
      "Epoch 74, Batch 141 Loss:0.008621742017567158\n",
      "Epoch 74, Batch 142 Loss:0.02719738893210888\n",
      "Epoch 74, Batch 143 Loss:0.01416725292801857\n",
      "Epoch 74, Batch 144 Loss:0.008915365673601627\n",
      "Epoch 74, Batch 145 Loss:0.014607767574489117\n",
      "Epoch 74, Batch 146 Loss:0.007782595697790384\n",
      "Epoch 74, Batch 147 Loss:0.010737022385001183\n",
      "Epoch 74, Batch 148 Loss:0.008673781529068947\n",
      "Epoch 74, Batch 149 Loss:0.009057268500328064\n",
      "Epoch 74, Batch 150 Loss:0.015934281051158905\n",
      "Epoch 74, Batch 151 Loss:0.019971387460827827\n",
      "Epoch 74, Batch 152 Loss:0.03235802799463272\n",
      "Epoch 74, Batch 153 Loss:0.0063811903819441795\n",
      "Epoch 74, Batch 154 Loss:0.007348170038312674\n",
      "Epoch 74, Batch 155 Loss:0.028444286435842514\n",
      "Epoch 74, Batch 156 Loss:0.018855871632695198\n",
      "Epoch 74, Batch 157 Loss:0.004829371348023415\n",
      "Epoch 74, Batch 158 Loss:0.014326645992696285\n",
      "Epoch 74, Batch 159 Loss:0.021397477015852928\n",
      "Epoch 74, Batch 160 Loss:0.04445217549800873\n",
      "Epoch 74, Batch 161 Loss:0.015919435769319534\n",
      "Epoch 74, Batch 162 Loss:0.019559741020202637\n",
      "Epoch 74, Batch 163 Loss:0.009114081040024757\n",
      "Epoch 74, Batch 164 Loss:0.032107532024383545\n",
      "Epoch 74, Batch 165 Loss:0.006873561069369316\n",
      "Epoch 74, Batch 166 Loss:0.024676945060491562\n",
      "Epoch 74, Batch 167 Loss:0.031965434551239014\n",
      "Epoch 74, Batch 168 Loss:0.02929702401161194\n",
      "Epoch 74, Batch 169 Loss:0.05587916076183319\n",
      "Epoch 74, Batch 170 Loss:0.027246342971920967\n",
      "Epoch 74, Batch 171 Loss:0.013153474777936935\n",
      "Epoch 74, Batch 172 Loss:0.012002427130937576\n",
      "Epoch 74, Batch 173 Loss:0.010445241816341877\n",
      "Epoch 74, Batch 174 Loss:0.010688217356801033\n",
      "Epoch 74, Batch 175 Loss:0.005982853472232819\n",
      "Epoch 74, Batch 176 Loss:0.044838834553956985\n",
      "Epoch 74, Batch 177 Loss:0.03996826335787773\n",
      "Epoch 74, Batch 178 Loss:0.005818147212266922\n",
      "Epoch 74, Batch 179 Loss:0.0191982239484787\n",
      "Epoch 74, Batch 180 Loss:0.02267426811158657\n",
      "Epoch 74, Batch 181 Loss:0.025327390059828758\n",
      "Epoch 74, Batch 182 Loss:0.005669165402650833\n",
      "Epoch 74, Batch 183 Loss:0.005413545295596123\n",
      "Epoch 74, Batch 184 Loss:0.012903029099106789\n",
      "Epoch 74, Batch 185 Loss:0.015620999038219452\n",
      "Epoch 74, Batch 186 Loss:0.022060519084334373\n",
      "Epoch 74, Batch 187 Loss:0.02401893213391304\n",
      "Epoch 74, Batch 188 Loss:0.0342920646071434\n",
      "Epoch 74, Batch 189 Loss:0.027479179203510284\n",
      "Epoch 74, Batch 190 Loss:0.0277559831738472\n",
      "Epoch 74, Batch 191 Loss:0.0080769844353199\n",
      "Epoch 74, Batch 192 Loss:0.020909518003463745\n",
      "Epoch 74, Batch 193 Loss:0.014770928770303726\n",
      "Epoch 74, Batch 194 Loss:0.01622810773551464\n",
      "Epoch 74, Batch 195 Loss:0.01771501824259758\n",
      "Epoch 74, Batch 196 Loss:0.053267952054739\n",
      "Epoch 74, Batch 197 Loss:0.006673059426248074\n",
      "Epoch 74, Batch 198 Loss:0.012696919031441212\n",
      "Epoch 74, Batch 199 Loss:0.05050289258360863\n",
      "Epoch 74, Batch 200 Loss:0.018749499693512917\n",
      "Epoch 74, Batch 201 Loss:0.012724044732749462\n",
      "Epoch 74, Batch 202 Loss:0.010061139240860939\n",
      "Epoch 74, Batch 203 Loss:0.006071117706596851\n",
      "Epoch 74, Batch 204 Loss:0.01707647368311882\n",
      "Epoch 74, Batch 205 Loss:0.019629215821623802\n",
      "Epoch 74, Batch 206 Loss:0.011953053995966911\n",
      "Epoch 74, Batch 207 Loss:0.016097143292427063\n",
      "Epoch 74, Batch 208 Loss:0.02147393301129341\n",
      "Epoch 74, Batch 209 Loss:0.007719977758824825\n",
      "Epoch 74, Batch 210 Loss:0.004513781983405352\n",
      "Epoch 74, Batch 211 Loss:0.005091013386845589\n",
      "Epoch 74, Batch 212 Loss:0.010968895629048347\n",
      "Epoch 74, Batch 213 Loss:0.01040150597691536\n",
      "Epoch 74, Batch 214 Loss:0.00469992496073246\n",
      "Epoch 74, Batch 215 Loss:0.04888906329870224\n",
      "Epoch 74, Batch 216 Loss:0.0033695956226438284\n",
      "Epoch 74, Batch 217 Loss:0.03117152862250805\n",
      "Epoch 74, Batch 218 Loss:0.021208174526691437\n",
      "Epoch 74, Batch 219 Loss:0.024079972878098488\n",
      "Epoch 74, Batch 220 Loss:0.03392759710550308\n",
      "Epoch 74, Batch 221 Loss:0.019522616639733315\n",
      "Epoch 74, Batch 222 Loss:0.018697842955589294\n",
      "Epoch 74, Batch 223 Loss:0.028134914115071297\n",
      "Epoch 74, Batch 224 Loss:0.03317686542868614\n",
      "Epoch 74, Batch 225 Loss:0.004378979094326496\n",
      "Epoch 74, Batch 226 Loss:0.007803337182849646\n",
      "Epoch 74, Batch 227 Loss:0.0333472341299057\n",
      "Epoch 74, Batch 228 Loss:0.006575917825102806\n",
      "Epoch 74, Batch 229 Loss:0.009882688522338867\n",
      "Epoch 74, Batch 230 Loss:0.004942608065903187\n",
      "Epoch 74, Batch 231 Loss:0.005860483273863792\n",
      "Epoch 74, Batch 232 Loss:0.028263568878173828\n",
      "Epoch 74, Batch 233 Loss:0.022413525730371475\n",
      "Loss in this Epoch is: 2.24135257304 %\n",
      "Accuracy in this Epoch is: 88.5599970818 %\n",
      "Epoch 75, Batch 0 Loss:0.016485074535012245\n",
      "Epoch 75, Batch 1 Loss:0.03626587986946106\n",
      "Epoch 75, Batch 2 Loss:0.006002346519380808\n",
      "Epoch 75, Batch 3 Loss:0.014411994256079197\n",
      "Epoch 75, Batch 4 Loss:0.0036312262527644634\n",
      "Epoch 75, Batch 5 Loss:0.04222281649708748\n",
      "Epoch 75, Batch 6 Loss:0.005413909442722797\n",
      "Epoch 75, Batch 7 Loss:0.011134125292301178\n",
      "Epoch 75, Batch 8 Loss:0.004243158269673586\n",
      "Epoch 75, Batch 9 Loss:0.007173270918428898\n",
      "Epoch 75, Batch 10 Loss:0.011476565152406693\n",
      "Epoch 75, Batch 11 Loss:0.0077081345953047276\n",
      "Epoch 75, Batch 12 Loss:0.005618956871330738\n",
      "Epoch 75, Batch 13 Loss:0.015049299225211143\n",
      "Epoch 75, Batch 14 Loss:0.01056990772485733\n",
      "Epoch 75, Batch 15 Loss:0.02096867747604847\n",
      "Epoch 75, Batch 16 Loss:0.03595712408423424\n",
      "Epoch 75, Batch 17 Loss:0.04116670414805412\n",
      "Epoch 75, Batch 18 Loss:0.012559890747070312\n",
      "Epoch 75, Batch 19 Loss:0.00902977678924799\n",
      "Epoch 75, Batch 20 Loss:0.01521114632487297\n",
      "Epoch 75, Batch 21 Loss:0.03186890482902527\n",
      "Epoch 75, Batch 22 Loss:0.014020727016031742\n",
      "Epoch 75, Batch 23 Loss:0.03412385657429695\n",
      "Epoch 75, Batch 24 Loss:0.009180432185530663\n",
      "Epoch 75, Batch 25 Loss:0.008968531154096127\n",
      "Epoch 75, Batch 26 Loss:0.01817205920815468\n",
      "Epoch 75, Batch 27 Loss:0.005951407365500927\n",
      "Epoch 75, Batch 28 Loss:0.0140409916639328\n",
      "Epoch 75, Batch 29 Loss:0.00457295635715127\n",
      "Epoch 75, Batch 30 Loss:0.029764913022518158\n",
      "Epoch 75, Batch 31 Loss:0.007909752428531647\n",
      "Epoch 75, Batch 32 Loss:0.019049542024731636\n",
      "Epoch 75, Batch 33 Loss:0.01311381347477436\n",
      "Epoch 75, Batch 34 Loss:0.011037955991923809\n",
      "Epoch 75, Batch 35 Loss:0.032488543540239334\n",
      "Epoch 75, Batch 36 Loss:0.03305009752511978\n",
      "Epoch 75, Batch 37 Loss:0.035553064197301865\n",
      "Epoch 75, Batch 38 Loss:0.061825770884752274\n",
      "Epoch 75, Batch 39 Loss:0.028401175513863564\n",
      "Epoch 75, Batch 40 Loss:0.026328157633543015\n",
      "Epoch 75, Batch 41 Loss:0.001095716841518879\n",
      "Epoch 75, Batch 42 Loss:0.006993663031607866\n",
      "Epoch 75, Batch 43 Loss:0.014943420886993408\n",
      "Epoch 75, Batch 44 Loss:0.00834177527576685\n",
      "Epoch 75, Batch 45 Loss:0.013509451411664486\n",
      "Epoch 75, Batch 46 Loss:0.0049044294282794\n",
      "Epoch 75, Batch 47 Loss:0.011334662325680256\n",
      "Epoch 75, Batch 48 Loss:0.008401810191571712\n",
      "Epoch 75, Batch 49 Loss:0.006800534203648567\n",
      "Epoch 75, Batch 50 Loss:0.02056804858148098\n",
      "Epoch 75, Batch 51 Loss:0.025902539491653442\n",
      "Epoch 75, Batch 52 Loss:0.01123049110174179\n",
      "Epoch 75, Batch 53 Loss:0.02345339022576809\n",
      "Epoch 75, Batch 54 Loss:0.008985229767858982\n",
      "Epoch 75, Batch 55 Loss:0.005841345060616732\n",
      "Epoch 75, Batch 56 Loss:0.0035355191212147474\n",
      "Epoch 75, Batch 57 Loss:0.00801397580653429\n",
      "Epoch 75, Batch 58 Loss:0.006777192931622267\n",
      "Epoch 75, Batch 59 Loss:0.009222391992807388\n",
      "Epoch 75, Batch 60 Loss:0.0058243065141141415\n",
      "Epoch 75, Batch 61 Loss:0.01975400000810623\n",
      "Epoch 75, Batch 62 Loss:0.005696299951523542\n",
      "Epoch 75, Batch 63 Loss:0.005559696815907955\n",
      "Epoch 75, Batch 64 Loss:0.002910047769546509\n",
      "Epoch 75, Batch 65 Loss:0.0209667831659317\n",
      "Epoch 75, Batch 66 Loss:0.011697623878717422\n",
      "Epoch 75, Batch 67 Loss:0.006277202628552914\n",
      "Epoch 75, Batch 68 Loss:0.012968096882104874\n",
      "Epoch 75, Batch 69 Loss:0.008787880651652813\n",
      "Epoch 75, Batch 70 Loss:0.016196416690945625\n",
      "Epoch 75, Batch 71 Loss:0.018675053492188454\n",
      "Epoch 75, Batch 72 Loss:0.03064454160630703\n",
      "Epoch 75, Batch 73 Loss:0.024578062817454338\n",
      "Epoch 75, Batch 74 Loss:0.010971732437610626\n",
      "Epoch 75, Batch 75 Loss:0.01377930212765932\n",
      "Epoch 75, Batch 76 Loss:0.011312171816825867\n",
      "Epoch 75, Batch 77 Loss:0.006605308968573809\n",
      "Epoch 75, Batch 78 Loss:0.011271107941865921\n",
      "Epoch 75, Batch 79 Loss:0.03272119536995888\n",
      "Epoch 75, Batch 80 Loss:0.006692210212349892\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75, Batch 81 Loss:0.01725579798221588\n",
      "Epoch 75, Batch 82 Loss:0.030385777354240417\n",
      "Epoch 75, Batch 83 Loss:0.009223478846251965\n",
      "Epoch 75, Batch 84 Loss:0.004709227010607719\n",
      "Epoch 75, Batch 85 Loss:0.002793017541989684\n",
      "Epoch 75, Batch 86 Loss:0.03770218417048454\n",
      "Epoch 75, Batch 87 Loss:0.05632783845067024\n",
      "Epoch 75, Batch 88 Loss:0.0027963188476860523\n",
      "Epoch 75, Batch 89 Loss:0.012816264294087887\n",
      "Epoch 75, Batch 90 Loss:0.01896948553621769\n",
      "Epoch 75, Batch 91 Loss:0.035569921135902405\n",
      "Epoch 75, Batch 92 Loss:0.0027248344849795103\n",
      "Epoch 75, Batch 93 Loss:0.0038155431393533945\n",
      "Epoch 75, Batch 94 Loss:0.006724084261804819\n",
      "Epoch 75, Batch 95 Loss:0.02509869821369648\n",
      "Epoch 75, Batch 96 Loss:0.007224944420158863\n",
      "Epoch 75, Batch 97 Loss:0.006894286721944809\n",
      "Epoch 75, Batch 98 Loss:0.010741794481873512\n",
      "Epoch 75, Batch 99 Loss:0.005080723203718662\n",
      "Epoch 75, Batch 100 Loss:0.01940753124654293\n",
      "Epoch 75, Batch 101 Loss:0.0042020161636173725\n",
      "Epoch 75, Batch 102 Loss:0.029193343594670296\n",
      "Epoch 75, Batch 103 Loss:0.004394157789647579\n",
      "Epoch 75, Batch 104 Loss:0.0030632512643933296\n",
      "Epoch 75, Batch 105 Loss:0.008570837788283825\n",
      "Epoch 75, Batch 106 Loss:0.0068229567259550095\n",
      "Epoch 75, Batch 107 Loss:0.011831710115075111\n",
      "Epoch 75, Batch 108 Loss:0.009896276518702507\n",
      "Epoch 75, Batch 109 Loss:0.007939279079437256\n",
      "Epoch 75, Batch 110 Loss:0.030173426494002342\n",
      "Epoch 75, Batch 111 Loss:0.015999503433704376\n",
      "Epoch 75, Batch 112 Loss:0.008023450151085854\n",
      "Epoch 75, Batch 113 Loss:0.0020476453937590122\n",
      "Epoch 75, Batch 114 Loss:0.01063480507582426\n",
      "Epoch 75, Batch 115 Loss:0.034310463815927505\n",
      "Epoch 75, Batch 116 Loss:0.00939115509390831\n",
      "Epoch 75, Batch 117 Loss:0.006409783381968737\n",
      "Epoch 75, Batch 118 Loss:0.010489099659025669\n",
      "Epoch 75, Batch 119 Loss:0.006941014435142279\n",
      "Epoch 75, Batch 120 Loss:0.006177449133247137\n",
      "Epoch 75, Batch 121 Loss:0.006198915187269449\n",
      "Epoch 75, Batch 122 Loss:0.007517114281654358\n",
      "Epoch 75, Batch 123 Loss:0.014494908042252064\n",
      "Epoch 75, Batch 124 Loss:0.004519193433225155\n",
      "Epoch 75, Batch 125 Loss:0.0030185820069164038\n",
      "Epoch 75, Batch 126 Loss:0.004683276172727346\n",
      "Epoch 75, Batch 127 Loss:0.0025505833327770233\n",
      "Epoch 75, Batch 128 Loss:0.005840660072863102\n",
      "Epoch 75, Batch 129 Loss:0.004806223791092634\n",
      "Epoch 75, Batch 130 Loss:0.005877532996237278\n",
      "Epoch 75, Batch 131 Loss:0.014831333421170712\n",
      "Epoch 75, Batch 132 Loss:0.013110408559441566\n",
      "Epoch 75, Batch 133 Loss:0.0534185953438282\n",
      "Epoch 75, Batch 134 Loss:0.017465151846408844\n",
      "Epoch 75, Batch 135 Loss:0.021545812487602234\n",
      "Epoch 75, Batch 136 Loss:0.017228394746780396\n",
      "Epoch 75, Batch 137 Loss:0.0039269765838980675\n",
      "Epoch 75, Batch 138 Loss:0.01053742878139019\n",
      "Epoch 75, Batch 139 Loss:0.00966255646198988\n",
      "Epoch 75, Batch 140 Loss:0.0198060292750597\n",
      "Epoch 75, Batch 141 Loss:0.004724962171167135\n",
      "Epoch 75, Batch 142 Loss:0.04214171692728996\n",
      "Epoch 75, Batch 143 Loss:0.02483704313635826\n",
      "Epoch 75, Batch 144 Loss:0.018005304038524628\n",
      "Epoch 75, Batch 145 Loss:0.011803530156612396\n",
      "Epoch 75, Batch 146 Loss:0.029999006539583206\n",
      "Epoch 75, Batch 147 Loss:0.07617831230163574\n",
      "Epoch 75, Batch 148 Loss:0.008545893244445324\n",
      "Epoch 75, Batch 149 Loss:0.03461151570081711\n",
      "Epoch 75, Batch 150 Loss:0.0045653716661036015\n",
      "Epoch 75, Batch 151 Loss:0.00687304325401783\n",
      "Epoch 75, Batch 152 Loss:0.028512628749012947\n",
      "Epoch 75, Batch 153 Loss:0.019631657749414444\n",
      "Epoch 75, Batch 154 Loss:0.027156172320246696\n",
      "Epoch 75, Batch 155 Loss:0.018490198999643326\n",
      "Epoch 75, Batch 156 Loss:0.03411422297358513\n",
      "Epoch 75, Batch 157 Loss:0.01830669492483139\n",
      "Epoch 75, Batch 158 Loss:0.008678951300680637\n",
      "Epoch 75, Batch 159 Loss:0.023146944120526314\n",
      "Epoch 75, Batch 160 Loss:0.0211503729224205\n",
      "Epoch 75, Batch 161 Loss:0.011904841288924217\n",
      "Epoch 75, Batch 162 Loss:0.004305069334805012\n",
      "Epoch 75, Batch 163 Loss:0.012599736452102661\n",
      "Epoch 75, Batch 164 Loss:0.010523639619350433\n",
      "Epoch 75, Batch 165 Loss:0.018581582233309746\n",
      "Epoch 75, Batch 166 Loss:0.04052586853504181\n",
      "Epoch 75, Batch 167 Loss:0.028530649840831757\n",
      "Epoch 75, Batch 168 Loss:0.006698804907500744\n",
      "Epoch 75, Batch 169 Loss:0.019466016441583633\n",
      "Epoch 75, Batch 170 Loss:0.01948956586420536\n",
      "Epoch 75, Batch 171 Loss:0.029881317168474197\n",
      "Epoch 75, Batch 172 Loss:0.010441549122333527\n",
      "Epoch 75, Batch 173 Loss:0.008525003679096699\n",
      "Epoch 75, Batch 174 Loss:0.03300905600190163\n",
      "Epoch 75, Batch 175 Loss:0.010033313184976578\n",
      "Epoch 75, Batch 176 Loss:0.005459459498524666\n",
      "Epoch 75, Batch 177 Loss:0.007658906280994415\n",
      "Epoch 75, Batch 178 Loss:0.011316485702991486\n",
      "Epoch 75, Batch 179 Loss:0.009763773530721664\n",
      "Epoch 75, Batch 180 Loss:0.008429087698459625\n",
      "Epoch 75, Batch 181 Loss:0.01058080792427063\n",
      "Epoch 75, Batch 182 Loss:0.030198855325579643\n",
      "Epoch 75, Batch 183 Loss:0.023050811141729355\n",
      "Epoch 75, Batch 184 Loss:0.011750674806535244\n",
      "Epoch 75, Batch 185 Loss:0.022528082132339478\n",
      "Epoch 75, Batch 186 Loss:0.009550197049975395\n",
      "Epoch 75, Batch 187 Loss:0.022760089486837387\n",
      "Epoch 75, Batch 188 Loss:0.0276058167219162\n",
      "Epoch 75, Batch 189 Loss:0.004491942003369331\n",
      "Epoch 75, Batch 190 Loss:0.013895969837903976\n",
      "Epoch 75, Batch 191 Loss:0.02790118008852005\n",
      "Epoch 75, Batch 192 Loss:0.001943616196513176\n",
      "Epoch 75, Batch 193 Loss:0.007090101484209299\n",
      "Epoch 75, Batch 194 Loss:0.01705649308860302\n",
      "Epoch 75, Batch 195 Loss:0.02452019229531288\n",
      "Epoch 75, Batch 196 Loss:0.005260857753455639\n",
      "Epoch 75, Batch 197 Loss:0.005746687762439251\n",
      "Epoch 75, Batch 198 Loss:0.009946438483893871\n",
      "Epoch 75, Batch 199 Loss:0.01019631139934063\n",
      "Epoch 75, Batch 200 Loss:0.0300715621560812\n",
      "Epoch 75, Batch 201 Loss:0.04807833582162857\n",
      "Epoch 75, Batch 202 Loss:0.017818480730056763\n",
      "Epoch 75, Batch 203 Loss:0.01390549074858427\n",
      "Epoch 75, Batch 204 Loss:0.05474896728992462\n",
      "Epoch 75, Batch 205 Loss:0.023367276415228844\n",
      "Epoch 75, Batch 206 Loss:0.018796702846884727\n",
      "Epoch 75, Batch 207 Loss:0.009441620670258999\n",
      "Epoch 75, Batch 208 Loss:0.008084094151854515\n",
      "Epoch 75, Batch 209 Loss:0.022479064762592316\n",
      "Epoch 75, Batch 210 Loss:0.024031145498156548\n",
      "Epoch 75, Batch 211 Loss:0.014447801746428013\n",
      "Epoch 75, Batch 212 Loss:0.048625484108924866\n",
      "Epoch 75, Batch 213 Loss:0.04680117592215538\n",
      "Epoch 75, Batch 214 Loss:0.017268134281039238\n",
      "Epoch 75, Batch 215 Loss:0.012771716341376305\n",
      "Epoch 75, Batch 216 Loss:0.013163464143872261\n",
      "Epoch 75, Batch 217 Loss:0.0201430581510067\n",
      "Epoch 75, Batch 218 Loss:0.0783725380897522\n",
      "Epoch 75, Batch 219 Loss:0.05713290721178055\n",
      "Epoch 75, Batch 220 Loss:0.016810394823551178\n",
      "Epoch 75, Batch 221 Loss:0.032185107469558716\n",
      "Epoch 75, Batch 222 Loss:0.03339064121246338\n",
      "Epoch 75, Batch 223 Loss:0.028393087908625603\n",
      "Epoch 75, Batch 224 Loss:0.010444557294249535\n",
      "Epoch 75, Batch 225 Loss:0.0329204760491848\n",
      "Epoch 75, Batch 226 Loss:0.049216121435165405\n",
      "Epoch 75, Batch 227 Loss:0.021700667217373848\n",
      "Epoch 75, Batch 228 Loss:0.03517139330506325\n",
      "Epoch 75, Batch 229 Loss:0.007574558723717928\n",
      "Epoch 75, Batch 230 Loss:0.04167508706450462\n",
      "Epoch 75, Batch 231 Loss:0.05383521318435669\n",
      "Epoch 75, Batch 232 Loss:0.009696798399090767\n",
      "Epoch 75, Batch 233 Loss:0.016517186537384987\n",
      "Loss in this Epoch is: 1.65171865374 %\n",
      "Accuracy in this Epoch is: 88.4299993515 %\n",
      "Epoch 76, Batch 0 Loss:0.024566082283854485\n",
      "Epoch 76, Batch 1 Loss:0.03275790438055992\n",
      "Epoch 76, Batch 2 Loss:0.010210034437477589\n",
      "Epoch 76, Batch 3 Loss:0.02005675435066223\n",
      "Epoch 76, Batch 4 Loss:0.01030946895480156\n",
      "Epoch 76, Batch 5 Loss:0.014664895832538605\n",
      "Epoch 76, Batch 6 Loss:0.018395109102129936\n",
      "Epoch 76, Batch 7 Loss:0.01736840233206749\n",
      "Epoch 76, Batch 8 Loss:0.0771583765745163\n",
      "Epoch 76, Batch 9 Loss:0.015269541181623936\n",
      "Epoch 76, Batch 10 Loss:0.01612086594104767\n",
      "Epoch 76, Batch 11 Loss:0.035967662930488586\n",
      "Epoch 76, Batch 12 Loss:0.03357771039009094\n",
      "Epoch 76, Batch 13 Loss:0.018350230529904366\n",
      "Epoch 76, Batch 14 Loss:0.014881293289363384\n",
      "Epoch 76, Batch 15 Loss:0.021720079705119133\n",
      "Epoch 76, Batch 16 Loss:0.02272888831794262\n",
      "Epoch 76, Batch 17 Loss:0.02324013039469719\n",
      "Epoch 76, Batch 18 Loss:0.007080651354044676\n",
      "Epoch 76, Batch 19 Loss:0.009189223870635033\n",
      "Epoch 76, Batch 20 Loss:0.009432913735508919\n",
      "Epoch 76, Batch 21 Loss:0.007815741002559662\n",
      "Epoch 76, Batch 22 Loss:0.012864834628999233\n",
      "Epoch 76, Batch 23 Loss:0.0417967215180397\n",
      "Epoch 76, Batch 24 Loss:0.006021252367645502\n",
      "Epoch 76, Batch 25 Loss:0.023349205031991005\n",
      "Epoch 76, Batch 26 Loss:0.014011967927217484\n",
      "Epoch 76, Batch 27 Loss:0.010496790520846844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Batch 28 Loss:0.00652895076200366\n",
      "Epoch 76, Batch 29 Loss:0.007323519326746464\n",
      "Epoch 76, Batch 30 Loss:0.0039331791922450066\n",
      "Epoch 76, Batch 31 Loss:0.017930932343006134\n",
      "Epoch 76, Batch 32 Loss:0.004005156457424164\n",
      "Epoch 76, Batch 33 Loss:0.012631861492991447\n",
      "Epoch 76, Batch 34 Loss:0.010891880840063095\n",
      "Epoch 76, Batch 35 Loss:0.011162268929183483\n",
      "Epoch 76, Batch 36 Loss:0.009642290882766247\n",
      "Epoch 76, Batch 37 Loss:0.02819526195526123\n",
      "Epoch 76, Batch 38 Loss:0.0033343525137752295\n",
      "Epoch 76, Batch 39 Loss:0.002463236916810274\n",
      "Epoch 76, Batch 40 Loss:0.008560421876609325\n",
      "Epoch 76, Batch 41 Loss:0.008894095197319984\n",
      "Epoch 76, Batch 42 Loss:0.007887042127549648\n",
      "Epoch 76, Batch 43 Loss:0.004011824727058411\n",
      "Epoch 76, Batch 44 Loss:0.015366845764219761\n",
      "Epoch 76, Batch 45 Loss:0.022683264687657356\n",
      "Epoch 76, Batch 46 Loss:0.009864489547908306\n",
      "Epoch 76, Batch 47 Loss:0.0077586243860423565\n",
      "Epoch 76, Batch 48 Loss:0.01703975349664688\n",
      "Epoch 76, Batch 49 Loss:0.031054381281137466\n",
      "Epoch 76, Batch 50 Loss:0.01170254498720169\n",
      "Epoch 76, Batch 51 Loss:0.01681770570576191\n",
      "Epoch 76, Batch 52 Loss:0.006789267063140869\n",
      "Epoch 76, Batch 53 Loss:0.0079857362434268\n",
      "Epoch 76, Batch 54 Loss:0.003307066857814789\n",
      "Epoch 76, Batch 55 Loss:0.014773120172321796\n",
      "Epoch 76, Batch 56 Loss:0.008754481561481953\n",
      "Epoch 76, Batch 57 Loss:0.023603785783052444\n",
      "Epoch 76, Batch 58 Loss:0.005885072983801365\n",
      "Epoch 76, Batch 59 Loss:0.006819084286689758\n",
      "Epoch 76, Batch 60 Loss:0.00401966692879796\n",
      "Epoch 76, Batch 61 Loss:0.006880141794681549\n",
      "Epoch 76, Batch 62 Loss:0.0064263842068612576\n",
      "Epoch 76, Batch 63 Loss:0.006823109462857246\n",
      "Epoch 76, Batch 64 Loss:0.014612143859267235\n",
      "Epoch 76, Batch 65 Loss:0.02065480872988701\n",
      "Epoch 76, Batch 66 Loss:0.00782898161560297\n",
      "Epoch 76, Batch 67 Loss:0.034340981394052505\n",
      "Epoch 76, Batch 68 Loss:0.006486306898295879\n",
      "Epoch 76, Batch 69 Loss:0.04163520410656929\n",
      "Epoch 76, Batch 70 Loss:0.018708691000938416\n",
      "Epoch 76, Batch 71 Loss:0.007090425118803978\n",
      "Epoch 76, Batch 72 Loss:0.04597577825188637\n",
      "Epoch 76, Batch 73 Loss:0.0034305956214666367\n",
      "Epoch 76, Batch 74 Loss:0.005340239033102989\n",
      "Epoch 76, Batch 75 Loss:0.03698939457535744\n",
      "Epoch 76, Batch 76 Loss:0.01781988888978958\n",
      "Epoch 76, Batch 77 Loss:0.013966446742415428\n",
      "Epoch 76, Batch 78 Loss:0.010484921745955944\n",
      "Epoch 76, Batch 79 Loss:0.031051533296704292\n",
      "Epoch 76, Batch 80 Loss:0.021079055964946747\n",
      "Epoch 76, Batch 81 Loss:0.024266421794891357\n",
      "Epoch 76, Batch 82 Loss:0.006476361304521561\n",
      "Epoch 76, Batch 83 Loss:0.007040694355964661\n",
      "Epoch 76, Batch 84 Loss:0.013063294813036919\n",
      "Epoch 76, Batch 85 Loss:0.00610183784738183\n",
      "Epoch 76, Batch 86 Loss:0.0029774578288197517\n",
      "Epoch 76, Batch 87 Loss:0.03426588326692581\n",
      "Epoch 76, Batch 88 Loss:0.006715794093906879\n",
      "Epoch 76, Batch 89 Loss:0.01674187369644642\n",
      "Epoch 76, Batch 90 Loss:0.031061064451932907\n",
      "Epoch 76, Batch 91 Loss:0.015571474097669125\n",
      "Epoch 76, Batch 92 Loss:0.01519075222313404\n",
      "Epoch 76, Batch 93 Loss:0.04959294945001602\n",
      "Epoch 76, Batch 94 Loss:0.01973916031420231\n",
      "Epoch 76, Batch 95 Loss:0.02032361924648285\n",
      "Epoch 76, Batch 96 Loss:0.014712313190102577\n",
      "Epoch 76, Batch 97 Loss:0.03993283212184906\n",
      "Epoch 76, Batch 98 Loss:0.012552893720567226\n",
      "Epoch 76, Batch 99 Loss:0.018820103257894516\n",
      "Epoch 76, Batch 100 Loss:0.06200648844242096\n",
      "Epoch 76, Batch 101 Loss:0.0172344408929348\n",
      "Epoch 76, Batch 102 Loss:0.035314809530973434\n",
      "Epoch 76, Batch 103 Loss:0.01546449214220047\n",
      "Epoch 76, Batch 104 Loss:0.023772284388542175\n",
      "Epoch 76, Batch 105 Loss:0.02812138758599758\n",
      "Epoch 76, Batch 106 Loss:0.018167056143283844\n",
      "Epoch 76, Batch 107 Loss:0.04972756654024124\n",
      "Epoch 76, Batch 108 Loss:0.06721578538417816\n",
      "Epoch 76, Batch 109 Loss:0.046557165682315826\n",
      "Epoch 76, Batch 110 Loss:0.02742912620306015\n",
      "Epoch 76, Batch 111 Loss:0.022511901333928108\n",
      "Epoch 76, Batch 112 Loss:0.11428681015968323\n",
      "Epoch 76, Batch 113 Loss:0.05482715740799904\n",
      "Epoch 76, Batch 114 Loss:0.02450154721736908\n",
      "Epoch 76, Batch 115 Loss:0.041701894253492355\n",
      "Epoch 76, Batch 116 Loss:0.03869368135929108\n",
      "Epoch 76, Batch 117 Loss:0.009135350584983826\n",
      "Epoch 76, Batch 118 Loss:0.05137117952108383\n",
      "Epoch 76, Batch 119 Loss:0.03292539343237877\n",
      "Epoch 76, Batch 120 Loss:0.026744533330202103\n",
      "Epoch 76, Batch 121 Loss:0.018315348774194717\n",
      "Epoch 76, Batch 122 Loss:0.03641386330127716\n",
      "Epoch 76, Batch 123 Loss:0.016604624688625336\n",
      "Epoch 76, Batch 124 Loss:0.05566456913948059\n",
      "Epoch 76, Batch 125 Loss:0.04816699028015137\n",
      "Epoch 76, Batch 126 Loss:0.21797581017017365\n",
      "Epoch 76, Batch 127 Loss:0.03231801092624664\n",
      "Epoch 76, Batch 128 Loss:0.024686040356755257\n",
      "Epoch 76, Batch 129 Loss:0.014071401208639145\n",
      "Epoch 76, Batch 130 Loss:0.06098707765340805\n",
      "Epoch 76, Batch 131 Loss:0.04331998527050018\n",
      "Epoch 76, Batch 132 Loss:0.015832582488656044\n",
      "Epoch 76, Batch 133 Loss:0.030284054577350616\n",
      "Epoch 76, Batch 134 Loss:0.058232877403497696\n",
      "Epoch 76, Batch 135 Loss:0.009601479396224022\n",
      "Epoch 76, Batch 136 Loss:0.02533090114593506\n",
      "Epoch 76, Batch 137 Loss:0.014936154708266258\n",
      "Epoch 76, Batch 138 Loss:0.04972229152917862\n",
      "Epoch 76, Batch 139 Loss:0.03381495177745819\n",
      "Epoch 76, Batch 140 Loss:0.04440644383430481\n",
      "Epoch 76, Batch 141 Loss:0.033389877527952194\n",
      "Epoch 76, Batch 142 Loss:0.03787248209118843\n",
      "Epoch 76, Batch 143 Loss:0.036253757774829865\n",
      "Epoch 76, Batch 144 Loss:0.05315554141998291\n",
      "Epoch 76, Batch 145 Loss:0.06879955530166626\n",
      "Epoch 76, Batch 146 Loss:0.05577543377876282\n",
      "Epoch 76, Batch 147 Loss:0.05239228159189224\n",
      "Epoch 76, Batch 148 Loss:0.034914568066596985\n",
      "Epoch 76, Batch 149 Loss:0.05023137852549553\n",
      "Epoch 76, Batch 150 Loss:0.03158466890454292\n",
      "Epoch 76, Batch 151 Loss:0.01912076398730278\n",
      "Epoch 76, Batch 152 Loss:0.036267299205064774\n",
      "Epoch 76, Batch 153 Loss:0.024798136204481125\n",
      "Epoch 76, Batch 154 Loss:0.07658671587705612\n",
      "Epoch 76, Batch 155 Loss:0.049791716039180756\n",
      "Epoch 76, Batch 156 Loss:0.018603269010782242\n",
      "Epoch 76, Batch 157 Loss:0.02015632390975952\n",
      "Epoch 76, Batch 158 Loss:0.05262468755245209\n",
      "Epoch 76, Batch 159 Loss:0.024510404095053673\n",
      "Epoch 76, Batch 160 Loss:0.027303453534841537\n",
      "Epoch 76, Batch 161 Loss:0.054171860218048096\n",
      "Epoch 76, Batch 162 Loss:0.04207650572061539\n",
      "Epoch 76, Batch 163 Loss:0.04695110023021698\n",
      "Epoch 76, Batch 164 Loss:0.031455881893634796\n",
      "Epoch 76, Batch 165 Loss:0.010174471884965897\n",
      "Epoch 76, Batch 166 Loss:0.025987543165683746\n",
      "Epoch 76, Batch 167 Loss:0.036076001822948456\n",
      "Epoch 76, Batch 168 Loss:0.0172092504799366\n",
      "Epoch 76, Batch 169 Loss:0.06057808920741081\n",
      "Epoch 76, Batch 170 Loss:0.03356897085905075\n",
      "Epoch 76, Batch 171 Loss:0.03482351824641228\n",
      "Epoch 76, Batch 172 Loss:0.010665320791304111\n",
      "Epoch 76, Batch 173 Loss:0.031893953680992126\n",
      "Epoch 76, Batch 174 Loss:0.034402698278427124\n",
      "Epoch 76, Batch 175 Loss:0.018157586455345154\n",
      "Epoch 76, Batch 176 Loss:0.04101722687482834\n",
      "Epoch 76, Batch 177 Loss:0.013200437650084496\n",
      "Epoch 76, Batch 178 Loss:0.09869439899921417\n",
      "Epoch 76, Batch 179 Loss:0.03497746214270592\n",
      "Epoch 76, Batch 180 Loss:0.0351656898856163\n",
      "Epoch 76, Batch 181 Loss:0.08870120346546173\n",
      "Epoch 76, Batch 182 Loss:0.057554297149181366\n",
      "Epoch 76, Batch 183 Loss:0.016042940318584442\n",
      "Epoch 76, Batch 184 Loss:0.04147975519299507\n",
      "Epoch 76, Batch 185 Loss:0.023051436990499496\n",
      "Epoch 76, Batch 186 Loss:0.022261232137680054\n",
      "Epoch 76, Batch 187 Loss:0.01915723644196987\n",
      "Epoch 76, Batch 188 Loss:0.018012233078479767\n",
      "Epoch 76, Batch 189 Loss:0.05045534670352936\n",
      "Epoch 76, Batch 190 Loss:0.024200694635510445\n",
      "Epoch 76, Batch 191 Loss:0.06599798798561096\n",
      "Epoch 76, Batch 192 Loss:0.05106848478317261\n",
      "Epoch 76, Batch 193 Loss:0.03171473741531372\n",
      "Epoch 76, Batch 194 Loss:0.05063951388001442\n",
      "Epoch 76, Batch 195 Loss:0.0670822262763977\n",
      "Epoch 76, Batch 196 Loss:0.038466472178697586\n",
      "Epoch 76, Batch 197 Loss:0.036595750600099564\n",
      "Epoch 76, Batch 198 Loss:0.08852370828390121\n",
      "Epoch 76, Batch 199 Loss:0.03853573650121689\n",
      "Epoch 76, Batch 200 Loss:0.02124478667974472\n",
      "Epoch 76, Batch 201 Loss:0.015849268063902855\n",
      "Epoch 76, Batch 202 Loss:0.038232021033763885\n",
      "Epoch 76, Batch 203 Loss:0.056711383163928986\n",
      "Epoch 76, Batch 204 Loss:0.03146228939294815\n",
      "Epoch 76, Batch 205 Loss:0.019030075520277023\n",
      "Epoch 76, Batch 206 Loss:0.015919240191578865\n",
      "Epoch 76, Batch 207 Loss:0.014920588582754135\n",
      "Epoch 76, Batch 208 Loss:0.01569289155304432\n",
      "Epoch 76, Batch 209 Loss:0.0209113247692585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76, Batch 210 Loss:0.006863967515528202\n",
      "Epoch 76, Batch 211 Loss:0.022173652425408363\n",
      "Epoch 76, Batch 212 Loss:0.03272949904203415\n",
      "Epoch 76, Batch 213 Loss:0.02233268693089485\n",
      "Epoch 76, Batch 214 Loss:0.016274118795990944\n",
      "Epoch 76, Batch 215 Loss:0.05988854169845581\n",
      "Epoch 76, Batch 216 Loss:0.014311116188764572\n",
      "Epoch 76, Batch 217 Loss:0.032332222908735275\n",
      "Epoch 76, Batch 218 Loss:0.017223384231328964\n",
      "Epoch 76, Batch 219 Loss:0.04721509665250778\n",
      "Epoch 76, Batch 220 Loss:0.02107344940304756\n",
      "Epoch 76, Batch 221 Loss:0.010447554290294647\n",
      "Epoch 76, Batch 222 Loss:0.006321291904896498\n",
      "Epoch 76, Batch 223 Loss:0.006560721900314093\n",
      "Epoch 76, Batch 224 Loss:0.05127368122339249\n",
      "Epoch 76, Batch 225 Loss:0.020535744726657867\n",
      "Epoch 76, Batch 226 Loss:0.0531025230884552\n",
      "Epoch 76, Batch 227 Loss:0.022802315652370453\n",
      "Epoch 76, Batch 228 Loss:0.011055968701839447\n",
      "Epoch 76, Batch 229 Loss:0.027723900973796844\n",
      "Epoch 76, Batch 230 Loss:0.044499754905700684\n",
      "Epoch 76, Batch 231 Loss:0.017314299941062927\n",
      "Epoch 76, Batch 232 Loss:0.012831749394536018\n",
      "Epoch 76, Batch 233 Loss:0.027499068528413773\n",
      "Loss in this Epoch is: 2.74990685284 %\n",
      "Accuracy in this Epoch is: 88.480001688 %\n",
      "Epoch 77, Batch 0 Loss:0.007463686168193817\n",
      "Epoch 77, Batch 1 Loss:0.011284774169325829\n",
      "Epoch 77, Batch 2 Loss:0.004496656823903322\n",
      "Epoch 77, Batch 3 Loss:0.009706482291221619\n",
      "Epoch 77, Batch 4 Loss:0.006773331668227911\n",
      "Epoch 77, Batch 5 Loss:0.01975833810865879\n",
      "Epoch 77, Batch 6 Loss:0.023186175152659416\n",
      "Epoch 77, Batch 7 Loss:0.010376954451203346\n",
      "Epoch 77, Batch 8 Loss:0.0051260460168123245\n",
      "Epoch 77, Batch 9 Loss:0.01203969493508339\n",
      "Epoch 77, Batch 10 Loss:0.004441737197339535\n",
      "Epoch 77, Batch 11 Loss:0.00932143721729517\n",
      "Epoch 77, Batch 12 Loss:0.009397396817803383\n",
      "Epoch 77, Batch 13 Loss:0.013965237885713577\n",
      "Epoch 77, Batch 14 Loss:0.0038605511654168367\n",
      "Epoch 77, Batch 15 Loss:0.013564830645918846\n",
      "Epoch 77, Batch 16 Loss:0.007714450359344482\n",
      "Epoch 77, Batch 17 Loss:0.008958777412772179\n",
      "Epoch 77, Batch 18 Loss:0.017187483608722687\n",
      "Epoch 77, Batch 19 Loss:0.004724225495010614\n",
      "Epoch 77, Batch 20 Loss:0.00833181757479906\n",
      "Epoch 77, Batch 21 Loss:0.0332435742020607\n",
      "Epoch 77, Batch 22 Loss:0.040855955332517624\n",
      "Epoch 77, Batch 23 Loss:0.009707094170153141\n",
      "Epoch 77, Batch 24 Loss:0.013788516633212566\n",
      "Epoch 77, Batch 25 Loss:0.01631554216146469\n",
      "Epoch 77, Batch 26 Loss:0.0037393560633063316\n",
      "Epoch 77, Batch 27 Loss:0.048727843910455704\n",
      "Epoch 77, Batch 28 Loss:0.023524614050984383\n",
      "Epoch 77, Batch 29 Loss:0.01748504303395748\n",
      "Epoch 77, Batch 30 Loss:0.009683279320597649\n",
      "Epoch 77, Batch 31 Loss:0.014773695729672909\n",
      "Epoch 77, Batch 32 Loss:0.02189970389008522\n",
      "Epoch 77, Batch 33 Loss:0.005486825946718454\n",
      "Epoch 77, Batch 34 Loss:0.012418105266988277\n",
      "Epoch 77, Batch 35 Loss:0.007546180859208107\n",
      "Epoch 77, Batch 36 Loss:0.01464741863310337\n",
      "Epoch 77, Batch 37 Loss:0.0032021424267441034\n",
      "Epoch 77, Batch 38 Loss:0.013025395572185516\n",
      "Epoch 77, Batch 39 Loss:0.01115062739700079\n",
      "Epoch 77, Batch 40 Loss:0.008527486585080624\n",
      "Epoch 77, Batch 41 Loss:0.010318981483578682\n",
      "Epoch 77, Batch 42 Loss:0.02588282898068428\n",
      "Epoch 77, Batch 43 Loss:0.009346296079456806\n",
      "Epoch 77, Batch 44 Loss:0.015333333984017372\n",
      "Epoch 77, Batch 45 Loss:0.01879088208079338\n",
      "Epoch 77, Batch 46 Loss:0.007856090553104877\n",
      "Epoch 77, Batch 47 Loss:0.005074927117675543\n",
      "Epoch 77, Batch 48 Loss:0.01317316573113203\n",
      "Epoch 77, Batch 49 Loss:0.016182916238904\n",
      "Epoch 77, Batch 50 Loss:0.003173254895955324\n",
      "Epoch 77, Batch 51 Loss:0.003213942749425769\n",
      "Epoch 77, Batch 52 Loss:0.004376567434519529\n",
      "Epoch 77, Batch 53 Loss:0.005316347815096378\n",
      "Epoch 77, Batch 54 Loss:0.005428774282336235\n",
      "Epoch 77, Batch 55 Loss:0.01895921677350998\n",
      "Epoch 77, Batch 56 Loss:0.014331555925309658\n",
      "Epoch 77, Batch 57 Loss:0.013803993351757526\n",
      "Epoch 77, Batch 58 Loss:0.02657313644886017\n",
      "Epoch 77, Batch 59 Loss:0.006338654085993767\n",
      "Epoch 77, Batch 60 Loss:0.057234026491642\n",
      "Epoch 77, Batch 61 Loss:0.022920740768313408\n",
      "Epoch 77, Batch 62 Loss:0.007768326438963413\n",
      "Epoch 77, Batch 63 Loss:0.012662569992244244\n",
      "Epoch 77, Batch 64 Loss:0.0043866680935025215\n",
      "Epoch 77, Batch 65 Loss:0.004848720505833626\n",
      "Epoch 77, Batch 66 Loss:0.005668896250426769\n",
      "Epoch 77, Batch 67 Loss:0.026971498504281044\n",
      "Epoch 77, Batch 68 Loss:0.004990519490092993\n",
      "Epoch 77, Batch 69 Loss:0.004375936463475227\n",
      "Epoch 77, Batch 70 Loss:0.0046308389864861965\n",
      "Epoch 77, Batch 71 Loss:0.046299368143081665\n",
      "Epoch 77, Batch 72 Loss:0.029878556728363037\n",
      "Epoch 77, Batch 73 Loss:0.0032534694764763117\n",
      "Epoch 77, Batch 74 Loss:0.004114503040909767\n",
      "Epoch 77, Batch 75 Loss:0.009679747745394707\n",
      "Epoch 77, Batch 76 Loss:0.0052569275721907616\n",
      "Epoch 77, Batch 77 Loss:0.0104005616158247\n",
      "Epoch 77, Batch 78 Loss:0.00886501930654049\n",
      "Epoch 77, Batch 79 Loss:0.03703716769814491\n",
      "Epoch 77, Batch 80 Loss:0.011187595315277576\n",
      "Epoch 77, Batch 81 Loss:0.01788579486310482\n",
      "Epoch 77, Batch 82 Loss:0.02166159823536873\n",
      "Epoch 77, Batch 83 Loss:0.014960340224206448\n",
      "Epoch 77, Batch 84 Loss:0.006957042962312698\n",
      "Epoch 77, Batch 85 Loss:0.0028635410126298666\n",
      "Epoch 77, Batch 86 Loss:0.008649648167192936\n",
      "Epoch 77, Batch 87 Loss:0.006488119252026081\n",
      "Epoch 77, Batch 88 Loss:0.0037184578832238913\n",
      "Epoch 77, Batch 89 Loss:0.0048315394669771194\n",
      "Epoch 77, Batch 90 Loss:0.012263882905244827\n",
      "Epoch 77, Batch 91 Loss:0.015548644587397575\n",
      "Epoch 77, Batch 92 Loss:0.007038833573460579\n",
      "Epoch 77, Batch 93 Loss:0.01746542751789093\n",
      "Epoch 77, Batch 94 Loss:0.003990853205323219\n",
      "Epoch 77, Batch 95 Loss:0.03579585254192352\n",
      "Epoch 77, Batch 96 Loss:0.022344261407852173\n",
      "Epoch 77, Batch 97 Loss:0.0018658936023712158\n",
      "Epoch 77, Batch 98 Loss:0.008392726071178913\n",
      "Epoch 77, Batch 99 Loss:0.007405427750200033\n",
      "Epoch 77, Batch 100 Loss:0.01780712604522705\n",
      "Epoch 77, Batch 101 Loss:0.005226728972047567\n",
      "Epoch 77, Batch 102 Loss:0.006984431762248278\n",
      "Epoch 77, Batch 103 Loss:0.01012873649597168\n",
      "Epoch 77, Batch 104 Loss:0.009872978553175926\n",
      "Epoch 77, Batch 105 Loss:0.0061778901144862175\n",
      "Epoch 77, Batch 106 Loss:0.006196882575750351\n",
      "Epoch 77, Batch 107 Loss:0.002908725757151842\n",
      "Epoch 77, Batch 108 Loss:0.011910493485629559\n",
      "Epoch 77, Batch 109 Loss:0.0027280226349830627\n",
      "Epoch 77, Batch 110 Loss:0.0056503708474338055\n",
      "Epoch 77, Batch 111 Loss:0.00667208805680275\n",
      "Epoch 77, Batch 112 Loss:0.010431822389364243\n",
      "Epoch 77, Batch 113 Loss:0.01004971656948328\n",
      "Epoch 77, Batch 114 Loss:0.02299514040350914\n",
      "Epoch 77, Batch 115 Loss:0.011874165385961533\n",
      "Epoch 77, Batch 116 Loss:0.00785108096897602\n",
      "Epoch 77, Batch 117 Loss:0.009833546355366707\n",
      "Epoch 77, Batch 118 Loss:0.004651925060898066\n",
      "Epoch 77, Batch 119 Loss:0.02253924496471882\n",
      "Epoch 77, Batch 120 Loss:0.01244456134736538\n",
      "Epoch 77, Batch 121 Loss:0.00959131121635437\n",
      "Epoch 77, Batch 122 Loss:0.006304099690169096\n",
      "Epoch 77, Batch 123 Loss:0.006650014780461788\n",
      "Epoch 77, Batch 124 Loss:0.015195016749203205\n",
      "Epoch 77, Batch 125 Loss:0.006279272027313709\n",
      "Epoch 77, Batch 126 Loss:0.012406522408127785\n",
      "Epoch 77, Batch 127 Loss:0.07244240492582321\n",
      "Epoch 77, Batch 128 Loss:0.005195776931941509\n",
      "Epoch 77, Batch 129 Loss:0.002717600902542472\n",
      "Epoch 77, Batch 130 Loss:0.007058168761432171\n",
      "Epoch 77, Batch 131 Loss:0.004170773085206747\n",
      "Epoch 77, Batch 132 Loss:0.008889500051736832\n",
      "Epoch 77, Batch 133 Loss:0.041447803378105164\n",
      "Epoch 77, Batch 134 Loss:0.012961233034729958\n",
      "Epoch 77, Batch 135 Loss:0.008852832950651646\n",
      "Epoch 77, Batch 136 Loss:0.005552315153181553\n",
      "Epoch 77, Batch 137 Loss:0.005558476783335209\n",
      "Epoch 77, Batch 138 Loss:0.022367112338542938\n",
      "Epoch 77, Batch 139 Loss:0.02478082850575447\n",
      "Epoch 77, Batch 140 Loss:0.004992557689547539\n",
      "Epoch 77, Batch 141 Loss:0.0032267780043184757\n",
      "Epoch 77, Batch 142 Loss:0.021004682406783104\n",
      "Epoch 77, Batch 143 Loss:0.01747303456068039\n",
      "Epoch 77, Batch 144 Loss:0.002138626528903842\n",
      "Epoch 77, Batch 145 Loss:0.018820365890860558\n",
      "Epoch 77, Batch 146 Loss:0.042337995022535324\n",
      "Epoch 77, Batch 147 Loss:0.0787004679441452\n",
      "Epoch 77, Batch 148 Loss:0.01675710454583168\n",
      "Epoch 77, Batch 149 Loss:0.006926806643605232\n",
      "Epoch 77, Batch 150 Loss:0.05402977764606476\n",
      "Epoch 77, Batch 151 Loss:0.017261430621147156\n",
      "Epoch 77, Batch 152 Loss:0.0073294080793857574\n",
      "Epoch 77, Batch 153 Loss:0.021328940987586975\n",
      "Epoch 77, Batch 154 Loss:0.013938575983047485\n",
      "Epoch 77, Batch 155 Loss:0.010511036030948162\n",
      "Epoch 77, Batch 156 Loss:0.009059114381670952\n",
      "Epoch 77, Batch 157 Loss:0.01602490432560444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77, Batch 158 Loss:0.03472357243299484\n",
      "Epoch 77, Batch 159 Loss:0.022495048120617867\n",
      "Epoch 77, Batch 160 Loss:0.0351438894867897\n",
      "Epoch 77, Batch 161 Loss:0.031888991594314575\n",
      "Epoch 77, Batch 162 Loss:0.01071840152144432\n",
      "Epoch 77, Batch 163 Loss:0.010968193411827087\n",
      "Epoch 77, Batch 164 Loss:0.03704685717821121\n",
      "Epoch 77, Batch 165 Loss:0.09853899478912354\n",
      "Epoch 77, Batch 166 Loss:0.012092746794223785\n",
      "Epoch 77, Batch 167 Loss:0.011017382144927979\n",
      "Epoch 77, Batch 168 Loss:0.010406296700239182\n",
      "Epoch 77, Batch 169 Loss:0.020576752722263336\n",
      "Epoch 77, Batch 170 Loss:0.011437060311436653\n",
      "Epoch 77, Batch 171 Loss:0.039956800639629364\n",
      "Epoch 77, Batch 172 Loss:0.03814985603094101\n",
      "Epoch 77, Batch 173 Loss:0.04345241189002991\n",
      "Epoch 77, Batch 174 Loss:0.02583150751888752\n",
      "Epoch 77, Batch 175 Loss:0.01837944984436035\n",
      "Epoch 77, Batch 176 Loss:0.011504504829645157\n",
      "Epoch 77, Batch 177 Loss:0.032766859978437424\n",
      "Epoch 77, Batch 178 Loss:0.01886981539428234\n",
      "Epoch 77, Batch 179 Loss:0.031163251027464867\n",
      "Epoch 77, Batch 180 Loss:0.008564943447709084\n",
      "Epoch 77, Batch 181 Loss:0.019793644547462463\n",
      "Epoch 77, Batch 182 Loss:0.015842732042074203\n",
      "Epoch 77, Batch 183 Loss:0.01792418211698532\n",
      "Epoch 77, Batch 184 Loss:0.02704266831278801\n",
      "Epoch 77, Batch 185 Loss:0.01387715619057417\n",
      "Epoch 77, Batch 186 Loss:0.02246689237654209\n",
      "Epoch 77, Batch 187 Loss:0.014665592461824417\n",
      "Epoch 77, Batch 188 Loss:0.05183771252632141\n",
      "Epoch 77, Batch 189 Loss:0.035817064344882965\n",
      "Epoch 77, Batch 190 Loss:0.016263071447610855\n",
      "Epoch 77, Batch 191 Loss:0.03320954740047455\n",
      "Epoch 77, Batch 192 Loss:0.034539200365543365\n",
      "Epoch 77, Batch 193 Loss:0.02058083564043045\n",
      "Epoch 77, Batch 194 Loss:0.021310262382030487\n",
      "Epoch 77, Batch 195 Loss:0.008042938075959682\n",
      "Epoch 77, Batch 196 Loss:0.02737296000123024\n",
      "Epoch 77, Batch 197 Loss:0.009721627458930016\n",
      "Epoch 77, Batch 198 Loss:0.01018473319709301\n",
      "Epoch 77, Batch 199 Loss:0.011314582079648972\n",
      "Epoch 77, Batch 200 Loss:0.006497722119092941\n",
      "Epoch 77, Batch 201 Loss:0.04314551502466202\n",
      "Epoch 77, Batch 202 Loss:0.005290939472615719\n",
      "Epoch 77, Batch 203 Loss:0.02336612157523632\n",
      "Epoch 77, Batch 204 Loss:0.008571946993470192\n",
      "Epoch 77, Batch 205 Loss:0.009410877712070942\n",
      "Epoch 77, Batch 206 Loss:0.02872520312666893\n",
      "Epoch 77, Batch 207 Loss:0.015331743285059929\n",
      "Epoch 77, Batch 208 Loss:0.01033379603177309\n",
      "Epoch 77, Batch 209 Loss:0.08695194125175476\n",
      "Epoch 77, Batch 210 Loss:0.04883122444152832\n",
      "Epoch 77, Batch 211 Loss:0.011829601600766182\n",
      "Epoch 77, Batch 212 Loss:0.02591109648346901\n",
      "Epoch 77, Batch 213 Loss:0.03191656246781349\n",
      "Epoch 77, Batch 214 Loss:0.03004814125597477\n",
      "Epoch 77, Batch 215 Loss:0.03815457969903946\n",
      "Epoch 77, Batch 216 Loss:0.045593295246362686\n",
      "Epoch 77, Batch 217 Loss:0.04289599135518074\n",
      "Epoch 77, Batch 218 Loss:0.01924261823296547\n",
      "Epoch 77, Batch 219 Loss:0.017797743901610374\n",
      "Epoch 77, Batch 220 Loss:0.006701200269162655\n",
      "Epoch 77, Batch 221 Loss:0.02786824107170105\n",
      "Epoch 77, Batch 222 Loss:0.015409115701913834\n",
      "Epoch 77, Batch 223 Loss:0.02444358915090561\n",
      "Epoch 77, Batch 224 Loss:0.012451032176613808\n",
      "Epoch 77, Batch 225 Loss:0.029394876211881638\n",
      "Epoch 77, Batch 226 Loss:0.018764521926641464\n",
      "Epoch 77, Batch 227 Loss:0.00901373103260994\n",
      "Epoch 77, Batch 228 Loss:0.029914457350969315\n",
      "Epoch 77, Batch 229 Loss:0.07797281444072723\n",
      "Epoch 77, Batch 230 Loss:0.011358783580362797\n",
      "Epoch 77, Batch 231 Loss:0.01700400561094284\n",
      "Epoch 77, Batch 232 Loss:0.014035234227776527\n",
      "Epoch 77, Batch 233 Loss:0.009034475311636925\n",
      "Loss in this Epoch is: 0.903447531164 %\n",
      "Accuracy in this Epoch is: 88.5999977589 %\n",
      "Epoch 78, Batch 0 Loss:0.008856814354658127\n",
      "Epoch 78, Batch 1 Loss:0.0071905385702848434\n",
      "Epoch 78, Batch 2 Loss:0.011272025294601917\n",
      "Epoch 78, Batch 3 Loss:0.00567644601687789\n",
      "Epoch 78, Batch 4 Loss:0.0037247450090944767\n",
      "Epoch 78, Batch 5 Loss:0.015094522386789322\n",
      "Epoch 78, Batch 6 Loss:0.024892963469028473\n",
      "Epoch 78, Batch 7 Loss:0.008125174790620804\n",
      "Epoch 78, Batch 8 Loss:0.02368970215320587\n",
      "Epoch 78, Batch 9 Loss:0.004362286999821663\n",
      "Epoch 78, Batch 10 Loss:0.009853478521108627\n",
      "Epoch 78, Batch 11 Loss:0.03786385804414749\n",
      "Epoch 78, Batch 12 Loss:0.014551928266882896\n",
      "Epoch 78, Batch 13 Loss:0.01131837721914053\n",
      "Epoch 78, Batch 14 Loss:0.042802780866622925\n",
      "Epoch 78, Batch 15 Loss:0.009181664325296879\n",
      "Epoch 78, Batch 16 Loss:0.011452315375208855\n",
      "Epoch 78, Batch 17 Loss:0.006850437261164188\n",
      "Epoch 78, Batch 18 Loss:0.013471833430230618\n",
      "Epoch 78, Batch 19 Loss:0.004575288388878107\n",
      "Epoch 78, Batch 20 Loss:0.01971346326172352\n",
      "Epoch 78, Batch 21 Loss:0.013644610531628132\n",
      "Epoch 78, Batch 22 Loss:0.009634078480303288\n",
      "Epoch 78, Batch 23 Loss:0.013279561884701252\n",
      "Epoch 78, Batch 24 Loss:0.00895678997039795\n",
      "Epoch 78, Batch 25 Loss:0.024597669020295143\n",
      "Epoch 78, Batch 26 Loss:0.0179471243172884\n",
      "Epoch 78, Batch 27 Loss:0.004643383901566267\n",
      "Epoch 78, Batch 28 Loss:0.03158031031489372\n",
      "Epoch 78, Batch 29 Loss:0.008068892173469067\n",
      "Epoch 78, Batch 30 Loss:0.018872752785682678\n",
      "Epoch 78, Batch 31 Loss:0.024893445894122124\n",
      "Epoch 78, Batch 32 Loss:0.0035804477520287037\n",
      "Epoch 78, Batch 33 Loss:0.004722721874713898\n",
      "Epoch 78, Batch 34 Loss:0.013087371364235878\n",
      "Epoch 78, Batch 35 Loss:0.012662501074373722\n",
      "Epoch 78, Batch 36 Loss:0.009292270056903362\n",
      "Epoch 78, Batch 37 Loss:0.021864820271730423\n",
      "Epoch 78, Batch 38 Loss:0.013632887043058872\n",
      "Epoch 78, Batch 39 Loss:0.016135644167661667\n",
      "Epoch 78, Batch 40 Loss:0.007838165387511253\n",
      "Epoch 78, Batch 41 Loss:0.006852423772215843\n",
      "Epoch 78, Batch 42 Loss:0.004224932752549648\n",
      "Epoch 78, Batch 43 Loss:0.010972416959702969\n",
      "Epoch 78, Batch 44 Loss:0.025308841839432716\n",
      "Epoch 78, Batch 45 Loss:0.0048165698535740376\n",
      "Epoch 78, Batch 46 Loss:0.023177405819296837\n",
      "Epoch 78, Batch 47 Loss:0.007647423539310694\n",
      "Epoch 78, Batch 48 Loss:0.00740642286837101\n",
      "Epoch 78, Batch 49 Loss:0.010122371837496758\n",
      "Epoch 78, Batch 50 Loss:0.005138643551617861\n",
      "Epoch 78, Batch 51 Loss:0.006964962929487228\n",
      "Epoch 78, Batch 52 Loss:0.0068261330015957355\n",
      "Epoch 78, Batch 53 Loss:0.006552776787430048\n",
      "Epoch 78, Batch 54 Loss:0.008291682228446007\n",
      "Epoch 78, Batch 55 Loss:0.034653205424547195\n",
      "Epoch 78, Batch 56 Loss:0.0037602894008159637\n",
      "Epoch 78, Batch 57 Loss:0.00932842306792736\n",
      "Epoch 78, Batch 58 Loss:0.0015307283028960228\n",
      "Epoch 78, Batch 59 Loss:0.005790302064269781\n",
      "Epoch 78, Batch 60 Loss:0.0016513783484697342\n",
      "Epoch 78, Batch 61 Loss:0.00940687581896782\n",
      "Epoch 78, Batch 62 Loss:0.002173695247620344\n",
      "Epoch 78, Batch 63 Loss:0.0034989938139915466\n",
      "Epoch 78, Batch 64 Loss:0.0028970541898161173\n",
      "Epoch 78, Batch 65 Loss:0.00884194765239954\n",
      "Epoch 78, Batch 66 Loss:0.029323328286409378\n",
      "Epoch 78, Batch 67 Loss:0.0030701523646712303\n",
      "Epoch 78, Batch 68 Loss:0.0067505305632948875\n",
      "Epoch 78, Batch 69 Loss:0.002070849761366844\n",
      "Epoch 78, Batch 70 Loss:0.004924385342746973\n",
      "Epoch 78, Batch 71 Loss:0.0031883607152849436\n",
      "Epoch 78, Batch 72 Loss:0.008422868326306343\n",
      "Epoch 78, Batch 73 Loss:0.00381294684484601\n",
      "Epoch 78, Batch 74 Loss:0.010455524548888206\n",
      "Epoch 78, Batch 75 Loss:0.02870439924299717\n",
      "Epoch 78, Batch 76 Loss:0.004024394787847996\n",
      "Epoch 78, Batch 77 Loss:0.0029430417343974113\n",
      "Epoch 78, Batch 78 Loss:0.019613929092884064\n",
      "Epoch 78, Batch 79 Loss:0.0033287061378359795\n",
      "Epoch 78, Batch 80 Loss:0.03117223270237446\n",
      "Epoch 78, Batch 81 Loss:0.01173522137105465\n",
      "Epoch 78, Batch 82 Loss:0.0343644917011261\n",
      "Epoch 78, Batch 83 Loss:0.02240620367228985\n",
      "Epoch 78, Batch 84 Loss:0.039586663246154785\n",
      "Epoch 78, Batch 85 Loss:0.020206760615110397\n",
      "Epoch 78, Batch 86 Loss:0.01693141460418701\n",
      "Epoch 78, Batch 87 Loss:0.027291197329759598\n",
      "Epoch 78, Batch 88 Loss:0.04853010177612305\n",
      "Epoch 78, Batch 89 Loss:0.011741231195628643\n",
      "Epoch 78, Batch 90 Loss:0.015700114890933037\n",
      "Epoch 78, Batch 91 Loss:0.053342144936323166\n",
      "Epoch 78, Batch 92 Loss:0.014212162233889103\n",
      "Epoch 78, Batch 93 Loss:0.00964929535984993\n",
      "Epoch 78, Batch 94 Loss:0.0528518371284008\n",
      "Epoch 78, Batch 95 Loss:0.03999561443924904\n",
      "Epoch 78, Batch 96 Loss:0.013360407203435898\n",
      "Epoch 78, Batch 97 Loss:0.024397823959589005\n",
      "Epoch 78, Batch 98 Loss:0.015389801934361458\n",
      "Epoch 78, Batch 99 Loss:0.02751712128520012\n",
      "Epoch 78, Batch 100 Loss:0.05230296403169632\n",
      "Epoch 78, Batch 101 Loss:0.016452889889478683\n",
      "Epoch 78, Batch 102 Loss:0.023881565779447556\n",
      "Epoch 78, Batch 103 Loss:0.10894593596458435\n",
      "Epoch 78, Batch 104 Loss:0.05780500918626785\n",
      "Epoch 78, Batch 105 Loss:0.015757568180561066\n",
      "Epoch 78, Batch 106 Loss:0.024514254182577133\n",
      "Epoch 78, Batch 107 Loss:0.014525522477924824\n",
      "Epoch 78, Batch 108 Loss:0.06163093075156212\n",
      "Epoch 78, Batch 109 Loss:0.037014592438936234\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78, Batch 110 Loss:0.01638764888048172\n",
      "Epoch 78, Batch 111 Loss:0.05273379758000374\n",
      "Epoch 78, Batch 112 Loss:0.03803214803338051\n",
      "Epoch 78, Batch 113 Loss:0.01114562526345253\n",
      "Epoch 78, Batch 114 Loss:0.07254377007484436\n",
      "Epoch 78, Batch 115 Loss:0.023469118401408195\n",
      "Epoch 78, Batch 116 Loss:0.013108563609421253\n",
      "Epoch 78, Batch 117 Loss:0.035555075854063034\n",
      "Epoch 78, Batch 118 Loss:0.007800134364515543\n",
      "Epoch 78, Batch 119 Loss:0.013723628595471382\n",
      "Epoch 78, Batch 120 Loss:0.024583203718066216\n",
      "Epoch 78, Batch 121 Loss:0.025117293000221252\n",
      "Epoch 78, Batch 122 Loss:0.03051014617085457\n",
      "Epoch 78, Batch 123 Loss:0.007007737644016743\n",
      "Epoch 78, Batch 124 Loss:0.051557112485170364\n",
      "Epoch 78, Batch 125 Loss:0.009157629683613777\n",
      "Epoch 78, Batch 126 Loss:0.019947431981563568\n",
      "Epoch 78, Batch 127 Loss:0.030623000115156174\n",
      "Epoch 78, Batch 128 Loss:0.009443577378988266\n",
      "Epoch 78, Batch 129 Loss:0.06443174183368683\n",
      "Epoch 78, Batch 130 Loss:0.008166121318936348\n",
      "Epoch 78, Batch 131 Loss:0.035791926085948944\n",
      "Epoch 78, Batch 132 Loss:0.008347691968083382\n",
      "Epoch 78, Batch 133 Loss:0.027210699394345284\n",
      "Epoch 78, Batch 134 Loss:0.016946088522672653\n",
      "Epoch 78, Batch 135 Loss:0.020945977419614792\n",
      "Epoch 78, Batch 136 Loss:0.028963064774870872\n",
      "Epoch 78, Batch 137 Loss:0.0051202536560595036\n",
      "Epoch 78, Batch 138 Loss:0.021402165293693542\n",
      "Epoch 78, Batch 139 Loss:0.023848479613661766\n",
      "Epoch 78, Batch 140 Loss:0.015660816803574562\n",
      "Epoch 78, Batch 141 Loss:0.01784290187060833\n",
      "Epoch 78, Batch 142 Loss:0.03718651831150055\n",
      "Epoch 78, Batch 143 Loss:0.032191041857004166\n",
      "Epoch 78, Batch 144 Loss:0.024795930832624435\n",
      "Epoch 78, Batch 145 Loss:0.01748659461736679\n",
      "Epoch 78, Batch 146 Loss:0.012685280293226242\n",
      "Epoch 78, Batch 147 Loss:0.02501743659377098\n",
      "Epoch 78, Batch 148 Loss:0.02156275510787964\n",
      "Epoch 78, Batch 149 Loss:0.008222628384828568\n",
      "Epoch 78, Batch 150 Loss:0.01705913245677948\n",
      "Epoch 78, Batch 151 Loss:0.01892043650150299\n",
      "Epoch 78, Batch 152 Loss:0.023824378848075867\n",
      "Epoch 78, Batch 153 Loss:0.0122643718495965\n",
      "Epoch 78, Batch 154 Loss:0.05993267148733139\n",
      "Epoch 78, Batch 155 Loss:0.01698075607419014\n",
      "Epoch 78, Batch 156 Loss:0.016688011586666107\n",
      "Epoch 78, Batch 157 Loss:0.00952960830181837\n",
      "Epoch 78, Batch 158 Loss:0.0116448774933815\n",
      "Epoch 78, Batch 159 Loss:0.017800375819206238\n",
      "Epoch 78, Batch 160 Loss:0.03778894618153572\n",
      "Epoch 78, Batch 161 Loss:0.0027118457946926355\n",
      "Epoch 78, Batch 162 Loss:0.00992493610829115\n",
      "Epoch 78, Batch 163 Loss:0.01779426820576191\n",
      "Epoch 78, Batch 164 Loss:0.029370218515396118\n",
      "Epoch 78, Batch 165 Loss:0.004873358644545078\n",
      "Epoch 78, Batch 166 Loss:0.041603222489356995\n",
      "Epoch 78, Batch 167 Loss:0.04100792109966278\n",
      "Epoch 78, Batch 168 Loss:0.022383660078048706\n",
      "Epoch 78, Batch 169 Loss:0.015716606751084328\n",
      "Epoch 78, Batch 170 Loss:0.03674234449863434\n",
      "Epoch 78, Batch 171 Loss:0.0531596913933754\n",
      "Epoch 78, Batch 172 Loss:0.010481463745236397\n",
      "Epoch 78, Batch 173 Loss:0.04496386647224426\n",
      "Epoch 78, Batch 174 Loss:0.06950992345809937\n",
      "Epoch 78, Batch 175 Loss:0.0866946280002594\n",
      "Epoch 78, Batch 176 Loss:0.043417491018772125\n",
      "Epoch 78, Batch 177 Loss:0.07291509211063385\n",
      "Epoch 78, Batch 178 Loss:0.033701345324516296\n",
      "Epoch 78, Batch 179 Loss:0.033956438302993774\n",
      "Epoch 78, Batch 180 Loss:0.08808331191539764\n",
      "Epoch 78, Batch 181 Loss:0.024236127734184265\n",
      "Epoch 78, Batch 182 Loss:0.06422668695449829\n",
      "Epoch 78, Batch 183 Loss:0.015170210972428322\n",
      "Epoch 78, Batch 184 Loss:0.0495196133852005\n",
      "Epoch 78, Batch 185 Loss:0.01202760636806488\n",
      "Epoch 78, Batch 186 Loss:0.04935090243816376\n",
      "Epoch 78, Batch 187 Loss:0.028787439689040184\n",
      "Epoch 78, Batch 188 Loss:0.048649296164512634\n",
      "Epoch 78, Batch 189 Loss:0.03753652423620224\n",
      "Epoch 78, Batch 190 Loss:0.011950687505304813\n",
      "Epoch 78, Batch 191 Loss:0.028687169775366783\n",
      "Epoch 78, Batch 192 Loss:0.03687199950218201\n",
      "Epoch 78, Batch 193 Loss:0.04337485134601593\n",
      "Epoch 78, Batch 194 Loss:0.05241071432828903\n",
      "Epoch 78, Batch 195 Loss:0.03946404159069061\n",
      "Epoch 78, Batch 196 Loss:0.03631781041622162\n",
      "Epoch 78, Batch 197 Loss:0.011904008686542511\n",
      "Epoch 78, Batch 198 Loss:0.039796747267246246\n",
      "Epoch 78, Batch 199 Loss:0.06687048077583313\n",
      "Epoch 78, Batch 200 Loss:0.020748348906636238\n",
      "Epoch 78, Batch 201 Loss:0.018313707783818245\n",
      "Epoch 78, Batch 202 Loss:0.03786048665642738\n",
      "Epoch 78, Batch 203 Loss:0.0286015085875988\n",
      "Epoch 78, Batch 204 Loss:0.02687891013920307\n",
      "Epoch 78, Batch 205 Loss:0.07932737469673157\n",
      "Epoch 78, Batch 206 Loss:0.12199300527572632\n",
      "Epoch 78, Batch 207 Loss:0.10171502828598022\n",
      "Epoch 78, Batch 208 Loss:0.13520509004592896\n",
      "Epoch 78, Batch 209 Loss:0.0960695669054985\n",
      "Epoch 78, Batch 210 Loss:0.10446736216545105\n",
      "Epoch 78, Batch 211 Loss:0.05901665613055229\n",
      "Epoch 78, Batch 212 Loss:0.06331255286931992\n",
      "Epoch 78, Batch 213 Loss:0.09978601336479187\n",
      "Epoch 78, Batch 214 Loss:0.055155396461486816\n",
      "Epoch 78, Batch 215 Loss:0.09812486171722412\n",
      "Epoch 78, Batch 216 Loss:0.039019931107759476\n",
      "Epoch 78, Batch 217 Loss:0.06534511595964432\n",
      "Epoch 78, Batch 218 Loss:0.04248763620853424\n",
      "Epoch 78, Batch 219 Loss:0.05748814344406128\n",
      "Epoch 78, Batch 220 Loss:0.03416237235069275\n",
      "Epoch 78, Batch 221 Loss:0.041513945907354355\n",
      "Epoch 78, Batch 222 Loss:0.06328382343053818\n",
      "Epoch 78, Batch 223 Loss:0.04154381901025772\n",
      "Epoch 78, Batch 224 Loss:0.10213491320610046\n",
      "Epoch 78, Batch 225 Loss:0.08155597746372223\n",
      "Epoch 78, Batch 226 Loss:0.038001660257577896\n",
      "Epoch 78, Batch 227 Loss:0.08756500482559204\n",
      "Epoch 78, Batch 228 Loss:0.03203227370977402\n",
      "Epoch 78, Batch 229 Loss:0.04147988557815552\n",
      "Epoch 78, Batch 230 Loss:0.06905540823936462\n",
      "Epoch 78, Batch 231 Loss:0.029315736144781113\n",
      "Epoch 78, Batch 232 Loss:0.05777623876929283\n",
      "Epoch 78, Batch 233 Loss:0.01607203297317028\n",
      "Loss in this Epoch is: 1.60720329732 %\n",
      "Accuracy in this Epoch is: 88.6699974537 %\n",
      "Epoch 79, Batch 0 Loss:0.03648712858557701\n",
      "Epoch 79, Batch 1 Loss:0.04594118520617485\n",
      "Epoch 79, Batch 2 Loss:0.037474799901247025\n",
      "Epoch 79, Batch 3 Loss:0.007846226915717125\n",
      "Epoch 79, Batch 4 Loss:0.05752788484096527\n",
      "Epoch 79, Batch 5 Loss:0.01928562857210636\n",
      "Epoch 79, Batch 6 Loss:0.015241051092743874\n",
      "Epoch 79, Batch 7 Loss:0.025910597294569016\n",
      "Epoch 79, Batch 8 Loss:0.02009800635278225\n",
      "Epoch 79, Batch 9 Loss:0.05248142033815384\n",
      "Epoch 79, Batch 10 Loss:0.010643505491316319\n",
      "Epoch 79, Batch 11 Loss:0.008134426549077034\n",
      "Epoch 79, Batch 12 Loss:0.012513382360339165\n",
      "Epoch 79, Batch 13 Loss:0.021405527368187904\n",
      "Epoch 79, Batch 14 Loss:0.04422064125537872\n",
      "Epoch 79, Batch 15 Loss:0.01633019931614399\n",
      "Epoch 79, Batch 16 Loss:0.013751499354839325\n",
      "Epoch 79, Batch 17 Loss:0.011368626728653908\n",
      "Epoch 79, Batch 18 Loss:0.03506271541118622\n",
      "Epoch 79, Batch 19 Loss:0.06308779120445251\n",
      "Epoch 79, Batch 20 Loss:0.009846091270446777\n",
      "Epoch 79, Batch 21 Loss:0.012460469268262386\n",
      "Epoch 79, Batch 22 Loss:0.022025614976882935\n",
      "Epoch 79, Batch 23 Loss:0.02769364044070244\n",
      "Epoch 79, Batch 24 Loss:0.036300063133239746\n",
      "Epoch 79, Batch 25 Loss:0.06584580987691879\n",
      "Epoch 79, Batch 26 Loss:0.07299874722957611\n",
      "Epoch 79, Batch 27 Loss:0.06378139555454254\n",
      "Epoch 79, Batch 28 Loss:0.02211269736289978\n",
      "Epoch 79, Batch 29 Loss:0.024500751867890358\n",
      "Epoch 79, Batch 30 Loss:0.11692118644714355\n",
      "Epoch 79, Batch 31 Loss:0.033577024936676025\n",
      "Epoch 79, Batch 32 Loss:0.03780505806207657\n",
      "Epoch 79, Batch 33 Loss:0.045478641986846924\n",
      "Epoch 79, Batch 34 Loss:0.033162616193294525\n",
      "Epoch 79, Batch 35 Loss:0.17163051664829254\n",
      "Epoch 79, Batch 36 Loss:0.014593876898288727\n",
      "Epoch 79, Batch 37 Loss:0.046043142676353455\n",
      "Epoch 79, Batch 38 Loss:0.035953737795352936\n",
      "Epoch 79, Batch 39 Loss:0.03724635764956474\n",
      "Epoch 79, Batch 40 Loss:0.043892838060855865\n",
      "Epoch 79, Batch 41 Loss:0.02455325424671173\n",
      "Epoch 79, Batch 42 Loss:0.012173746712505817\n",
      "Epoch 79, Batch 43 Loss:0.021878991276025772\n",
      "Epoch 79, Batch 44 Loss:0.05842563882470131\n",
      "Epoch 79, Batch 45 Loss:0.028602812439203262\n",
      "Epoch 79, Batch 46 Loss:0.029047813266515732\n",
      "Epoch 79, Batch 47 Loss:0.0361466258764267\n",
      "Epoch 79, Batch 48 Loss:0.014666871167719364\n",
      "Epoch 79, Batch 49 Loss:0.05230724811553955\n",
      "Epoch 79, Batch 50 Loss:0.05728357285261154\n",
      "Epoch 79, Batch 51 Loss:0.07314160466194153\n",
      "Epoch 79, Batch 52 Loss:0.04831317812204361\n",
      "Epoch 79, Batch 53 Loss:0.021526457741856575\n",
      "Epoch 79, Batch 54 Loss:0.044931523501873016\n",
      "Epoch 79, Batch 55 Loss:0.01121685653924942\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79, Batch 56 Loss:0.02672438696026802\n",
      "Epoch 79, Batch 57 Loss:0.050960443913936615\n",
      "Epoch 79, Batch 58 Loss:0.04575648531317711\n",
      "Epoch 79, Batch 59 Loss:0.11865752935409546\n",
      "Epoch 79, Batch 60 Loss:0.03921442851424217\n",
      "Epoch 79, Batch 61 Loss:0.025345856323838234\n",
      "Epoch 79, Batch 62 Loss:0.03543064743280411\n",
      "Epoch 79, Batch 63 Loss:0.06131025403738022\n",
      "Epoch 79, Batch 64 Loss:0.027005603536963463\n",
      "Epoch 79, Batch 65 Loss:0.029254546388983727\n",
      "Epoch 79, Batch 66 Loss:0.012240919284522533\n",
      "Epoch 79, Batch 67 Loss:0.05028718709945679\n",
      "Epoch 79, Batch 68 Loss:0.05276552587747574\n",
      "Epoch 79, Batch 69 Loss:0.06668632477521896\n",
      "Epoch 79, Batch 70 Loss:0.03238607943058014\n",
      "Epoch 79, Batch 71 Loss:0.041734881699085236\n",
      "Epoch 79, Batch 72 Loss:0.01348458044230938\n",
      "Epoch 79, Batch 73 Loss:0.030838431790471077\n",
      "Epoch 79, Batch 74 Loss:0.01961851492524147\n",
      "Epoch 79, Batch 75 Loss:0.030501194298267365\n",
      "Epoch 79, Batch 76 Loss:0.09033094346523285\n",
      "Epoch 79, Batch 77 Loss:0.024395329877734184\n",
      "Epoch 79, Batch 78 Loss:0.07365622371435165\n",
      "Epoch 79, Batch 79 Loss:0.034074343740940094\n",
      "Epoch 79, Batch 80 Loss:0.04653588682413101\n",
      "Epoch 79, Batch 81 Loss:0.04840104281902313\n",
      "Epoch 79, Batch 82 Loss:0.038242436945438385\n",
      "Epoch 79, Batch 83 Loss:0.07039837539196014\n",
      "Epoch 79, Batch 84 Loss:0.03664655610918999\n",
      "Epoch 79, Batch 85 Loss:0.029804063960909843\n",
      "Epoch 79, Batch 86 Loss:0.051146313548088074\n",
      "Epoch 79, Batch 87 Loss:0.03326324746012688\n",
      "Epoch 79, Batch 88 Loss:0.031263601034879684\n",
      "Epoch 79, Batch 89 Loss:0.0307247843593359\n",
      "Epoch 79, Batch 90 Loss:0.028920700773596764\n",
      "Epoch 79, Batch 91 Loss:0.027531471103429794\n",
      "Epoch 79, Batch 92 Loss:0.0611797533929348\n",
      "Epoch 79, Batch 93 Loss:0.04805267974734306\n",
      "Epoch 79, Batch 94 Loss:0.03393347188830376\n",
      "Epoch 79, Batch 95 Loss:0.05535753816366196\n",
      "Epoch 79, Batch 96 Loss:0.04809089004993439\n",
      "Epoch 79, Batch 97 Loss:0.02964572049677372\n",
      "Epoch 79, Batch 98 Loss:0.07758438587188721\n",
      "Epoch 79, Batch 99 Loss:0.07114866375923157\n",
      "Epoch 79, Batch 100 Loss:0.061831772327423096\n",
      "Epoch 79, Batch 101 Loss:0.055858269333839417\n",
      "Epoch 79, Batch 102 Loss:0.04648777097463608\n",
      "Epoch 79, Batch 103 Loss:0.021492362022399902\n",
      "Epoch 79, Batch 104 Loss:0.038827210664749146\n",
      "Epoch 79, Batch 105 Loss:0.052887074649333954\n",
      "Epoch 79, Batch 106 Loss:0.09642520546913147\n",
      "Epoch 79, Batch 107 Loss:0.022177081555128098\n",
      "Epoch 79, Batch 108 Loss:0.03686966747045517\n",
      "Epoch 79, Batch 109 Loss:0.03238595277070999\n",
      "Epoch 79, Batch 110 Loss:0.06217225641012192\n",
      "Epoch 79, Batch 111 Loss:0.06771817803382874\n",
      "Epoch 79, Batch 112 Loss:0.023745499551296234\n",
      "Epoch 79, Batch 113 Loss:0.014673115685582161\n",
      "Epoch 79, Batch 114 Loss:0.053491704165935516\n",
      "Epoch 79, Batch 115 Loss:0.03595993295311928\n",
      "Epoch 79, Batch 116 Loss:0.059371426701545715\n",
      "Epoch 79, Batch 117 Loss:0.026973364874720573\n",
      "Epoch 79, Batch 118 Loss:0.009085454046726227\n",
      "Epoch 79, Batch 119 Loss:0.08315585553646088\n",
      "Epoch 79, Batch 120 Loss:0.015397511422634125\n",
      "Epoch 79, Batch 121 Loss:0.04108969867229462\n",
      "Epoch 79, Batch 122 Loss:0.034800317138433456\n",
      "Epoch 79, Batch 123 Loss:0.03673810884356499\n",
      "Epoch 79, Batch 124 Loss:0.028839174658060074\n",
      "Epoch 79, Batch 125 Loss:0.03983229771256447\n",
      "Epoch 79, Batch 126 Loss:0.009654728695750237\n",
      "Epoch 79, Batch 127 Loss:0.011178442277014256\n",
      "Epoch 79, Batch 128 Loss:0.02687256783246994\n",
      "Epoch 79, Batch 129 Loss:0.058629073202610016\n",
      "Epoch 79, Batch 130 Loss:0.04347455874085426\n",
      "Epoch 79, Batch 131 Loss:0.01478060707449913\n",
      "Epoch 79, Batch 132 Loss:0.032111577689647675\n",
      "Epoch 79, Batch 133 Loss:0.02345721237361431\n",
      "Epoch 79, Batch 134 Loss:0.025532115250825882\n",
      "Epoch 79, Batch 135 Loss:0.07155153900384903\n",
      "Epoch 79, Batch 136 Loss:0.010633330792188644\n",
      "Epoch 79, Batch 137 Loss:0.022785354405641556\n",
      "Epoch 79, Batch 138 Loss:0.10655362904071808\n",
      "Epoch 79, Batch 139 Loss:0.05110570415854454\n",
      "Epoch 79, Batch 140 Loss:0.04154188930988312\n",
      "Epoch 79, Batch 141 Loss:0.03061051107943058\n",
      "Epoch 79, Batch 142 Loss:0.03520379215478897\n",
      "Epoch 79, Batch 143 Loss:0.035633064806461334\n",
      "Epoch 79, Batch 144 Loss:0.02839372307062149\n",
      "Epoch 79, Batch 145 Loss:0.008096814155578613\n",
      "Epoch 79, Batch 146 Loss:0.025861959904432297\n",
      "Epoch 79, Batch 147 Loss:0.03268660977482796\n",
      "Epoch 79, Batch 148 Loss:0.0066457269713282585\n",
      "Epoch 79, Batch 149 Loss:0.03921473026275635\n",
      "Epoch 79, Batch 150 Loss:0.023164566606283188\n",
      "Epoch 79, Batch 151 Loss:0.04138140007853508\n",
      "Epoch 79, Batch 152 Loss:0.03227581828832626\n",
      "Epoch 79, Batch 153 Loss:0.012883415445685387\n",
      "Epoch 79, Batch 154 Loss:0.02561921998858452\n",
      "Epoch 79, Batch 155 Loss:0.011943201534450054\n",
      "Epoch 79, Batch 156 Loss:0.019441360607743263\n",
      "Epoch 79, Batch 157 Loss:0.020730579271912575\n",
      "Epoch 79, Batch 158 Loss:0.026384880766272545\n",
      "Epoch 79, Batch 159 Loss:0.056087322533130646\n",
      "Epoch 79, Batch 160 Loss:0.008853737264871597\n",
      "Epoch 79, Batch 161 Loss:0.025654681026935577\n",
      "Epoch 79, Batch 162 Loss:0.034036457538604736\n",
      "Epoch 79, Batch 163 Loss:0.019886983558535576\n",
      "Epoch 79, Batch 164 Loss:0.05223298445343971\n",
      "Epoch 79, Batch 165 Loss:0.04106892645359039\n",
      "Epoch 79, Batch 166 Loss:0.055956970900297165\n",
      "Epoch 79, Batch 167 Loss:0.08541715145111084\n",
      "Epoch 79, Batch 168 Loss:0.046659186482429504\n",
      "Epoch 79, Batch 169 Loss:0.050060562789440155\n",
      "Epoch 79, Batch 170 Loss:0.013348002918064594\n",
      "Epoch 79, Batch 171 Loss:0.08530385047197342\n",
      "Epoch 79, Batch 172 Loss:0.04727768152952194\n",
      "Epoch 79, Batch 173 Loss:0.033328719437122345\n",
      "Epoch 79, Batch 174 Loss:0.024734025821089745\n",
      "Epoch 79, Batch 175 Loss:0.010287070646882057\n",
      "Epoch 79, Batch 176 Loss:0.02379840612411499\n",
      "Epoch 79, Batch 177 Loss:0.02209467813372612\n",
      "Epoch 79, Batch 178 Loss:0.022266391664743423\n",
      "Epoch 79, Batch 179 Loss:0.014920305460691452\n",
      "Epoch 79, Batch 180 Loss:0.0299589354544878\n",
      "Epoch 79, Batch 181 Loss:0.03133130073547363\n",
      "Epoch 79, Batch 182 Loss:0.011884527280926704\n",
      "Epoch 79, Batch 183 Loss:0.03176024556159973\n",
      "Epoch 79, Batch 184 Loss:0.03627292439341545\n",
      "Epoch 79, Batch 185 Loss:0.05159009248018265\n",
      "Epoch 79, Batch 186 Loss:0.012126604095101357\n",
      "Epoch 79, Batch 187 Loss:0.04047350585460663\n",
      "Epoch 79, Batch 188 Loss:0.013110391795635223\n",
      "Epoch 79, Batch 189 Loss:0.03387315198779106\n",
      "Epoch 79, Batch 190 Loss:0.019511252641677856\n",
      "Epoch 79, Batch 191 Loss:0.029694324359297752\n",
      "Epoch 79, Batch 192 Loss:0.004410526715219021\n",
      "Epoch 79, Batch 193 Loss:0.03352956101298332\n",
      "Epoch 79, Batch 194 Loss:0.044479191303253174\n",
      "Epoch 79, Batch 195 Loss:0.0398063138127327\n",
      "Epoch 79, Batch 196 Loss:0.03392801061272621\n",
      "Epoch 79, Batch 197 Loss:0.008020956069231033\n",
      "Epoch 79, Batch 198 Loss:0.013509131968021393\n",
      "Epoch 79, Batch 199 Loss:0.012241318821907043\n",
      "Epoch 79, Batch 200 Loss:0.02612192928791046\n",
      "Epoch 79, Batch 201 Loss:0.025493565946817398\n",
      "Epoch 79, Batch 202 Loss:0.012533701956272125\n",
      "Epoch 79, Batch 203 Loss:0.037208713591098785\n",
      "Epoch 79, Batch 204 Loss:0.04058416560292244\n",
      "Epoch 79, Batch 205 Loss:0.03789500147104263\n",
      "Epoch 79, Batch 206 Loss:0.024004271253943443\n",
      "Epoch 79, Batch 207 Loss:0.03735464811325073\n",
      "Epoch 79, Batch 208 Loss:0.033715665340423584\n",
      "Epoch 79, Batch 209 Loss:0.017498280853033066\n",
      "Epoch 79, Batch 210 Loss:0.025053968653082848\n",
      "Epoch 79, Batch 211 Loss:0.009164458140730858\n",
      "Epoch 79, Batch 212 Loss:0.014019354246556759\n",
      "Epoch 79, Batch 213 Loss:0.0200275257229805\n",
      "Epoch 79, Batch 214 Loss:0.010406816378235817\n",
      "Epoch 79, Batch 215 Loss:0.016137590631842613\n",
      "Epoch 79, Batch 216 Loss:0.058565668761730194\n",
      "Epoch 79, Batch 217 Loss:0.017055314034223557\n",
      "Epoch 79, Batch 218 Loss:0.01350684929639101\n",
      "Epoch 79, Batch 219 Loss:0.07061532139778137\n",
      "Epoch 79, Batch 220 Loss:0.06511540710926056\n",
      "Epoch 79, Batch 221 Loss:0.09373997151851654\n",
      "Epoch 79, Batch 222 Loss:0.01732972264289856\n",
      "Epoch 79, Batch 223 Loss:0.010959802195429802\n",
      "Epoch 79, Batch 224 Loss:0.010220835916697979\n",
      "Epoch 79, Batch 225 Loss:0.05216802656650543\n",
      "Epoch 79, Batch 226 Loss:0.03356138616800308\n",
      "Epoch 79, Batch 227 Loss:0.0255889929831028\n",
      "Epoch 79, Batch 228 Loss:0.04495958238840103\n",
      "Epoch 79, Batch 229 Loss:0.07045397162437439\n",
      "Epoch 79, Batch 230 Loss:0.019747601822018623\n",
      "Epoch 79, Batch 231 Loss:0.036683134734630585\n",
      "Epoch 79, Batch 232 Loss:0.0092092826962471\n",
      "Epoch 79, Batch 233 Loss:0.013268992304801941\n",
      "Loss in this Epoch is: 1.32689923048 %\n",
      "Accuracy in this Epoch is: 88.6600017548 %\n",
      "Epoch 80, Batch 0 Loss:0.006127217318862677\n",
      "Epoch 80, Batch 1 Loss:0.045547451823949814\n",
      "Epoch 80, Batch 2 Loss:0.024066131561994553\n",
      "Epoch 80, Batch 3 Loss:0.060040056705474854\n",
      "Epoch 80, Batch 4 Loss:0.06203058734536171\n",
      "Epoch 80, Batch 5 Loss:0.039260413497686386\n",
      "Epoch 80, Batch 6 Loss:0.029705578461289406\n",
      "Epoch 80, Batch 7 Loss:0.03096817247569561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Batch 8 Loss:0.009777460247278214\n",
      "Epoch 80, Batch 9 Loss:0.01750417985022068\n",
      "Epoch 80, Batch 10 Loss:0.008533510379493237\n",
      "Epoch 80, Batch 11 Loss:0.02495650015771389\n",
      "Epoch 80, Batch 12 Loss:0.006930262316018343\n",
      "Epoch 80, Batch 13 Loss:0.0072838421911001205\n",
      "Epoch 80, Batch 14 Loss:0.0289597325026989\n",
      "Epoch 80, Batch 15 Loss:0.01455876138061285\n",
      "Epoch 80, Batch 16 Loss:0.010945994406938553\n",
      "Epoch 80, Batch 17 Loss:0.00787321012467146\n",
      "Epoch 80, Batch 18 Loss:0.016650836914777756\n",
      "Epoch 80, Batch 19 Loss:0.015132677741348743\n",
      "Epoch 80, Batch 20 Loss:0.01759115792810917\n",
      "Epoch 80, Batch 21 Loss:0.029435066506266594\n",
      "Epoch 80, Batch 22 Loss:0.03050186112523079\n",
      "Epoch 80, Batch 23 Loss:0.01731821894645691\n",
      "Epoch 80, Batch 24 Loss:0.012947501614689827\n",
      "Epoch 80, Batch 25 Loss:0.009515751153230667\n",
      "Epoch 80, Batch 26 Loss:0.0263005830347538\n",
      "Epoch 80, Batch 27 Loss:0.009628222323954105\n",
      "Epoch 80, Batch 28 Loss:0.016698304563760757\n",
      "Epoch 80, Batch 29 Loss:0.046608101576566696\n",
      "Epoch 80, Batch 30 Loss:0.033638980239629745\n",
      "Epoch 80, Batch 31 Loss:0.029639974236488342\n",
      "Epoch 80, Batch 32 Loss:0.020793382078409195\n",
      "Epoch 80, Batch 33 Loss:0.03315642103552818\n",
      "Epoch 80, Batch 34 Loss:0.01548790279775858\n",
      "Epoch 80, Batch 35 Loss:0.005727064795792103\n",
      "Epoch 80, Batch 36 Loss:0.009658143855631351\n",
      "Epoch 80, Batch 37 Loss:0.021317731589078903\n",
      "Epoch 80, Batch 38 Loss:0.020732248201966286\n",
      "Epoch 80, Batch 39 Loss:0.007072616368532181\n",
      "Epoch 80, Batch 40 Loss:0.05570323392748833\n",
      "Epoch 80, Batch 41 Loss:0.02578289806842804\n",
      "Epoch 80, Batch 42 Loss:0.00743440305814147\n",
      "Epoch 80, Batch 43 Loss:0.007500941399484873\n",
      "Epoch 80, Batch 44 Loss:0.019498812034726143\n",
      "Epoch 80, Batch 45 Loss:0.054489292204380035\n",
      "Epoch 80, Batch 46 Loss:0.03671804070472717\n",
      "Epoch 80, Batch 47 Loss:0.002109306864440441\n",
      "Epoch 80, Batch 48 Loss:0.02307470142841339\n",
      "Epoch 80, Batch 49 Loss:0.03963898867368698\n",
      "Epoch 80, Batch 50 Loss:0.003353231819346547\n",
      "Epoch 80, Batch 51 Loss:0.013803704641759396\n",
      "Epoch 80, Batch 52 Loss:0.006113555282354355\n",
      "Epoch 80, Batch 53 Loss:0.011270133778452873\n",
      "Epoch 80, Batch 54 Loss:0.009208128787577152\n",
      "Epoch 80, Batch 55 Loss:0.008114885538816452\n",
      "Epoch 80, Batch 56 Loss:0.014903654344379902\n",
      "Epoch 80, Batch 57 Loss:0.029312925413250923\n",
      "Epoch 80, Batch 58 Loss:0.042751245200634\n",
      "Epoch 80, Batch 59 Loss:0.0041876137256622314\n",
      "Epoch 80, Batch 60 Loss:0.017746716737747192\n",
      "Epoch 80, Batch 61 Loss:0.012981168925762177\n",
      "Epoch 80, Batch 62 Loss:0.036805085837841034\n",
      "Epoch 80, Batch 63 Loss:0.005362859461456537\n",
      "Epoch 80, Batch 64 Loss:0.013226049020886421\n",
      "Epoch 80, Batch 65 Loss:0.0887337401509285\n",
      "Epoch 80, Batch 66 Loss:0.01355928648263216\n",
      "Epoch 80, Batch 67 Loss:0.043712783604860306\n",
      "Epoch 80, Batch 68 Loss:0.016546687111258507\n",
      "Epoch 80, Batch 69 Loss:0.03267769515514374\n",
      "Epoch 80, Batch 70 Loss:0.045429907739162445\n",
      "Epoch 80, Batch 71 Loss:0.01875760406255722\n",
      "Epoch 80, Batch 72 Loss:0.036984238773584366\n",
      "Epoch 80, Batch 73 Loss:0.0609971284866333\n",
      "Epoch 80, Batch 74 Loss:0.0389857292175293\n",
      "Epoch 80, Batch 75 Loss:0.04124265909194946\n",
      "Epoch 80, Batch 76 Loss:0.03317553177475929\n",
      "Epoch 80, Batch 77 Loss:0.03935931250452995\n",
      "Epoch 80, Batch 78 Loss:0.05750846490263939\n",
      "Epoch 80, Batch 79 Loss:0.007304153870791197\n",
      "Epoch 80, Batch 80 Loss:0.009463734924793243\n",
      "Epoch 80, Batch 81 Loss:0.026915349066257477\n",
      "Epoch 80, Batch 82 Loss:0.03651507943868637\n",
      "Epoch 80, Batch 83 Loss:0.009684717282652855\n",
      "Epoch 80, Batch 84 Loss:0.02091819792985916\n",
      "Epoch 80, Batch 85 Loss:0.012601100839674473\n",
      "Epoch 80, Batch 86 Loss:0.04099193587899208\n",
      "Epoch 80, Batch 87 Loss:0.02104511670768261\n",
      "Epoch 80, Batch 88 Loss:0.02935134619474411\n",
      "Epoch 80, Batch 89 Loss:0.014196835458278656\n",
      "Epoch 80, Batch 90 Loss:0.028359264135360718\n",
      "Epoch 80, Batch 91 Loss:0.02508026547729969\n",
      "Epoch 80, Batch 92 Loss:0.030371520668268204\n",
      "Epoch 80, Batch 93 Loss:0.05324118211865425\n",
      "Epoch 80, Batch 94 Loss:0.01637515053153038\n",
      "Epoch 80, Batch 95 Loss:0.051290594041347504\n",
      "Epoch 80, Batch 96 Loss:0.017344048246741295\n",
      "Epoch 80, Batch 97 Loss:0.011417550966143608\n",
      "Epoch 80, Batch 98 Loss:0.009434402920305729\n",
      "Epoch 80, Batch 99 Loss:0.0379829928278923\n",
      "Epoch 80, Batch 100 Loss:0.03868628293275833\n",
      "Epoch 80, Batch 101 Loss:0.07977226376533508\n",
      "Epoch 80, Batch 102 Loss:0.022537067532539368\n",
      "Epoch 80, Batch 103 Loss:0.02793431282043457\n",
      "Epoch 80, Batch 104 Loss:0.01453522127121687\n",
      "Epoch 80, Batch 105 Loss:0.0237744078040123\n",
      "Epoch 80, Batch 106 Loss:0.027846431359648705\n",
      "Epoch 80, Batch 107 Loss:0.03230232745409012\n",
      "Epoch 80, Batch 108 Loss:0.07419899851083755\n",
      "Epoch 80, Batch 109 Loss:0.011921728029847145\n",
      "Epoch 80, Batch 110 Loss:0.051542654633522034\n",
      "Epoch 80, Batch 111 Loss:0.021380648016929626\n",
      "Epoch 80, Batch 112 Loss:0.010116628371179104\n",
      "Epoch 80, Batch 113 Loss:0.017656318843364716\n",
      "Epoch 80, Batch 114 Loss:0.05698463320732117\n",
      "Epoch 80, Batch 115 Loss:0.028315193951129913\n",
      "Epoch 80, Batch 116 Loss:0.05377858132123947\n",
      "Epoch 80, Batch 117 Loss:0.01630406640470028\n",
      "Epoch 80, Batch 118 Loss:0.060327332466840744\n",
      "Epoch 80, Batch 119 Loss:0.04162809997797012\n",
      "Epoch 80, Batch 120 Loss:0.009783344343304634\n",
      "Epoch 80, Batch 121 Loss:0.020234592258930206\n",
      "Epoch 80, Batch 122 Loss:0.04645879566669464\n",
      "Epoch 80, Batch 123 Loss:0.039513375610113144\n",
      "Epoch 80, Batch 124 Loss:0.008647749200463295\n",
      "Epoch 80, Batch 125 Loss:0.017310723662376404\n",
      "Epoch 80, Batch 126 Loss:0.032270438969135284\n",
      "Epoch 80, Batch 127 Loss:0.04088393598794937\n",
      "Epoch 80, Batch 128 Loss:0.018633153289556503\n",
      "Epoch 80, Batch 129 Loss:0.00852307677268982\n",
      "Epoch 80, Batch 130 Loss:0.03414862975478172\n",
      "Epoch 80, Batch 131 Loss:0.013689074665307999\n",
      "Epoch 80, Batch 132 Loss:0.020284920930862427\n",
      "Epoch 80, Batch 133 Loss:0.034062184393405914\n",
      "Epoch 80, Batch 134 Loss:0.02268882468342781\n",
      "Epoch 80, Batch 135 Loss:0.009322190657258034\n",
      "Epoch 80, Batch 136 Loss:0.021402837708592415\n",
      "Epoch 80, Batch 137 Loss:0.007081243675202131\n",
      "Epoch 80, Batch 138 Loss:0.01312851533293724\n",
      "Epoch 80, Batch 139 Loss:0.013641748577356339\n",
      "Epoch 80, Batch 140 Loss:0.032329194247722626\n",
      "Epoch 80, Batch 141 Loss:0.019766760990023613\n",
      "Epoch 80, Batch 142 Loss:0.023180905729532242\n",
      "Epoch 80, Batch 143 Loss:0.050085797905921936\n",
      "Epoch 80, Batch 144 Loss:0.03909861296415329\n",
      "Epoch 80, Batch 145 Loss:0.012410784140229225\n",
      "Epoch 80, Batch 146 Loss:0.01903989352285862\n",
      "Epoch 80, Batch 147 Loss:0.03388557955622673\n",
      "Epoch 80, Batch 148 Loss:0.01841173879802227\n",
      "Epoch 80, Batch 149 Loss:0.022718625143170357\n",
      "Epoch 80, Batch 150 Loss:0.003730987897142768\n",
      "Epoch 80, Batch 151 Loss:0.018751125782728195\n",
      "Epoch 80, Batch 152 Loss:0.029852692037820816\n",
      "Epoch 80, Batch 153 Loss:0.021829700097441673\n",
      "Epoch 80, Batch 154 Loss:0.02383599616587162\n",
      "Epoch 80, Batch 155 Loss:0.025786468759179115\n",
      "Epoch 80, Batch 156 Loss:0.034340210258960724\n",
      "Epoch 80, Batch 157 Loss:0.04643081873655319\n",
      "Epoch 80, Batch 158 Loss:0.012880098074674606\n",
      "Epoch 80, Batch 159 Loss:0.018081393092870712\n",
      "Epoch 80, Batch 160 Loss:0.03343347832560539\n",
      "Epoch 80, Batch 161 Loss:0.02539568766951561\n",
      "Epoch 80, Batch 162 Loss:0.028753826394677162\n",
      "Epoch 80, Batch 163 Loss:0.023180270567536354\n",
      "Epoch 80, Batch 164 Loss:0.026922540739178658\n",
      "Epoch 80, Batch 165 Loss:0.02139252796769142\n",
      "Epoch 80, Batch 166 Loss:0.033773962408304214\n",
      "Epoch 80, Batch 167 Loss:0.008068198338150978\n",
      "Epoch 80, Batch 168 Loss:0.01056806743144989\n",
      "Epoch 80, Batch 169 Loss:0.016393136233091354\n",
      "Epoch 80, Batch 170 Loss:0.05527520924806595\n",
      "Epoch 80, Batch 171 Loss:0.03959210216999054\n",
      "Epoch 80, Batch 172 Loss:0.0030904337763786316\n",
      "Epoch 80, Batch 173 Loss:0.018583402037620544\n",
      "Epoch 80, Batch 174 Loss:0.030109751969575882\n",
      "Epoch 80, Batch 175 Loss:0.047226715832948685\n",
      "Epoch 80, Batch 176 Loss:0.04275493323802948\n",
      "Epoch 80, Batch 177 Loss:0.012740587815642357\n",
      "Epoch 80, Batch 178 Loss:0.018149424344301224\n",
      "Epoch 80, Batch 179 Loss:0.009445223957300186\n",
      "Epoch 80, Batch 180 Loss:0.04957035556435585\n",
      "Epoch 80, Batch 181 Loss:0.0281491968780756\n",
      "Epoch 80, Batch 182 Loss:0.05822473019361496\n",
      "Epoch 80, Batch 183 Loss:0.024971146136522293\n",
      "Epoch 80, Batch 184 Loss:0.015490151941776276\n",
      "Epoch 80, Batch 185 Loss:0.03594681993126869\n",
      "Epoch 80, Batch 186 Loss:0.06391793489456177\n",
      "Epoch 80, Batch 187 Loss:0.037044502794742584\n",
      "Epoch 80, Batch 188 Loss:0.050463609397411346\n",
      "Epoch 80, Batch 189 Loss:0.03646555542945862\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 80, Batch 190 Loss:0.011073085479438305\n",
      "Epoch 80, Batch 191 Loss:0.027966473251581192\n",
      "Epoch 80, Batch 192 Loss:0.023183688521385193\n",
      "Epoch 80, Batch 193 Loss:0.022949855774641037\n",
      "Epoch 80, Batch 194 Loss:0.06785927712917328\n",
      "Epoch 80, Batch 195 Loss:0.029483698308467865\n",
      "Epoch 80, Batch 196 Loss:0.03653052821755409\n",
      "Epoch 80, Batch 197 Loss:0.03224591910839081\n",
      "Epoch 80, Batch 198 Loss:0.03144880756735802\n",
      "Epoch 80, Batch 199 Loss:0.013376914896070957\n",
      "Epoch 80, Batch 200 Loss:0.020417548716068268\n",
      "Epoch 80, Batch 201 Loss:0.015795832499861717\n",
      "Epoch 80, Batch 202 Loss:0.0218186192214489\n",
      "Epoch 80, Batch 203 Loss:0.029691901057958603\n",
      "Epoch 80, Batch 204 Loss:0.030403846874833107\n",
      "Epoch 80, Batch 205 Loss:0.01732918992638588\n",
      "Epoch 80, Batch 206 Loss:0.012510070577263832\n",
      "Epoch 80, Batch 207 Loss:0.009398387745022774\n",
      "Epoch 80, Batch 208 Loss:0.012215310707688332\n",
      "Epoch 80, Batch 209 Loss:0.024140439927577972\n",
      "Epoch 80, Batch 210 Loss:0.01791980490088463\n",
      "Epoch 80, Batch 211 Loss:0.04797736555337906\n",
      "Epoch 80, Batch 212 Loss:0.027375802397727966\n",
      "Epoch 80, Batch 213 Loss:0.024234354496002197\n",
      "Epoch 80, Batch 214 Loss:0.021308407187461853\n",
      "Epoch 80, Batch 215 Loss:0.013983355835080147\n",
      "Epoch 80, Batch 216 Loss:0.028141245245933533\n",
      "Epoch 80, Batch 217 Loss:0.01489200908690691\n",
      "Epoch 80, Batch 218 Loss:0.014553721994161606\n",
      "Epoch 80, Batch 219 Loss:0.022470416501164436\n",
      "Epoch 80, Batch 220 Loss:0.019966518506407738\n",
      "Epoch 80, Batch 221 Loss:0.006013990379869938\n",
      "Epoch 80, Batch 222 Loss:0.026064474135637283\n",
      "Epoch 80, Batch 223 Loss:0.023747414350509644\n",
      "Epoch 80, Batch 224 Loss:0.02441679686307907\n",
      "Epoch 80, Batch 225 Loss:0.007709805388003588\n",
      "Epoch 80, Batch 226 Loss:0.027757229283452034\n",
      "Epoch 80, Batch 227 Loss:0.009195851162075996\n",
      "Epoch 80, Batch 228 Loss:0.011319841258227825\n",
      "Epoch 80, Batch 229 Loss:0.008952109143137932\n",
      "Epoch 80, Batch 230 Loss:0.00961616076529026\n",
      "Epoch 80, Batch 231 Loss:0.033170029520988464\n",
      "Epoch 80, Batch 232 Loss:0.043076932430267334\n",
      "Epoch 80, Batch 233 Loss:0.03625212982296944\n",
      "Loss in this Epoch is: 3.6252129823 %\n",
      "Accuracy in this Epoch is: 88.2499992847 %\n",
      "Epoch 81, Batch 0 Loss:0.04380185902118683\n",
      "Epoch 81, Batch 1 Loss:0.027656354010105133\n",
      "Epoch 81, Batch 2 Loss:0.021230418235063553\n",
      "Epoch 81, Batch 3 Loss:0.08058863878250122\n",
      "Epoch 81, Batch 4 Loss:0.03295007348060608\n",
      "Epoch 81, Batch 5 Loss:0.023305492475628853\n",
      "Epoch 81, Batch 6 Loss:0.015940099954605103\n",
      "Epoch 81, Batch 7 Loss:0.018257075920701027\n",
      "Epoch 81, Batch 8 Loss:0.02401449903845787\n",
      "Epoch 81, Batch 9 Loss:0.039534762501716614\n",
      "Epoch 81, Batch 10 Loss:0.02585201896727085\n",
      "Epoch 81, Batch 11 Loss:0.07661242038011551\n",
      "Epoch 81, Batch 12 Loss:0.07196881622076035\n",
      "Epoch 81, Batch 13 Loss:0.010526794008910656\n",
      "Epoch 81, Batch 14 Loss:0.03186915069818497\n",
      "Epoch 81, Batch 15 Loss:0.052654385566711426\n",
      "Epoch 81, Batch 16 Loss:0.043872199952602386\n",
      "Epoch 81, Batch 17 Loss:0.05996020510792732\n",
      "Epoch 81, Batch 18 Loss:0.03528903052210808\n",
      "Epoch 81, Batch 19 Loss:0.07327000796794891\n",
      "Epoch 81, Batch 20 Loss:0.08176061511039734\n",
      "Epoch 81, Batch 21 Loss:0.02932032197713852\n",
      "Epoch 81, Batch 22 Loss:0.054082490503787994\n",
      "Epoch 81, Batch 23 Loss:0.07664434611797333\n",
      "Epoch 81, Batch 24 Loss:0.05754672363400459\n",
      "Epoch 81, Batch 25 Loss:0.03380071371793747\n",
      "Epoch 81, Batch 26 Loss:0.024033239111304283\n",
      "Epoch 81, Batch 27 Loss:0.045632604509592056\n",
      "Epoch 81, Batch 28 Loss:0.03730999305844307\n",
      "Epoch 81, Batch 29 Loss:0.04341248795390129\n",
      "Epoch 81, Batch 30 Loss:0.014556875452399254\n",
      "Epoch 81, Batch 31 Loss:0.015187387354671955\n",
      "Epoch 81, Batch 32 Loss:0.09399883449077606\n",
      "Epoch 81, Batch 33 Loss:0.05071067065000534\n",
      "Epoch 81, Batch 34 Loss:0.005362237803637981\n",
      "Epoch 81, Batch 35 Loss:0.02662966027855873\n",
      "Epoch 81, Batch 36 Loss:0.01236698031425476\n",
      "Epoch 81, Batch 37 Loss:0.02589547447860241\n",
      "Epoch 81, Batch 38 Loss:0.013180258683860302\n",
      "Epoch 81, Batch 39 Loss:0.03158388286828995\n",
      "Epoch 81, Batch 40 Loss:0.020111575722694397\n",
      "Epoch 81, Batch 41 Loss:0.020087720826268196\n",
      "Epoch 81, Batch 42 Loss:0.009432555176317692\n",
      "Epoch 81, Batch 43 Loss:0.004251218866556883\n",
      "Epoch 81, Batch 44 Loss:0.043582648038864136\n",
      "Epoch 81, Batch 45 Loss:0.03245456516742706\n",
      "Epoch 81, Batch 46 Loss:0.019217610359191895\n",
      "Epoch 81, Batch 47 Loss:0.024974243715405464\n",
      "Epoch 81, Batch 48 Loss:0.006032266654074192\n",
      "Epoch 81, Batch 49 Loss:0.04088246822357178\n",
      "Epoch 81, Batch 50 Loss:0.04112103953957558\n",
      "Epoch 81, Batch 51 Loss:0.0665052980184555\n",
      "Epoch 81, Batch 52 Loss:0.03261606767773628\n",
      "Epoch 81, Batch 53 Loss:0.011843172833323479\n",
      "Epoch 81, Batch 54 Loss:0.026183966547250748\n",
      "Epoch 81, Batch 55 Loss:0.025759300217032433\n",
      "Epoch 81, Batch 56 Loss:0.024468226358294487\n",
      "Epoch 81, Batch 57 Loss:0.016102878376841545\n",
      "Epoch 81, Batch 58 Loss:0.02816341258585453\n",
      "Epoch 81, Batch 59 Loss:0.01772591471672058\n",
      "Epoch 81, Batch 60 Loss:0.021442005410790443\n",
      "Epoch 81, Batch 61 Loss:0.05550230294466019\n",
      "Epoch 81, Batch 62 Loss:0.00990620069205761\n",
      "Epoch 81, Batch 63 Loss:0.020567139610648155\n",
      "Epoch 81, Batch 64 Loss:0.025770733132958412\n",
      "Epoch 81, Batch 65 Loss:0.016212986782193184\n",
      "Epoch 81, Batch 66 Loss:0.019863655790686607\n",
      "Epoch 81, Batch 67 Loss:0.011701629497110844\n",
      "Epoch 81, Batch 68 Loss:0.01726536639034748\n",
      "Epoch 81, Batch 69 Loss:0.007426464464515448\n",
      "Epoch 81, Batch 70 Loss:0.009869074448943138\n",
      "Epoch 81, Batch 71 Loss:0.02656686119735241\n",
      "Epoch 81, Batch 72 Loss:0.027996910735964775\n",
      "Epoch 81, Batch 73 Loss:0.026461610570549965\n",
      "Epoch 81, Batch 74 Loss:0.004908385686576366\n",
      "Epoch 81, Batch 75 Loss:0.027190055698156357\n",
      "Epoch 81, Batch 76 Loss:0.07687997072935104\n",
      "Epoch 81, Batch 77 Loss:0.013021225109696388\n",
      "Epoch 81, Batch 78 Loss:0.055209189653396606\n",
      "Epoch 81, Batch 79 Loss:0.0487925261259079\n",
      "Epoch 81, Batch 80 Loss:0.02147025428712368\n",
      "Epoch 81, Batch 81 Loss:0.039982836693525314\n",
      "Epoch 81, Batch 82 Loss:0.016104379668831825\n",
      "Epoch 81, Batch 83 Loss:0.02484314888715744\n",
      "Epoch 81, Batch 84 Loss:0.031793806701898575\n",
      "Epoch 81, Batch 85 Loss:0.024086277931928635\n",
      "Epoch 81, Batch 86 Loss:0.027385499328374863\n",
      "Epoch 81, Batch 87 Loss:0.0168314129114151\n",
      "Epoch 81, Batch 88 Loss:0.007495309691876173\n",
      "Epoch 81, Batch 89 Loss:0.003919883165508509\n",
      "Epoch 81, Batch 90 Loss:0.005494887474924326\n",
      "Epoch 81, Batch 91 Loss:0.029943501576781273\n",
      "Epoch 81, Batch 92 Loss:0.026079051196575165\n",
      "Epoch 81, Batch 93 Loss:0.031819526106119156\n",
      "Epoch 81, Batch 94 Loss:0.020045733079314232\n",
      "Epoch 81, Batch 95 Loss:0.020411286503076553\n",
      "Epoch 81, Batch 96 Loss:0.028101110830903053\n",
      "Epoch 81, Batch 97 Loss:0.004673340357840061\n",
      "Epoch 81, Batch 98 Loss:0.012926190160214901\n",
      "Epoch 81, Batch 99 Loss:0.0238693505525589\n",
      "Epoch 81, Batch 100 Loss:0.016595149412751198\n",
      "Epoch 81, Batch 101 Loss:0.013394414447247982\n",
      "Epoch 81, Batch 102 Loss:0.015273193828761578\n",
      "Epoch 81, Batch 103 Loss:0.028554793447256088\n",
      "Epoch 81, Batch 104 Loss:0.03237370774149895\n",
      "Epoch 81, Batch 105 Loss:0.028997313231229782\n",
      "Epoch 81, Batch 106 Loss:0.019100096076726913\n",
      "Epoch 81, Batch 107 Loss:0.016282442957162857\n",
      "Epoch 81, Batch 108 Loss:0.011013584211468697\n",
      "Epoch 81, Batch 109 Loss:0.004777953028678894\n",
      "Epoch 81, Batch 110 Loss:0.0024451976642012596\n",
      "Epoch 81, Batch 111 Loss:0.042062949389219284\n",
      "Epoch 81, Batch 112 Loss:0.019337037578225136\n",
      "Epoch 81, Batch 113 Loss:0.0017755692824721336\n",
      "Epoch 81, Batch 114 Loss:0.023458003997802734\n",
      "Epoch 81, Batch 115 Loss:0.008646225556731224\n",
      "Epoch 81, Batch 116 Loss:0.021590450778603554\n",
      "Epoch 81, Batch 117 Loss:0.009049009531736374\n",
      "Epoch 81, Batch 118 Loss:0.009587196633219719\n",
      "Epoch 81, Batch 119 Loss:0.008459395729005337\n",
      "Epoch 81, Batch 120 Loss:0.009315339848399162\n",
      "Epoch 81, Batch 121 Loss:0.026517588645219803\n",
      "Epoch 81, Batch 122 Loss:0.024483036249876022\n",
      "Epoch 81, Batch 123 Loss:0.010850705206394196\n",
      "Epoch 81, Batch 124 Loss:0.003305264748632908\n",
      "Epoch 81, Batch 125 Loss:0.03567209467291832\n",
      "Epoch 81, Batch 126 Loss:0.041533198207616806\n",
      "Epoch 81, Batch 127 Loss:0.03144025802612305\n",
      "Epoch 81, Batch 128 Loss:0.002522732364013791\n",
      "Epoch 81, Batch 129 Loss:0.013089395128190517\n",
      "Epoch 81, Batch 130 Loss:0.020428307354450226\n",
      "Epoch 81, Batch 131 Loss:0.025629254058003426\n",
      "Epoch 81, Batch 132 Loss:0.058402128517627716\n",
      "Epoch 81, Batch 133 Loss:0.006801805458962917\n",
      "Epoch 81, Batch 134 Loss:0.026965303346514702\n",
      "Epoch 81, Batch 135 Loss:0.008246775716543198\n",
      "Epoch 81, Batch 136 Loss:0.010246971622109413\n",
      "Epoch 81, Batch 137 Loss:0.006465534679591656\n",
      "Epoch 81, Batch 138 Loss:0.005036104470491409\n",
      "Epoch 81, Batch 139 Loss:0.0687289759516716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81, Batch 140 Loss:0.00338272238150239\n",
      "Epoch 81, Batch 141 Loss:0.024123679846525192\n",
      "Epoch 81, Batch 142 Loss:0.008549138903617859\n",
      "Epoch 81, Batch 143 Loss:0.010300951078534126\n",
      "Epoch 81, Batch 144 Loss:0.01249583251774311\n",
      "Epoch 81, Batch 145 Loss:0.015811897814273834\n",
      "Epoch 81, Batch 146 Loss:0.00929314736276865\n",
      "Epoch 81, Batch 147 Loss:0.0071395039558410645\n",
      "Epoch 81, Batch 148 Loss:0.024854999035596848\n",
      "Epoch 81, Batch 149 Loss:0.0039586154744029045\n",
      "Epoch 81, Batch 150 Loss:0.022082677111029625\n",
      "Epoch 81, Batch 151 Loss:0.017341574653983116\n",
      "Epoch 81, Batch 152 Loss:0.004486773628741503\n",
      "Epoch 81, Batch 153 Loss:0.027858875691890717\n",
      "Epoch 81, Batch 154 Loss:0.028206709772348404\n",
      "Epoch 81, Batch 155 Loss:0.019357912242412567\n",
      "Epoch 81, Batch 156 Loss:0.04174267500638962\n",
      "Epoch 81, Batch 157 Loss:0.01803516037762165\n",
      "Epoch 81, Batch 158 Loss:0.016969293355941772\n",
      "Epoch 81, Batch 159 Loss:0.027946723625063896\n",
      "Epoch 81, Batch 160 Loss:0.013379795476794243\n",
      "Epoch 81, Batch 161 Loss:0.03920428454875946\n",
      "Epoch 81, Batch 162 Loss:0.022388145327568054\n",
      "Epoch 81, Batch 163 Loss:0.02972075715661049\n",
      "Epoch 81, Batch 164 Loss:0.019181180745363235\n",
      "Epoch 81, Batch 165 Loss:0.013031993061304092\n",
      "Epoch 81, Batch 166 Loss:0.03746853768825531\n",
      "Epoch 81, Batch 167 Loss:0.06140059605240822\n",
      "Epoch 81, Batch 168 Loss:0.08729802817106247\n",
      "Epoch 81, Batch 169 Loss:0.04353567212820053\n",
      "Epoch 81, Batch 170 Loss:0.015309341251850128\n",
      "Epoch 81, Batch 171 Loss:0.010977561585605145\n",
      "Epoch 81, Batch 172 Loss:0.05012361705303192\n",
      "Epoch 81, Batch 173 Loss:0.05435171723365784\n",
      "Epoch 81, Batch 174 Loss:0.021812764927744865\n",
      "Epoch 81, Batch 175 Loss:0.07578834891319275\n",
      "Epoch 81, Batch 176 Loss:0.05991244316101074\n",
      "Epoch 81, Batch 177 Loss:0.03961388021707535\n",
      "Epoch 81, Batch 178 Loss:0.03253156691789627\n",
      "Epoch 81, Batch 179 Loss:0.0701431855559349\n",
      "Epoch 81, Batch 180 Loss:0.13748377561569214\n",
      "Epoch 81, Batch 181 Loss:0.013349100016057491\n",
      "Epoch 81, Batch 182 Loss:0.030406109988689423\n",
      "Epoch 81, Batch 183 Loss:0.04288055747747421\n",
      "Epoch 81, Batch 184 Loss:0.038439810276031494\n",
      "Epoch 81, Batch 185 Loss:0.0468594953417778\n",
      "Epoch 81, Batch 186 Loss:0.020611781626939774\n",
      "Epoch 81, Batch 187 Loss:0.07414153963327408\n",
      "Epoch 81, Batch 188 Loss:0.07338554412126541\n",
      "Epoch 81, Batch 189 Loss:0.10181492567062378\n",
      "Epoch 81, Batch 190 Loss:0.11376570165157318\n",
      "Epoch 81, Batch 191 Loss:0.0863681435585022\n",
      "Epoch 81, Batch 192 Loss:0.036432720720767975\n",
      "Epoch 81, Batch 193 Loss:0.08774136006832123\n",
      "Epoch 81, Batch 194 Loss:0.06475815922021866\n",
      "Epoch 81, Batch 195 Loss:0.06349262595176697\n",
      "Epoch 81, Batch 196 Loss:0.02799404226243496\n",
      "Epoch 81, Batch 197 Loss:0.025654057040810585\n",
      "Epoch 81, Batch 198 Loss:0.024137215688824654\n",
      "Epoch 81, Batch 199 Loss:0.03997064381837845\n",
      "Epoch 81, Batch 200 Loss:0.03376861661672592\n",
      "Epoch 81, Batch 201 Loss:0.14029589295387268\n",
      "Epoch 81, Batch 202 Loss:0.028105929493904114\n",
      "Epoch 81, Batch 203 Loss:0.17263163626194\n",
      "Epoch 81, Batch 204 Loss:0.018828563392162323\n",
      "Epoch 81, Batch 205 Loss:0.029343174770474434\n",
      "Epoch 81, Batch 206 Loss:0.007413266226649284\n",
      "Epoch 81, Batch 207 Loss:0.02672600746154785\n",
      "Epoch 81, Batch 208 Loss:0.03647627681493759\n",
      "Epoch 81, Batch 209 Loss:0.060461558401584625\n",
      "Epoch 81, Batch 210 Loss:0.0364685095846653\n",
      "Epoch 81, Batch 211 Loss:0.0213374774903059\n",
      "Epoch 81, Batch 212 Loss:0.05983823537826538\n",
      "Epoch 81, Batch 213 Loss:0.0564449168741703\n",
      "Epoch 81, Batch 214 Loss:0.046578548848629\n",
      "Epoch 81, Batch 215 Loss:0.01038682833313942\n",
      "Epoch 81, Batch 216 Loss:0.019254790619015694\n",
      "Epoch 81, Batch 217 Loss:0.03568797558546066\n",
      "Epoch 81, Batch 218 Loss:0.014045794494450092\n",
      "Epoch 81, Batch 219 Loss:0.033242467790842056\n",
      "Epoch 81, Batch 220 Loss:0.01022491604089737\n",
      "Epoch 81, Batch 221 Loss:0.014114548452198505\n",
      "Epoch 81, Batch 222 Loss:0.009110328741371632\n",
      "Epoch 81, Batch 223 Loss:0.005907800979912281\n",
      "Epoch 81, Batch 224 Loss:0.00961251836270094\n",
      "Epoch 81, Batch 225 Loss:0.02158898301422596\n",
      "Epoch 81, Batch 226 Loss:0.029350940138101578\n",
      "Epoch 81, Batch 227 Loss:0.04077073931694031\n",
      "Epoch 81, Batch 228 Loss:0.0359015092253685\n",
      "Epoch 81, Batch 229 Loss:0.008996463380753994\n",
      "Epoch 81, Batch 230 Loss:0.011613685637712479\n",
      "Epoch 81, Batch 231 Loss:0.030217882245779037\n",
      "Epoch 81, Batch 232 Loss:0.033825505524873734\n",
      "Epoch 81, Batch 233 Loss:0.027330618351697922\n",
      "Loss in this Epoch is: 2.73306183517 %\n",
      "Accuracy in this Epoch is: 88.480001688 %\n",
      "Epoch 82, Batch 0 Loss:0.009640918113291264\n",
      "Epoch 82, Batch 1 Loss:0.005493065807968378\n",
      "Epoch 82, Batch 2 Loss:0.008451521396636963\n",
      "Epoch 82, Batch 3 Loss:0.06271320581436157\n",
      "Epoch 82, Batch 4 Loss:0.05019473657011986\n",
      "Epoch 82, Batch 5 Loss:0.06328553706407547\n",
      "Epoch 82, Batch 6 Loss:0.0383683443069458\n",
      "Epoch 82, Batch 7 Loss:0.029565580189228058\n",
      "Epoch 82, Batch 8 Loss:0.06655775010585785\n",
      "Epoch 82, Batch 9 Loss:0.032419245690107346\n",
      "Epoch 82, Batch 10 Loss:0.028416791930794716\n",
      "Epoch 82, Batch 11 Loss:0.03247933089733124\n",
      "Epoch 82, Batch 12 Loss:0.043729521334171295\n",
      "Epoch 82, Batch 13 Loss:0.03245887905359268\n",
      "Epoch 82, Batch 14 Loss:0.03705476596951485\n",
      "Epoch 82, Batch 15 Loss:0.06465090811252594\n",
      "Epoch 82, Batch 16 Loss:0.03230772539973259\n",
      "Epoch 82, Batch 17 Loss:0.08715364336967468\n",
      "Epoch 82, Batch 18 Loss:0.0565398707985878\n",
      "Epoch 82, Batch 19 Loss:0.021961845457553864\n",
      "Epoch 82, Batch 20 Loss:0.07452953606843948\n",
      "Epoch 82, Batch 21 Loss:0.020235011354088783\n",
      "Epoch 82, Batch 22 Loss:0.0332968533039093\n",
      "Epoch 82, Batch 23 Loss:0.02310393564403057\n",
      "Epoch 82, Batch 24 Loss:0.01181873120367527\n",
      "Epoch 82, Batch 25 Loss:0.07661613076925278\n",
      "Epoch 82, Batch 26 Loss:0.03862924501299858\n",
      "Epoch 82, Batch 27 Loss:0.03579261526465416\n",
      "Epoch 82, Batch 28 Loss:0.023479552939534187\n",
      "Epoch 82, Batch 29 Loss:0.00815557036548853\n",
      "Epoch 82, Batch 30 Loss:0.0055679865181446075\n",
      "Epoch 82, Batch 31 Loss:0.004743899218738079\n",
      "Epoch 82, Batch 32 Loss:0.037748660892248154\n",
      "Epoch 82, Batch 33 Loss:0.04484173282980919\n",
      "Epoch 82, Batch 34 Loss:0.04372483864426613\n",
      "Epoch 82, Batch 35 Loss:0.008442526683211327\n",
      "Epoch 82, Batch 36 Loss:0.02929060161113739\n",
      "Epoch 82, Batch 37 Loss:0.006526496261358261\n",
      "Epoch 82, Batch 38 Loss:0.010051725432276726\n",
      "Epoch 82, Batch 39 Loss:0.009370951913297176\n",
      "Epoch 82, Batch 40 Loss:0.01789085939526558\n",
      "Epoch 82, Batch 41 Loss:0.007656111381947994\n",
      "Epoch 82, Batch 42 Loss:0.02393929474055767\n",
      "Epoch 82, Batch 43 Loss:0.007379418704658747\n",
      "Epoch 82, Batch 44 Loss:0.006744502577930689\n",
      "Epoch 82, Batch 45 Loss:0.016711419448256493\n",
      "Epoch 82, Batch 46 Loss:0.022731231525540352\n",
      "Epoch 82, Batch 47 Loss:0.01984984427690506\n",
      "Epoch 82, Batch 48 Loss:0.013738678768277168\n",
      "Epoch 82, Batch 49 Loss:0.01591583713889122\n",
      "Epoch 82, Batch 50 Loss:0.0388067364692688\n",
      "Epoch 82, Batch 51 Loss:0.009743880480527878\n",
      "Epoch 82, Batch 52 Loss:0.010823515243828297\n",
      "Epoch 82, Batch 53 Loss:0.018750159069895744\n",
      "Epoch 82, Batch 54 Loss:0.007217648904770613\n",
      "Epoch 82, Batch 55 Loss:0.003378969617187977\n",
      "Epoch 82, Batch 56 Loss:0.0020271961111575365\n",
      "Epoch 82, Batch 57 Loss:0.00729647371917963\n",
      "Epoch 82, Batch 58 Loss:0.006941804196685553\n",
      "Epoch 82, Batch 59 Loss:0.015139434486627579\n",
      "Epoch 82, Batch 60 Loss:0.003207549685612321\n",
      "Epoch 82, Batch 61 Loss:0.01147481705993414\n",
      "Epoch 82, Batch 62 Loss:0.016546843573451042\n",
      "Epoch 82, Batch 63 Loss:0.012525035068392754\n",
      "Epoch 82, Batch 64 Loss:0.021589890122413635\n",
      "Epoch 82, Batch 65 Loss:0.04099496081471443\n",
      "Epoch 82, Batch 66 Loss:0.01023022923618555\n",
      "Epoch 82, Batch 67 Loss:0.008849449455738068\n",
      "Epoch 82, Batch 68 Loss:0.010402319952845573\n",
      "Epoch 82, Batch 69 Loss:0.009515665471553802\n",
      "Epoch 82, Batch 70 Loss:0.0020483671687543392\n",
      "Epoch 82, Batch 71 Loss:0.001609056955203414\n",
      "Epoch 82, Batch 72 Loss:0.0033504159655421972\n",
      "Epoch 82, Batch 73 Loss:0.019880540668964386\n",
      "Epoch 82, Batch 74 Loss:0.03483497351408005\n",
      "Epoch 82, Batch 75 Loss:0.010794950649142265\n",
      "Epoch 82, Batch 76 Loss:0.014796603471040726\n",
      "Epoch 82, Batch 77 Loss:0.004809076897799969\n",
      "Epoch 82, Batch 78 Loss:0.005411999300122261\n",
      "Epoch 82, Batch 79 Loss:0.05021650344133377\n",
      "Epoch 82, Batch 80 Loss:0.02789110690355301\n",
      "Epoch 82, Batch 81 Loss:0.006078579928725958\n",
      "Epoch 82, Batch 82 Loss:0.005526045337319374\n",
      "Epoch 82, Batch 83 Loss:0.02999040111899376\n",
      "Epoch 82, Batch 84 Loss:0.011314275674521923\n",
      "Epoch 82, Batch 85 Loss:0.003661707043647766\n",
      "Epoch 82, Batch 86 Loss:0.0070269666612148285\n",
      "Epoch 82, Batch 87 Loss:0.004026426468044519\n",
      "Epoch 82, Batch 88 Loss:0.007906447164714336\n",
      "Epoch 82, Batch 89 Loss:0.02591942809522152\n",
      "Epoch 82, Batch 90 Loss:0.047808945178985596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82, Batch 91 Loss:0.005655235610902309\n",
      "Epoch 82, Batch 92 Loss:0.0032908355351537466\n",
      "Epoch 82, Batch 93 Loss:0.005404922179877758\n",
      "Epoch 82, Batch 94 Loss:0.007596844341605902\n",
      "Epoch 82, Batch 95 Loss:0.002375551965087652\n",
      "Epoch 82, Batch 96 Loss:0.005029871128499508\n",
      "Epoch 82, Batch 97 Loss:0.01205911859869957\n",
      "Epoch 82, Batch 98 Loss:0.01474403589963913\n",
      "Epoch 82, Batch 99 Loss:0.018066490069031715\n",
      "Epoch 82, Batch 100 Loss:0.027503374963998795\n",
      "Epoch 82, Batch 101 Loss:0.04255502298474312\n",
      "Epoch 82, Batch 102 Loss:0.01184153649955988\n",
      "Epoch 82, Batch 103 Loss:0.01040881872177124\n",
      "Epoch 82, Batch 104 Loss:0.008742750622332096\n",
      "Epoch 82, Batch 105 Loss:0.015251822769641876\n",
      "Epoch 82, Batch 106 Loss:0.006147637963294983\n",
      "Epoch 82, Batch 107 Loss:0.0032042530365288258\n",
      "Epoch 82, Batch 108 Loss:0.020938899368047714\n",
      "Epoch 82, Batch 109 Loss:0.004896110855042934\n",
      "Epoch 82, Batch 110 Loss:0.011031894013285637\n",
      "Epoch 82, Batch 111 Loss:0.04426008090376854\n",
      "Epoch 82, Batch 112 Loss:0.028776079416275024\n",
      "Epoch 82, Batch 113 Loss:0.005951021797955036\n",
      "Epoch 82, Batch 114 Loss:0.020094234496355057\n",
      "Epoch 82, Batch 115 Loss:0.038562968373298645\n",
      "Epoch 82, Batch 116 Loss:0.011627083644270897\n",
      "Epoch 82, Batch 117 Loss:0.014265257865190506\n",
      "Epoch 82, Batch 118 Loss:0.03675565868616104\n",
      "Epoch 82, Batch 119 Loss:0.017247483134269714\n",
      "Epoch 82, Batch 120 Loss:0.027090681716799736\n",
      "Epoch 82, Batch 121 Loss:0.022256897762417793\n",
      "Epoch 82, Batch 122 Loss:0.006182339042425156\n",
      "Epoch 82, Batch 123 Loss:0.010877053253352642\n",
      "Epoch 82, Batch 124 Loss:0.027740037068724632\n",
      "Epoch 82, Batch 125 Loss:0.015267488546669483\n",
      "Epoch 82, Batch 126 Loss:0.023717639967799187\n",
      "Epoch 82, Batch 127 Loss:0.009391576051712036\n",
      "Epoch 82, Batch 128 Loss:0.05393883213400841\n",
      "Epoch 82, Batch 129 Loss:0.02095225639641285\n",
      "Epoch 82, Batch 130 Loss:0.015416965819895267\n",
      "Epoch 82, Batch 131 Loss:0.023720044642686844\n",
      "Epoch 82, Batch 132 Loss:0.030315320938825607\n",
      "Epoch 82, Batch 133 Loss:0.06137850135564804\n",
      "Epoch 82, Batch 134 Loss:0.05719427764415741\n",
      "Epoch 82, Batch 135 Loss:0.01592831127345562\n",
      "Epoch 82, Batch 136 Loss:0.015970464795827866\n",
      "Epoch 82, Batch 137 Loss:0.008795158006250858\n",
      "Epoch 82, Batch 138 Loss:0.0261936467140913\n",
      "Epoch 82, Batch 139 Loss:0.015646053478121758\n",
      "Epoch 82, Batch 140 Loss:0.017084361985325813\n",
      "Epoch 82, Batch 141 Loss:0.04397942125797272\n",
      "Epoch 82, Batch 142 Loss:0.011722135357558727\n",
      "Epoch 82, Batch 143 Loss:0.01581786572933197\n",
      "Epoch 82, Batch 144 Loss:0.011394585482776165\n",
      "Epoch 82, Batch 145 Loss:0.014591283164918423\n",
      "Epoch 82, Batch 146 Loss:0.03456167131662369\n",
      "Epoch 82, Batch 147 Loss:0.013962904922664165\n",
      "Epoch 82, Batch 148 Loss:0.011909060180187225\n",
      "Epoch 82, Batch 149 Loss:0.0202188640832901\n",
      "Epoch 82, Batch 150 Loss:0.011404689401388168\n",
      "Epoch 82, Batch 151 Loss:0.009184116497635841\n",
      "Epoch 82, Batch 152 Loss:0.011961698532104492\n",
      "Epoch 82, Batch 153 Loss:0.037237707525491714\n",
      "Epoch 82, Batch 154 Loss:0.009465699084103107\n",
      "Epoch 82, Batch 155 Loss:0.0037980456836521626\n",
      "Epoch 82, Batch 156 Loss:0.024772316217422485\n",
      "Epoch 82, Batch 157 Loss:0.011698516085743904\n",
      "Epoch 82, Batch 158 Loss:0.008009734563529491\n",
      "Epoch 82, Batch 159 Loss:0.029863746836781502\n",
      "Epoch 82, Batch 160 Loss:0.018527545034885406\n",
      "Epoch 82, Batch 161 Loss:0.011412003077566624\n",
      "Epoch 82, Batch 162 Loss:0.015445182099938393\n",
      "Epoch 82, Batch 163 Loss:0.03216719254851341\n",
      "Epoch 82, Batch 164 Loss:0.011660568416118622\n",
      "Epoch 82, Batch 165 Loss:0.05245305597782135\n",
      "Epoch 82, Batch 166 Loss:0.03949708491563797\n",
      "Epoch 82, Batch 167 Loss:0.017800714820623398\n",
      "Epoch 82, Batch 168 Loss:0.055519621819257736\n",
      "Epoch 82, Batch 169 Loss:0.009588360786437988\n",
      "Epoch 82, Batch 170 Loss:0.0191997941583395\n",
      "Epoch 82, Batch 171 Loss:0.012090800330042839\n",
      "Epoch 82, Batch 172 Loss:0.01099746860563755\n",
      "Epoch 82, Batch 173 Loss:0.009411653503775597\n",
      "Epoch 82, Batch 174 Loss:0.017029335722327232\n",
      "Epoch 82, Batch 175 Loss:0.012068867683410645\n",
      "Epoch 82, Batch 176 Loss:0.04898886755108833\n",
      "Epoch 82, Batch 177 Loss:0.02552592195570469\n",
      "Epoch 82, Batch 178 Loss:0.009158878587186337\n",
      "Epoch 82, Batch 179 Loss:0.008342823013663292\n",
      "Epoch 82, Batch 180 Loss:0.031156063079833984\n",
      "Epoch 82, Batch 181 Loss:0.036911334842443466\n",
      "Epoch 82, Batch 182 Loss:0.01747298799455166\n",
      "Epoch 82, Batch 183 Loss:0.02635742351412773\n",
      "Epoch 82, Batch 184 Loss:0.015024267137050629\n",
      "Epoch 82, Batch 185 Loss:0.022262156009674072\n",
      "Epoch 82, Batch 186 Loss:0.011250527575612068\n",
      "Epoch 82, Batch 187 Loss:0.017653688788414\n",
      "Epoch 82, Batch 188 Loss:0.019613251090049744\n",
      "Epoch 82, Batch 189 Loss:0.015148449689149857\n",
      "Epoch 82, Batch 190 Loss:0.01181752234697342\n",
      "Epoch 82, Batch 191 Loss:0.05733874440193176\n",
      "Epoch 82, Batch 192 Loss:0.023768814280629158\n",
      "Epoch 82, Batch 193 Loss:0.015738127753138542\n",
      "Epoch 82, Batch 194 Loss:0.007317192852497101\n",
      "Epoch 82, Batch 195 Loss:0.016976796090602875\n",
      "Epoch 82, Batch 196 Loss:0.007199285086244345\n",
      "Epoch 82, Batch 197 Loss:0.012622158974409103\n",
      "Epoch 82, Batch 198 Loss:0.004657197743654251\n",
      "Epoch 82, Batch 199 Loss:0.03308083489537239\n",
      "Epoch 82, Batch 200 Loss:0.007891202345490456\n",
      "Epoch 82, Batch 201 Loss:0.01784355752170086\n",
      "Epoch 82, Batch 202 Loss:0.020255012437701225\n",
      "Epoch 82, Batch 203 Loss:0.008432378061115742\n",
      "Epoch 82, Batch 204 Loss:0.011581746861338615\n",
      "Epoch 82, Batch 205 Loss:0.02252846583724022\n",
      "Epoch 82, Batch 206 Loss:0.05894729122519493\n",
      "Epoch 82, Batch 207 Loss:0.05681011080741882\n",
      "Epoch 82, Batch 208 Loss:0.040166959166526794\n",
      "Epoch 82, Batch 209 Loss:0.031059052795171738\n",
      "Epoch 82, Batch 210 Loss:0.004258908797055483\n",
      "Epoch 82, Batch 211 Loss:0.018596716225147247\n",
      "Epoch 82, Batch 212 Loss:0.007121926173567772\n",
      "Epoch 82, Batch 213 Loss:0.026789873838424683\n",
      "Epoch 82, Batch 214 Loss:0.023612992838025093\n",
      "Epoch 82, Batch 215 Loss:0.019479725509881973\n",
      "Epoch 82, Batch 216 Loss:0.06615924835205078\n",
      "Epoch 82, Batch 217 Loss:0.009119409136474133\n",
      "Epoch 82, Batch 218 Loss:0.03376109153032303\n",
      "Epoch 82, Batch 219 Loss:0.01827765628695488\n",
      "Epoch 82, Batch 220 Loss:0.006378783844411373\n",
      "Epoch 82, Batch 221 Loss:0.021761883050203323\n",
      "Epoch 82, Batch 222 Loss:0.0359405018389225\n",
      "Epoch 82, Batch 223 Loss:0.023886390030384064\n",
      "Epoch 82, Batch 224 Loss:0.02972949668765068\n",
      "Epoch 82, Batch 225 Loss:0.008122357539832592\n",
      "Epoch 82, Batch 226 Loss:0.013092199340462685\n",
      "Epoch 82, Batch 227 Loss:0.003847365267574787\n",
      "Epoch 82, Batch 228 Loss:0.01570926606655121\n",
      "Epoch 82, Batch 229 Loss:0.00862487219274044\n",
      "Epoch 82, Batch 230 Loss:0.0073668756522238255\n",
      "Epoch 82, Batch 231 Loss:0.007870660163462162\n",
      "Epoch 82, Batch 232 Loss:0.030036184936761856\n",
      "Epoch 82, Batch 233 Loss:0.003617567243054509\n",
      "Loss in this Epoch is: 0.361756724305 %\n",
      "Accuracy in this Epoch is: 88.789999485 %\n",
      "Epoch 83, Batch 0 Loss:0.00706896185874939\n",
      "Epoch 83, Batch 1 Loss:0.005199988838285208\n",
      "Epoch 83, Batch 2 Loss:0.005409537814557552\n",
      "Epoch 83, Batch 3 Loss:0.006319648586213589\n",
      "Epoch 83, Batch 4 Loss:0.008434578776359558\n",
      "Epoch 83, Batch 5 Loss:0.013173996470868587\n",
      "Epoch 83, Batch 6 Loss:0.016711775213479996\n",
      "Epoch 83, Batch 7 Loss:0.01177456509321928\n",
      "Epoch 83, Batch 8 Loss:0.004007678013294935\n",
      "Epoch 83, Batch 9 Loss:0.008072488009929657\n",
      "Epoch 83, Batch 10 Loss:0.0024696625769138336\n",
      "Epoch 83, Batch 11 Loss:0.003143521025776863\n",
      "Epoch 83, Batch 12 Loss:0.018255053088068962\n",
      "Epoch 83, Batch 13 Loss:0.014172335155308247\n",
      "Epoch 83, Batch 14 Loss:0.011102831922471523\n",
      "Epoch 83, Batch 15 Loss:0.007408914156258106\n",
      "Epoch 83, Batch 16 Loss:0.005052344873547554\n",
      "Epoch 83, Batch 17 Loss:0.003055638400837779\n",
      "Epoch 83, Batch 18 Loss:0.012532238848507404\n",
      "Epoch 83, Batch 19 Loss:0.006838739383965731\n",
      "Epoch 83, Batch 20 Loss:0.011736324056982994\n",
      "Epoch 83, Batch 21 Loss:0.006175714079290628\n",
      "Epoch 83, Batch 22 Loss:0.0023851946461945772\n",
      "Epoch 83, Batch 23 Loss:0.036396533250808716\n",
      "Epoch 83, Batch 24 Loss:0.029196707531809807\n",
      "Epoch 83, Batch 25 Loss:0.008021514862775803\n",
      "Epoch 83, Batch 26 Loss:0.012737981975078583\n",
      "Epoch 83, Batch 27 Loss:0.002214704640209675\n",
      "Epoch 83, Batch 28 Loss:0.0036543842870742083\n",
      "Epoch 83, Batch 29 Loss:0.009273038245737553\n",
      "Epoch 83, Batch 30 Loss:0.010799498297274113\n",
      "Epoch 83, Batch 31 Loss:0.011310738511383533\n",
      "Epoch 83, Batch 32 Loss:0.002113252878189087\n",
      "Epoch 83, Batch 33 Loss:0.011829796247184277\n",
      "Epoch 83, Batch 34 Loss:0.011994970962405205\n",
      "Epoch 83, Batch 35 Loss:0.0048798429779708385\n",
      "Epoch 83, Batch 36 Loss:0.01581127941608429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Batch 37 Loss:0.003526257583871484\n",
      "Epoch 83, Batch 38 Loss:0.018425006419420242\n",
      "Epoch 83, Batch 39 Loss:0.01144715677946806\n",
      "Epoch 83, Batch 40 Loss:0.002431741915643215\n",
      "Epoch 83, Batch 41 Loss:0.0075889709405601025\n",
      "Epoch 83, Batch 42 Loss:0.005843012593686581\n",
      "Epoch 83, Batch 43 Loss:0.018252091482281685\n",
      "Epoch 83, Batch 44 Loss:0.005308754742145538\n",
      "Epoch 83, Batch 45 Loss:0.004636368714272976\n",
      "Epoch 83, Batch 46 Loss:0.004187095444649458\n",
      "Epoch 83, Batch 47 Loss:0.006667675916105509\n",
      "Epoch 83, Batch 48 Loss:0.00791204534471035\n",
      "Epoch 83, Batch 49 Loss:0.013791135512292385\n",
      "Epoch 83, Batch 50 Loss:0.0027456162497401237\n",
      "Epoch 83, Batch 51 Loss:0.002164639765396714\n",
      "Epoch 83, Batch 52 Loss:0.01614336483180523\n",
      "Epoch 83, Batch 53 Loss:0.014697529375553131\n",
      "Epoch 83, Batch 54 Loss:0.002371265087276697\n",
      "Epoch 83, Batch 55 Loss:0.009550725109875202\n",
      "Epoch 83, Batch 56 Loss:0.0019768434576690197\n",
      "Epoch 83, Batch 57 Loss:0.0036548899952322245\n",
      "Epoch 83, Batch 58 Loss:0.005558294244110584\n",
      "Epoch 83, Batch 59 Loss:0.0055194939486682415\n",
      "Epoch 83, Batch 60 Loss:0.021101156249642372\n",
      "Epoch 83, Batch 61 Loss:0.0033682966604828835\n",
      "Epoch 83, Batch 62 Loss:0.01334682572633028\n",
      "Epoch 83, Batch 63 Loss:0.00821700319647789\n",
      "Epoch 83, Batch 64 Loss:0.0009999731555581093\n",
      "Epoch 83, Batch 65 Loss:0.0029207663610577583\n",
      "Epoch 83, Batch 66 Loss:0.0016033579595386982\n",
      "Epoch 83, Batch 67 Loss:0.005540008191019297\n",
      "Epoch 83, Batch 68 Loss:0.007511441130191088\n",
      "Epoch 83, Batch 69 Loss:0.0048433043994009495\n",
      "Epoch 83, Batch 70 Loss:0.008673365227878094\n",
      "Epoch 83, Batch 71 Loss:0.010321585461497307\n",
      "Epoch 83, Batch 72 Loss:0.00929404515773058\n",
      "Epoch 83, Batch 73 Loss:0.00456543592736125\n",
      "Epoch 83, Batch 74 Loss:0.021562770009040833\n",
      "Epoch 83, Batch 75 Loss:0.0021176759619265795\n",
      "Epoch 83, Batch 76 Loss:0.0022405553609132767\n",
      "Epoch 83, Batch 77 Loss:0.005312435328960419\n",
      "Epoch 83, Batch 78 Loss:0.018705038353800774\n",
      "Epoch 83, Batch 79 Loss:0.0023872004821896553\n",
      "Epoch 83, Batch 80 Loss:0.0040449644438922405\n",
      "Epoch 83, Batch 81 Loss:0.004924594424664974\n",
      "Epoch 83, Batch 82 Loss:0.0015339350793510675\n",
      "Epoch 83, Batch 83 Loss:0.0020279104355722666\n",
      "Epoch 83, Batch 84 Loss:0.005772636272013187\n",
      "Epoch 83, Batch 85 Loss:0.0017483265837654471\n",
      "Epoch 83, Batch 86 Loss:0.015050536021590233\n",
      "Epoch 83, Batch 87 Loss:0.028527328744530678\n",
      "Epoch 83, Batch 88 Loss:0.04450090229511261\n",
      "Epoch 83, Batch 89 Loss:0.007773114368319511\n",
      "Epoch 83, Batch 90 Loss:0.010333971120417118\n",
      "Epoch 83, Batch 91 Loss:0.012761001475155354\n",
      "Epoch 83, Batch 92 Loss:0.02989255264401436\n",
      "Epoch 83, Batch 93 Loss:0.006409377790987492\n",
      "Epoch 83, Batch 94 Loss:0.07081205397844315\n",
      "Epoch 83, Batch 95 Loss:0.053027886897325516\n",
      "Epoch 83, Batch 96 Loss:0.013229343108832836\n",
      "Epoch 83, Batch 97 Loss:0.06785665452480316\n",
      "Epoch 83, Batch 98 Loss:0.017984099686145782\n",
      "Epoch 83, Batch 99 Loss:0.02618386223912239\n",
      "Epoch 83, Batch 100 Loss:0.0036914884112775326\n",
      "Epoch 83, Batch 101 Loss:0.00508748646825552\n",
      "Epoch 83, Batch 102 Loss:0.004469168372452259\n",
      "Epoch 83, Batch 103 Loss:0.0030360498931258917\n",
      "Epoch 83, Batch 104 Loss:0.006499649491161108\n",
      "Epoch 83, Batch 105 Loss:0.023691419512033463\n",
      "Epoch 83, Batch 106 Loss:0.02983880415558815\n",
      "Epoch 83, Batch 107 Loss:0.013850092887878418\n",
      "Epoch 83, Batch 108 Loss:0.01147957518696785\n",
      "Epoch 83, Batch 109 Loss:0.008922508917748928\n",
      "Epoch 83, Batch 110 Loss:0.004765590187162161\n",
      "Epoch 83, Batch 111 Loss:0.008098793216049671\n",
      "Epoch 83, Batch 112 Loss:0.010707622393965721\n",
      "Epoch 83, Batch 113 Loss:0.0024953412357717752\n",
      "Epoch 83, Batch 114 Loss:0.005541758146136999\n",
      "Epoch 83, Batch 115 Loss:0.0068417866714298725\n",
      "Epoch 83, Batch 116 Loss:0.00743331853300333\n",
      "Epoch 83, Batch 117 Loss:0.006868685595691204\n",
      "Epoch 83, Batch 118 Loss:0.00796344131231308\n",
      "Epoch 83, Batch 119 Loss:0.01957356184720993\n",
      "Epoch 83, Batch 120 Loss:0.002811991609632969\n",
      "Epoch 83, Batch 121 Loss:0.009722413495182991\n",
      "Epoch 83, Batch 122 Loss:0.030375512316823006\n",
      "Epoch 83, Batch 123 Loss:0.01477043703198433\n",
      "Epoch 83, Batch 124 Loss:0.006999277509748936\n",
      "Epoch 83, Batch 125 Loss:0.017625486478209496\n",
      "Epoch 83, Batch 126 Loss:0.01556091196835041\n",
      "Epoch 83, Batch 127 Loss:0.0027730849105864763\n",
      "Epoch 83, Batch 128 Loss:0.015669893473386765\n",
      "Epoch 83, Batch 129 Loss:0.00493038771674037\n",
      "Epoch 83, Batch 130 Loss:0.004651160910725594\n",
      "Epoch 83, Batch 131 Loss:0.00665360176935792\n",
      "Epoch 83, Batch 132 Loss:0.035502463579177856\n",
      "Epoch 83, Batch 133 Loss:0.009872673079371452\n",
      "Epoch 83, Batch 134 Loss:0.005193350836634636\n",
      "Epoch 83, Batch 135 Loss:0.002238037995994091\n",
      "Epoch 83, Batch 136 Loss:0.0037730298936367035\n",
      "Epoch 83, Batch 137 Loss:0.024295173585414886\n",
      "Epoch 83, Batch 138 Loss:0.0053914738819003105\n",
      "Epoch 83, Batch 139 Loss:0.03374160826206207\n",
      "Epoch 83, Batch 140 Loss:0.07775898277759552\n",
      "Epoch 83, Batch 141 Loss:0.007235656026750803\n",
      "Epoch 83, Batch 142 Loss:0.012384330853819847\n",
      "Epoch 83, Batch 143 Loss:0.0032975166104733944\n",
      "Epoch 83, Batch 144 Loss:0.024119166657328606\n",
      "Epoch 83, Batch 145 Loss:0.00816249381750822\n",
      "Epoch 83, Batch 146 Loss:0.013030169531702995\n",
      "Epoch 83, Batch 147 Loss:0.005955325439572334\n",
      "Epoch 83, Batch 148 Loss:0.011946538463234901\n",
      "Epoch 83, Batch 149 Loss:0.017185097560286522\n",
      "Epoch 83, Batch 150 Loss:0.018027182668447495\n",
      "Epoch 83, Batch 151 Loss:0.008026028983294964\n",
      "Epoch 83, Batch 152 Loss:0.011907439678907394\n",
      "Epoch 83, Batch 153 Loss:0.010781904682517052\n",
      "Epoch 83, Batch 154 Loss:0.07185832411050797\n",
      "Epoch 83, Batch 155 Loss:0.02218112349510193\n",
      "Epoch 83, Batch 156 Loss:0.007606480270624161\n",
      "Epoch 83, Batch 157 Loss:0.020926402881741524\n",
      "Epoch 83, Batch 158 Loss:0.023584259673953056\n",
      "Epoch 83, Batch 159 Loss:0.02492581680417061\n",
      "Epoch 83, Batch 160 Loss:0.03514133393764496\n",
      "Epoch 83, Batch 161 Loss:0.021333150565624237\n",
      "Epoch 83, Batch 162 Loss:0.014082593843340874\n",
      "Epoch 83, Batch 163 Loss:0.04733556881546974\n",
      "Epoch 83, Batch 164 Loss:0.028296487405896187\n",
      "Epoch 83, Batch 165 Loss:0.00472443550825119\n",
      "Epoch 83, Batch 166 Loss:0.007114099338650703\n",
      "Epoch 83, Batch 167 Loss:0.01926908828318119\n",
      "Epoch 83, Batch 168 Loss:0.00992891751229763\n",
      "Epoch 83, Batch 169 Loss:0.020093640312552452\n",
      "Epoch 83, Batch 170 Loss:0.01483095996081829\n",
      "Epoch 83, Batch 171 Loss:0.058620333671569824\n",
      "Epoch 83, Batch 172 Loss:0.007944775745272636\n",
      "Epoch 83, Batch 173 Loss:0.018019819632172585\n",
      "Epoch 83, Batch 174 Loss:0.022520970553159714\n",
      "Epoch 83, Batch 175 Loss:0.04649628326296806\n",
      "Epoch 83, Batch 176 Loss:0.030542079359292984\n",
      "Epoch 83, Batch 177 Loss:0.03383856266736984\n",
      "Epoch 83, Batch 178 Loss:0.012286238372325897\n",
      "Epoch 83, Batch 179 Loss:0.033518124371767044\n",
      "Epoch 83, Batch 180 Loss:0.029492268338799477\n",
      "Epoch 83, Batch 181 Loss:0.03505781292915344\n",
      "Epoch 83, Batch 182 Loss:0.021640993654727936\n",
      "Epoch 83, Batch 183 Loss:0.021480072289705276\n",
      "Epoch 83, Batch 184 Loss:0.010755792260169983\n",
      "Epoch 83, Batch 185 Loss:0.012333735823631287\n",
      "Epoch 83, Batch 186 Loss:0.03686722740530968\n",
      "Epoch 83, Batch 187 Loss:0.023144422098994255\n",
      "Epoch 83, Batch 188 Loss:0.011272139847278595\n",
      "Epoch 83, Batch 189 Loss:0.0075617642141878605\n",
      "Epoch 83, Batch 190 Loss:0.030103346332907677\n",
      "Epoch 83, Batch 191 Loss:0.034708812832832336\n",
      "Epoch 83, Batch 192 Loss:0.007889922708272934\n",
      "Epoch 83, Batch 193 Loss:0.012871377170085907\n",
      "Epoch 83, Batch 194 Loss:0.019477732479572296\n",
      "Epoch 83, Batch 195 Loss:0.04047195240855217\n",
      "Epoch 83, Batch 196 Loss:0.06512320786714554\n",
      "Epoch 83, Batch 197 Loss:0.01751636154949665\n",
      "Epoch 83, Batch 198 Loss:0.007760009728372097\n",
      "Epoch 83, Batch 199 Loss:0.008005266077816486\n",
      "Epoch 83, Batch 200 Loss:0.014213516376912594\n",
      "Epoch 83, Batch 201 Loss:0.025980044156312943\n",
      "Epoch 83, Batch 202 Loss:0.019672051072120667\n",
      "Epoch 83, Batch 203 Loss:0.03347984328866005\n",
      "Epoch 83, Batch 204 Loss:0.08197291195392609\n",
      "Epoch 83, Batch 205 Loss:0.024473756551742554\n",
      "Epoch 83, Batch 206 Loss:0.021609582006931305\n",
      "Epoch 83, Batch 207 Loss:0.010904193855822086\n",
      "Epoch 83, Batch 208 Loss:0.033496033400297165\n",
      "Epoch 83, Batch 209 Loss:0.01818782091140747\n",
      "Epoch 83, Batch 210 Loss:0.059623610228300095\n",
      "Epoch 83, Batch 211 Loss:0.01478731818497181\n",
      "Epoch 83, Batch 212 Loss:0.007705069147050381\n",
      "Epoch 83, Batch 213 Loss:0.021819256246089935\n",
      "Epoch 83, Batch 214 Loss:0.026591848582029343\n",
      "Epoch 83, Batch 215 Loss:0.009844984859228134\n",
      "Epoch 83, Batch 216 Loss:0.040761273354291916\n",
      "Epoch 83, Batch 217 Loss:0.028550725430250168\n",
      "Epoch 83, Batch 218 Loss:0.022473949939012527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83, Batch 219 Loss:0.012549486011266708\n",
      "Epoch 83, Batch 220 Loss:0.017707716673612595\n",
      "Epoch 83, Batch 221 Loss:0.01479375921189785\n",
      "Epoch 83, Batch 222 Loss:0.011337410658597946\n",
      "Epoch 83, Batch 223 Loss:0.02199152484536171\n",
      "Epoch 83, Batch 224 Loss:0.005354307591915131\n",
      "Epoch 83, Batch 225 Loss:0.02445290796458721\n",
      "Epoch 83, Batch 226 Loss:0.021614637225866318\n",
      "Epoch 83, Batch 227 Loss:0.03051094152033329\n",
      "Epoch 83, Batch 228 Loss:0.015016157180070877\n",
      "Epoch 83, Batch 229 Loss:0.0468163788318634\n",
      "Epoch 83, Batch 230 Loss:0.007021833211183548\n",
      "Epoch 83, Batch 231 Loss:0.008595841936767101\n",
      "Epoch 83, Batch 232 Loss:0.004214349202811718\n",
      "Epoch 83, Batch 233 Loss:0.011110127903521061\n",
      "Loss in this Epoch is: 1.11101279035 %\n",
      "Accuracy in this Epoch is: 88.630002737 %\n",
      "Epoch 84, Batch 0 Loss:0.02381162717938423\n",
      "Epoch 84, Batch 1 Loss:0.01075921393930912\n",
      "Epoch 84, Batch 2 Loss:0.011909225955605507\n",
      "Epoch 84, Batch 3 Loss:0.017511999234557152\n",
      "Epoch 84, Batch 4 Loss:0.04308321326971054\n",
      "Epoch 84, Batch 5 Loss:0.050167400389909744\n",
      "Epoch 84, Batch 6 Loss:0.025078419595956802\n",
      "Epoch 84, Batch 7 Loss:0.004267001524567604\n",
      "Epoch 84, Batch 8 Loss:0.03924046829342842\n",
      "Epoch 84, Batch 9 Loss:0.004127843305468559\n",
      "Epoch 84, Batch 10 Loss:0.00875539518892765\n",
      "Epoch 84, Batch 11 Loss:0.008178815245628357\n",
      "Epoch 84, Batch 12 Loss:0.005689384415745735\n",
      "Epoch 84, Batch 13 Loss:0.0170233566313982\n",
      "Epoch 84, Batch 14 Loss:0.005631968379020691\n",
      "Epoch 84, Batch 15 Loss:0.006928936578333378\n",
      "Epoch 84, Batch 16 Loss:0.02451154589653015\n",
      "Epoch 84, Batch 17 Loss:0.005370419006794691\n",
      "Epoch 84, Batch 18 Loss:0.02210814878344536\n",
      "Epoch 84, Batch 19 Loss:0.007497611455619335\n",
      "Epoch 84, Batch 20 Loss:0.023835303261876106\n",
      "Epoch 84, Batch 21 Loss:0.013098237104713917\n",
      "Epoch 84, Batch 22 Loss:0.017391212284564972\n",
      "Epoch 84, Batch 23 Loss:0.00937214307487011\n",
      "Epoch 84, Batch 24 Loss:0.008845740929245949\n",
      "Epoch 84, Batch 25 Loss:0.010397129692137241\n",
      "Epoch 84, Batch 26 Loss:0.008074238896369934\n",
      "Epoch 84, Batch 27 Loss:0.007340439595282078\n",
      "Epoch 84, Batch 28 Loss:0.0011328441323712468\n",
      "Epoch 84, Batch 29 Loss:0.0047483197413384914\n",
      "Epoch 84, Batch 30 Loss:0.008381976746022701\n",
      "Epoch 84, Batch 31 Loss:0.0033904556185007095\n",
      "Epoch 84, Batch 32 Loss:0.011984876357018948\n",
      "Epoch 84, Batch 33 Loss:0.026100575923919678\n",
      "Epoch 84, Batch 34 Loss:0.004658819176256657\n",
      "Epoch 84, Batch 35 Loss:0.01463232934474945\n",
      "Epoch 84, Batch 36 Loss:0.029122259467840195\n",
      "Epoch 84, Batch 37 Loss:0.006496816407889128\n",
      "Epoch 84, Batch 38 Loss:0.005055743735283613\n",
      "Epoch 84, Batch 39 Loss:0.004580691922456026\n",
      "Epoch 84, Batch 40 Loss:0.010194625705480576\n",
      "Epoch 84, Batch 41 Loss:0.005583796184509993\n",
      "Epoch 84, Batch 42 Loss:0.003575616516172886\n",
      "Epoch 84, Batch 43 Loss:0.008145893923938274\n",
      "Epoch 84, Batch 44 Loss:0.00830928049981594\n",
      "Epoch 84, Batch 45 Loss:0.06539248675107956\n",
      "Epoch 84, Batch 46 Loss:0.015053070150315762\n",
      "Epoch 84, Batch 47 Loss:0.008145217783749104\n",
      "Epoch 84, Batch 48 Loss:0.01858827844262123\n",
      "Epoch 84, Batch 49 Loss:0.004649206064641476\n",
      "Epoch 84, Batch 50 Loss:0.013712062500417233\n",
      "Epoch 84, Batch 51 Loss:0.017498347908258438\n",
      "Epoch 84, Batch 52 Loss:0.0345015712082386\n",
      "Epoch 84, Batch 53 Loss:0.016671642661094666\n",
      "Epoch 84, Batch 54 Loss:0.004299588967114687\n",
      "Epoch 84, Batch 55 Loss:0.015263749286532402\n",
      "Epoch 84, Batch 56 Loss:0.005294131580740213\n",
      "Epoch 84, Batch 57 Loss:0.0033391534816473722\n",
      "Epoch 84, Batch 58 Loss:0.02872771956026554\n",
      "Epoch 84, Batch 59 Loss:0.004919008817523718\n",
      "Epoch 84, Batch 60 Loss:0.012212730944156647\n",
      "Epoch 84, Batch 61 Loss:0.024945242330431938\n",
      "Epoch 84, Batch 62 Loss:0.0057681165635585785\n",
      "Epoch 84, Batch 63 Loss:0.004054655786603689\n",
      "Epoch 84, Batch 64 Loss:0.019931908696889877\n",
      "Epoch 84, Batch 65 Loss:0.003682861104607582\n",
      "Epoch 84, Batch 66 Loss:0.005694289226084948\n",
      "Epoch 84, Batch 67 Loss:0.0032060404773801565\n",
      "Epoch 84, Batch 68 Loss:0.01941833645105362\n",
      "Epoch 84, Batch 69 Loss:0.003471656935289502\n",
      "Epoch 84, Batch 70 Loss:0.010725526139140129\n",
      "Epoch 84, Batch 71 Loss:0.014832244254648685\n",
      "Epoch 84, Batch 72 Loss:0.014364676550030708\n",
      "Epoch 84, Batch 73 Loss:0.01086872722953558\n",
      "Epoch 84, Batch 74 Loss:0.012583712115883827\n",
      "Epoch 84, Batch 75 Loss:0.003463748376816511\n",
      "Epoch 84, Batch 76 Loss:0.0186922550201416\n",
      "Epoch 84, Batch 77 Loss:0.002175161149352789\n",
      "Epoch 84, Batch 78 Loss:0.0042670173570513725\n",
      "Epoch 84, Batch 79 Loss:0.02614310011267662\n",
      "Epoch 84, Batch 80 Loss:0.0034173086751252413\n",
      "Epoch 84, Batch 81 Loss:0.01401776634156704\n",
      "Epoch 84, Batch 82 Loss:0.011622088961303234\n",
      "Epoch 84, Batch 83 Loss:0.002727988176047802\n",
      "Epoch 84, Batch 84 Loss:0.004065223969519138\n",
      "Epoch 84, Batch 85 Loss:0.016714125871658325\n",
      "Epoch 84, Batch 86 Loss:0.010326710529625416\n",
      "Epoch 84, Batch 87 Loss:0.008662317879498005\n",
      "Epoch 84, Batch 88 Loss:0.012251478619873524\n",
      "Epoch 84, Batch 89 Loss:0.0032654565293341875\n",
      "Epoch 84, Batch 90 Loss:0.004350451752543449\n",
      "Epoch 84, Batch 91 Loss:0.005551584996283054\n",
      "Epoch 84, Batch 92 Loss:0.02621493488550186\n",
      "Epoch 84, Batch 93 Loss:0.01686619594693184\n",
      "Epoch 84, Batch 94 Loss:0.0035540263634175062\n",
      "Epoch 84, Batch 95 Loss:0.0022646624129265547\n",
      "Epoch 84, Batch 96 Loss:0.0031550105195492506\n",
      "Epoch 84, Batch 97 Loss:0.0033549731597304344\n",
      "Epoch 84, Batch 98 Loss:0.014217838644981384\n",
      "Epoch 84, Batch 99 Loss:0.007096041459590197\n",
      "Epoch 84, Batch 100 Loss:0.003452182514593005\n",
      "Epoch 84, Batch 101 Loss:0.018610520288348198\n",
      "Epoch 84, Batch 102 Loss:0.004601586610078812\n",
      "Epoch 84, Batch 103 Loss:0.004631950519979\n",
      "Epoch 84, Batch 104 Loss:0.009471442550420761\n",
      "Epoch 84, Batch 105 Loss:0.002307418268173933\n",
      "Epoch 84, Batch 106 Loss:0.003369989339262247\n",
      "Epoch 84, Batch 107 Loss:0.01167771965265274\n",
      "Epoch 84, Batch 108 Loss:0.03951285406947136\n",
      "Epoch 84, Batch 109 Loss:0.020149827003479004\n",
      "Epoch 84, Batch 110 Loss:0.007289686240255833\n",
      "Epoch 84, Batch 111 Loss:0.007742088288068771\n",
      "Epoch 84, Batch 112 Loss:0.003649703226983547\n",
      "Epoch 84, Batch 113 Loss:0.020449504256248474\n",
      "Epoch 84, Batch 114 Loss:0.0076176878064870834\n",
      "Epoch 84, Batch 115 Loss:0.009626812301576138\n",
      "Epoch 84, Batch 116 Loss:0.002223359188064933\n",
      "Epoch 84, Batch 117 Loss:0.004162290599197149\n",
      "Epoch 84, Batch 118 Loss:0.0017859870567917824\n",
      "Epoch 84, Batch 119 Loss:0.0021868953481316566\n",
      "Epoch 84, Batch 120 Loss:0.00925501063466072\n",
      "Epoch 84, Batch 121 Loss:0.007776572834700346\n",
      "Epoch 84, Batch 122 Loss:0.009450075216591358\n",
      "Epoch 84, Batch 123 Loss:0.005372472107410431\n",
      "Epoch 84, Batch 124 Loss:0.012977536767721176\n",
      "Epoch 84, Batch 125 Loss:0.0043677655048668385\n",
      "Epoch 84, Batch 126 Loss:0.024849817156791687\n",
      "Epoch 84, Batch 127 Loss:0.02984800562262535\n",
      "Epoch 84, Batch 128 Loss:0.007386208511888981\n",
      "Epoch 84, Batch 129 Loss:0.004077096004039049\n",
      "Epoch 84, Batch 130 Loss:0.023537086322903633\n",
      "Epoch 84, Batch 131 Loss:0.011171480640769005\n",
      "Epoch 84, Batch 132 Loss:0.055398084223270416\n",
      "Epoch 84, Batch 133 Loss:0.0009323221747763455\n",
      "Epoch 84, Batch 134 Loss:0.004765759687870741\n",
      "Epoch 84, Batch 135 Loss:0.059233322739601135\n",
      "Epoch 84, Batch 136 Loss:0.011631343513727188\n",
      "Epoch 84, Batch 137 Loss:0.009044450707733631\n",
      "Epoch 84, Batch 138 Loss:0.0025906020309776068\n",
      "Epoch 84, Batch 139 Loss:0.0016654020873829722\n",
      "Epoch 84, Batch 140 Loss:0.004014834761619568\n",
      "Epoch 84, Batch 141 Loss:0.018912149593234062\n",
      "Epoch 84, Batch 142 Loss:0.00465391157194972\n",
      "Epoch 84, Batch 143 Loss:0.014171878807246685\n",
      "Epoch 84, Batch 144 Loss:0.009167008101940155\n",
      "Epoch 84, Batch 145 Loss:0.0044081928208470345\n",
      "Epoch 84, Batch 146 Loss:0.00767100416123867\n",
      "Epoch 84, Batch 147 Loss:0.032204583287239075\n",
      "Epoch 84, Batch 148 Loss:0.005909719038754702\n",
      "Epoch 84, Batch 149 Loss:0.010801978409290314\n",
      "Epoch 84, Batch 150 Loss:0.0242619551718235\n",
      "Epoch 84, Batch 151 Loss:0.0035155927762389183\n",
      "Epoch 84, Batch 152 Loss:0.009950869716703892\n",
      "Epoch 84, Batch 153 Loss:0.014374881982803345\n",
      "Epoch 84, Batch 154 Loss:0.007778037339448929\n",
      "Epoch 84, Batch 155 Loss:0.01379466988146305\n",
      "Epoch 84, Batch 156 Loss:0.008352239616215229\n",
      "Epoch 84, Batch 157 Loss:0.0037162930238991976\n",
      "Epoch 84, Batch 158 Loss:0.012781566008925438\n",
      "Epoch 84, Batch 159 Loss:0.02124079316854477\n",
      "Epoch 84, Batch 160 Loss:0.019557278603315353\n",
      "Epoch 84, Batch 161 Loss:0.013070376589894295\n",
      "Epoch 84, Batch 162 Loss:0.010766234248876572\n",
      "Epoch 84, Batch 163 Loss:0.011873053386807442\n",
      "Epoch 84, Batch 164 Loss:0.012086428701877594\n",
      "Epoch 84, Batch 165 Loss:0.008352860808372498\n",
      "Epoch 84, Batch 166 Loss:0.010214464738965034\n",
      "Epoch 84, Batch 167 Loss:0.019546225666999817\n",
      "Epoch 84, Batch 168 Loss:0.06000513583421707\n",
      "Epoch 84, Batch 169 Loss:0.02816867083311081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84, Batch 170 Loss:0.035542961210012436\n",
      "Epoch 84, Batch 171 Loss:0.029964473098516464\n",
      "Epoch 84, Batch 172 Loss:0.012761609628796577\n",
      "Epoch 84, Batch 173 Loss:0.0113002834841609\n",
      "Epoch 84, Batch 174 Loss:0.04747979715466499\n",
      "Epoch 84, Batch 175 Loss:0.02607777528464794\n",
      "Epoch 84, Batch 176 Loss:0.01162193063646555\n",
      "Epoch 84, Batch 177 Loss:0.041833531111478806\n",
      "Epoch 84, Batch 178 Loss:0.03964810445904732\n",
      "Epoch 84, Batch 179 Loss:0.011169884353876114\n",
      "Epoch 84, Batch 180 Loss:0.03306303173303604\n",
      "Epoch 84, Batch 181 Loss:0.024731874465942383\n",
      "Epoch 84, Batch 182 Loss:0.02588709443807602\n",
      "Epoch 84, Batch 183 Loss:0.028281543403863907\n",
      "Epoch 84, Batch 184 Loss:0.01546294055879116\n",
      "Epoch 84, Batch 185 Loss:0.04437347128987312\n",
      "Epoch 84, Batch 186 Loss:0.058438632637262344\n",
      "Epoch 84, Batch 187 Loss:0.0029358169995248318\n",
      "Epoch 84, Batch 188 Loss:0.01971181109547615\n",
      "Epoch 84, Batch 189 Loss:0.02147628739476204\n",
      "Epoch 84, Batch 190 Loss:0.0459415502846241\n",
      "Epoch 84, Batch 191 Loss:0.030837643891572952\n",
      "Epoch 84, Batch 192 Loss:0.031530559062957764\n",
      "Epoch 84, Batch 193 Loss:0.04236231744289398\n",
      "Epoch 84, Batch 194 Loss:0.005435308441519737\n",
      "Epoch 84, Batch 195 Loss:0.024554798379540443\n",
      "Epoch 84, Batch 196 Loss:0.022735947743058205\n",
      "Epoch 84, Batch 197 Loss:0.008068098686635494\n",
      "Epoch 84, Batch 198 Loss:0.009171669371426105\n",
      "Epoch 84, Batch 199 Loss:0.01975177600979805\n",
      "Epoch 84, Batch 200 Loss:0.020545370876789093\n",
      "Epoch 84, Batch 201 Loss:0.011199848726391792\n",
      "Epoch 84, Batch 202 Loss:0.008963773027062416\n",
      "Epoch 84, Batch 203 Loss:0.012082952074706554\n",
      "Epoch 84, Batch 204 Loss:0.04297029972076416\n",
      "Epoch 84, Batch 205 Loss:0.005681752227246761\n",
      "Epoch 84, Batch 206 Loss:0.007785443682223558\n",
      "Epoch 84, Batch 207 Loss:0.01300773210823536\n",
      "Epoch 84, Batch 208 Loss:0.03856423497200012\n",
      "Epoch 84, Batch 209 Loss:0.043924011290073395\n",
      "Epoch 84, Batch 210 Loss:0.014893781393766403\n",
      "Epoch 84, Batch 211 Loss:0.012607279233634472\n",
      "Epoch 84, Batch 212 Loss:0.014696281403303146\n",
      "Epoch 84, Batch 213 Loss:0.018488001078367233\n",
      "Epoch 84, Batch 214 Loss:0.049645375460386276\n",
      "Epoch 84, Batch 215 Loss:0.00477199861779809\n",
      "Epoch 84, Batch 216 Loss:0.019721655175089836\n",
      "Epoch 84, Batch 217 Loss:0.014335067011415958\n",
      "Epoch 84, Batch 218 Loss:0.008113132789731026\n",
      "Epoch 84, Batch 219 Loss:0.045577362179756165\n",
      "Epoch 84, Batch 220 Loss:0.009065894410014153\n",
      "Epoch 84, Batch 221 Loss:0.0043649207800626755\n",
      "Epoch 84, Batch 222 Loss:0.00660281628370285\n",
      "Epoch 84, Batch 223 Loss:0.00613334309309721\n",
      "Epoch 84, Batch 224 Loss:0.015161404386162758\n",
      "Epoch 84, Batch 225 Loss:0.01762397401034832\n",
      "Epoch 84, Batch 226 Loss:0.023751327767968178\n",
      "Epoch 84, Batch 227 Loss:0.02173144370317459\n",
      "Epoch 84, Batch 228 Loss:0.01586637645959854\n",
      "Epoch 84, Batch 229 Loss:0.03200291842222214\n",
      "Epoch 84, Batch 230 Loss:0.007147553376853466\n",
      "Epoch 84, Batch 231 Loss:0.01766710914671421\n",
      "Epoch 84, Batch 232 Loss:0.027767330408096313\n",
      "Epoch 84, Batch 233 Loss:0.008260702714323997\n",
      "Loss in this Epoch is: 0.826070271432 %\n",
      "Accuracy in this Epoch is: 88.5500013828 %\n",
      "Epoch 85, Batch 0 Loss:0.014333680272102356\n",
      "Epoch 85, Batch 1 Loss:0.007295658346265554\n",
      "Epoch 85, Batch 2 Loss:0.009838540107011795\n",
      "Epoch 85, Batch 3 Loss:0.00760521600022912\n",
      "Epoch 85, Batch 4 Loss:0.02390841767191887\n",
      "Epoch 85, Batch 5 Loss:0.0029002458322793245\n",
      "Epoch 85, Batch 6 Loss:0.017405109480023384\n",
      "Epoch 85, Batch 7 Loss:0.01875731535255909\n",
      "Epoch 85, Batch 8 Loss:0.03234102204442024\n",
      "Epoch 85, Batch 9 Loss:0.009836189448833466\n",
      "Epoch 85, Batch 10 Loss:0.0019004412461072206\n",
      "Epoch 85, Batch 11 Loss:0.006883852183818817\n",
      "Epoch 85, Batch 12 Loss:0.008702315390110016\n",
      "Epoch 85, Batch 13 Loss:0.03925547003746033\n",
      "Epoch 85, Batch 14 Loss:0.004716305527836084\n",
      "Epoch 85, Batch 15 Loss:0.013868550769984722\n",
      "Epoch 85, Batch 16 Loss:0.005600190255790949\n",
      "Epoch 85, Batch 17 Loss:0.001891737338155508\n",
      "Epoch 85, Batch 18 Loss:0.02032751962542534\n",
      "Epoch 85, Batch 19 Loss:0.014996952377259731\n",
      "Epoch 85, Batch 20 Loss:0.008035263977944851\n",
      "Epoch 85, Batch 21 Loss:0.012991856783628464\n",
      "Epoch 85, Batch 22 Loss:0.0038937628269195557\n",
      "Epoch 85, Batch 23 Loss:0.0059824977070093155\n",
      "Epoch 85, Batch 24 Loss:0.008319714106619358\n",
      "Epoch 85, Batch 25 Loss:0.011273459531366825\n",
      "Epoch 85, Batch 26 Loss:0.0019773326348513365\n",
      "Epoch 85, Batch 27 Loss:0.005083763971924782\n",
      "Epoch 85, Batch 28 Loss:0.002181707415729761\n",
      "Epoch 85, Batch 29 Loss:0.005152857396751642\n",
      "Epoch 85, Batch 30 Loss:0.017091507092118263\n",
      "Epoch 85, Batch 31 Loss:0.005903847981244326\n",
      "Epoch 85, Batch 32 Loss:0.012940014712512493\n",
      "Epoch 85, Batch 33 Loss:0.007847648113965988\n",
      "Epoch 85, Batch 34 Loss:0.0033213517162948847\n",
      "Epoch 85, Batch 35 Loss:0.011749077588319778\n",
      "Epoch 85, Batch 36 Loss:0.038375820964574814\n",
      "Epoch 85, Batch 37 Loss:0.017512844875454903\n",
      "Epoch 85, Batch 38 Loss:0.009466219693422318\n",
      "Epoch 85, Batch 39 Loss:0.003177957609295845\n",
      "Epoch 85, Batch 40 Loss:0.0025505535304546356\n",
      "Epoch 85, Batch 41 Loss:0.020941853523254395\n",
      "Epoch 85, Batch 42 Loss:0.013781571760773659\n",
      "Epoch 85, Batch 43 Loss:0.01468183659017086\n",
      "Epoch 85, Batch 44 Loss:0.006897899322211742\n",
      "Epoch 85, Batch 45 Loss:0.04738720506429672\n",
      "Epoch 85, Batch 46 Loss:0.03297852352261543\n",
      "Epoch 85, Batch 47 Loss:0.0094760088250041\n",
      "Epoch 85, Batch 48 Loss:0.021747315302491188\n",
      "Epoch 85, Batch 49 Loss:0.014637919142842293\n",
      "Epoch 85, Batch 50 Loss:0.02122572436928749\n",
      "Epoch 85, Batch 51 Loss:0.051108378916978836\n",
      "Epoch 85, Batch 52 Loss:0.009376008063554764\n",
      "Epoch 85, Batch 53 Loss:0.006452652160078287\n",
      "Epoch 85, Batch 54 Loss:0.006014613434672356\n",
      "Epoch 85, Batch 55 Loss:0.003427180228754878\n",
      "Epoch 85, Batch 56 Loss:0.012577342800796032\n",
      "Epoch 85, Batch 57 Loss:0.02236194908618927\n",
      "Epoch 85, Batch 58 Loss:0.014562519267201424\n",
      "Epoch 85, Batch 59 Loss:0.009336384013295174\n",
      "Epoch 85, Batch 60 Loss:0.00790751539170742\n",
      "Epoch 85, Batch 61 Loss:0.02796068973839283\n",
      "Epoch 85, Batch 62 Loss:0.01011934969574213\n",
      "Epoch 85, Batch 63 Loss:0.009151309728622437\n",
      "Epoch 85, Batch 64 Loss:0.007173760328441858\n",
      "Epoch 85, Batch 65 Loss:0.01694387197494507\n",
      "Epoch 85, Batch 66 Loss:0.01453208364546299\n",
      "Epoch 85, Batch 67 Loss:0.008775347843766212\n",
      "Epoch 85, Batch 68 Loss:0.023634815588593483\n",
      "Epoch 85, Batch 69 Loss:0.03583439812064171\n",
      "Epoch 85, Batch 70 Loss:0.009500620886683464\n",
      "Epoch 85, Batch 71 Loss:0.009673257358372211\n",
      "Epoch 85, Batch 72 Loss:0.009186964482069016\n",
      "Epoch 85, Batch 73 Loss:0.004733134061098099\n",
      "Epoch 85, Batch 74 Loss:0.006748308893293142\n",
      "Epoch 85, Batch 75 Loss:0.035462476313114166\n",
      "Epoch 85, Batch 76 Loss:0.021973218768835068\n",
      "Epoch 85, Batch 77 Loss:0.004150021355599165\n",
      "Epoch 85, Batch 78 Loss:0.027983857318758965\n",
      "Epoch 85, Batch 79 Loss:0.024342505261301994\n",
      "Epoch 85, Batch 80 Loss:0.015598460100591183\n",
      "Epoch 85, Batch 81 Loss:0.04688815772533417\n",
      "Epoch 85, Batch 82 Loss:0.012844929471611977\n",
      "Epoch 85, Batch 83 Loss:0.014162061735987663\n",
      "Epoch 85, Batch 84 Loss:0.011213790625333786\n",
      "Epoch 85, Batch 85 Loss:0.016184115782380104\n",
      "Epoch 85, Batch 86 Loss:0.007924476638436317\n",
      "Epoch 85, Batch 87 Loss:0.009640011005103588\n",
      "Epoch 85, Batch 88 Loss:0.0036892478819936514\n",
      "Epoch 85, Batch 89 Loss:0.012051071040332317\n",
      "Epoch 85, Batch 90 Loss:0.005483943969011307\n",
      "Epoch 85, Batch 91 Loss:0.00574704771861434\n",
      "Epoch 85, Batch 92 Loss:0.021679647266864777\n",
      "Epoch 85, Batch 93 Loss:0.01758500374853611\n",
      "Epoch 85, Batch 94 Loss:0.03318827971816063\n",
      "Epoch 85, Batch 95 Loss:0.03605885058641434\n",
      "Epoch 85, Batch 96 Loss:0.01268715038895607\n",
      "Epoch 85, Batch 97 Loss:0.010291432961821556\n",
      "Epoch 85, Batch 98 Loss:0.008133979514241219\n",
      "Epoch 85, Batch 99 Loss:0.018911462277173996\n",
      "Epoch 85, Batch 100 Loss:0.018570540472865105\n",
      "Epoch 85, Batch 101 Loss:0.042166806757450104\n",
      "Epoch 85, Batch 102 Loss:0.021940942853689194\n",
      "Epoch 85, Batch 103 Loss:0.007558227516710758\n",
      "Epoch 85, Batch 104 Loss:0.01733744889497757\n",
      "Epoch 85, Batch 105 Loss:0.018522735685110092\n",
      "Epoch 85, Batch 106 Loss:0.011139645241200924\n",
      "Epoch 85, Batch 107 Loss:0.00661400007084012\n",
      "Epoch 85, Batch 108 Loss:0.023917458951473236\n",
      "Epoch 85, Batch 109 Loss:0.002695638220757246\n",
      "Epoch 85, Batch 110 Loss:0.008093686774373055\n",
      "Epoch 85, Batch 111 Loss:0.007207032758742571\n",
      "Epoch 85, Batch 112 Loss:0.018021054565906525\n",
      "Epoch 85, Batch 113 Loss:0.009311680682003498\n",
      "Epoch 85, Batch 114 Loss:0.009402705356478691\n",
      "Epoch 85, Batch 115 Loss:0.007827149704098701\n",
      "Epoch 85, Batch 116 Loss:0.03614574670791626\n",
      "Epoch 85, Batch 117 Loss:0.007042840123176575\n",
      "Epoch 85, Batch 118 Loss:0.006787549238651991\n",
      "Epoch 85, Batch 119 Loss:0.014644280076026917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85, Batch 120 Loss:0.031142285093665123\n",
      "Epoch 85, Batch 121 Loss:0.010005589574575424\n",
      "Epoch 85, Batch 122 Loss:0.027990883216261864\n",
      "Epoch 85, Batch 123 Loss:0.006193557754158974\n",
      "Epoch 85, Batch 124 Loss:0.01794970966875553\n",
      "Epoch 85, Batch 125 Loss:0.015523931942880154\n",
      "Epoch 85, Batch 126 Loss:0.04251867160201073\n",
      "Epoch 85, Batch 127 Loss:0.012374922633171082\n",
      "Epoch 85, Batch 128 Loss:0.08752794563770294\n",
      "Epoch 85, Batch 129 Loss:0.009568030945956707\n",
      "Epoch 85, Batch 130 Loss:0.024164486676454544\n",
      "Epoch 85, Batch 131 Loss:0.017886321991682053\n",
      "Epoch 85, Batch 132 Loss:0.011096281930804253\n",
      "Epoch 85, Batch 133 Loss:0.026681825518608093\n",
      "Epoch 85, Batch 134 Loss:0.023269152268767357\n",
      "Epoch 85, Batch 135 Loss:0.008043677546083927\n",
      "Epoch 85, Batch 136 Loss:0.007676979526877403\n",
      "Epoch 85, Batch 137 Loss:0.00655785109847784\n",
      "Epoch 85, Batch 138 Loss:0.016078894957900047\n",
      "Epoch 85, Batch 139 Loss:0.023423636332154274\n",
      "Epoch 85, Batch 140 Loss:0.032382212579250336\n",
      "Epoch 85, Batch 141 Loss:0.014305878430604935\n",
      "Epoch 85, Batch 142 Loss:0.01435450091958046\n",
      "Epoch 85, Batch 143 Loss:0.010682949796319008\n",
      "Epoch 85, Batch 144 Loss:0.009179254062473774\n",
      "Epoch 85, Batch 145 Loss:0.028343528509140015\n",
      "Epoch 85, Batch 146 Loss:0.0066160839051008224\n",
      "Epoch 85, Batch 147 Loss:0.014930233359336853\n",
      "Epoch 85, Batch 148 Loss:0.02063662000000477\n",
      "Epoch 85, Batch 149 Loss:0.012629063799977303\n",
      "Epoch 85, Batch 150 Loss:0.014331755228340626\n",
      "Epoch 85, Batch 151 Loss:0.005400184541940689\n",
      "Epoch 85, Batch 152 Loss:0.015198798850178719\n",
      "Epoch 85, Batch 153 Loss:0.008768774569034576\n",
      "Epoch 85, Batch 154 Loss:0.0021682209335267544\n",
      "Epoch 85, Batch 155 Loss:0.010425492189824581\n",
      "Epoch 85, Batch 156 Loss:0.009829185903072357\n",
      "Epoch 85, Batch 157 Loss:0.021786238998174667\n",
      "Epoch 85, Batch 158 Loss:0.018923459574580193\n",
      "Epoch 85, Batch 159 Loss:0.013334534130990505\n",
      "Epoch 85, Batch 160 Loss:0.007126646116375923\n",
      "Epoch 85, Batch 161 Loss:0.007832389324903488\n",
      "Epoch 85, Batch 162 Loss:0.008230416104197502\n",
      "Epoch 85, Batch 163 Loss:0.01152109820395708\n",
      "Epoch 85, Batch 164 Loss:0.02159867063164711\n",
      "Epoch 85, Batch 165 Loss:0.013967795297503471\n",
      "Epoch 85, Batch 166 Loss:0.02033882774412632\n",
      "Epoch 85, Batch 167 Loss:0.010685646906495094\n",
      "Epoch 85, Batch 168 Loss:0.00942079909145832\n",
      "Epoch 85, Batch 169 Loss:0.005884304642677307\n",
      "Epoch 85, Batch 170 Loss:0.003797895973548293\n",
      "Epoch 85, Batch 171 Loss:0.0062378281727433205\n",
      "Epoch 85, Batch 172 Loss:0.009317061863839626\n",
      "Epoch 85, Batch 173 Loss:0.009879127144813538\n",
      "Epoch 85, Batch 174 Loss:0.0073781730607151985\n",
      "Epoch 85, Batch 175 Loss:0.00639358488842845\n",
      "Epoch 85, Batch 176 Loss:0.023340310901403427\n",
      "Epoch 85, Batch 177 Loss:0.04326900467276573\n",
      "Epoch 85, Batch 178 Loss:0.021133480593562126\n",
      "Epoch 85, Batch 179 Loss:0.006903514266014099\n",
      "Epoch 85, Batch 180 Loss:0.0032333852723240852\n",
      "Epoch 85, Batch 181 Loss:0.022978270426392555\n",
      "Epoch 85, Batch 182 Loss:0.02017076127231121\n",
      "Epoch 85, Batch 183 Loss:0.016166698187589645\n",
      "Epoch 85, Batch 184 Loss:0.015863792970776558\n",
      "Epoch 85, Batch 185 Loss:0.006049309857189655\n",
      "Epoch 85, Batch 186 Loss:0.004291427321732044\n",
      "Epoch 85, Batch 187 Loss:0.008834712207317352\n",
      "Epoch 85, Batch 188 Loss:0.01456582359969616\n",
      "Epoch 85, Batch 189 Loss:0.011267558671534061\n",
      "Epoch 85, Batch 190 Loss:0.01417368184775114\n",
      "Epoch 85, Batch 191 Loss:0.031118370592594147\n",
      "Epoch 85, Batch 192 Loss:0.033343978226184845\n",
      "Epoch 85, Batch 193 Loss:0.013509024865925312\n",
      "Epoch 85, Batch 194 Loss:0.03613860905170441\n",
      "Epoch 85, Batch 195 Loss:0.012352710589766502\n",
      "Epoch 85, Batch 196 Loss:0.06020712852478027\n",
      "Epoch 85, Batch 197 Loss:0.0606352761387825\n",
      "Epoch 85, Batch 198 Loss:0.006822830066084862\n",
      "Epoch 85, Batch 199 Loss:0.0290323905646801\n",
      "Epoch 85, Batch 200 Loss:0.010310027748346329\n",
      "Epoch 85, Batch 201 Loss:0.029040953144431114\n",
      "Epoch 85, Batch 202 Loss:0.011520163156092167\n",
      "Epoch 85, Batch 203 Loss:0.0185872633010149\n",
      "Epoch 85, Batch 204 Loss:0.036862172186374664\n",
      "Epoch 85, Batch 205 Loss:0.017744015902280807\n",
      "Epoch 85, Batch 206 Loss:0.03389325737953186\n",
      "Epoch 85, Batch 207 Loss:0.027130112051963806\n",
      "Epoch 85, Batch 208 Loss:0.02473316341638565\n",
      "Epoch 85, Batch 209 Loss:0.02099272608757019\n",
      "Epoch 85, Batch 210 Loss:0.03661463037133217\n",
      "Epoch 85, Batch 211 Loss:0.035073887556791306\n",
      "Epoch 85, Batch 212 Loss:0.03633752837777138\n",
      "Epoch 85, Batch 213 Loss:0.017136642709374428\n",
      "Epoch 85, Batch 214 Loss:0.0827435553073883\n",
      "Epoch 85, Batch 215 Loss:0.0574972964823246\n",
      "Epoch 85, Batch 216 Loss:0.05354432016611099\n",
      "Epoch 85, Batch 217 Loss:0.034684620797634125\n",
      "Epoch 85, Batch 218 Loss:0.022199722006917\n",
      "Epoch 85, Batch 219 Loss:0.007764879148453474\n",
      "Epoch 85, Batch 220 Loss:0.06262771785259247\n",
      "Epoch 85, Batch 221 Loss:0.041295282542705536\n",
      "Epoch 85, Batch 222 Loss:0.028814831748604774\n",
      "Epoch 85, Batch 223 Loss:0.0481460876762867\n",
      "Epoch 85, Batch 224 Loss:0.04220833256840706\n",
      "Epoch 85, Batch 225 Loss:0.030408009886741638\n",
      "Epoch 85, Batch 226 Loss:0.04386426508426666\n",
      "Epoch 85, Batch 227 Loss:0.029302220791578293\n",
      "Epoch 85, Batch 228 Loss:0.019916534423828125\n",
      "Epoch 85, Batch 229 Loss:0.010086476802825928\n",
      "Epoch 85, Batch 230 Loss:0.022393859922885895\n",
      "Epoch 85, Batch 231 Loss:0.05368940904736519\n",
      "Epoch 85, Batch 232 Loss:0.018904799595475197\n",
      "Epoch 85, Batch 233 Loss:0.008480190299451351\n",
      "Loss in this Epoch is: 0.848019029945 %\n",
      "Accuracy in this Epoch is: 88.480001688 %\n",
      "Epoch 86, Batch 0 Loss:0.004948671907186508\n",
      "Epoch 86, Batch 1 Loss:0.01710350252687931\n",
      "Epoch 86, Batch 2 Loss:0.032373834401369095\n",
      "Epoch 86, Batch 3 Loss:0.14019784331321716\n",
      "Epoch 86, Batch 4 Loss:0.011166979558765888\n",
      "Epoch 86, Batch 5 Loss:0.006994168274104595\n",
      "Epoch 86, Batch 6 Loss:0.02544340305030346\n",
      "Epoch 86, Batch 7 Loss:0.03838352486491203\n",
      "Epoch 86, Batch 8 Loss:0.033400196582078934\n",
      "Epoch 86, Batch 9 Loss:0.031245725229382515\n",
      "Epoch 86, Batch 10 Loss:0.016087904572486877\n",
      "Epoch 86, Batch 11 Loss:0.03209058195352554\n",
      "Epoch 86, Batch 12 Loss:0.02478715032339096\n",
      "Epoch 86, Batch 13 Loss:0.012164236046373844\n",
      "Epoch 86, Batch 14 Loss:0.028702549636363983\n",
      "Epoch 86, Batch 15 Loss:0.034694451838731766\n",
      "Epoch 86, Batch 16 Loss:0.022185424342751503\n",
      "Epoch 86, Batch 17 Loss:0.026302851736545563\n",
      "Epoch 86, Batch 18 Loss:0.02309330552816391\n",
      "Epoch 86, Batch 19 Loss:0.006067204289138317\n",
      "Epoch 86, Batch 20 Loss:0.03842976316809654\n",
      "Epoch 86, Batch 21 Loss:0.02364017814397812\n",
      "Epoch 86, Batch 22 Loss:0.010150307789444923\n",
      "Epoch 86, Batch 23 Loss:0.0572969987988472\n",
      "Epoch 86, Batch 24 Loss:0.03635335713624954\n",
      "Epoch 86, Batch 25 Loss:0.00785051565617323\n",
      "Epoch 86, Batch 26 Loss:0.012666766531765461\n",
      "Epoch 86, Batch 27 Loss:0.011882515624165535\n",
      "Epoch 86, Batch 28 Loss:0.012682742439210415\n",
      "Epoch 86, Batch 29 Loss:0.06880869716405869\n",
      "Epoch 86, Batch 30 Loss:0.03132600337266922\n",
      "Epoch 86, Batch 31 Loss:0.023775452747941017\n",
      "Epoch 86, Batch 32 Loss:0.018716102465987206\n",
      "Epoch 86, Batch 33 Loss:0.016459735110402107\n",
      "Epoch 86, Batch 34 Loss:0.019876625388860703\n",
      "Epoch 86, Batch 35 Loss:0.010966641828417778\n",
      "Epoch 86, Batch 36 Loss:0.008720996789634228\n",
      "Epoch 86, Batch 37 Loss:0.017804840579628944\n",
      "Epoch 86, Batch 38 Loss:0.01202269084751606\n",
      "Epoch 86, Batch 39 Loss:0.009383012540638447\n",
      "Epoch 86, Batch 40 Loss:0.01301558967679739\n",
      "Epoch 86, Batch 41 Loss:0.00976639799773693\n",
      "Epoch 86, Batch 42 Loss:0.00612694351002574\n",
      "Epoch 86, Batch 43 Loss:0.0026765100192278624\n",
      "Epoch 86, Batch 44 Loss:0.00831917766481638\n",
      "Epoch 86, Batch 45 Loss:0.010000722482800484\n",
      "Epoch 86, Batch 46 Loss:0.005593897774815559\n",
      "Epoch 86, Batch 47 Loss:0.0043967245146632195\n",
      "Epoch 86, Batch 48 Loss:0.005356458481401205\n",
      "Epoch 86, Batch 49 Loss:0.03533237427473068\n",
      "Epoch 86, Batch 50 Loss:0.007179620210081339\n",
      "Epoch 86, Batch 51 Loss:0.004449100233614445\n",
      "Epoch 86, Batch 52 Loss:0.007179549895226955\n",
      "Epoch 86, Batch 53 Loss:0.0033560271840542555\n",
      "Epoch 86, Batch 54 Loss:0.018449150025844574\n",
      "Epoch 86, Batch 55 Loss:0.0019352614181116223\n",
      "Epoch 86, Batch 56 Loss:0.005993517115712166\n",
      "Epoch 86, Batch 57 Loss:0.018175864592194557\n",
      "Epoch 86, Batch 58 Loss:0.022410603240132332\n",
      "Epoch 86, Batch 59 Loss:0.0247197262942791\n",
      "Epoch 86, Batch 60 Loss:0.03168197721242905\n",
      "Epoch 86, Batch 61 Loss:0.005303194746375084\n",
      "Epoch 86, Batch 62 Loss:0.0331667959690094\n",
      "Epoch 86, Batch 63 Loss:0.0011912236222997308\n",
      "Epoch 86, Batch 64 Loss:0.00362201314419508\n",
      "Epoch 86, Batch 65 Loss:0.003268742933869362\n",
      "Epoch 86, Batch 66 Loss:0.008641231805086136\n",
      "Epoch 86, Batch 67 Loss:0.009045086801052094\n",
      "Epoch 86, Batch 68 Loss:0.013572950847446918\n",
      "Epoch 86, Batch 69 Loss:0.005821180995553732\n",
      "Epoch 86, Batch 70 Loss:0.012766925618052483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86, Batch 71 Loss:0.009959074668586254\n",
      "Epoch 86, Batch 72 Loss:0.01977998949587345\n",
      "Epoch 86, Batch 73 Loss:0.0074053797870874405\n",
      "Epoch 86, Batch 74 Loss:0.005376334767788649\n",
      "Epoch 86, Batch 75 Loss:0.004105994012206793\n",
      "Epoch 86, Batch 76 Loss:0.015463939867913723\n",
      "Epoch 86, Batch 77 Loss:0.02809661254286766\n",
      "Epoch 86, Batch 78 Loss:0.009173428639769554\n",
      "Epoch 86, Batch 79 Loss:0.005359558388590813\n",
      "Epoch 86, Batch 80 Loss:0.028059395030140877\n",
      "Epoch 86, Batch 81 Loss:0.00319114257581532\n",
      "Epoch 86, Batch 82 Loss:0.0046400404535233974\n",
      "Epoch 86, Batch 83 Loss:0.013480925001204014\n",
      "Epoch 86, Batch 84 Loss:0.02980717271566391\n",
      "Epoch 86, Batch 85 Loss:0.027552468702197075\n",
      "Epoch 86, Batch 86 Loss:0.032206494361162186\n",
      "Epoch 86, Batch 87 Loss:0.041764505207538605\n",
      "Epoch 86, Batch 88 Loss:0.012346252799034119\n",
      "Epoch 86, Batch 89 Loss:0.006076138466596603\n",
      "Epoch 86, Batch 90 Loss:0.009650252759456635\n",
      "Epoch 86, Batch 91 Loss:0.002516582142561674\n",
      "Epoch 86, Batch 92 Loss:0.0028625826817005873\n",
      "Epoch 86, Batch 93 Loss:0.016097329556941986\n",
      "Epoch 86, Batch 94 Loss:0.0047295005060732365\n",
      "Epoch 86, Batch 95 Loss:0.022161345928907394\n",
      "Epoch 86, Batch 96 Loss:0.006367793306708336\n",
      "Epoch 86, Batch 97 Loss:0.003407461801543832\n",
      "Epoch 86, Batch 98 Loss:0.012265583500266075\n",
      "Epoch 86, Batch 99 Loss:0.009308134205639362\n",
      "Epoch 86, Batch 100 Loss:0.015543388202786446\n",
      "Epoch 86, Batch 101 Loss:0.030881647020578384\n",
      "Epoch 86, Batch 102 Loss:0.004623901564627886\n",
      "Epoch 86, Batch 103 Loss:0.006242415867745876\n",
      "Epoch 86, Batch 104 Loss:0.02499322220683098\n",
      "Epoch 86, Batch 105 Loss:0.014424614608287811\n",
      "Epoch 86, Batch 106 Loss:0.038751885294914246\n",
      "Epoch 86, Batch 107 Loss:0.0474955178797245\n",
      "Epoch 86, Batch 108 Loss:0.010520102456212044\n",
      "Epoch 86, Batch 109 Loss:0.03309367969632149\n",
      "Epoch 86, Batch 110 Loss:0.05357770994305611\n",
      "Epoch 86, Batch 111 Loss:0.010651360265910625\n",
      "Epoch 86, Batch 112 Loss:0.0033296493347734213\n",
      "Epoch 86, Batch 113 Loss:0.020515598356723785\n",
      "Epoch 86, Batch 114 Loss:0.00680149532854557\n",
      "Epoch 86, Batch 115 Loss:0.007120253052562475\n",
      "Epoch 86, Batch 116 Loss:0.004689340479671955\n",
      "Epoch 86, Batch 117 Loss:0.004956841468811035\n",
      "Epoch 86, Batch 118 Loss:0.009828843176364899\n",
      "Epoch 86, Batch 119 Loss:0.008864002302289009\n",
      "Epoch 86, Batch 120 Loss:0.014765916392207146\n",
      "Epoch 86, Batch 121 Loss:0.019869031384587288\n",
      "Epoch 86, Batch 122 Loss:0.016063354909420013\n",
      "Epoch 86, Batch 123 Loss:0.01974198780953884\n",
      "Epoch 86, Batch 124 Loss:0.002157175214961171\n",
      "Epoch 86, Batch 125 Loss:0.005283998791128397\n",
      "Epoch 86, Batch 126 Loss:0.0028388826176524162\n",
      "Epoch 86, Batch 127 Loss:0.012760406360030174\n",
      "Epoch 86, Batch 128 Loss:0.0032669175416231155\n",
      "Epoch 86, Batch 129 Loss:0.02193191461265087\n",
      "Epoch 86, Batch 130 Loss:0.0037694419734179974\n",
      "Epoch 86, Batch 131 Loss:0.0051242574118077755\n",
      "Epoch 86, Batch 132 Loss:0.004598942585289478\n",
      "Epoch 86, Batch 133 Loss:0.004930455237627029\n",
      "Epoch 86, Batch 134 Loss:0.0036842201370745897\n",
      "Epoch 86, Batch 135 Loss:0.009957672096788883\n",
      "Epoch 86, Batch 136 Loss:0.006816423498094082\n",
      "Epoch 86, Batch 137 Loss:0.01863553747534752\n",
      "Epoch 86, Batch 138 Loss:0.003598080947995186\n",
      "Epoch 86, Batch 139 Loss:0.00557800754904747\n",
      "Epoch 86, Batch 140 Loss:0.005656005814671516\n",
      "Epoch 86, Batch 141 Loss:0.002617290709167719\n",
      "Epoch 86, Batch 142 Loss:0.05258573591709137\n",
      "Epoch 86, Batch 143 Loss:0.010470814071595669\n",
      "Epoch 86, Batch 144 Loss:0.029702629894018173\n",
      "Epoch 86, Batch 145 Loss:0.004372729919850826\n",
      "Epoch 86, Batch 146 Loss:0.03318312019109726\n",
      "Epoch 86, Batch 147 Loss:0.031510960310697556\n",
      "Epoch 86, Batch 148 Loss:0.019581450149416924\n",
      "Epoch 86, Batch 149 Loss:0.004349487833678722\n",
      "Epoch 86, Batch 150 Loss:0.010105572640895844\n",
      "Epoch 86, Batch 151 Loss:0.01119480561465025\n",
      "Epoch 86, Batch 152 Loss:0.028271527960896492\n",
      "Epoch 86, Batch 153 Loss:0.018099654465913773\n",
      "Epoch 86, Batch 154 Loss:0.03435693308711052\n",
      "Epoch 86, Batch 155 Loss:0.03623579442501068\n",
      "Epoch 86, Batch 156 Loss:0.025337209925055504\n",
      "Epoch 86, Batch 157 Loss:0.06566743552684784\n",
      "Epoch 86, Batch 158 Loss:0.0418226458132267\n",
      "Epoch 86, Batch 159 Loss:0.08328008651733398\n",
      "Epoch 86, Batch 160 Loss:0.058716051280498505\n",
      "Epoch 86, Batch 161 Loss:0.0604611337184906\n",
      "Epoch 86, Batch 162 Loss:0.008473692461848259\n",
      "Epoch 86, Batch 163 Loss:0.050530530512332916\n",
      "Epoch 86, Batch 164 Loss:0.06731100380420685\n",
      "Epoch 86, Batch 165 Loss:0.0638551190495491\n",
      "Epoch 86, Batch 166 Loss:0.05182894319295883\n",
      "Epoch 86, Batch 167 Loss:0.061328426003456116\n",
      "Epoch 86, Batch 168 Loss:0.05100974068045616\n",
      "Epoch 86, Batch 169 Loss:0.037900492548942566\n",
      "Epoch 86, Batch 170 Loss:0.04104107618331909\n",
      "Epoch 86, Batch 171 Loss:0.05709923803806305\n",
      "Epoch 86, Batch 172 Loss:0.011764533817768097\n",
      "Epoch 86, Batch 173 Loss:0.032469429075717926\n",
      "Epoch 86, Batch 174 Loss:0.04531507194042206\n",
      "Epoch 86, Batch 175 Loss:0.05255207046866417\n",
      "Epoch 86, Batch 176 Loss:0.019870974123477936\n",
      "Epoch 86, Batch 177 Loss:0.09553185105323792\n",
      "Epoch 86, Batch 178 Loss:0.029701553285121918\n",
      "Epoch 86, Batch 179 Loss:0.04333781450986862\n",
      "Epoch 86, Batch 180 Loss:0.027957724407315254\n",
      "Epoch 86, Batch 181 Loss:0.015921935439109802\n",
      "Epoch 86, Batch 182 Loss:0.031392667442560196\n",
      "Epoch 86, Batch 183 Loss:0.02261228859424591\n",
      "Epoch 86, Batch 184 Loss:0.07774584740400314\n",
      "Epoch 86, Batch 185 Loss:0.01634187623858452\n",
      "Epoch 86, Batch 186 Loss:0.06148218363523483\n",
      "Epoch 86, Batch 187 Loss:0.025007136166095734\n",
      "Epoch 86, Batch 188 Loss:0.010549322701990604\n",
      "Epoch 86, Batch 189 Loss:0.015110986307263374\n",
      "Epoch 86, Batch 190 Loss:0.015867654234170914\n",
      "Epoch 86, Batch 191 Loss:0.08518244326114655\n",
      "Epoch 86, Batch 192 Loss:0.01330289151519537\n",
      "Epoch 86, Batch 193 Loss:0.016484970226883888\n",
      "Epoch 86, Batch 194 Loss:0.01752772368490696\n",
      "Epoch 86, Batch 195 Loss:0.03047354891896248\n",
      "Epoch 86, Batch 196 Loss:0.032043807208538055\n",
      "Epoch 86, Batch 197 Loss:0.005768968258053064\n",
      "Epoch 86, Batch 198 Loss:0.016662312671542168\n",
      "Epoch 86, Batch 199 Loss:0.027841612696647644\n",
      "Epoch 86, Batch 200 Loss:0.03720541670918465\n",
      "Epoch 86, Batch 201 Loss:0.012465735897421837\n",
      "Epoch 86, Batch 202 Loss:0.03425869345664978\n",
      "Epoch 86, Batch 203 Loss:0.03358840197324753\n",
      "Epoch 86, Batch 204 Loss:0.009991533122956753\n",
      "Epoch 86, Batch 205 Loss:0.04359354451298714\n",
      "Epoch 86, Batch 206 Loss:0.023536741733551025\n",
      "Epoch 86, Batch 207 Loss:0.02705872431397438\n",
      "Epoch 86, Batch 208 Loss:0.078528493642807\n",
      "Epoch 86, Batch 209 Loss:0.01694372668862343\n",
      "Epoch 86, Batch 210 Loss:0.06223304197192192\n",
      "Epoch 86, Batch 211 Loss:0.0541546493768692\n",
      "Epoch 86, Batch 212 Loss:0.011283074505627155\n",
      "Epoch 86, Batch 213 Loss:0.04587914049625397\n",
      "Epoch 86, Batch 214 Loss:0.02261160872876644\n",
      "Epoch 86, Batch 215 Loss:0.020385658368468285\n",
      "Epoch 86, Batch 216 Loss:0.04359643533825874\n",
      "Epoch 86, Batch 217 Loss:0.033572860062122345\n",
      "Epoch 86, Batch 218 Loss:0.030017230659723282\n",
      "Epoch 86, Batch 219 Loss:0.04072416201233864\n",
      "Epoch 86, Batch 220 Loss:0.022071603685617447\n",
      "Epoch 86, Batch 221 Loss:0.027164282277226448\n",
      "Epoch 86, Batch 222 Loss:0.006352616939693689\n",
      "Epoch 86, Batch 223 Loss:0.028925135731697083\n",
      "Epoch 86, Batch 224 Loss:0.026559993624687195\n",
      "Epoch 86, Batch 225 Loss:0.01119232177734375\n",
      "Epoch 86, Batch 226 Loss:0.028911098837852478\n",
      "Epoch 86, Batch 227 Loss:0.042373839765787125\n",
      "Epoch 86, Batch 228 Loss:0.02648991532623768\n",
      "Epoch 86, Batch 229 Loss:0.007733932696282864\n",
      "Epoch 86, Batch 230 Loss:0.013681909069418907\n",
      "Epoch 86, Batch 231 Loss:0.011736946180462837\n",
      "Epoch 86, Batch 232 Loss:0.029374005272984505\n",
      "Epoch 86, Batch 233 Loss:0.0410551019012928\n",
      "Loss in this Epoch is: 4.10551019013 %\n",
      "Accuracy in this Epoch is: 88.8199985027 %\n",
      "Epoch 87, Batch 0 Loss:0.008655953221023083\n",
      "Epoch 87, Batch 1 Loss:0.028356241062283516\n",
      "Epoch 87, Batch 2 Loss:0.014426693320274353\n",
      "Epoch 87, Batch 3 Loss:0.010927396826446056\n",
      "Epoch 87, Batch 4 Loss:0.02232867106795311\n",
      "Epoch 87, Batch 5 Loss:0.0171176977455616\n",
      "Epoch 87, Batch 6 Loss:0.005811241455376148\n",
      "Epoch 87, Batch 7 Loss:0.005372699815779924\n",
      "Epoch 87, Batch 8 Loss:0.014419790357351303\n",
      "Epoch 87, Batch 9 Loss:0.007582020480185747\n",
      "Epoch 87, Batch 10 Loss:0.014169309288263321\n",
      "Epoch 87, Batch 11 Loss:0.01171971671283245\n",
      "Epoch 87, Batch 12 Loss:0.023690007627010345\n",
      "Epoch 87, Batch 13 Loss:0.012976223602890968\n",
      "Epoch 87, Batch 14 Loss:0.005309781525284052\n",
      "Epoch 87, Batch 15 Loss:0.02595904842019081\n",
      "Epoch 87, Batch 16 Loss:0.02097080461680889\n",
      "Epoch 87, Batch 17 Loss:0.004559807945042849\n",
      "Epoch 87, Batch 18 Loss:0.008976850658655167\n",
      "Epoch 87, Batch 19 Loss:0.010111321695148945\n",
      "Epoch 87, Batch 20 Loss:0.007600847166031599\n",
      "Epoch 87, Batch 21 Loss:0.023011311888694763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Batch 22 Loss:0.014742373488843441\n",
      "Epoch 87, Batch 23 Loss:0.01972689852118492\n",
      "Epoch 87, Batch 24 Loss:0.028037410229444504\n",
      "Epoch 87, Batch 25 Loss:0.006321087013930082\n",
      "Epoch 87, Batch 26 Loss:0.0035701976157724857\n",
      "Epoch 87, Batch 27 Loss:0.013580631464719772\n",
      "Epoch 87, Batch 28 Loss:0.02145412005484104\n",
      "Epoch 87, Batch 29 Loss:0.0033096782863140106\n",
      "Epoch 87, Batch 30 Loss:0.03628982603549957\n",
      "Epoch 87, Batch 31 Loss:0.00591355562210083\n",
      "Epoch 87, Batch 32 Loss:0.005533131305128336\n",
      "Epoch 87, Batch 33 Loss:0.01441270299255848\n",
      "Epoch 87, Batch 34 Loss:0.0018544227350503206\n",
      "Epoch 87, Batch 35 Loss:0.04047098383307457\n",
      "Epoch 87, Batch 36 Loss:0.03331559896469116\n",
      "Epoch 87, Batch 37 Loss:0.006404329556971788\n",
      "Epoch 87, Batch 38 Loss:0.00566201563924551\n",
      "Epoch 87, Batch 39 Loss:0.004555015359073877\n",
      "Epoch 87, Batch 40 Loss:0.023227661848068237\n",
      "Epoch 87, Batch 41 Loss:0.019106445834040642\n",
      "Epoch 87, Batch 42 Loss:0.01837957836687565\n",
      "Epoch 87, Batch 43 Loss:0.006887954659759998\n",
      "Epoch 87, Batch 44 Loss:0.020314499735832214\n",
      "Epoch 87, Batch 45 Loss:0.007338047027587891\n",
      "Epoch 87, Batch 46 Loss:0.04889293387532234\n",
      "Epoch 87, Batch 47 Loss:0.08758030086755753\n",
      "Epoch 87, Batch 48 Loss:0.015715327113866806\n",
      "Epoch 87, Batch 49 Loss:0.009756574407219887\n",
      "Epoch 87, Batch 50 Loss:0.022885888814926147\n",
      "Epoch 87, Batch 51 Loss:0.04770807921886444\n",
      "Epoch 87, Batch 52 Loss:0.05389130115509033\n",
      "Epoch 87, Batch 53 Loss:0.015961239114403725\n",
      "Epoch 87, Batch 54 Loss:0.1398371011018753\n",
      "Epoch 87, Batch 55 Loss:0.034575946629047394\n",
      "Epoch 87, Batch 56 Loss:0.011228114366531372\n",
      "Epoch 87, Batch 57 Loss:0.051233433187007904\n",
      "Epoch 87, Batch 58 Loss:0.020541364327073097\n",
      "Epoch 87, Batch 59 Loss:0.020249273627996445\n",
      "Epoch 87, Batch 60 Loss:0.027320409193634987\n",
      "Epoch 87, Batch 61 Loss:0.018333859741687775\n",
      "Epoch 87, Batch 62 Loss:0.010551423765718937\n",
      "Epoch 87, Batch 63 Loss:0.011709661222994328\n",
      "Epoch 87, Batch 64 Loss:0.012378309853374958\n",
      "Epoch 87, Batch 65 Loss:0.023609105497598648\n",
      "Epoch 87, Batch 66 Loss:0.009755745530128479\n",
      "Epoch 87, Batch 67 Loss:0.05432168394327164\n",
      "Epoch 87, Batch 68 Loss:0.022515958175063133\n",
      "Epoch 87, Batch 69 Loss:0.04248867928981781\n",
      "Epoch 87, Batch 70 Loss:0.04865172132849693\n",
      "Epoch 87, Batch 71 Loss:0.0074521396309137344\n",
      "Epoch 87, Batch 72 Loss:0.04420261085033417\n",
      "Epoch 87, Batch 73 Loss:0.02468426711857319\n",
      "Epoch 87, Batch 74 Loss:0.02353528141975403\n",
      "Epoch 87, Batch 75 Loss:0.008686767891049385\n",
      "Epoch 87, Batch 76 Loss:0.04085918143391609\n",
      "Epoch 87, Batch 77 Loss:0.02526596561074257\n",
      "Epoch 87, Batch 78 Loss:0.020281966775655746\n",
      "Epoch 87, Batch 79 Loss:0.01265267375856638\n",
      "Epoch 87, Batch 80 Loss:0.01732972264289856\n",
      "Epoch 87, Batch 81 Loss:0.03250715509057045\n",
      "Epoch 87, Batch 82 Loss:0.04177074134349823\n",
      "Epoch 87, Batch 83 Loss:0.05276089906692505\n",
      "Epoch 87, Batch 84 Loss:0.03909088671207428\n",
      "Epoch 87, Batch 85 Loss:0.00947355106472969\n",
      "Epoch 87, Batch 86 Loss:0.017978450283408165\n",
      "Epoch 87, Batch 87 Loss:0.06382927298545837\n",
      "Epoch 87, Batch 88 Loss:0.009258642792701721\n",
      "Epoch 87, Batch 89 Loss:0.029829323291778564\n",
      "Epoch 87, Batch 90 Loss:0.017709944397211075\n",
      "Epoch 87, Batch 91 Loss:0.018830519169569016\n",
      "Epoch 87, Batch 92 Loss:0.01198940072208643\n",
      "Epoch 87, Batch 93 Loss:0.0074234819039702415\n",
      "Epoch 87, Batch 94 Loss:0.009517551399767399\n",
      "Epoch 87, Batch 95 Loss:0.05377615615725517\n",
      "Epoch 87, Batch 96 Loss:0.02567794732749462\n",
      "Epoch 87, Batch 97 Loss:0.02790912427008152\n",
      "Epoch 87, Batch 98 Loss:0.01709212176501751\n",
      "Epoch 87, Batch 99 Loss:0.009866312146186829\n",
      "Epoch 87, Batch 100 Loss:0.03754899278283119\n",
      "Epoch 87, Batch 101 Loss:0.03948234021663666\n",
      "Epoch 87, Batch 102 Loss:0.008192124776542187\n",
      "Epoch 87, Batch 103 Loss:0.01667739450931549\n",
      "Epoch 87, Batch 104 Loss:0.011191925033926964\n",
      "Epoch 87, Batch 105 Loss:0.04335363209247589\n",
      "Epoch 87, Batch 106 Loss:0.03016258403658867\n",
      "Epoch 87, Batch 107 Loss:0.003668637480586767\n",
      "Epoch 87, Batch 108 Loss:0.004015606828033924\n",
      "Epoch 87, Batch 109 Loss:0.021438710391521454\n",
      "Epoch 87, Batch 110 Loss:0.0026811794377863407\n",
      "Epoch 87, Batch 111 Loss:0.008058415725827217\n",
      "Epoch 87, Batch 112 Loss:0.006268403492867947\n",
      "Epoch 87, Batch 113 Loss:0.00865864660590887\n",
      "Epoch 87, Batch 114 Loss:0.01114582922309637\n",
      "Epoch 87, Batch 115 Loss:0.012533845379948616\n",
      "Epoch 87, Batch 116 Loss:0.008994420990347862\n",
      "Epoch 87, Batch 117 Loss:0.008200700394809246\n",
      "Epoch 87, Batch 118 Loss:0.018433647230267525\n",
      "Epoch 87, Batch 119 Loss:0.024972721934318542\n",
      "Epoch 87, Batch 120 Loss:0.017798058688640594\n",
      "Epoch 87, Batch 121 Loss:0.014334416016936302\n",
      "Epoch 87, Batch 122 Loss:0.0028515320736914873\n",
      "Epoch 87, Batch 123 Loss:0.011610886082053185\n",
      "Epoch 87, Batch 124 Loss:0.02503647841513157\n",
      "Epoch 87, Batch 125 Loss:0.013732299208641052\n",
      "Epoch 87, Batch 126 Loss:0.035533610731363297\n",
      "Epoch 87, Batch 127 Loss:0.03537531569600105\n",
      "Epoch 87, Batch 128 Loss:0.10863444209098816\n",
      "Epoch 87, Batch 129 Loss:0.01527685858309269\n",
      "Epoch 87, Batch 130 Loss:0.02922644279897213\n",
      "Epoch 87, Batch 131 Loss:0.02718508243560791\n",
      "Epoch 87, Batch 132 Loss:0.011025488376617432\n",
      "Epoch 87, Batch 133 Loss:0.03819303214550018\n",
      "Epoch 87, Batch 134 Loss:0.009533445350825787\n",
      "Epoch 87, Batch 135 Loss:0.010881435126066208\n",
      "Epoch 87, Batch 136 Loss:0.06164325773715973\n",
      "Epoch 87, Batch 137 Loss:0.03187906742095947\n",
      "Epoch 87, Batch 138 Loss:0.08113204687833786\n",
      "Epoch 87, Batch 139 Loss:0.03440621495246887\n",
      "Epoch 87, Batch 140 Loss:0.06280843913555145\n",
      "Epoch 87, Batch 141 Loss:0.0869956910610199\n",
      "Epoch 87, Batch 142 Loss:0.03584223985671997\n",
      "Epoch 87, Batch 143 Loss:0.023396652191877365\n",
      "Epoch 87, Batch 144 Loss:0.043491560965776443\n",
      "Epoch 87, Batch 145 Loss:0.05983404442667961\n",
      "Epoch 87, Batch 146 Loss:0.02447829395532608\n",
      "Epoch 87, Batch 147 Loss:0.07055646181106567\n",
      "Epoch 87, Batch 148 Loss:0.014947853051126003\n",
      "Epoch 87, Batch 149 Loss:0.049867916852235794\n",
      "Epoch 87, Batch 150 Loss:0.03329342603683472\n",
      "Epoch 87, Batch 151 Loss:0.05596978962421417\n",
      "Epoch 87, Batch 152 Loss:0.08150389045476913\n",
      "Epoch 87, Batch 153 Loss:0.05071717128157616\n",
      "Epoch 87, Batch 154 Loss:0.03800107538700104\n",
      "Epoch 87, Batch 155 Loss:0.06776243448257446\n",
      "Epoch 87, Batch 156 Loss:0.07181384414434433\n",
      "Epoch 87, Batch 157 Loss:0.06806064397096634\n",
      "Epoch 87, Batch 158 Loss:0.12176782637834549\n",
      "Epoch 87, Batch 159 Loss:0.039436329156160355\n",
      "Epoch 87, Batch 160 Loss:0.06358139961957932\n",
      "Epoch 87, Batch 161 Loss:0.03562043234705925\n",
      "Epoch 87, Batch 162 Loss:0.05016051232814789\n",
      "Epoch 87, Batch 163 Loss:0.10446539521217346\n",
      "Epoch 87, Batch 164 Loss:0.0702739804983139\n",
      "Epoch 87, Batch 165 Loss:0.03538554161787033\n",
      "Epoch 87, Batch 166 Loss:0.06486409902572632\n",
      "Epoch 87, Batch 167 Loss:0.09757854044437408\n",
      "Epoch 87, Batch 168 Loss:0.05111970007419586\n",
      "Epoch 87, Batch 169 Loss:0.04318356141448021\n",
      "Epoch 87, Batch 170 Loss:0.023978129029273987\n",
      "Epoch 87, Batch 171 Loss:0.06146231293678284\n",
      "Epoch 87, Batch 172 Loss:0.03519474342465401\n",
      "Epoch 87, Batch 173 Loss:0.05566722899675369\n",
      "Epoch 87, Batch 174 Loss:0.04201481491327286\n",
      "Epoch 87, Batch 175 Loss:0.008786730468273163\n",
      "Epoch 87, Batch 176 Loss:0.039416976273059845\n",
      "Epoch 87, Batch 177 Loss:0.028265632688999176\n",
      "Epoch 87, Batch 178 Loss:0.020735066384077072\n",
      "Epoch 87, Batch 179 Loss:0.04434932768344879\n",
      "Epoch 87, Batch 180 Loss:0.007469004951417446\n",
      "Epoch 87, Batch 181 Loss:0.039848893880844116\n",
      "Epoch 87, Batch 182 Loss:0.10455290228128433\n",
      "Epoch 87, Batch 183 Loss:0.011225417256355286\n",
      "Epoch 87, Batch 184 Loss:0.06165539473295212\n",
      "Epoch 87, Batch 185 Loss:0.03235954791307449\n",
      "Epoch 87, Batch 186 Loss:0.019439004361629486\n",
      "Epoch 87, Batch 187 Loss:0.06106319651007652\n",
      "Epoch 87, Batch 188 Loss:0.020420555025339127\n",
      "Epoch 87, Batch 189 Loss:0.023593351244926453\n",
      "Epoch 87, Batch 190 Loss:0.040635496377944946\n",
      "Epoch 87, Batch 191 Loss:0.052286166697740555\n",
      "Epoch 87, Batch 192 Loss:0.05090036615729332\n",
      "Epoch 87, Batch 193 Loss:0.02442435547709465\n",
      "Epoch 87, Batch 194 Loss:0.022938678041100502\n",
      "Epoch 87, Batch 195 Loss:0.05976412072777748\n",
      "Epoch 87, Batch 196 Loss:0.06454771757125854\n",
      "Epoch 87, Batch 197 Loss:0.03134283423423767\n",
      "Epoch 87, Batch 198 Loss:0.0400262176990509\n",
      "Epoch 87, Batch 199 Loss:0.061107903718948364\n",
      "Epoch 87, Batch 200 Loss:0.12194439023733139\n",
      "Epoch 87, Batch 201 Loss:0.04515475407242775\n",
      "Epoch 87, Batch 202 Loss:0.05611712858080864\n",
      "Epoch 87, Batch 203 Loss:0.05681042745709419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87, Batch 204 Loss:0.055892568081617355\n",
      "Epoch 87, Batch 205 Loss:0.1866527795791626\n",
      "Epoch 87, Batch 206 Loss:0.09874065220355988\n",
      "Epoch 87, Batch 207 Loss:0.05413950979709625\n",
      "Epoch 87, Batch 208 Loss:0.11413311213254929\n",
      "Epoch 87, Batch 209 Loss:0.09011541306972504\n",
      "Epoch 87, Batch 210 Loss:0.09613707661628723\n",
      "Epoch 87, Batch 211 Loss:0.0527169331908226\n",
      "Epoch 87, Batch 212 Loss:0.053715601563453674\n",
      "Epoch 87, Batch 213 Loss:0.09864737838506699\n",
      "Epoch 87, Batch 214 Loss:0.06219131872057915\n",
      "Epoch 87, Batch 215 Loss:0.020530816167593002\n",
      "Epoch 87, Batch 216 Loss:0.03148786351084709\n",
      "Epoch 87, Batch 217 Loss:0.0708567351102829\n",
      "Epoch 87, Batch 218 Loss:0.04227668046951294\n",
      "Epoch 87, Batch 219 Loss:0.054694242775440216\n",
      "Epoch 87, Batch 220 Loss:0.04375837743282318\n",
      "Epoch 87, Batch 221 Loss:0.021997660398483276\n",
      "Epoch 87, Batch 222 Loss:0.05539947748184204\n",
      "Epoch 87, Batch 223 Loss:0.03733731433749199\n",
      "Epoch 87, Batch 224 Loss:0.05713015794754028\n",
      "Epoch 87, Batch 225 Loss:0.0568506196141243\n",
      "Epoch 87, Batch 226 Loss:0.036987703293561935\n",
      "Epoch 87, Batch 227 Loss:0.03263280540704727\n",
      "Epoch 87, Batch 228 Loss:0.01728558912873268\n",
      "Epoch 87, Batch 229 Loss:0.040039606392383575\n",
      "Epoch 87, Batch 230 Loss:0.02646102011203766\n",
      "Epoch 87, Batch 231 Loss:0.019358297809958458\n",
      "Epoch 87, Batch 232 Loss:0.03586753085255623\n",
      "Epoch 87, Batch 233 Loss:0.03103959932923317\n",
      "Loss in this Epoch is: 3.10395993292 %\n",
      "Accuracy in this Epoch is: 88.2399976254 %\n",
      "Epoch 88, Batch 0 Loss:0.028416350483894348\n",
      "Epoch 88, Batch 1 Loss:0.034819092601537704\n",
      "Epoch 88, Batch 2 Loss:0.029085399582982063\n",
      "Epoch 88, Batch 3 Loss:0.04478202760219574\n",
      "Epoch 88, Batch 4 Loss:0.025469506159424782\n",
      "Epoch 88, Batch 5 Loss:0.01864723488688469\n",
      "Epoch 88, Batch 6 Loss:0.010555177927017212\n",
      "Epoch 88, Batch 7 Loss:0.019229713827371597\n",
      "Epoch 88, Batch 8 Loss:0.027082569897174835\n",
      "Epoch 88, Batch 9 Loss:0.01674116589128971\n",
      "Epoch 88, Batch 10 Loss:0.018976418301463127\n",
      "Epoch 88, Batch 11 Loss:0.005084407050162554\n",
      "Epoch 88, Batch 12 Loss:0.025171147659420967\n",
      "Epoch 88, Batch 13 Loss:0.01794215478003025\n",
      "Epoch 88, Batch 14 Loss:0.032870423048734665\n",
      "Epoch 88, Batch 15 Loss:0.0389934666454792\n",
      "Epoch 88, Batch 16 Loss:0.014197471551597118\n",
      "Epoch 88, Batch 17 Loss:0.05204165726900101\n",
      "Epoch 88, Batch 18 Loss:0.019741863012313843\n",
      "Epoch 88, Batch 19 Loss:0.037210188806056976\n",
      "Epoch 88, Batch 20 Loss:0.03703921288251877\n",
      "Epoch 88, Batch 21 Loss:0.01657622680068016\n",
      "Epoch 88, Batch 22 Loss:0.007491901516914368\n",
      "Epoch 88, Batch 23 Loss:0.005719105713069439\n",
      "Epoch 88, Batch 24 Loss:0.023282654583454132\n",
      "Epoch 88, Batch 25 Loss:0.015029890462756157\n",
      "Epoch 88, Batch 26 Loss:0.018391959369182587\n",
      "Epoch 88, Batch 27 Loss:0.0238792784512043\n",
      "Epoch 88, Batch 28 Loss:0.00622766837477684\n",
      "Epoch 88, Batch 29 Loss:0.06676938384771347\n",
      "Epoch 88, Batch 30 Loss:0.012914767488837242\n",
      "Epoch 88, Batch 31 Loss:0.007331607397645712\n",
      "Epoch 88, Batch 32 Loss:0.0070841191336512566\n",
      "Epoch 88, Batch 33 Loss:0.011299166828393936\n",
      "Epoch 88, Batch 34 Loss:0.013665677048265934\n",
      "Epoch 88, Batch 35 Loss:0.008820045739412308\n",
      "Epoch 88, Batch 36 Loss:0.038212984800338745\n",
      "Epoch 88, Batch 37 Loss:0.012698574922978878\n",
      "Epoch 88, Batch 38 Loss:0.025968173518776894\n",
      "Epoch 88, Batch 39 Loss:0.010884354822337627\n",
      "Epoch 88, Batch 40 Loss:0.02834952622652054\n",
      "Epoch 88, Batch 41 Loss:0.09064070880413055\n",
      "Epoch 88, Batch 42 Loss:0.03673113137483597\n",
      "Epoch 88, Batch 43 Loss:0.009066403843462467\n",
      "Epoch 88, Batch 44 Loss:0.056620653718709946\n",
      "Epoch 88, Batch 45 Loss:0.04018305987119675\n",
      "Epoch 88, Batch 46 Loss:0.16071373224258423\n",
      "Epoch 88, Batch 47 Loss:0.07198215276002884\n",
      "Epoch 88, Batch 48 Loss:0.11522644013166428\n",
      "Epoch 88, Batch 49 Loss:0.059778474271297455\n",
      "Epoch 88, Batch 50 Loss:0.07674074918031693\n",
      "Epoch 88, Batch 51 Loss:0.1290239542722702\n",
      "Epoch 88, Batch 52 Loss:0.08177261054515839\n",
      "Epoch 88, Batch 53 Loss:0.16278284788131714\n",
      "Epoch 88, Batch 54 Loss:0.18851612508296967\n",
      "Epoch 88, Batch 55 Loss:0.06575619429349899\n",
      "Epoch 88, Batch 56 Loss:0.08260764181613922\n",
      "Epoch 88, Batch 57 Loss:0.1371716856956482\n",
      "Epoch 88, Batch 58 Loss:0.11218428611755371\n",
      "Epoch 88, Batch 59 Loss:0.08693680912256241\n",
      "Epoch 88, Batch 60 Loss:0.08253631740808487\n",
      "Epoch 88, Batch 61 Loss:0.06393146514892578\n",
      "Epoch 88, Batch 62 Loss:0.0666385143995285\n",
      "Epoch 88, Batch 63 Loss:0.08364962041378021\n",
      "Epoch 88, Batch 64 Loss:0.07583450525999069\n",
      "Epoch 88, Batch 65 Loss:0.11829303950071335\n",
      "Epoch 88, Batch 66 Loss:0.051967836916446686\n",
      "Epoch 88, Batch 67 Loss:0.07368062436580658\n",
      "Epoch 88, Batch 68 Loss:0.05603843182325363\n",
      "Epoch 88, Batch 69 Loss:0.050007205456495285\n",
      "Epoch 88, Batch 70 Loss:0.07503900676965714\n",
      "Epoch 88, Batch 71 Loss:0.07499253749847412\n",
      "Epoch 88, Batch 72 Loss:0.050690341740846634\n",
      "Epoch 88, Batch 73 Loss:0.05571182072162628\n",
      "Epoch 88, Batch 74 Loss:0.05119633302092552\n",
      "Epoch 88, Batch 75 Loss:0.07067917287349701\n",
      "Epoch 88, Batch 76 Loss:0.04452911764383316\n",
      "Epoch 88, Batch 77 Loss:0.04245169833302498\n",
      "Epoch 88, Batch 78 Loss:0.05569605901837349\n",
      "Epoch 88, Batch 79 Loss:0.021780263632535934\n",
      "Epoch 88, Batch 80 Loss:0.08404555916786194\n",
      "Epoch 88, Batch 81 Loss:0.04229942709207535\n",
      "Epoch 88, Batch 82 Loss:0.0246844831854105\n",
      "Epoch 88, Batch 83 Loss:0.03578498214483261\n",
      "Epoch 88, Batch 84 Loss:0.04908366873860359\n",
      "Epoch 88, Batch 85 Loss:0.03823232650756836\n",
      "Epoch 88, Batch 86 Loss:0.032740939408540726\n",
      "Epoch 88, Batch 87 Loss:0.019248267635703087\n",
      "Epoch 88, Batch 88 Loss:0.0417952835559845\n",
      "Epoch 88, Batch 89 Loss:0.024135151877999306\n",
      "Epoch 88, Batch 90 Loss:0.039470210671424866\n",
      "Epoch 88, Batch 91 Loss:0.08663526922464371\n",
      "Epoch 88, Batch 92 Loss:0.027497032657265663\n",
      "Epoch 88, Batch 93 Loss:0.03431516885757446\n",
      "Epoch 88, Batch 94 Loss:0.025663062930107117\n",
      "Epoch 88, Batch 95 Loss:0.03750849887728691\n",
      "Epoch 88, Batch 96 Loss:0.04098806530237198\n",
      "Epoch 88, Batch 97 Loss:0.05508628860116005\n",
      "Epoch 88, Batch 98 Loss:0.02536265179514885\n",
      "Epoch 88, Batch 99 Loss:0.01784338802099228\n",
      "Epoch 88, Batch 100 Loss:0.04185295104980469\n",
      "Epoch 88, Batch 101 Loss:0.04140074923634529\n",
      "Epoch 88, Batch 102 Loss:0.027955878525972366\n",
      "Epoch 88, Batch 103 Loss:0.025265567004680634\n",
      "Epoch 88, Batch 104 Loss:0.02687085047364235\n",
      "Epoch 88, Batch 105 Loss:0.04175185784697533\n",
      "Epoch 88, Batch 106 Loss:0.016628459095954895\n",
      "Epoch 88, Batch 107 Loss:0.03181735426187515\n",
      "Epoch 88, Batch 108 Loss:0.03915010020136833\n",
      "Epoch 88, Batch 109 Loss:0.014039908535778522\n",
      "Epoch 88, Batch 110 Loss:0.09880906343460083\n",
      "Epoch 88, Batch 111 Loss:0.03635763004422188\n",
      "Epoch 88, Batch 112 Loss:0.014596251770853996\n",
      "Epoch 88, Batch 113 Loss:0.045996759086847305\n",
      "Epoch 88, Batch 114 Loss:0.020259346812963486\n",
      "Epoch 88, Batch 115 Loss:0.07787991315126419\n",
      "Epoch 88, Batch 116 Loss:0.04240695387125015\n",
      "Epoch 88, Batch 117 Loss:0.01886604167521\n",
      "Epoch 88, Batch 118 Loss:0.03094133362174034\n",
      "Epoch 88, Batch 119 Loss:0.02985784038901329\n",
      "Epoch 88, Batch 120 Loss:0.030129529535770416\n",
      "Epoch 88, Batch 121 Loss:0.03867647051811218\n",
      "Epoch 88, Batch 122 Loss:0.08308243751525879\n",
      "Epoch 88, Batch 123 Loss:0.014709701761603355\n",
      "Epoch 88, Batch 124 Loss:0.03239297866821289\n",
      "Epoch 88, Batch 125 Loss:0.028474917635321617\n",
      "Epoch 88, Batch 126 Loss:0.01239919662475586\n",
      "Epoch 88, Batch 127 Loss:0.030625252053141594\n",
      "Epoch 88, Batch 128 Loss:0.038729358464479446\n",
      "Epoch 88, Batch 129 Loss:0.019055761396884918\n",
      "Epoch 88, Batch 130 Loss:0.027347810566425323\n",
      "Epoch 88, Batch 131 Loss:0.06163414567708969\n",
      "Epoch 88, Batch 132 Loss:0.006161570083349943\n",
      "Epoch 88, Batch 133 Loss:0.017085112631320953\n",
      "Epoch 88, Batch 134 Loss:0.023433741182088852\n",
      "Epoch 88, Batch 135 Loss:0.076592355966568\n",
      "Epoch 88, Batch 136 Loss:0.007155212573707104\n",
      "Epoch 88, Batch 137 Loss:0.04608087241649628\n",
      "Epoch 88, Batch 138 Loss:0.05920817703008652\n",
      "Epoch 88, Batch 139 Loss:0.04513886198401451\n",
      "Epoch 88, Batch 140 Loss:0.061401061713695526\n",
      "Epoch 88, Batch 141 Loss:0.0459185466170311\n",
      "Epoch 88, Batch 142 Loss:0.051982976496219635\n",
      "Epoch 88, Batch 143 Loss:0.02903839945793152\n",
      "Epoch 88, Batch 144 Loss:0.035052746534347534\n",
      "Epoch 88, Batch 145 Loss:0.05751714110374451\n",
      "Epoch 88, Batch 146 Loss:0.015345016494393349\n",
      "Epoch 88, Batch 147 Loss:0.02451312355697155\n",
      "Epoch 88, Batch 148 Loss:0.04560830444097519\n",
      "Epoch 88, Batch 149 Loss:0.02901289239525795\n",
      "Epoch 88, Batch 150 Loss:0.1022014319896698\n",
      "Epoch 88, Batch 151 Loss:0.17970210313796997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88, Batch 152 Loss:0.02573326602578163\n",
      "Epoch 88, Batch 153 Loss:0.03175147622823715\n",
      "Epoch 88, Batch 154 Loss:0.03594072163105011\n",
      "Epoch 88, Batch 155 Loss:0.02649334818124771\n",
      "Epoch 88, Batch 156 Loss:0.021760521456599236\n",
      "Epoch 88, Batch 157 Loss:0.04236695170402527\n",
      "Epoch 88, Batch 158 Loss:0.0292993001639843\n",
      "Epoch 88, Batch 159 Loss:0.03639651834964752\n",
      "Epoch 88, Batch 160 Loss:0.00905099231749773\n",
      "Epoch 88, Batch 161 Loss:0.04129301384091377\n",
      "Epoch 88, Batch 162 Loss:0.015096316114068031\n",
      "Epoch 88, Batch 163 Loss:0.053288377821445465\n",
      "Epoch 88, Batch 164 Loss:0.008712786249816418\n",
      "Epoch 88, Batch 165 Loss:0.06991477310657501\n",
      "Epoch 88, Batch 166 Loss:0.030205808579921722\n",
      "Epoch 88, Batch 167 Loss:0.022114640101790428\n",
      "Epoch 88, Batch 168 Loss:0.011462436988949776\n",
      "Epoch 88, Batch 169 Loss:0.04079706221818924\n",
      "Epoch 88, Batch 170 Loss:0.02060948684811592\n",
      "Epoch 88, Batch 171 Loss:0.012184921652078629\n",
      "Epoch 88, Batch 172 Loss:0.011570746079087257\n",
      "Epoch 88, Batch 173 Loss:0.032938454300165176\n",
      "Epoch 88, Batch 174 Loss:0.014024762436747551\n",
      "Epoch 88, Batch 175 Loss:0.014873976819217205\n",
      "Epoch 88, Batch 176 Loss:0.0440654382109642\n",
      "Epoch 88, Batch 177 Loss:0.05470363050699234\n",
      "Epoch 88, Batch 178 Loss:0.032987989485263824\n",
      "Epoch 88, Batch 179 Loss:0.022818271070718765\n",
      "Epoch 88, Batch 180 Loss:0.02804524451494217\n",
      "Epoch 88, Batch 181 Loss:0.03005167283117771\n",
      "Epoch 88, Batch 182 Loss:0.06604602187871933\n",
      "Epoch 88, Batch 183 Loss:0.028271906077861786\n",
      "Epoch 88, Batch 184 Loss:0.013003095053136349\n",
      "Epoch 88, Batch 185 Loss:0.007500448264181614\n",
      "Epoch 88, Batch 186 Loss:0.012971361167728901\n",
      "Epoch 88, Batch 187 Loss:0.0196610726416111\n",
      "Epoch 88, Batch 188 Loss:0.03146863356232643\n",
      "Epoch 88, Batch 189 Loss:0.023710114881396294\n",
      "Epoch 88, Batch 190 Loss:0.02265225350856781\n",
      "Epoch 88, Batch 191 Loss:0.01189466379582882\n",
      "Epoch 88, Batch 192 Loss:0.025178981944918633\n",
      "Epoch 88, Batch 193 Loss:0.02871111035346985\n",
      "Epoch 88, Batch 194 Loss:0.04162577539682388\n",
      "Epoch 88, Batch 195 Loss:0.041098110377788544\n",
      "Epoch 88, Batch 196 Loss:0.0202343687415123\n",
      "Epoch 88, Batch 197 Loss:0.06784120202064514\n",
      "Epoch 88, Batch 198 Loss:0.033927999436855316\n",
      "Epoch 88, Batch 199 Loss:0.031046323478221893\n",
      "Epoch 88, Batch 200 Loss:0.028393760323524475\n",
      "Epoch 88, Batch 201 Loss:0.023638831451535225\n",
      "Epoch 88, Batch 202 Loss:0.02766244299709797\n",
      "Epoch 88, Batch 203 Loss:0.027355486527085304\n",
      "Epoch 88, Batch 204 Loss:0.03321423754096031\n",
      "Epoch 88, Batch 205 Loss:0.014102151617407799\n",
      "Epoch 88, Batch 206 Loss:0.010404447093605995\n",
      "Epoch 88, Batch 207 Loss:0.010793721303343773\n",
      "Epoch 88, Batch 208 Loss:0.031870532780885696\n",
      "Epoch 88, Batch 209 Loss:0.017295805737376213\n",
      "Epoch 88, Batch 210 Loss:0.031919337809085846\n",
      "Epoch 88, Batch 211 Loss:0.0324125699698925\n",
      "Epoch 88, Batch 212 Loss:0.011296361684799194\n",
      "Epoch 88, Batch 213 Loss:0.03432080149650574\n",
      "Epoch 88, Batch 214 Loss:0.015987753868103027\n",
      "Epoch 88, Batch 215 Loss:0.04538410156965256\n",
      "Epoch 88, Batch 216 Loss:0.02302897721529007\n",
      "Epoch 88, Batch 217 Loss:0.01728121004998684\n",
      "Epoch 88, Batch 218 Loss:0.012417923659086227\n",
      "Epoch 88, Batch 219 Loss:0.03217901289463043\n",
      "Epoch 88, Batch 220 Loss:0.046808794140815735\n",
      "Epoch 88, Batch 221 Loss:0.011893469840288162\n",
      "Epoch 88, Batch 222 Loss:0.015299501828849316\n",
      "Epoch 88, Batch 223 Loss:0.010450021363794804\n",
      "Epoch 88, Batch 224 Loss:0.024057164788246155\n",
      "Epoch 88, Batch 225 Loss:0.021510059013962746\n",
      "Epoch 88, Batch 226 Loss:0.013333025388419628\n",
      "Epoch 88, Batch 227 Loss:0.03349480777978897\n",
      "Epoch 88, Batch 228 Loss:0.01611393503844738\n",
      "Epoch 88, Batch 229 Loss:0.018654249608516693\n",
      "Epoch 88, Batch 230 Loss:0.018520107492804527\n",
      "Epoch 88, Batch 231 Loss:0.015862317755818367\n",
      "Epoch 88, Batch 232 Loss:0.014390688389539719\n",
      "Epoch 88, Batch 233 Loss:0.03988577798008919\n",
      "Loss in this Epoch is: 3.98857779801 %\n",
      "Accuracy in this Epoch is: 88.220000267 %\n",
      "Epoch 89, Batch 0 Loss:0.013296394608914852\n",
      "Epoch 89, Batch 1 Loss:0.008733082562685013\n",
      "Epoch 89, Batch 2 Loss:0.025788452476263046\n",
      "Epoch 89, Batch 3 Loss:0.011176196858286858\n",
      "Epoch 89, Batch 4 Loss:0.006351944990456104\n",
      "Epoch 89, Batch 5 Loss:0.04005933552980423\n",
      "Epoch 89, Batch 6 Loss:0.04398588836193085\n",
      "Epoch 89, Batch 7 Loss:0.02199532464146614\n",
      "Epoch 89, Batch 8 Loss:0.0037109781987965107\n",
      "Epoch 89, Batch 9 Loss:0.010761380195617676\n",
      "Epoch 89, Batch 10 Loss:0.01921716518700123\n",
      "Epoch 89, Batch 11 Loss:0.041243188083171844\n",
      "Epoch 89, Batch 12 Loss:0.02085208334028721\n",
      "Epoch 89, Batch 13 Loss:0.0075345467776060104\n",
      "Epoch 89, Batch 14 Loss:0.018824314698576927\n",
      "Epoch 89, Batch 15 Loss:0.015983251854777336\n",
      "Epoch 89, Batch 16 Loss:0.019737884402275085\n",
      "Epoch 89, Batch 17 Loss:0.03715648874640465\n",
      "Epoch 89, Batch 18 Loss:0.023331377655267715\n",
      "Epoch 89, Batch 19 Loss:0.018263984471559525\n",
      "Epoch 89, Batch 20 Loss:0.05463920906186104\n",
      "Epoch 89, Batch 21 Loss:0.006102554500102997\n",
      "Epoch 89, Batch 22 Loss:0.0010298334527760744\n",
      "Epoch 89, Batch 23 Loss:0.0068568941205739975\n",
      "Epoch 89, Batch 24 Loss:0.02800397388637066\n",
      "Epoch 89, Batch 25 Loss:0.015023780055344105\n",
      "Epoch 89, Batch 26 Loss:0.00452596927061677\n",
      "Epoch 89, Batch 27 Loss:0.014909820631146431\n",
      "Epoch 89, Batch 28 Loss:0.022931525483727455\n",
      "Epoch 89, Batch 29 Loss:0.009242281317710876\n",
      "Epoch 89, Batch 30 Loss:0.006950776092708111\n",
      "Epoch 89, Batch 31 Loss:0.03769967705011368\n",
      "Epoch 89, Batch 32 Loss:0.006972193252295256\n",
      "Epoch 89, Batch 33 Loss:0.012248245067894459\n",
      "Epoch 89, Batch 34 Loss:0.014952512457966805\n",
      "Epoch 89, Batch 35 Loss:0.008196697570383549\n",
      "Epoch 89, Batch 36 Loss:0.009174400940537453\n",
      "Epoch 89, Batch 37 Loss:0.03323664516210556\n",
      "Epoch 89, Batch 38 Loss:0.027329571545124054\n",
      "Epoch 89, Batch 39 Loss:0.029456766322255135\n",
      "Epoch 89, Batch 40 Loss:0.019702566787600517\n",
      "Epoch 89, Batch 41 Loss:0.011169875040650368\n",
      "Epoch 89, Batch 42 Loss:0.03126169368624687\n",
      "Epoch 89, Batch 43 Loss:0.011035164818167686\n",
      "Epoch 89, Batch 44 Loss:0.0219204630702734\n",
      "Epoch 89, Batch 45 Loss:0.0327773280441761\n",
      "Epoch 89, Batch 46 Loss:0.008700783364474773\n",
      "Epoch 89, Batch 47 Loss:0.004633776377886534\n",
      "Epoch 89, Batch 48 Loss:0.021532393991947174\n",
      "Epoch 89, Batch 49 Loss:0.04915304854512215\n",
      "Epoch 89, Batch 50 Loss:0.03456202894449234\n",
      "Epoch 89, Batch 51 Loss:0.04026416316628456\n",
      "Epoch 89, Batch 52 Loss:0.021628662943840027\n",
      "Epoch 89, Batch 53 Loss:0.01975070871412754\n",
      "Epoch 89, Batch 54 Loss:0.015118268318474293\n",
      "Epoch 89, Batch 55 Loss:0.03924751281738281\n",
      "Epoch 89, Batch 56 Loss:0.036172471940517426\n",
      "Epoch 89, Batch 57 Loss:0.06036999449133873\n",
      "Epoch 89, Batch 58 Loss:0.04981645941734314\n",
      "Epoch 89, Batch 59 Loss:0.0389888696372509\n",
      "Epoch 89, Batch 60 Loss:0.013637246564030647\n",
      "Epoch 89, Batch 61 Loss:0.03236762061715126\n",
      "Epoch 89, Batch 62 Loss:0.036455485969781876\n",
      "Epoch 89, Batch 63 Loss:0.049228668212890625\n",
      "Epoch 89, Batch 64 Loss:0.010739780962467194\n",
      "Epoch 89, Batch 65 Loss:0.023910606279969215\n",
      "Epoch 89, Batch 66 Loss:0.007937345653772354\n",
      "Epoch 89, Batch 67 Loss:0.03793884068727493\n",
      "Epoch 89, Batch 68 Loss:0.019627442583441734\n",
      "Epoch 89, Batch 69 Loss:0.029216812923550606\n",
      "Epoch 89, Batch 70 Loss:0.037798527628183365\n",
      "Epoch 89, Batch 71 Loss:0.013465887866914272\n",
      "Epoch 89, Batch 72 Loss:0.02758932113647461\n",
      "Epoch 89, Batch 73 Loss:0.012392282485961914\n",
      "Epoch 89, Batch 74 Loss:0.038486678153276443\n",
      "Epoch 89, Batch 75 Loss:0.018262313678860664\n",
      "Epoch 89, Batch 76 Loss:0.036352869123220444\n",
      "Epoch 89, Batch 77 Loss:0.03131593391299248\n",
      "Epoch 89, Batch 78 Loss:0.01128944382071495\n",
      "Epoch 89, Batch 79 Loss:0.01058539841324091\n",
      "Epoch 89, Batch 80 Loss:0.005610617343336344\n",
      "Epoch 89, Batch 81 Loss:0.012471533380448818\n",
      "Epoch 89, Batch 82 Loss:0.020464107394218445\n",
      "Epoch 89, Batch 83 Loss:0.009089297614991665\n",
      "Epoch 89, Batch 84 Loss:0.026683416217565536\n",
      "Epoch 89, Batch 85 Loss:0.007535257376730442\n",
      "Epoch 89, Batch 86 Loss:0.015133128501474857\n",
      "Epoch 89, Batch 87 Loss:0.04181844741106033\n",
      "Epoch 89, Batch 88 Loss:0.015439016744494438\n",
      "Epoch 89, Batch 89 Loss:0.012049177661538124\n",
      "Epoch 89, Batch 90 Loss:0.03013267181813717\n",
      "Epoch 89, Batch 91 Loss:0.007202514912933111\n",
      "Epoch 89, Batch 92 Loss:0.0027237245813012123\n",
      "Epoch 89, Batch 93 Loss:0.004186794627457857\n",
      "Epoch 89, Batch 94 Loss:0.009850779548287392\n",
      "Epoch 89, Batch 95 Loss:0.025131914764642715\n",
      "Epoch 89, Batch 96 Loss:0.01921209506690502\n",
      "Epoch 89, Batch 97 Loss:0.0060162050649523735\n",
      "Epoch 89, Batch 98 Loss:0.038633786141872406\n",
      "Epoch 89, Batch 99 Loss:0.03406789153814316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89, Batch 100 Loss:0.004340972751379013\n",
      "Epoch 89, Batch 101 Loss:0.015507537871599197\n",
      "Epoch 89, Batch 102 Loss:0.010308600962162018\n",
      "Epoch 89, Batch 103 Loss:0.09834980964660645\n",
      "Epoch 89, Batch 104 Loss:0.01813441887497902\n",
      "Epoch 89, Batch 105 Loss:0.006703946739435196\n",
      "Epoch 89, Batch 106 Loss:0.016758324578404427\n",
      "Epoch 89, Batch 107 Loss:0.06387124955654144\n",
      "Epoch 89, Batch 108 Loss:0.04381054267287254\n",
      "Epoch 89, Batch 109 Loss:0.015944629907608032\n",
      "Epoch 89, Batch 110 Loss:0.03989455848932266\n",
      "Epoch 89, Batch 111 Loss:0.037098295986652374\n",
      "Epoch 89, Batch 112 Loss:0.03502343222498894\n",
      "Epoch 89, Batch 113 Loss:0.08091460913419724\n",
      "Epoch 89, Batch 114 Loss:0.09883067011833191\n",
      "Epoch 89, Batch 115 Loss:0.034982871264219284\n",
      "Epoch 89, Batch 116 Loss:0.055173639208078384\n",
      "Epoch 89, Batch 117 Loss:0.03779281675815582\n",
      "Epoch 89, Batch 118 Loss:0.041549209505319595\n",
      "Epoch 89, Batch 119 Loss:0.05955524742603302\n",
      "Epoch 89, Batch 120 Loss:0.05647954344749451\n",
      "Epoch 89, Batch 121 Loss:0.04788203537464142\n",
      "Epoch 89, Batch 122 Loss:0.07480742037296295\n",
      "Epoch 89, Batch 123 Loss:0.0462784469127655\n",
      "Epoch 89, Batch 124 Loss:0.06272965669631958\n",
      "Epoch 89, Batch 125 Loss:0.04117637872695923\n",
      "Epoch 89, Batch 126 Loss:0.05159696191549301\n",
      "Epoch 89, Batch 127 Loss:0.034939561039209366\n",
      "Epoch 89, Batch 128 Loss:0.07077766209840775\n",
      "Epoch 89, Batch 129 Loss:0.013232553377747536\n",
      "Epoch 89, Batch 130 Loss:0.017078381031751633\n",
      "Epoch 89, Batch 131 Loss:0.04136531054973602\n",
      "Epoch 89, Batch 132 Loss:0.02955779619514942\n",
      "Epoch 89, Batch 133 Loss:0.028433706611394882\n",
      "Epoch 89, Batch 134 Loss:0.08120197057723999\n",
      "Epoch 89, Batch 135 Loss:0.03397543355822563\n",
      "Epoch 89, Batch 136 Loss:0.05109240114688873\n",
      "Epoch 89, Batch 137 Loss:0.014256952330470085\n",
      "Epoch 89, Batch 138 Loss:0.029289133846759796\n",
      "Epoch 89, Batch 139 Loss:0.01747261732816696\n",
      "Epoch 89, Batch 140 Loss:0.05476108193397522\n",
      "Epoch 89, Batch 141 Loss:0.020271098241209984\n",
      "Epoch 89, Batch 142 Loss:0.05533606931567192\n",
      "Epoch 89, Batch 143 Loss:0.014282049611210823\n",
      "Epoch 89, Batch 144 Loss:0.07390841096639633\n",
      "Epoch 89, Batch 145 Loss:0.02825748547911644\n",
      "Epoch 89, Batch 146 Loss:0.04916800558567047\n",
      "Epoch 89, Batch 147 Loss:0.08717826753854752\n",
      "Epoch 89, Batch 148 Loss:0.034241221845149994\n",
      "Epoch 89, Batch 149 Loss:0.034358877688646317\n",
      "Epoch 89, Batch 150 Loss:0.03184584900736809\n",
      "Epoch 89, Batch 151 Loss:0.03772064670920372\n",
      "Epoch 89, Batch 152 Loss:0.02321074903011322\n",
      "Epoch 89, Batch 153 Loss:0.026554986834526062\n",
      "Epoch 89, Batch 154 Loss:0.0238069798797369\n",
      "Epoch 89, Batch 155 Loss:0.01953880675137043\n",
      "Epoch 89, Batch 156 Loss:0.031423237174749374\n",
      "Epoch 89, Batch 157 Loss:0.0480218380689621\n",
      "Epoch 89, Batch 158 Loss:0.029154811054468155\n",
      "Epoch 89, Batch 159 Loss:0.029766283929347992\n",
      "Epoch 89, Batch 160 Loss:0.02186618000268936\n",
      "Epoch 89, Batch 161 Loss:0.012217973358929157\n",
      "Epoch 89, Batch 162 Loss:0.035441093146800995\n",
      "Epoch 89, Batch 163 Loss:0.015567649155855179\n",
      "Epoch 89, Batch 164 Loss:0.020237522199749947\n",
      "Epoch 89, Batch 165 Loss:0.023457538336515427\n",
      "Epoch 89, Batch 166 Loss:0.030771635472774506\n",
      "Epoch 89, Batch 167 Loss:0.011925425380468369\n",
      "Epoch 89, Batch 168 Loss:0.011675925925374031\n",
      "Epoch 89, Batch 169 Loss:0.029851380735635757\n",
      "Epoch 89, Batch 170 Loss:0.02454579621553421\n",
      "Epoch 89, Batch 171 Loss:0.011845426633954048\n",
      "Epoch 89, Batch 172 Loss:0.006929805967956781\n",
      "Epoch 89, Batch 173 Loss:0.01927199959754944\n",
      "Epoch 89, Batch 174 Loss:0.04186660423874855\n",
      "Epoch 89, Batch 175 Loss:0.03557169437408447\n",
      "Epoch 89, Batch 176 Loss:0.03081505000591278\n",
      "Epoch 89, Batch 177 Loss:0.045578040182590485\n",
      "Epoch 89, Batch 178 Loss:0.018336694687604904\n",
      "Epoch 89, Batch 179 Loss:0.009696190245449543\n",
      "Epoch 89, Batch 180 Loss:0.015337919816374779\n",
      "Epoch 89, Batch 181 Loss:0.05215776339173317\n",
      "Epoch 89, Batch 182 Loss:0.05648685246706009\n",
      "Epoch 89, Batch 183 Loss:0.027145234867930412\n",
      "Epoch 89, Batch 184 Loss:0.01359877735376358\n",
      "Epoch 89, Batch 185 Loss:0.01707618497312069\n",
      "Epoch 89, Batch 186 Loss:0.01868784800171852\n",
      "Epoch 89, Batch 187 Loss:0.032278772443532944\n",
      "Epoch 89, Batch 188 Loss:0.00924110971391201\n",
      "Epoch 89, Batch 189 Loss:0.043751731514930725\n",
      "Epoch 89, Batch 190 Loss:0.08959604054689407\n",
      "Epoch 89, Batch 191 Loss:0.023624379187822342\n",
      "Epoch 89, Batch 192 Loss:0.010464029386639595\n",
      "Epoch 89, Batch 193 Loss:0.01520093996077776\n",
      "Epoch 89, Batch 194 Loss:0.012231574393808842\n",
      "Epoch 89, Batch 195 Loss:0.018445022404193878\n",
      "Epoch 89, Batch 196 Loss:0.04224879667162895\n",
      "Epoch 89, Batch 197 Loss:0.036906398832798004\n",
      "Epoch 89, Batch 198 Loss:0.04214230552315712\n",
      "Epoch 89, Batch 199 Loss:0.00266958586871624\n",
      "Epoch 89, Batch 200 Loss:0.02254834957420826\n",
      "Epoch 89, Batch 201 Loss:0.007902141660451889\n",
      "Epoch 89, Batch 202 Loss:0.02761334925889969\n",
      "Epoch 89, Batch 203 Loss:0.017457667738199234\n",
      "Epoch 89, Batch 204 Loss:0.0666128545999527\n",
      "Epoch 89, Batch 205 Loss:0.006194204092025757\n",
      "Epoch 89, Batch 206 Loss:0.023689694702625275\n",
      "Epoch 89, Batch 207 Loss:0.014033719897270203\n",
      "Epoch 89, Batch 208 Loss:0.0300179123878479\n",
      "Epoch 89, Batch 209 Loss:0.03339506313204765\n",
      "Epoch 89, Batch 210 Loss:0.09106478095054626\n",
      "Epoch 89, Batch 211 Loss:0.04995295777916908\n",
      "Epoch 89, Batch 212 Loss:0.02399817854166031\n",
      "Epoch 89, Batch 213 Loss:0.04813048243522644\n",
      "Epoch 89, Batch 214 Loss:0.01938501000404358\n",
      "Epoch 89, Batch 215 Loss:0.03209938853979111\n",
      "Epoch 89, Batch 216 Loss:0.037256546318531036\n",
      "Epoch 89, Batch 217 Loss:0.007768303155899048\n",
      "Epoch 89, Batch 218 Loss:0.039088450372219086\n",
      "Epoch 89, Batch 219 Loss:0.014777720905840397\n",
      "Epoch 89, Batch 220 Loss:0.020342575386166573\n",
      "Epoch 89, Batch 221 Loss:0.03847114369273186\n",
      "Epoch 89, Batch 222 Loss:0.022878950461745262\n",
      "Epoch 89, Batch 223 Loss:0.02872524969279766\n",
      "Epoch 89, Batch 224 Loss:0.054545801132917404\n",
      "Epoch 89, Batch 225 Loss:0.02788465842604637\n",
      "Epoch 89, Batch 226 Loss:0.028310786932706833\n",
      "Epoch 89, Batch 227 Loss:0.0684344545006752\n",
      "Epoch 89, Batch 228 Loss:0.049511220306158066\n",
      "Epoch 89, Batch 229 Loss:0.031251996755599976\n",
      "Epoch 89, Batch 230 Loss:0.01271192915737629\n",
      "Epoch 89, Batch 231 Loss:0.00942876935005188\n",
      "Epoch 89, Batch 232 Loss:0.02522851713001728\n",
      "Epoch 89, Batch 233 Loss:0.03658979386091232\n",
      "Loss in this Epoch is: 3.65897938609 %\n",
      "Accuracy in this Epoch is: 88.2499992847 %\n",
      "Epoch 90, Batch 0 Loss:0.06214635819196701\n",
      "Epoch 90, Batch 1 Loss:0.010597490705549717\n",
      "Epoch 90, Batch 2 Loss:0.05417785793542862\n",
      "Epoch 90, Batch 3 Loss:0.06954791396856308\n",
      "Epoch 90, Batch 4 Loss:0.008677682839334011\n",
      "Epoch 90, Batch 5 Loss:0.00720042921602726\n",
      "Epoch 90, Batch 6 Loss:0.007102165371179581\n",
      "Epoch 90, Batch 7 Loss:0.018666643649339676\n",
      "Epoch 90, Batch 8 Loss:0.015147468075156212\n",
      "Epoch 90, Batch 9 Loss:0.027434013783931732\n",
      "Epoch 90, Batch 10 Loss:0.022545646876096725\n",
      "Epoch 90, Batch 11 Loss:0.0064590442925691605\n",
      "Epoch 90, Batch 12 Loss:0.02929409220814705\n",
      "Epoch 90, Batch 13 Loss:0.0395507737994194\n",
      "Epoch 90, Batch 14 Loss:0.008155611343681812\n",
      "Epoch 90, Batch 15 Loss:0.06675352156162262\n",
      "Epoch 90, Batch 16 Loss:0.008050823584198952\n",
      "Epoch 90, Batch 17 Loss:0.022549958899617195\n",
      "Epoch 90, Batch 18 Loss:0.004831579048186541\n",
      "Epoch 90, Batch 19 Loss:0.040347348898649216\n",
      "Epoch 90, Batch 20 Loss:0.004827831871807575\n",
      "Epoch 90, Batch 21 Loss:0.013755721971392632\n",
      "Epoch 90, Batch 22 Loss:0.007648829836398363\n",
      "Epoch 90, Batch 23 Loss:0.011434502899646759\n",
      "Epoch 90, Batch 24 Loss:0.022765344008803368\n",
      "Epoch 90, Batch 25 Loss:0.039426881819963455\n",
      "Epoch 90, Batch 26 Loss:0.0037677150685340166\n",
      "Epoch 90, Batch 27 Loss:0.015023400075733662\n",
      "Epoch 90, Batch 28 Loss:0.006377308629453182\n",
      "Epoch 90, Batch 29 Loss:0.005556075368076563\n",
      "Epoch 90, Batch 30 Loss:0.00556871946901083\n",
      "Epoch 90, Batch 31 Loss:0.024121876806020737\n",
      "Epoch 90, Batch 32 Loss:0.004592015407979488\n",
      "Epoch 90, Batch 33 Loss:0.009619415737688541\n",
      "Epoch 90, Batch 34 Loss:0.02393101342022419\n",
      "Epoch 90, Batch 35 Loss:0.009153570048511028\n",
      "Epoch 90, Batch 36 Loss:0.049218274652957916\n",
      "Epoch 90, Batch 37 Loss:0.006314046215265989\n",
      "Epoch 90, Batch 38 Loss:0.013168650679290295\n",
      "Epoch 90, Batch 39 Loss:0.008412130177021027\n",
      "Epoch 90, Batch 40 Loss:0.0036493847146630287\n",
      "Epoch 90, Batch 41 Loss:0.005521727725863457\n",
      "Epoch 90, Batch 42 Loss:0.0030317804776132107\n",
      "Epoch 90, Batch 43 Loss:0.018699677661061287\n",
      "Epoch 90, Batch 44 Loss:0.005013828631490469\n",
      "Epoch 90, Batch 45 Loss:0.00424695760011673\n",
      "Epoch 90, Batch 46 Loss:0.0025676004588603973\n",
      "Epoch 90, Batch 47 Loss:0.005107066128402948\n",
      "Epoch 90, Batch 48 Loss:0.014182181097567081\n",
      "Epoch 90, Batch 49 Loss:0.01207499299198389\n",
      "Epoch 90, Batch 50 Loss:0.04145143926143646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 90, Batch 51 Loss:0.010947584174573421\n",
      "Epoch 90, Batch 52 Loss:0.014626464806497097\n",
      "Epoch 90, Batch 53 Loss:0.005355158820748329\n",
      "Epoch 90, Batch 54 Loss:0.043411724269390106\n",
      "Epoch 90, Batch 55 Loss:0.003576874267309904\n",
      "Epoch 90, Batch 56 Loss:0.011321409605443478\n",
      "Epoch 90, Batch 57 Loss:0.030646903440356255\n",
      "Epoch 90, Batch 58 Loss:0.018818406388163567\n",
      "Epoch 90, Batch 59 Loss:0.045809268951416016\n",
      "Epoch 90, Batch 60 Loss:0.0034302379935979843\n",
      "Epoch 90, Batch 61 Loss:0.011529252864420414\n",
      "Epoch 90, Batch 62 Loss:0.025493936613202095\n",
      "Epoch 90, Batch 63 Loss:0.004756411537528038\n",
      "Epoch 90, Batch 64 Loss:0.020386705175042152\n",
      "Epoch 90, Batch 65 Loss:0.004656973760575056\n",
      "Epoch 90, Batch 66 Loss:0.006242855452001095\n",
      "Epoch 90, Batch 67 Loss:0.01244635134935379\n",
      "Epoch 90, Batch 68 Loss:0.019996264949440956\n",
      "Epoch 90, Batch 69 Loss:0.0010711002396419644\n",
      "Epoch 90, Batch 70 Loss:0.005687021184712648\n",
      "Epoch 90, Batch 71 Loss:0.0034882333129644394\n",
      "Epoch 90, Batch 72 Loss:0.002031276235356927\n",
      "Epoch 90, Batch 73 Loss:0.014642440713942051\n",
      "Epoch 90, Batch 74 Loss:0.008440203964710236\n",
      "Epoch 90, Batch 75 Loss:0.0052818553522229195\n",
      "Epoch 90, Batch 76 Loss:0.003418175270780921\n",
      "Epoch 90, Batch 77 Loss:0.02583666890859604\n",
      "Epoch 90, Batch 78 Loss:0.010329480282962322\n",
      "Epoch 90, Batch 79 Loss:0.003207239555194974\n",
      "Epoch 90, Batch 80 Loss:0.009442494250833988\n",
      "Epoch 90, Batch 81 Loss:0.005708689801394939\n",
      "Epoch 90, Batch 82 Loss:0.008983828127384186\n",
      "Epoch 90, Batch 83 Loss:0.00760746980085969\n",
      "Epoch 90, Batch 84 Loss:0.007310184650123119\n",
      "Epoch 90, Batch 85 Loss:0.03900286182761192\n",
      "Epoch 90, Batch 86 Loss:0.007537451572716236\n",
      "Epoch 90, Batch 87 Loss:0.003760896623134613\n",
      "Epoch 90, Batch 88 Loss:0.027808785438537598\n",
      "Epoch 90, Batch 89 Loss:0.026287727057933807\n",
      "Epoch 90, Batch 90 Loss:0.025353161618113518\n",
      "Epoch 90, Batch 91 Loss:0.05461279675364494\n",
      "Epoch 90, Batch 92 Loss:0.006215682718902826\n",
      "Epoch 90, Batch 93 Loss:0.005735862068831921\n",
      "Epoch 90, Batch 94 Loss:0.028592735528945923\n",
      "Epoch 90, Batch 95 Loss:0.04460897669196129\n",
      "Epoch 90, Batch 96 Loss:0.004837460350245237\n",
      "Epoch 90, Batch 97 Loss:0.022768551483750343\n",
      "Epoch 90, Batch 98 Loss:0.0058352029882371426\n",
      "Epoch 90, Batch 99 Loss:0.04323294758796692\n",
      "Epoch 90, Batch 100 Loss:0.003353276289999485\n",
      "Epoch 90, Batch 101 Loss:0.005163170397281647\n",
      "Epoch 90, Batch 102 Loss:0.017096493393182755\n",
      "Epoch 90, Batch 103 Loss:0.00876002199947834\n",
      "Epoch 90, Batch 104 Loss:0.05416177585721016\n",
      "Epoch 90, Batch 105 Loss:0.015814967453479767\n",
      "Epoch 90, Batch 106 Loss:0.008382870815694332\n",
      "Epoch 90, Batch 107 Loss:0.0169280506670475\n",
      "Epoch 90, Batch 108 Loss:0.031059712171554565\n",
      "Epoch 90, Batch 109 Loss:0.0304388627409935\n",
      "Epoch 90, Batch 110 Loss:0.005700631067156792\n",
      "Epoch 90, Batch 111 Loss:0.01009119488298893\n",
      "Epoch 90, Batch 112 Loss:0.004654671065509319\n",
      "Epoch 90, Batch 113 Loss:0.04893132299184799\n",
      "Epoch 90, Batch 114 Loss:0.011153893545269966\n",
      "Epoch 90, Batch 115 Loss:0.004493612330406904\n",
      "Epoch 90, Batch 116 Loss:0.006061520893126726\n",
      "Epoch 90, Batch 117 Loss:0.004736870992928743\n",
      "Epoch 90, Batch 118 Loss:0.018009904772043228\n",
      "Epoch 90, Batch 119 Loss:0.003266585059463978\n",
      "Epoch 90, Batch 120 Loss:0.021018877625465393\n",
      "Epoch 90, Batch 121 Loss:0.021924061700701714\n",
      "Epoch 90, Batch 122 Loss:0.02206571400165558\n",
      "Epoch 90, Batch 123 Loss:0.012643938884139061\n",
      "Epoch 90, Batch 124 Loss:0.005646213889122009\n",
      "Epoch 90, Batch 125 Loss:0.01344822533428669\n",
      "Epoch 90, Batch 126 Loss:0.002626169240102172\n",
      "Epoch 90, Batch 127 Loss:0.007366192992776632\n",
      "Epoch 90, Batch 128 Loss:0.006179356947541237\n",
      "Epoch 90, Batch 129 Loss:0.02289770357310772\n",
      "Epoch 90, Batch 130 Loss:0.0069417147897183895\n",
      "Epoch 90, Batch 131 Loss:0.011068303138017654\n",
      "Epoch 90, Batch 132 Loss:0.0065998598001897335\n",
      "Epoch 90, Batch 133 Loss:0.015057876706123352\n",
      "Epoch 90, Batch 134 Loss:0.016805989667773247\n",
      "Epoch 90, Batch 135 Loss:0.013436776585876942\n",
      "Epoch 90, Batch 136 Loss:0.00699025671929121\n",
      "Epoch 90, Batch 137 Loss:0.036042485386133194\n",
      "Epoch 90, Batch 138 Loss:0.008760273456573486\n",
      "Epoch 90, Batch 139 Loss:0.014017258770763874\n",
      "Epoch 90, Batch 140 Loss:0.027790814638137817\n",
      "Epoch 90, Batch 141 Loss:0.0016209010500460863\n",
      "Epoch 90, Batch 142 Loss:0.031243037432432175\n",
      "Epoch 90, Batch 143 Loss:0.012208656407892704\n",
      "Epoch 90, Batch 144 Loss:0.028376586735248566\n",
      "Epoch 90, Batch 145 Loss:0.006266367621719837\n",
      "Epoch 90, Batch 146 Loss:0.012002785690128803\n",
      "Epoch 90, Batch 147 Loss:0.019712217152118683\n",
      "Epoch 90, Batch 148 Loss:0.005443958565592766\n",
      "Epoch 90, Batch 149 Loss:0.010835735127329826\n",
      "Epoch 90, Batch 150 Loss:0.01982744410634041\n",
      "Epoch 90, Batch 151 Loss:0.004403317347168922\n",
      "Epoch 90, Batch 152 Loss:0.04667455703020096\n",
      "Epoch 90, Batch 153 Loss:0.01070844754576683\n",
      "Epoch 90, Batch 154 Loss:0.03077319823205471\n",
      "Epoch 90, Batch 155 Loss:0.033778347074985504\n",
      "Epoch 90, Batch 156 Loss:0.013131408952176571\n",
      "Epoch 90, Batch 157 Loss:0.004663368687033653\n",
      "Epoch 90, Batch 158 Loss:0.02155168727040291\n",
      "Epoch 90, Batch 159 Loss:0.017238248139619827\n",
      "Epoch 90, Batch 160 Loss:0.016692960634827614\n",
      "Epoch 90, Batch 161 Loss:0.02012747898697853\n",
      "Epoch 90, Batch 162 Loss:0.033145491033792496\n",
      "Epoch 90, Batch 163 Loss:0.017307763919234276\n",
      "Epoch 90, Batch 164 Loss:0.022962674498558044\n",
      "Epoch 90, Batch 165 Loss:0.020164132118225098\n",
      "Epoch 90, Batch 166 Loss:0.01826080121099949\n",
      "Epoch 90, Batch 167 Loss:0.025750625878572464\n",
      "Epoch 90, Batch 168 Loss:0.02725214511156082\n",
      "Epoch 90, Batch 169 Loss:0.020436078310012817\n",
      "Epoch 90, Batch 170 Loss:0.01982843689620495\n",
      "Epoch 90, Batch 171 Loss:0.0414280891418457\n",
      "Epoch 90, Batch 172 Loss:0.0059870770201087\n",
      "Epoch 90, Batch 173 Loss:0.019032791256904602\n",
      "Epoch 90, Batch 174 Loss:0.009759150445461273\n",
      "Epoch 90, Batch 175 Loss:0.02116389200091362\n",
      "Epoch 90, Batch 176 Loss:0.02992762252688408\n",
      "Epoch 90, Batch 177 Loss:0.010446138679981232\n",
      "Epoch 90, Batch 178 Loss:0.016709119081497192\n",
      "Epoch 90, Batch 179 Loss:0.009147265926003456\n",
      "Epoch 90, Batch 180 Loss:0.0059188902378082275\n",
      "Epoch 90, Batch 181 Loss:0.06301064789295197\n",
      "Epoch 90, Batch 182 Loss:0.011414636857807636\n",
      "Epoch 90, Batch 183 Loss:0.011917117983102798\n",
      "Epoch 90, Batch 184 Loss:0.009603293612599373\n",
      "Epoch 90, Batch 185 Loss:0.016101084649562836\n",
      "Epoch 90, Batch 186 Loss:0.019609667360782623\n",
      "Epoch 90, Batch 187 Loss:0.009684773162007332\n",
      "Epoch 90, Batch 188 Loss:0.06112642586231232\n",
      "Epoch 90, Batch 189 Loss:0.023086920380592346\n",
      "Epoch 90, Batch 190 Loss:0.01165999099612236\n",
      "Epoch 90, Batch 191 Loss:0.029139284044504166\n",
      "Epoch 90, Batch 192 Loss:0.03993174806237221\n",
      "Epoch 90, Batch 193 Loss:0.022968512028455734\n",
      "Epoch 90, Batch 194 Loss:0.007877558469772339\n",
      "Epoch 90, Batch 195 Loss:0.01651545986533165\n",
      "Epoch 90, Batch 196 Loss:0.01919686794281006\n",
      "Epoch 90, Batch 197 Loss:0.01030876487493515\n",
      "Epoch 90, Batch 198 Loss:0.011225583031773567\n",
      "Epoch 90, Batch 199 Loss:0.03141309320926666\n",
      "Epoch 90, Batch 200 Loss:0.02082977257668972\n",
      "Epoch 90, Batch 201 Loss:0.034089066088199615\n",
      "Epoch 90, Batch 202 Loss:0.08880499750375748\n",
      "Epoch 90, Batch 203 Loss:0.007250198163092136\n",
      "Epoch 90, Batch 204 Loss:0.008573652245104313\n",
      "Epoch 90, Batch 205 Loss:0.0045868405140936375\n",
      "Epoch 90, Batch 206 Loss:0.007428902201354504\n",
      "Epoch 90, Batch 207 Loss:0.005529182031750679\n",
      "Epoch 90, Batch 208 Loss:0.020371325314044952\n",
      "Epoch 90, Batch 209 Loss:0.018194464966654778\n",
      "Epoch 90, Batch 210 Loss:0.029352262616157532\n",
      "Epoch 90, Batch 211 Loss:0.013160377740859985\n",
      "Epoch 90, Batch 212 Loss:0.01020133774727583\n",
      "Epoch 90, Batch 213 Loss:0.008957580663263798\n",
      "Epoch 90, Batch 214 Loss:0.008444197475910187\n",
      "Epoch 90, Batch 215 Loss:0.012972644530236721\n",
      "Epoch 90, Batch 216 Loss:0.01221549790352583\n",
      "Epoch 90, Batch 217 Loss:0.02144003100693226\n",
      "Epoch 90, Batch 218 Loss:0.01818990707397461\n",
      "Epoch 90, Batch 219 Loss:0.04946831241250038\n",
      "Epoch 90, Batch 220 Loss:0.01855682022869587\n",
      "Epoch 90, Batch 221 Loss:0.023039087653160095\n",
      "Epoch 90, Batch 222 Loss:0.019526388496160507\n",
      "Epoch 90, Batch 223 Loss:0.02509012445807457\n",
      "Epoch 90, Batch 224 Loss:0.02336404100060463\n",
      "Epoch 90, Batch 225 Loss:0.13381893932819366\n",
      "Epoch 90, Batch 226 Loss:0.03595781698822975\n",
      "Epoch 90, Batch 227 Loss:0.0222170390188694\n",
      "Epoch 90, Batch 228 Loss:0.0064304606057703495\n",
      "Epoch 90, Batch 229 Loss:0.043327100574970245\n",
      "Epoch 90, Batch 230 Loss:0.049737051129341125\n",
      "Epoch 90, Batch 231 Loss:0.0117728840559721\n",
      "Epoch 90, Batch 232 Loss:0.030422210693359375\n",
      "Epoch 90, Batch 233 Loss:0.03506479784846306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in this Epoch is: 3.50647978485 %\n",
      "Accuracy in this Epoch is: 87.9700005054 %\n",
      "Epoch 91, Batch 0 Loss:0.04153494909405708\n",
      "Epoch 91, Batch 1 Loss:0.03884745016694069\n",
      "Epoch 91, Batch 2 Loss:0.04541248455643654\n",
      "Epoch 91, Batch 3 Loss:0.029555601999163628\n",
      "Epoch 91, Batch 4 Loss:0.04829312488436699\n",
      "Epoch 91, Batch 5 Loss:0.020851371809840202\n",
      "Epoch 91, Batch 6 Loss:0.020199477672576904\n",
      "Epoch 91, Batch 7 Loss:0.03264816477894783\n",
      "Epoch 91, Batch 8 Loss:0.02828112617135048\n",
      "Epoch 91, Batch 9 Loss:0.02195722609758377\n",
      "Epoch 91, Batch 10 Loss:0.09564698487520218\n",
      "Epoch 91, Batch 11 Loss:0.04263805225491524\n",
      "Epoch 91, Batch 12 Loss:0.04517638683319092\n",
      "Epoch 91, Batch 13 Loss:0.03393871709704399\n",
      "Epoch 91, Batch 14 Loss:0.08079854398965836\n",
      "Epoch 91, Batch 15 Loss:0.028008362278342247\n",
      "Epoch 91, Batch 16 Loss:0.009215123951435089\n",
      "Epoch 91, Batch 17 Loss:0.03764142096042633\n",
      "Epoch 91, Batch 18 Loss:0.03847171738743782\n",
      "Epoch 91, Batch 19 Loss:0.042639076709747314\n",
      "Epoch 91, Batch 20 Loss:0.024163205176591873\n",
      "Epoch 91, Batch 21 Loss:0.01560244057327509\n",
      "Epoch 91, Batch 22 Loss:0.0162019282579422\n",
      "Epoch 91, Batch 23 Loss:0.04586372151970863\n",
      "Epoch 91, Batch 24 Loss:0.03174556791782379\n",
      "Epoch 91, Batch 25 Loss:0.007435816340148449\n",
      "Epoch 91, Batch 26 Loss:0.04311284422874451\n",
      "Epoch 91, Batch 27 Loss:0.038319021463394165\n",
      "Epoch 91, Batch 28 Loss:0.01821117289364338\n",
      "Epoch 91, Batch 29 Loss:0.06139150261878967\n",
      "Epoch 91, Batch 30 Loss:0.019520364701747894\n",
      "Epoch 91, Batch 31 Loss:0.012757633812725544\n",
      "Epoch 91, Batch 32 Loss:0.024280773475766182\n",
      "Epoch 91, Batch 33 Loss:0.013703213073313236\n",
      "Epoch 91, Batch 34 Loss:0.01736781932413578\n",
      "Epoch 91, Batch 35 Loss:0.010931499302387238\n",
      "Epoch 91, Batch 36 Loss:0.009528241120278835\n",
      "Epoch 91, Batch 37 Loss:0.042347561568021774\n",
      "Epoch 91, Batch 38 Loss:0.01941405050456524\n",
      "Epoch 91, Batch 39 Loss:0.026590557768940926\n",
      "Epoch 91, Batch 40 Loss:0.011022193357348442\n",
      "Epoch 91, Batch 41 Loss:0.07594062387943268\n",
      "Epoch 91, Batch 42 Loss:0.058958958834409714\n",
      "Epoch 91, Batch 43 Loss:0.027729421854019165\n",
      "Epoch 91, Batch 44 Loss:0.02532939240336418\n",
      "Epoch 91, Batch 45 Loss:0.016177305951714516\n",
      "Epoch 91, Batch 46 Loss:0.03841721639037132\n",
      "Epoch 91, Batch 47 Loss:0.03981427848339081\n",
      "Epoch 91, Batch 48 Loss:0.028280677273869514\n",
      "Epoch 91, Batch 49 Loss:0.02682666853070259\n",
      "Epoch 91, Batch 50 Loss:0.01723010651767254\n",
      "Epoch 91, Batch 51 Loss:0.007308771833777428\n",
      "Epoch 91, Batch 52 Loss:0.06098194420337677\n",
      "Epoch 91, Batch 53 Loss:0.054475776851177216\n",
      "Epoch 91, Batch 54 Loss:0.003474135883152485\n",
      "Epoch 91, Batch 55 Loss:0.024925967678427696\n",
      "Epoch 91, Batch 56 Loss:0.010498682968318462\n",
      "Epoch 91, Batch 57 Loss:0.015337062068283558\n",
      "Epoch 91, Batch 58 Loss:0.006359592545777559\n",
      "Epoch 91, Batch 59 Loss:0.011203647591173649\n",
      "Epoch 91, Batch 60 Loss:0.007850300520658493\n",
      "Epoch 91, Batch 61 Loss:0.031315628439188004\n",
      "Epoch 91, Batch 62 Loss:0.045332323759794235\n",
      "Epoch 91, Batch 63 Loss:0.009474659338593483\n",
      "Epoch 91, Batch 64 Loss:0.02519252710044384\n",
      "Epoch 91, Batch 65 Loss:0.029209505766630173\n",
      "Epoch 91, Batch 66 Loss:0.017946463078260422\n",
      "Epoch 91, Batch 67 Loss:0.012315633706748486\n",
      "Epoch 91, Batch 68 Loss:0.061700157821178436\n",
      "Epoch 91, Batch 69 Loss:0.002857454586774111\n",
      "Epoch 91, Batch 70 Loss:0.0806865245103836\n",
      "Epoch 91, Batch 71 Loss:0.06511998176574707\n",
      "Epoch 91, Batch 72 Loss:0.041186101734638214\n",
      "Epoch 91, Batch 73 Loss:0.0347103588283062\n",
      "Epoch 91, Batch 74 Loss:0.024233657866716385\n",
      "Epoch 91, Batch 75 Loss:0.06295347213745117\n",
      "Epoch 91, Batch 76 Loss:0.014155798591673374\n",
      "Epoch 91, Batch 77 Loss:0.03168563172221184\n",
      "Epoch 91, Batch 78 Loss:0.014756851829588413\n",
      "Epoch 91, Batch 79 Loss:0.02050713077187538\n",
      "Epoch 91, Batch 80 Loss:0.07418147474527359\n",
      "Epoch 91, Batch 81 Loss:0.019979188218712807\n",
      "Epoch 91, Batch 82 Loss:0.02347889170050621\n",
      "Epoch 91, Batch 83 Loss:0.0366278737783432\n",
      "Epoch 91, Batch 84 Loss:0.039902638643980026\n",
      "Epoch 91, Batch 85 Loss:0.04975387081503868\n",
      "Epoch 91, Batch 86 Loss:0.041727788746356964\n",
      "Epoch 91, Batch 87 Loss:0.025704268366098404\n",
      "Epoch 91, Batch 88 Loss:0.0482180081307888\n",
      "Epoch 91, Batch 89 Loss:0.06280242651700974\n",
      "Epoch 91, Batch 90 Loss:0.06488896906375885\n",
      "Epoch 91, Batch 91 Loss:0.030654480680823326\n",
      "Epoch 91, Batch 92 Loss:0.05324356630444527\n",
      "Epoch 91, Batch 93 Loss:0.08086352795362473\n",
      "Epoch 91, Batch 94 Loss:0.03571499139070511\n",
      "Epoch 91, Batch 95 Loss:0.06038222461938858\n",
      "Epoch 91, Batch 96 Loss:0.12414279580116272\n",
      "Epoch 91, Batch 97 Loss:0.07186529040336609\n",
      "Epoch 91, Batch 98 Loss:0.1001296415925026\n",
      "Epoch 91, Batch 99 Loss:0.050110604614019394\n",
      "Epoch 91, Batch 100 Loss:0.032189711928367615\n",
      "Epoch 91, Batch 101 Loss:0.028595395386219025\n",
      "Epoch 91, Batch 102 Loss:0.08828356862068176\n",
      "Epoch 91, Batch 103 Loss:0.05649564787745476\n",
      "Epoch 91, Batch 104 Loss:0.0245803389698267\n",
      "Epoch 91, Batch 105 Loss:0.024372167885303497\n",
      "Epoch 91, Batch 106 Loss:0.02292482927441597\n",
      "Epoch 91, Batch 107 Loss:0.05545853078365326\n",
      "Epoch 91, Batch 108 Loss:0.03039862960577011\n",
      "Epoch 91, Batch 109 Loss:0.008767593652009964\n",
      "Epoch 91, Batch 110 Loss:0.0633373036980629\n",
      "Epoch 91, Batch 111 Loss:0.059576407074928284\n",
      "Epoch 91, Batch 112 Loss:0.05106556788086891\n",
      "Epoch 91, Batch 113 Loss:0.08619874715805054\n",
      "Epoch 91, Batch 114 Loss:0.08248636871576309\n",
      "Epoch 91, Batch 115 Loss:0.05452723428606987\n",
      "Epoch 91, Batch 116 Loss:0.040400974452495575\n",
      "Epoch 91, Batch 117 Loss:0.062231361865997314\n",
      "Epoch 91, Batch 118 Loss:0.052538055926561356\n",
      "Epoch 91, Batch 119 Loss:0.06785116344690323\n",
      "Epoch 91, Batch 120 Loss:0.08539079129695892\n",
      "Epoch 91, Batch 121 Loss:0.09271174669265747\n",
      "Epoch 91, Batch 122 Loss:0.11997215449810028\n",
      "Epoch 91, Batch 123 Loss:0.10928511619567871\n",
      "Epoch 91, Batch 124 Loss:0.06832075119018555\n",
      "Epoch 91, Batch 125 Loss:0.12732096016407013\n",
      "Epoch 91, Batch 126 Loss:0.06572695076465607\n",
      "Epoch 91, Batch 127 Loss:0.06468897312879562\n",
      "Epoch 91, Batch 128 Loss:0.05458464100956917\n",
      "Epoch 91, Batch 129 Loss:0.12352587282657623\n",
      "Epoch 91, Batch 130 Loss:0.09779570996761322\n",
      "Epoch 91, Batch 131 Loss:0.09284552931785583\n",
      "Epoch 91, Batch 132 Loss:0.10316737741231918\n",
      "Epoch 91, Batch 133 Loss:0.06837689876556396\n",
      "Epoch 91, Batch 134 Loss:0.10785433650016785\n",
      "Epoch 91, Batch 135 Loss:0.0809597373008728\n",
      "Epoch 91, Batch 136 Loss:0.08125856518745422\n",
      "Epoch 91, Batch 137 Loss:0.1385977864265442\n",
      "Epoch 91, Batch 138 Loss:0.08124566823244095\n",
      "Epoch 91, Batch 139 Loss:0.0895528644323349\n",
      "Epoch 91, Batch 140 Loss:0.06571628153324127\n",
      "Epoch 91, Batch 141 Loss:0.04448587819933891\n",
      "Epoch 91, Batch 142 Loss:0.07732921838760376\n",
      "Epoch 91, Batch 143 Loss:0.10206372290849686\n",
      "Epoch 91, Batch 144 Loss:0.0567011758685112\n",
      "Epoch 91, Batch 145 Loss:0.06115613877773285\n",
      "Epoch 91, Batch 146 Loss:0.034559816122055054\n",
      "Epoch 91, Batch 147 Loss:0.0654381811618805\n",
      "Epoch 91, Batch 148 Loss:0.030482126399874687\n",
      "Epoch 91, Batch 149 Loss:0.06189066171646118\n",
      "Epoch 91, Batch 150 Loss:0.08871763944625854\n",
      "Epoch 91, Batch 151 Loss:0.018784357234835625\n",
      "Epoch 91, Batch 152 Loss:0.02334684319794178\n",
      "Epoch 91, Batch 153 Loss:0.024609358981251717\n",
      "Epoch 91, Batch 154 Loss:0.03009437397122383\n",
      "Epoch 91, Batch 155 Loss:0.007117862347513437\n",
      "Epoch 91, Batch 156 Loss:0.04709737002849579\n",
      "Epoch 91, Batch 157 Loss:0.03283822536468506\n",
      "Epoch 91, Batch 158 Loss:0.02024717628955841\n",
      "Epoch 91, Batch 159 Loss:0.02471305802464485\n",
      "Epoch 91, Batch 160 Loss:0.023938974365592003\n",
      "Epoch 91, Batch 161 Loss:0.014656677842140198\n",
      "Epoch 91, Batch 162 Loss:0.013157054781913757\n",
      "Epoch 91, Batch 163 Loss:0.026264507323503494\n",
      "Epoch 91, Batch 164 Loss:0.033715564757585526\n",
      "Epoch 91, Batch 165 Loss:0.018180804327130318\n",
      "Epoch 91, Batch 166 Loss:0.01235058344900608\n",
      "Epoch 91, Batch 167 Loss:0.019008994102478027\n",
      "Epoch 91, Batch 168 Loss:0.040186624974012375\n",
      "Epoch 91, Batch 169 Loss:0.06967952847480774\n",
      "Epoch 91, Batch 170 Loss:0.02501615136861801\n",
      "Epoch 91, Batch 171 Loss:0.019038191065192223\n",
      "Epoch 91, Batch 172 Loss:0.06205061823129654\n",
      "Epoch 91, Batch 173 Loss:0.01937238872051239\n",
      "Epoch 91, Batch 174 Loss:0.017250657081604004\n",
      "Epoch 91, Batch 175 Loss:0.038837824016809464\n",
      "Epoch 91, Batch 176 Loss:0.03282877430319786\n",
      "Epoch 91, Batch 177 Loss:0.0059164841659367085\n",
      "Epoch 91, Batch 178 Loss:0.01508515514433384\n",
      "Epoch 91, Batch 179 Loss:0.01816207729279995\n",
      "Epoch 91, Batch 180 Loss:0.022554783150553703\n",
      "Epoch 91, Batch 181 Loss:0.0358007438480854\n",
      "Epoch 91, Batch 182 Loss:0.023414436727762222\n",
      "Epoch 91, Batch 183 Loss:0.010020227171480656\n",
      "Epoch 91, Batch 184 Loss:0.009879834949970245\n",
      "Epoch 91, Batch 185 Loss:0.006811183411628008\n",
      "Epoch 91, Batch 186 Loss:0.03569503501057625\n",
      "Epoch 91, Batch 187 Loss:0.01633766107261181\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91, Batch 188 Loss:0.013379249721765518\n",
      "Epoch 91, Batch 189 Loss:0.021542806178331375\n",
      "Epoch 91, Batch 190 Loss:0.012016626074910164\n",
      "Epoch 91, Batch 191 Loss:0.043050721287727356\n",
      "Epoch 91, Batch 192 Loss:0.02077011577785015\n",
      "Epoch 91, Batch 193 Loss:0.032861486077308655\n",
      "Epoch 91, Batch 194 Loss:0.009900743141770363\n",
      "Epoch 91, Batch 195 Loss:0.045041270554065704\n",
      "Epoch 91, Batch 196 Loss:0.03398994356393814\n",
      "Epoch 91, Batch 197 Loss:0.031178437173366547\n",
      "Epoch 91, Batch 198 Loss:0.026064343750476837\n",
      "Epoch 91, Batch 199 Loss:0.03987529128789902\n",
      "Epoch 91, Batch 200 Loss:0.027276646345853806\n",
      "Epoch 91, Batch 201 Loss:0.033245861530303955\n",
      "Epoch 91, Batch 202 Loss:0.046650346368551254\n",
      "Epoch 91, Batch 203 Loss:0.00904940627515316\n",
      "Epoch 91, Batch 204 Loss:0.04474084824323654\n",
      "Epoch 91, Batch 205 Loss:0.0247645303606987\n",
      "Epoch 91, Batch 206 Loss:0.029202094301581383\n",
      "Epoch 91, Batch 207 Loss:0.01105584017932415\n",
      "Epoch 91, Batch 208 Loss:0.04682891443371773\n",
      "Epoch 91, Batch 209 Loss:0.023220419883728027\n",
      "Epoch 91, Batch 210 Loss:0.03848206251859665\n",
      "Epoch 91, Batch 211 Loss:0.03315690904855728\n",
      "Epoch 91, Batch 212 Loss:0.0341341570019722\n",
      "Epoch 91, Batch 213 Loss:0.008069436997175217\n",
      "Epoch 91, Batch 214 Loss:0.010178761556744576\n",
      "Epoch 91, Batch 215 Loss:0.0341876819729805\n",
      "Epoch 91, Batch 216 Loss:0.009515514597296715\n",
      "Epoch 91, Batch 217 Loss:0.008624756708741188\n",
      "Epoch 91, Batch 218 Loss:0.07528099417686462\n",
      "Epoch 91, Batch 219 Loss:0.0576777309179306\n",
      "Epoch 91, Batch 220 Loss:0.047332994639873505\n",
      "Epoch 91, Batch 221 Loss:0.03281928226351738\n",
      "Epoch 91, Batch 222 Loss:0.005930842831730843\n",
      "Epoch 91, Batch 223 Loss:0.027355113998055458\n",
      "Epoch 91, Batch 224 Loss:0.04509098082780838\n",
      "Epoch 91, Batch 225 Loss:0.014481326565146446\n",
      "Epoch 91, Batch 226 Loss:0.08866065740585327\n",
      "Epoch 91, Batch 227 Loss:0.02948017790913582\n",
      "Epoch 91, Batch 228 Loss:0.011467763222754002\n",
      "Epoch 91, Batch 229 Loss:0.04597782343626022\n",
      "Epoch 91, Batch 230 Loss:0.03382540121674538\n",
      "Epoch 91, Batch 231 Loss:0.05594987794756889\n",
      "Epoch 91, Batch 232 Loss:0.14267495274543762\n",
      "Epoch 91, Batch 233 Loss:0.016141604632139206\n",
      "Loss in this Epoch is: 1.61416046321 %\n",
      "Accuracy in this Epoch is: 88.0299985409 %\n",
      "Epoch 92, Batch 0 Loss:0.019143832847476006\n",
      "Epoch 92, Batch 1 Loss:0.010713295079767704\n",
      "Epoch 92, Batch 2 Loss:0.00977036077529192\n",
      "Epoch 92, Batch 3 Loss:0.0097882729023695\n",
      "Epoch 92, Batch 4 Loss:0.019061867147684097\n",
      "Epoch 92, Batch 5 Loss:0.015576970763504505\n",
      "Epoch 92, Batch 6 Loss:0.04857281967997551\n",
      "Epoch 92, Batch 7 Loss:0.011937183327972889\n",
      "Epoch 92, Batch 8 Loss:0.006574322935193777\n",
      "Epoch 92, Batch 9 Loss:0.006370908580720425\n",
      "Epoch 92, Batch 10 Loss:0.011245612055063248\n",
      "Epoch 92, Batch 11 Loss:0.019735202193260193\n",
      "Epoch 92, Batch 12 Loss:0.009567838162183762\n",
      "Epoch 92, Batch 13 Loss:0.0250295028090477\n",
      "Epoch 92, Batch 14 Loss:0.03075931780040264\n",
      "Epoch 92, Batch 15 Loss:0.01779468171298504\n",
      "Epoch 92, Batch 16 Loss:0.03187799081206322\n",
      "Epoch 92, Batch 17 Loss:0.016708362847566605\n",
      "Epoch 92, Batch 18 Loss:0.008116859011352062\n",
      "Epoch 92, Batch 19 Loss:0.005440636072307825\n",
      "Epoch 92, Batch 20 Loss:0.01772608421742916\n",
      "Epoch 92, Batch 21 Loss:0.003557017305865884\n",
      "Epoch 92, Batch 22 Loss:0.021137110888957977\n",
      "Epoch 92, Batch 23 Loss:0.006443055346608162\n",
      "Epoch 92, Batch 24 Loss:0.031459517776966095\n",
      "Epoch 92, Batch 25 Loss:0.011816385202109814\n",
      "Epoch 92, Batch 26 Loss:0.009804844856262207\n",
      "Epoch 92, Batch 27 Loss:0.02148817852139473\n",
      "Epoch 92, Batch 28 Loss:0.023790862411260605\n",
      "Epoch 92, Batch 29 Loss:0.010399472899734974\n",
      "Epoch 92, Batch 30 Loss:0.007243064697831869\n",
      "Epoch 92, Batch 31 Loss:0.009694055654108524\n",
      "Epoch 92, Batch 32 Loss:0.013173678889870644\n",
      "Epoch 92, Batch 33 Loss:0.020257942378520966\n",
      "Epoch 92, Batch 34 Loss:0.04174209386110306\n",
      "Epoch 92, Batch 35 Loss:0.005689787212759256\n",
      "Epoch 92, Batch 36 Loss:0.005252036731690168\n",
      "Epoch 92, Batch 37 Loss:0.06042753532528877\n",
      "Epoch 92, Batch 38 Loss:0.010601062327623367\n",
      "Epoch 92, Batch 39 Loss:0.03982842341065407\n",
      "Epoch 92, Batch 40 Loss:0.03854964300990105\n",
      "Epoch 92, Batch 41 Loss:0.006745839491486549\n",
      "Epoch 92, Batch 42 Loss:0.05877066031098366\n",
      "Epoch 92, Batch 43 Loss:0.02431218884885311\n",
      "Epoch 92, Batch 44 Loss:0.01567792519927025\n",
      "Epoch 92, Batch 45 Loss:0.0109030120074749\n",
      "Epoch 92, Batch 46 Loss:0.003257100936025381\n",
      "Epoch 92, Batch 47 Loss:0.009577658958733082\n",
      "Epoch 92, Batch 48 Loss:0.00361237907782197\n",
      "Epoch 92, Batch 49 Loss:0.014920329675078392\n",
      "Epoch 92, Batch 50 Loss:0.015001965686678886\n",
      "Epoch 92, Batch 51 Loss:0.002327528316527605\n",
      "Epoch 92, Batch 52 Loss:0.03515751287341118\n",
      "Epoch 92, Batch 53 Loss:0.003874534275382757\n",
      "Epoch 92, Batch 54 Loss:0.00689004035666585\n",
      "Epoch 92, Batch 55 Loss:0.006110501009970903\n",
      "Epoch 92, Batch 56 Loss:0.017246078699827194\n",
      "Epoch 92, Batch 57 Loss:0.013340299017727375\n",
      "Epoch 92, Batch 58 Loss:0.004388911183923483\n",
      "Epoch 92, Batch 59 Loss:0.005705070216208696\n",
      "Epoch 92, Batch 60 Loss:0.0030900840647518635\n",
      "Epoch 92, Batch 61 Loss:0.004157549701631069\n",
      "Epoch 92, Batch 62 Loss:0.009768747724592686\n",
      "Epoch 92, Batch 63 Loss:0.015425809659063816\n",
      "Epoch 92, Batch 64 Loss:0.012858700007200241\n",
      "Epoch 92, Batch 65 Loss:0.02693196013569832\n",
      "Epoch 92, Batch 66 Loss:0.0044158706441521645\n",
      "Epoch 92, Batch 67 Loss:0.008672750554978848\n",
      "Epoch 92, Batch 68 Loss:0.022826122120022774\n",
      "Epoch 92, Batch 69 Loss:0.003272182308137417\n",
      "Epoch 92, Batch 70 Loss:0.0023401493672281504\n",
      "Epoch 92, Batch 71 Loss:0.004285864997655153\n",
      "Epoch 92, Batch 72 Loss:0.00224398635327816\n",
      "Epoch 92, Batch 73 Loss:0.007563751190900803\n",
      "Epoch 92, Batch 74 Loss:0.012275749817490578\n",
      "Epoch 92, Batch 75 Loss:0.012046333402395248\n",
      "Epoch 92, Batch 76 Loss:0.004087619949132204\n",
      "Epoch 92, Batch 77 Loss:0.006938012316823006\n",
      "Epoch 92, Batch 78 Loss:0.03327982500195503\n",
      "Epoch 92, Batch 79 Loss:0.04945271089673042\n",
      "Epoch 92, Batch 80 Loss:0.005484133958816528\n",
      "Epoch 92, Batch 81 Loss:0.004480073694139719\n",
      "Epoch 92, Batch 82 Loss:0.03766900673508644\n",
      "Epoch 92, Batch 83 Loss:0.007635164074599743\n",
      "Epoch 92, Batch 84 Loss:0.006611349061131477\n",
      "Epoch 92, Batch 85 Loss:0.005586585495620966\n",
      "Epoch 92, Batch 86 Loss:0.03753970190882683\n",
      "Epoch 92, Batch 87 Loss:0.003669736674055457\n",
      "Epoch 92, Batch 88 Loss:0.015633482486009598\n",
      "Epoch 92, Batch 89 Loss:0.0047251260839402676\n",
      "Epoch 92, Batch 90 Loss:0.005633518099784851\n",
      "Epoch 92, Batch 91 Loss:0.029303032904863358\n",
      "Epoch 92, Batch 92 Loss:0.009199175052344799\n",
      "Epoch 92, Batch 93 Loss:0.004849748220294714\n",
      "Epoch 92, Batch 94 Loss:0.0017609187634661794\n",
      "Epoch 92, Batch 95 Loss:0.03585568070411682\n",
      "Epoch 92, Batch 96 Loss:0.005628837272524834\n",
      "Epoch 92, Batch 97 Loss:0.005066420882940292\n",
      "Epoch 92, Batch 98 Loss:0.028222711756825447\n",
      "Epoch 92, Batch 99 Loss:0.011221320368349552\n",
      "Epoch 92, Batch 100 Loss:0.011358575895428658\n",
      "Epoch 92, Batch 101 Loss:0.004433524329215288\n",
      "Epoch 92, Batch 102 Loss:0.012572200037539005\n",
      "Epoch 92, Batch 103 Loss:0.010794875212013721\n",
      "Epoch 92, Batch 104 Loss:0.015284569934010506\n",
      "Epoch 92, Batch 105 Loss:0.01188761554658413\n",
      "Epoch 92, Batch 106 Loss:0.008439435623586178\n",
      "Epoch 92, Batch 107 Loss:0.0046328832395374775\n",
      "Epoch 92, Batch 108 Loss:0.014684388414025307\n",
      "Epoch 92, Batch 109 Loss:0.006789894308894873\n",
      "Epoch 92, Batch 110 Loss:0.01441004779189825\n",
      "Epoch 92, Batch 111 Loss:0.001851900713518262\n",
      "Epoch 92, Batch 112 Loss:0.008013797923922539\n",
      "Epoch 92, Batch 113 Loss:0.019507383927702904\n",
      "Epoch 92, Batch 114 Loss:0.021949250251054764\n",
      "Epoch 92, Batch 115 Loss:0.011709571816027164\n",
      "Epoch 92, Batch 116 Loss:0.0022171963937580585\n",
      "Epoch 92, Batch 117 Loss:0.011668803170323372\n",
      "Epoch 92, Batch 118 Loss:0.0055361646227538586\n",
      "Epoch 92, Batch 119 Loss:0.010443383827805519\n",
      "Epoch 92, Batch 120 Loss:0.0035019046626985073\n",
      "Epoch 92, Batch 121 Loss:0.004211081191897392\n",
      "Epoch 92, Batch 122 Loss:0.005863129161298275\n",
      "Epoch 92, Batch 123 Loss:0.003507572691887617\n",
      "Epoch 92, Batch 124 Loss:0.005218734033405781\n",
      "Epoch 92, Batch 125 Loss:0.0013510058633983135\n",
      "Epoch 92, Batch 126 Loss:0.005394786596298218\n",
      "Epoch 92, Batch 127 Loss:0.0016343051102012396\n",
      "Epoch 92, Batch 128 Loss:0.017145510762929916\n",
      "Epoch 92, Batch 129 Loss:0.003115633502602577\n",
      "Epoch 92, Batch 130 Loss:0.019529391080141068\n",
      "Epoch 92, Batch 131 Loss:0.004879084415733814\n",
      "Epoch 92, Batch 132 Loss:0.005141286179423332\n",
      "Epoch 92, Batch 133 Loss:0.015579210594296455\n",
      "Epoch 92, Batch 134 Loss:0.010140656493604183\n",
      "Epoch 92, Batch 135 Loss:0.002768306527286768\n",
      "Epoch 92, Batch 136 Loss:0.0023274857085198164\n",
      "Epoch 92, Batch 137 Loss:0.002051639137789607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92, Batch 138 Loss:0.02569752186536789\n",
      "Epoch 92, Batch 139 Loss:0.006111874245107174\n",
      "Epoch 92, Batch 140 Loss:0.01307469978928566\n",
      "Epoch 92, Batch 141 Loss:0.005232327617704868\n",
      "Epoch 92, Batch 142 Loss:0.0043160030618309975\n",
      "Epoch 92, Batch 143 Loss:0.00761518906801939\n",
      "Epoch 92, Batch 144 Loss:0.005226835608482361\n",
      "Epoch 92, Batch 145 Loss:0.0037015771958976984\n",
      "Epoch 92, Batch 146 Loss:0.006663571577519178\n",
      "Epoch 92, Batch 147 Loss:0.00867740623652935\n",
      "Epoch 92, Batch 148 Loss:0.019896822050213814\n",
      "Epoch 92, Batch 149 Loss:0.0022691753692924976\n",
      "Epoch 92, Batch 150 Loss:0.0021883700974285603\n",
      "Epoch 92, Batch 151 Loss:0.010612748563289642\n",
      "Epoch 92, Batch 152 Loss:0.0029974631033837795\n",
      "Epoch 92, Batch 153 Loss:0.00220185867510736\n",
      "Epoch 92, Batch 154 Loss:0.017930621281266212\n",
      "Epoch 92, Batch 155 Loss:0.0030085272155702114\n",
      "Epoch 92, Batch 156 Loss:0.0028668679296970367\n",
      "Epoch 92, Batch 157 Loss:0.0024320771917700768\n",
      "Epoch 92, Batch 158 Loss:0.0031672632321715355\n",
      "Epoch 92, Batch 159 Loss:0.0030536202248185873\n",
      "Epoch 92, Batch 160 Loss:0.0014148266054689884\n",
      "Epoch 92, Batch 161 Loss:0.0038347982335835695\n",
      "Epoch 92, Batch 162 Loss:0.0030975951813161373\n",
      "Epoch 92, Batch 163 Loss:0.0023201541043817997\n",
      "Epoch 92, Batch 164 Loss:0.003967213910073042\n",
      "Epoch 92, Batch 165 Loss:0.00797551590949297\n",
      "Epoch 92, Batch 166 Loss:0.018117031082510948\n",
      "Epoch 92, Batch 167 Loss:0.0014922048430889845\n",
      "Epoch 92, Batch 168 Loss:0.001783285173587501\n",
      "Epoch 92, Batch 169 Loss:0.0015253028832376003\n",
      "Epoch 92, Batch 170 Loss:0.002986438339576125\n",
      "Epoch 92, Batch 171 Loss:0.014646442607045174\n",
      "Epoch 92, Batch 172 Loss:0.00562676228582859\n",
      "Epoch 92, Batch 173 Loss:0.008866570889949799\n",
      "Epoch 92, Batch 174 Loss:0.004150649532675743\n",
      "Epoch 92, Batch 175 Loss:0.0044920663349330425\n",
      "Epoch 92, Batch 176 Loss:0.01462700217962265\n",
      "Epoch 92, Batch 177 Loss:0.004209219478070736\n",
      "Epoch 92, Batch 178 Loss:0.007797567173838615\n",
      "Epoch 92, Batch 179 Loss:0.0023442846722900867\n",
      "Epoch 92, Batch 180 Loss:0.023050034418702126\n",
      "Epoch 92, Batch 181 Loss:0.004598748870193958\n",
      "Epoch 92, Batch 182 Loss:0.044480662792921066\n",
      "Epoch 92, Batch 183 Loss:0.0033598020672798157\n",
      "Epoch 92, Batch 184 Loss:0.03573848307132721\n",
      "Epoch 92, Batch 185 Loss:0.06813827902078629\n",
      "Epoch 92, Batch 186 Loss:0.0017042802646756172\n",
      "Epoch 92, Batch 187 Loss:0.008283456787467003\n",
      "Epoch 92, Batch 188 Loss:0.002230581361800432\n",
      "Epoch 92, Batch 189 Loss:0.03216998279094696\n",
      "Epoch 92, Batch 190 Loss:0.005744886118918657\n",
      "Epoch 92, Batch 191 Loss:0.01518929097801447\n",
      "Epoch 92, Batch 192 Loss:0.0020037516951560974\n",
      "Epoch 92, Batch 193 Loss:0.03475594520568848\n",
      "Epoch 92, Batch 194 Loss:0.0029376186430454254\n",
      "Epoch 92, Batch 195 Loss:0.004486032761633396\n",
      "Epoch 92, Batch 196 Loss:0.0026832078583538532\n",
      "Epoch 92, Batch 197 Loss:0.005194758530706167\n",
      "Epoch 92, Batch 198 Loss:0.005418218206614256\n",
      "Epoch 92, Batch 199 Loss:0.015820298343896866\n",
      "Epoch 92, Batch 200 Loss:0.04452783241868019\n",
      "Epoch 92, Batch 201 Loss:0.003065538825467229\n",
      "Epoch 92, Batch 202 Loss:0.0036541763693094254\n",
      "Epoch 92, Batch 203 Loss:0.010519309900701046\n",
      "Epoch 92, Batch 204 Loss:0.004953361116349697\n",
      "Epoch 92, Batch 205 Loss:0.013075640425086021\n",
      "Epoch 92, Batch 206 Loss:0.012584235519170761\n",
      "Epoch 92, Batch 207 Loss:0.009821898303925991\n",
      "Epoch 92, Batch 208 Loss:0.03086872026324272\n",
      "Epoch 92, Batch 209 Loss:0.0030474422965198755\n",
      "Epoch 92, Batch 210 Loss:0.01386741641908884\n",
      "Epoch 92, Batch 211 Loss:0.044255331158638\n",
      "Epoch 92, Batch 212 Loss:0.0023655034601688385\n",
      "Epoch 92, Batch 213 Loss:0.004074724856764078\n",
      "Epoch 92, Batch 214 Loss:0.003510607173666358\n",
      "Epoch 92, Batch 215 Loss:0.02371574006974697\n",
      "Epoch 92, Batch 216 Loss:0.0017107712337747216\n",
      "Epoch 92, Batch 217 Loss:0.016546420753002167\n",
      "Epoch 92, Batch 218 Loss:0.010828151367604733\n",
      "Epoch 92, Batch 219 Loss:0.004090487025678158\n",
      "Epoch 92, Batch 220 Loss:0.029182545840740204\n",
      "Epoch 92, Batch 221 Loss:0.08488630503416061\n",
      "Epoch 92, Batch 222 Loss:0.0410495363175869\n",
      "Epoch 92, Batch 223 Loss:0.02911984547972679\n",
      "Epoch 92, Batch 224 Loss:0.029857449233531952\n",
      "Epoch 92, Batch 225 Loss:0.022959277033805847\n",
      "Epoch 92, Batch 226 Loss:0.006692860741168261\n",
      "Epoch 92, Batch 227 Loss:0.03247307613492012\n",
      "Epoch 92, Batch 228 Loss:0.03577195852994919\n",
      "Epoch 92, Batch 229 Loss:0.00948502030223608\n",
      "Epoch 92, Batch 230 Loss:0.0035628429614007473\n",
      "Epoch 92, Batch 231 Loss:0.02966637909412384\n",
      "Epoch 92, Batch 232 Loss:0.02626205049455166\n",
      "Epoch 92, Batch 233 Loss:0.008628030307590961\n",
      "Loss in this Epoch is: 0.862803030759 %\n",
      "Accuracy in this Epoch is: 88.4599983692 %\n",
      "Epoch 93, Batch 0 Loss:0.04205433279275894\n",
      "Epoch 93, Batch 1 Loss:0.005474386736750603\n",
      "Epoch 93, Batch 2 Loss:0.004068938549607992\n",
      "Epoch 93, Batch 3 Loss:0.005693539045751095\n",
      "Epoch 93, Batch 4 Loss:0.00951070711016655\n",
      "Epoch 93, Batch 5 Loss:0.004021406173706055\n",
      "Epoch 93, Batch 6 Loss:0.010339254513382912\n",
      "Epoch 93, Batch 7 Loss:0.006578221917152405\n",
      "Epoch 93, Batch 8 Loss:0.013678444549441338\n",
      "Epoch 93, Batch 9 Loss:0.004005384631454945\n",
      "Epoch 93, Batch 10 Loss:0.027637703344225883\n",
      "Epoch 93, Batch 11 Loss:0.004910028539597988\n",
      "Epoch 93, Batch 12 Loss:0.029259128496050835\n",
      "Epoch 93, Batch 13 Loss:0.01158083975315094\n",
      "Epoch 93, Batch 14 Loss:0.0704709067940712\n",
      "Epoch 93, Batch 15 Loss:0.060200441628694534\n",
      "Epoch 93, Batch 16 Loss:0.03348793461918831\n",
      "Epoch 93, Batch 17 Loss:0.04601757600903511\n",
      "Epoch 93, Batch 18 Loss:0.03761250525712967\n",
      "Epoch 93, Batch 19 Loss:0.034843310713768005\n",
      "Epoch 93, Batch 20 Loss:0.01212033536285162\n",
      "Epoch 93, Batch 21 Loss:0.024689875543117523\n",
      "Epoch 93, Batch 22 Loss:0.07239625602960587\n",
      "Epoch 93, Batch 23 Loss:0.03505364805459976\n",
      "Epoch 93, Batch 24 Loss:0.07379447668790817\n",
      "Epoch 93, Batch 25 Loss:0.009925931692123413\n",
      "Epoch 93, Batch 26 Loss:0.02397744171321392\n",
      "Epoch 93, Batch 27 Loss:0.01826419122517109\n",
      "Epoch 93, Batch 28 Loss:0.02313072979450226\n",
      "Epoch 93, Batch 29 Loss:0.007710157427936792\n",
      "Epoch 93, Batch 30 Loss:0.011918545700609684\n",
      "Epoch 93, Batch 31 Loss:0.030249377712607384\n",
      "Epoch 93, Batch 32 Loss:0.006045455113053322\n",
      "Epoch 93, Batch 33 Loss:0.02217579260468483\n",
      "Epoch 93, Batch 34 Loss:0.030570095404982567\n",
      "Epoch 93, Batch 35 Loss:0.016120344400405884\n",
      "Epoch 93, Batch 36 Loss:0.005564476829022169\n",
      "Epoch 93, Batch 37 Loss:0.039914101362228394\n",
      "Epoch 93, Batch 38 Loss:0.014455822296440601\n",
      "Epoch 93, Batch 39 Loss:0.03049394115805626\n",
      "Epoch 93, Batch 40 Loss:0.05840175971388817\n",
      "Epoch 93, Batch 41 Loss:0.016436848789453506\n",
      "Epoch 93, Batch 42 Loss:0.004574325401335955\n",
      "Epoch 93, Batch 43 Loss:0.03400919586420059\n",
      "Epoch 93, Batch 44 Loss:0.01020345464348793\n",
      "Epoch 93, Batch 45 Loss:0.02176894247531891\n",
      "Epoch 93, Batch 46 Loss:0.020512383431196213\n",
      "Epoch 93, Batch 47 Loss:0.02209671214222908\n",
      "Epoch 93, Batch 48 Loss:0.026354560628533363\n",
      "Epoch 93, Batch 49 Loss:0.017095856368541718\n",
      "Epoch 93, Batch 50 Loss:0.03154532611370087\n",
      "Epoch 93, Batch 51 Loss:0.004511702340096235\n",
      "Epoch 93, Batch 52 Loss:0.0071754599921405315\n",
      "Epoch 93, Batch 53 Loss:0.032690584659576416\n",
      "Epoch 93, Batch 54 Loss:0.02300301566720009\n",
      "Epoch 93, Batch 55 Loss:0.016744008287787437\n",
      "Epoch 93, Batch 56 Loss:0.02672993578016758\n",
      "Epoch 93, Batch 57 Loss:0.028556572273373604\n",
      "Epoch 93, Batch 58 Loss:0.041388869285583496\n",
      "Epoch 93, Batch 59 Loss:0.014308333396911621\n",
      "Epoch 93, Batch 60 Loss:0.01029579620808363\n",
      "Epoch 93, Batch 61 Loss:0.00835869088768959\n",
      "Epoch 93, Batch 62 Loss:0.012164901942014694\n",
      "Epoch 93, Batch 63 Loss:0.00865220371633768\n",
      "Epoch 93, Batch 64 Loss:0.011312965303659439\n",
      "Epoch 93, Batch 65 Loss:0.03844008967280388\n",
      "Epoch 93, Batch 66 Loss:0.043823279440402985\n",
      "Epoch 93, Batch 67 Loss:0.010740173980593681\n",
      "Epoch 93, Batch 68 Loss:0.015714872628450394\n",
      "Epoch 93, Batch 69 Loss:0.0033136343117803335\n",
      "Epoch 93, Batch 70 Loss:0.01909083127975464\n",
      "Epoch 93, Batch 71 Loss:0.006386438850313425\n",
      "Epoch 93, Batch 72 Loss:0.0051197451539337635\n",
      "Epoch 93, Batch 73 Loss:0.005258575547486544\n",
      "Epoch 93, Batch 74 Loss:0.010340714827179909\n",
      "Epoch 93, Batch 75 Loss:0.002929113805294037\n",
      "Epoch 93, Batch 76 Loss:0.006146442145109177\n",
      "Epoch 93, Batch 77 Loss:0.015207825228571892\n",
      "Epoch 93, Batch 78 Loss:0.0037502991035580635\n",
      "Epoch 93, Batch 79 Loss:0.007923631928861141\n",
      "Epoch 93, Batch 80 Loss:0.004365011118352413\n",
      "Epoch 93, Batch 81 Loss:0.03445837274193764\n",
      "Epoch 93, Batch 82 Loss:0.013448493555188179\n",
      "Epoch 93, Batch 83 Loss:0.002107044914737344\n",
      "Epoch 93, Batch 84 Loss:0.02305188588798046\n",
      "Epoch 93, Batch 85 Loss:0.0014268512604758143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, Batch 86 Loss:0.018292387947440147\n",
      "Epoch 93, Batch 87 Loss:0.004570758435875177\n",
      "Epoch 93, Batch 88 Loss:0.00774500984698534\n",
      "Epoch 93, Batch 89 Loss:0.0016038723988458514\n",
      "Epoch 93, Batch 90 Loss:0.013859058730304241\n",
      "Epoch 93, Batch 91 Loss:0.01784687675535679\n",
      "Epoch 93, Batch 92 Loss:0.006223451346158981\n",
      "Epoch 93, Batch 93 Loss:0.008509522303938866\n",
      "Epoch 93, Batch 94 Loss:0.005650702398270369\n",
      "Epoch 93, Batch 95 Loss:0.015337320975959301\n",
      "Epoch 93, Batch 96 Loss:0.0056583452969789505\n",
      "Epoch 93, Batch 97 Loss:0.029888713732361794\n",
      "Epoch 93, Batch 98 Loss:0.03868769109249115\n",
      "Epoch 93, Batch 99 Loss:0.09381946921348572\n",
      "Epoch 93, Batch 100 Loss:0.021181471645832062\n",
      "Epoch 93, Batch 101 Loss:0.002288315212354064\n",
      "Epoch 93, Batch 102 Loss:0.003373963525518775\n",
      "Epoch 93, Batch 103 Loss:0.0033960500732064247\n",
      "Epoch 93, Batch 104 Loss:0.044170182198286057\n",
      "Epoch 93, Batch 105 Loss:0.004762474447488785\n",
      "Epoch 93, Batch 106 Loss:0.009554894641041756\n",
      "Epoch 93, Batch 107 Loss:0.0020187068730592728\n",
      "Epoch 93, Batch 108 Loss:0.007831621915102005\n",
      "Epoch 93, Batch 109 Loss:0.002564207650721073\n",
      "Epoch 93, Batch 110 Loss:0.016605915501713753\n",
      "Epoch 93, Batch 111 Loss:0.011785116046667099\n",
      "Epoch 93, Batch 112 Loss:0.0020752872806042433\n",
      "Epoch 93, Batch 113 Loss:0.003841136582195759\n",
      "Epoch 93, Batch 114 Loss:0.011635439470410347\n",
      "Epoch 93, Batch 115 Loss:0.02143392525613308\n",
      "Epoch 93, Batch 116 Loss:0.0032932760659605265\n",
      "Epoch 93, Batch 117 Loss:0.008929532952606678\n",
      "Epoch 93, Batch 118 Loss:0.013610799796879292\n",
      "Epoch 93, Batch 119 Loss:0.018362335860729218\n",
      "Epoch 93, Batch 120 Loss:0.009804783388972282\n",
      "Epoch 93, Batch 121 Loss:0.005359278991818428\n",
      "Epoch 93, Batch 122 Loss:0.004233888350427151\n",
      "Epoch 93, Batch 123 Loss:0.006315859500318766\n",
      "Epoch 93, Batch 124 Loss:0.005747605115175247\n",
      "Epoch 93, Batch 125 Loss:0.039380453526973724\n",
      "Epoch 93, Batch 126 Loss:0.01679077371954918\n",
      "Epoch 93, Batch 127 Loss:0.0093079824000597\n",
      "Epoch 93, Batch 128 Loss:0.012815594673156738\n",
      "Epoch 93, Batch 129 Loss:0.007434960454702377\n",
      "Epoch 93, Batch 130 Loss:0.01324343029409647\n",
      "Epoch 93, Batch 131 Loss:0.006132420152425766\n",
      "Epoch 93, Batch 132 Loss:0.004086284898221493\n",
      "Epoch 93, Batch 133 Loss:0.017649691551923752\n",
      "Epoch 93, Batch 134 Loss:0.004249114077538252\n",
      "Epoch 93, Batch 135 Loss:0.02136228233575821\n",
      "Epoch 93, Batch 136 Loss:0.005493560805916786\n",
      "Epoch 93, Batch 137 Loss:0.010777896270155907\n",
      "Epoch 93, Batch 138 Loss:0.027542462572455406\n",
      "Epoch 93, Batch 139 Loss:0.022546496242284775\n",
      "Epoch 93, Batch 140 Loss:0.03196314349770546\n",
      "Epoch 93, Batch 141 Loss:0.03288334608078003\n",
      "Epoch 93, Batch 142 Loss:0.02673693373799324\n",
      "Epoch 93, Batch 143 Loss:0.020347973331809044\n",
      "Epoch 93, Batch 144 Loss:0.041087619960308075\n",
      "Epoch 93, Batch 145 Loss:0.03773801773786545\n",
      "Epoch 93, Batch 146 Loss:0.005875471048057079\n",
      "Epoch 93, Batch 147 Loss:0.023965224623680115\n",
      "Epoch 93, Batch 148 Loss:0.026054436340928078\n",
      "Epoch 93, Batch 149 Loss:0.02291838638484478\n",
      "Epoch 93, Batch 150 Loss:0.023460067808628082\n",
      "Epoch 93, Batch 151 Loss:0.014205161482095718\n",
      "Epoch 93, Batch 152 Loss:0.025419583544135094\n",
      "Epoch 93, Batch 153 Loss:0.05011766403913498\n",
      "Epoch 93, Batch 154 Loss:0.023793397471308708\n",
      "Epoch 93, Batch 155 Loss:0.03688577935099602\n",
      "Epoch 93, Batch 156 Loss:0.09026896953582764\n",
      "Epoch 93, Batch 157 Loss:0.009580392390489578\n",
      "Epoch 93, Batch 158 Loss:0.05248592048883438\n",
      "Epoch 93, Batch 159 Loss:0.023264102637767792\n",
      "Epoch 93, Batch 160 Loss:0.019265858456492424\n",
      "Epoch 93, Batch 161 Loss:0.03875541687011719\n",
      "Epoch 93, Batch 162 Loss:0.052175093442201614\n",
      "Epoch 93, Batch 163 Loss:0.03699769452214241\n",
      "Epoch 93, Batch 164 Loss:0.012033773586153984\n",
      "Epoch 93, Batch 165 Loss:0.031177625060081482\n",
      "Epoch 93, Batch 166 Loss:0.01978517323732376\n",
      "Epoch 93, Batch 167 Loss:0.03450219705700874\n",
      "Epoch 93, Batch 168 Loss:0.032845936715602875\n",
      "Epoch 93, Batch 169 Loss:0.043026529252529144\n",
      "Epoch 93, Batch 170 Loss:0.012438380159437656\n",
      "Epoch 93, Batch 171 Loss:0.04177909344434738\n",
      "Epoch 93, Batch 172 Loss:0.026781532913446426\n",
      "Epoch 93, Batch 173 Loss:0.05970238894224167\n",
      "Epoch 93, Batch 174 Loss:0.03306818753480911\n",
      "Epoch 93, Batch 175 Loss:0.029642298817634583\n",
      "Epoch 93, Batch 176 Loss:0.01121564395725727\n",
      "Epoch 93, Batch 177 Loss:0.03709380328655243\n",
      "Epoch 93, Batch 178 Loss:0.02775680646300316\n",
      "Epoch 93, Batch 179 Loss:0.02417428232729435\n",
      "Epoch 93, Batch 180 Loss:0.05003790184855461\n",
      "Epoch 93, Batch 181 Loss:0.020559925585985184\n",
      "Epoch 93, Batch 182 Loss:0.0105914156883955\n",
      "Epoch 93, Batch 183 Loss:0.012620365247130394\n",
      "Epoch 93, Batch 184 Loss:0.015339682810008526\n",
      "Epoch 93, Batch 185 Loss:0.007731431629508734\n",
      "Epoch 93, Batch 186 Loss:0.05290099233388901\n",
      "Epoch 93, Batch 187 Loss:0.01034662127494812\n",
      "Epoch 93, Batch 188 Loss:0.004121582489460707\n",
      "Epoch 93, Batch 189 Loss:0.02031133323907852\n",
      "Epoch 93, Batch 190 Loss:0.005678095854818821\n",
      "Epoch 93, Batch 191 Loss:0.03437066823244095\n",
      "Epoch 93, Batch 192 Loss:0.037163227796554565\n",
      "Epoch 93, Batch 193 Loss:0.007291962392628193\n",
      "Epoch 93, Batch 194 Loss:0.008848246186971664\n",
      "Epoch 93, Batch 195 Loss:0.017889222130179405\n",
      "Epoch 93, Batch 196 Loss:0.017526883631944656\n",
      "Epoch 93, Batch 197 Loss:0.006955562625080347\n",
      "Epoch 93, Batch 198 Loss:0.06426404416561127\n",
      "Epoch 93, Batch 199 Loss:0.021187858656048775\n",
      "Epoch 93, Batch 200 Loss:0.008707448840141296\n",
      "Epoch 93, Batch 201 Loss:0.006212332285940647\n",
      "Epoch 93, Batch 202 Loss:0.010685186833143234\n",
      "Epoch 93, Batch 203 Loss:0.04475555568933487\n",
      "Epoch 93, Batch 204 Loss:0.038419559597969055\n",
      "Epoch 93, Batch 205 Loss:0.01785489171743393\n",
      "Epoch 93, Batch 206 Loss:0.053622566163539886\n",
      "Epoch 93, Batch 207 Loss:0.038371741771698\n",
      "Epoch 93, Batch 208 Loss:0.010124793276190758\n",
      "Epoch 93, Batch 209 Loss:0.020938672125339508\n",
      "Epoch 93, Batch 210 Loss:0.024518968537449837\n",
      "Epoch 93, Batch 211 Loss:0.011975021101534367\n",
      "Epoch 93, Batch 212 Loss:0.0054067689925432205\n",
      "Epoch 93, Batch 213 Loss:0.021187517791986465\n",
      "Epoch 93, Batch 214 Loss:0.006685500964522362\n",
      "Epoch 93, Batch 215 Loss:0.011060742661356926\n",
      "Epoch 93, Batch 216 Loss:0.004518507048487663\n",
      "Epoch 93, Batch 217 Loss:0.010069283656775951\n",
      "Epoch 93, Batch 218 Loss:0.004101286642253399\n",
      "Epoch 93, Batch 219 Loss:0.01268380880355835\n",
      "Epoch 93, Batch 220 Loss:0.01623542234301567\n",
      "Epoch 93, Batch 221 Loss:0.009361768141388893\n",
      "Epoch 93, Batch 222 Loss:0.006230583414435387\n",
      "Epoch 93, Batch 223 Loss:0.010347213596105576\n",
      "Epoch 93, Batch 224 Loss:0.04009392857551575\n",
      "Epoch 93, Batch 225 Loss:0.012256277725100517\n",
      "Epoch 93, Batch 226 Loss:0.008809215389192104\n",
      "Epoch 93, Batch 227 Loss:0.01154888141900301\n",
      "Epoch 93, Batch 228 Loss:0.0059119113720953465\n",
      "Epoch 93, Batch 229 Loss:0.02123570255935192\n",
      "Epoch 93, Batch 230 Loss:0.054701194167137146\n",
      "Epoch 93, Batch 231 Loss:0.004999028984457254\n",
      "Epoch 93, Batch 232 Loss:0.02587953768670559\n",
      "Epoch 93, Batch 233 Loss:0.006832439918071032\n",
      "Loss in this Epoch is: 0.683243991807 %\n",
      "Accuracy in this Epoch is: 88.7000024319 %\n",
      "Epoch 94, Batch 0 Loss:0.005164232105016708\n",
      "Epoch 94, Batch 1 Loss:0.011066632345318794\n",
      "Epoch 94, Batch 2 Loss:0.0048782904632389545\n",
      "Epoch 94, Batch 3 Loss:0.006280239671468735\n",
      "Epoch 94, Batch 4 Loss:0.0186618622392416\n",
      "Epoch 94, Batch 5 Loss:0.01210867054760456\n",
      "Epoch 94, Batch 6 Loss:0.004384545609354973\n",
      "Epoch 94, Batch 7 Loss:0.027338320389389992\n",
      "Epoch 94, Batch 8 Loss:0.01059150230139494\n",
      "Epoch 94, Batch 9 Loss:0.005609516054391861\n",
      "Epoch 94, Batch 10 Loss:0.038788940757513046\n",
      "Epoch 94, Batch 11 Loss:0.007138003595173359\n",
      "Epoch 94, Batch 12 Loss:0.0069679999724030495\n",
      "Epoch 94, Batch 13 Loss:0.02138558402657509\n",
      "Epoch 94, Batch 14 Loss:0.006128197535872459\n",
      "Epoch 94, Batch 15 Loss:0.004106760956346989\n",
      "Epoch 94, Batch 16 Loss:0.01187235675752163\n",
      "Epoch 94, Batch 17 Loss:0.00785570964217186\n",
      "Epoch 94, Batch 18 Loss:0.023780789226293564\n",
      "Epoch 94, Batch 19 Loss:0.007934845052659512\n",
      "Epoch 94, Batch 20 Loss:0.01142590120434761\n",
      "Epoch 94, Batch 21 Loss:0.0063957106322050095\n",
      "Epoch 94, Batch 22 Loss:0.006150256842374802\n",
      "Epoch 94, Batch 23 Loss:0.004754766821861267\n",
      "Epoch 94, Batch 24 Loss:0.006865113042294979\n",
      "Epoch 94, Batch 25 Loss:0.005739707034081221\n",
      "Epoch 94, Batch 26 Loss:0.04548287391662598\n",
      "Epoch 94, Batch 27 Loss:0.02086801826953888\n",
      "Epoch 94, Batch 28 Loss:0.00789218582212925\n",
      "Epoch 94, Batch 29 Loss:0.0032191865611821413\n",
      "Epoch 94, Batch 30 Loss:0.004543297458440065\n",
      "Epoch 94, Batch 31 Loss:0.007691578008234501\n",
      "Epoch 94, Batch 32 Loss:0.01617441140115261\n",
      "Epoch 94, Batch 33 Loss:0.0026345732621848583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Batch 34 Loss:0.005648306105285883\n",
      "Epoch 94, Batch 35 Loss:0.05100884288549423\n",
      "Epoch 94, Batch 36 Loss:0.00711809890344739\n",
      "Epoch 94, Batch 37 Loss:0.009986895136535168\n",
      "Epoch 94, Batch 38 Loss:0.006364583037793636\n",
      "Epoch 94, Batch 39 Loss:0.0030953483656048775\n",
      "Epoch 94, Batch 40 Loss:0.02975846640765667\n",
      "Epoch 94, Batch 41 Loss:0.0029153702780604362\n",
      "Epoch 94, Batch 42 Loss:0.008968561887741089\n",
      "Epoch 94, Batch 43 Loss:0.005154908634722233\n",
      "Epoch 94, Batch 44 Loss:0.004196642432361841\n",
      "Epoch 94, Batch 45 Loss:0.0025209656450897455\n",
      "Epoch 94, Batch 46 Loss:0.007696794345974922\n",
      "Epoch 94, Batch 47 Loss:0.027238376438617706\n",
      "Epoch 94, Batch 48 Loss:0.0073403543792665005\n",
      "Epoch 94, Batch 49 Loss:0.004316415637731552\n",
      "Epoch 94, Batch 50 Loss:0.004775866866111755\n",
      "Epoch 94, Batch 51 Loss:0.04423986002802849\n",
      "Epoch 94, Batch 52 Loss:0.0032035643234848976\n",
      "Epoch 94, Batch 53 Loss:0.005918300710618496\n",
      "Epoch 94, Batch 54 Loss:0.008710975758731365\n",
      "Epoch 94, Batch 55 Loss:0.017310647293925285\n",
      "Epoch 94, Batch 56 Loss:0.015957560390233994\n",
      "Epoch 94, Batch 57 Loss:0.0034823131281882524\n",
      "Epoch 94, Batch 58 Loss:0.007316176779568195\n",
      "Epoch 94, Batch 59 Loss:0.0033093434758484364\n",
      "Epoch 94, Batch 60 Loss:0.0025269738398492336\n",
      "Epoch 94, Batch 61 Loss:0.00553591875359416\n",
      "Epoch 94, Batch 62 Loss:0.0025658058002591133\n",
      "Epoch 94, Batch 63 Loss:0.010503323748707771\n",
      "Epoch 94, Batch 64 Loss:0.004556245170533657\n",
      "Epoch 94, Batch 65 Loss:0.005583766382187605\n",
      "Epoch 94, Batch 66 Loss:0.014534705318510532\n",
      "Epoch 94, Batch 67 Loss:0.004328643437474966\n",
      "Epoch 94, Batch 68 Loss:0.004005339462310076\n",
      "Epoch 94, Batch 69 Loss:0.003183357883244753\n",
      "Epoch 94, Batch 70 Loss:0.011715640313923359\n",
      "Epoch 94, Batch 71 Loss:0.0029927706345915794\n",
      "Epoch 94, Batch 72 Loss:0.024022625759243965\n",
      "Epoch 94, Batch 73 Loss:0.012480238452553749\n",
      "Epoch 94, Batch 74 Loss:0.001831273315474391\n",
      "Epoch 94, Batch 75 Loss:0.003621090669184923\n",
      "Epoch 94, Batch 76 Loss:0.001726955291815102\n",
      "Epoch 94, Batch 77 Loss:0.0015871389769017696\n",
      "Epoch 94, Batch 78 Loss:0.0018513445975258946\n",
      "Epoch 94, Batch 79 Loss:0.0044890460558235645\n",
      "Epoch 94, Batch 80 Loss:0.0017533150967210531\n",
      "Epoch 94, Batch 81 Loss:0.008068387396633625\n",
      "Epoch 94, Batch 82 Loss:0.00853610597550869\n",
      "Epoch 94, Batch 83 Loss:0.0039907521568238735\n",
      "Epoch 94, Batch 84 Loss:0.0016334439860656857\n",
      "Epoch 94, Batch 85 Loss:0.02133643627166748\n",
      "Epoch 94, Batch 86 Loss:0.012748866342008114\n",
      "Epoch 94, Batch 87 Loss:0.008565074764192104\n",
      "Epoch 94, Batch 88 Loss:0.007801136467605829\n",
      "Epoch 94, Batch 89 Loss:0.040902916342020035\n",
      "Epoch 94, Batch 90 Loss:0.004036984406411648\n",
      "Epoch 94, Batch 91 Loss:0.002833365462720394\n",
      "Epoch 94, Batch 92 Loss:0.010512753389775753\n",
      "Epoch 94, Batch 93 Loss:0.006558835506439209\n",
      "Epoch 94, Batch 94 Loss:0.02798871323466301\n",
      "Epoch 94, Batch 95 Loss:0.002305038506165147\n",
      "Epoch 94, Batch 96 Loss:0.005694329272955656\n",
      "Epoch 94, Batch 97 Loss:0.03258147090673447\n",
      "Epoch 94, Batch 98 Loss:0.016746800392866135\n",
      "Epoch 94, Batch 99 Loss:0.021975157782435417\n",
      "Epoch 94, Batch 100 Loss:0.003046085126698017\n",
      "Epoch 94, Batch 101 Loss:0.030620090663433075\n",
      "Epoch 94, Batch 102 Loss:0.005052612163126469\n",
      "Epoch 94, Batch 103 Loss:0.01300081703811884\n",
      "Epoch 94, Batch 104 Loss:0.0015262978849932551\n",
      "Epoch 94, Batch 105 Loss:0.005850045941770077\n",
      "Epoch 94, Batch 106 Loss:0.005773643963038921\n",
      "Epoch 94, Batch 107 Loss:0.008486337959766388\n",
      "Epoch 94, Batch 108 Loss:0.003989933989942074\n",
      "Epoch 94, Batch 109 Loss:0.007393081206828356\n",
      "Epoch 94, Batch 110 Loss:0.004827640950679779\n",
      "Epoch 94, Batch 111 Loss:0.004839329048991203\n",
      "Epoch 94, Batch 112 Loss:0.006167664658278227\n",
      "Epoch 94, Batch 113 Loss:0.010202562436461449\n",
      "Epoch 94, Batch 114 Loss:0.006537678651511669\n",
      "Epoch 94, Batch 115 Loss:0.007138357963413\n",
      "Epoch 94, Batch 116 Loss:0.010891333222389221\n",
      "Epoch 94, Batch 117 Loss:0.0035129247698932886\n",
      "Epoch 94, Batch 118 Loss:0.002712247194722295\n",
      "Epoch 94, Batch 119 Loss:0.0017787594115361571\n",
      "Epoch 94, Batch 120 Loss:0.0014266439247876406\n",
      "Epoch 94, Batch 121 Loss:0.01552865281701088\n",
      "Epoch 94, Batch 122 Loss:0.0015613328432664275\n",
      "Epoch 94, Batch 123 Loss:0.001901881885714829\n",
      "Epoch 94, Batch 124 Loss:0.0017340469639748335\n",
      "Epoch 94, Batch 125 Loss:0.011044505052268505\n",
      "Epoch 94, Batch 126 Loss:0.002763184253126383\n",
      "Epoch 94, Batch 127 Loss:0.0019423117628321052\n",
      "Epoch 94, Batch 128 Loss:0.00983204785734415\n",
      "Epoch 94, Batch 129 Loss:0.0015739238588139415\n",
      "Epoch 94, Batch 130 Loss:0.0017709441017359495\n",
      "Epoch 94, Batch 131 Loss:0.0025494967121630907\n",
      "Epoch 94, Batch 132 Loss:0.003520044032484293\n",
      "Epoch 94, Batch 133 Loss:0.0012675754260271788\n",
      "Epoch 94, Batch 134 Loss:0.0017776869935914874\n",
      "Epoch 94, Batch 135 Loss:0.0027564819902181625\n",
      "Epoch 94, Batch 136 Loss:0.0011307994136586785\n",
      "Epoch 94, Batch 137 Loss:0.0014380542561411858\n",
      "Epoch 94, Batch 138 Loss:0.0009610009146854281\n",
      "Epoch 94, Batch 139 Loss:0.0009701852686703205\n",
      "Epoch 94, Batch 140 Loss:0.004843452945351601\n",
      "Epoch 94, Batch 141 Loss:0.009002042934298515\n",
      "Epoch 94, Batch 142 Loss:0.0026146506424993277\n",
      "Epoch 94, Batch 143 Loss:0.00289915781468153\n",
      "Epoch 94, Batch 144 Loss:0.003399816807359457\n",
      "Epoch 94, Batch 145 Loss:0.012989627197384834\n",
      "Epoch 94, Batch 146 Loss:0.0022225240245461464\n",
      "Epoch 94, Batch 147 Loss:0.0015428324695676565\n",
      "Epoch 94, Batch 148 Loss:0.0006955258431844413\n",
      "Epoch 94, Batch 149 Loss:0.005867816507816315\n",
      "Epoch 94, Batch 150 Loss:0.0014778927434235811\n",
      "Epoch 94, Batch 151 Loss:0.005890741478651762\n",
      "Epoch 94, Batch 152 Loss:0.0030095563270151615\n",
      "Epoch 94, Batch 153 Loss:0.009543786756694317\n",
      "Epoch 94, Batch 154 Loss:0.0019778560381382704\n",
      "Epoch 94, Batch 155 Loss:0.004694809205830097\n",
      "Epoch 94, Batch 156 Loss:0.0021196561865508556\n",
      "Epoch 94, Batch 157 Loss:0.004091601353138685\n",
      "Epoch 94, Batch 158 Loss:0.006183558143675327\n",
      "Epoch 94, Batch 159 Loss:0.0042518125846982\n",
      "Epoch 94, Batch 160 Loss:0.0015943682519719005\n",
      "Epoch 94, Batch 161 Loss:0.0187283493578434\n",
      "Epoch 94, Batch 162 Loss:0.008317897096276283\n",
      "Epoch 94, Batch 163 Loss:0.00120903505012393\n",
      "Epoch 94, Batch 164 Loss:0.0006738460506312549\n",
      "Epoch 94, Batch 165 Loss:0.01967730186879635\n",
      "Epoch 94, Batch 166 Loss:0.004224301315844059\n",
      "Epoch 94, Batch 167 Loss:0.016239572316408157\n",
      "Epoch 94, Batch 168 Loss:0.003974458202719688\n",
      "Epoch 94, Batch 169 Loss:0.00313668348826468\n",
      "Epoch 94, Batch 170 Loss:0.010163523256778717\n",
      "Epoch 94, Batch 171 Loss:0.004016042686998844\n",
      "Epoch 94, Batch 172 Loss:0.0018776629585772753\n",
      "Epoch 94, Batch 173 Loss:0.0008380832150578499\n",
      "Epoch 94, Batch 174 Loss:0.019276754930615425\n",
      "Epoch 94, Batch 175 Loss:0.021980907768011093\n",
      "Epoch 94, Batch 176 Loss:0.0015632931608706713\n",
      "Epoch 94, Batch 177 Loss:0.03728988394141197\n",
      "Epoch 94, Batch 178 Loss:0.0018192336428910494\n",
      "Epoch 94, Batch 179 Loss:0.023625124245882034\n",
      "Epoch 94, Batch 180 Loss:0.012028612196445465\n",
      "Epoch 94, Batch 181 Loss:0.013519741594791412\n",
      "Epoch 94, Batch 182 Loss:0.002237525535747409\n",
      "Epoch 94, Batch 183 Loss:0.009287558495998383\n",
      "Epoch 94, Batch 184 Loss:0.00440002279356122\n",
      "Epoch 94, Batch 185 Loss:0.06964521110057831\n",
      "Epoch 94, Batch 186 Loss:0.0058036260306835175\n",
      "Epoch 94, Batch 187 Loss:0.009304681792855263\n",
      "Epoch 94, Batch 188 Loss:0.005917929112911224\n",
      "Epoch 94, Batch 189 Loss:0.021512070670723915\n",
      "Epoch 94, Batch 190 Loss:0.0030027683824300766\n",
      "Epoch 94, Batch 191 Loss:0.005314997397363186\n",
      "Epoch 94, Batch 192 Loss:0.03050396591424942\n",
      "Epoch 94, Batch 193 Loss:0.0063415709882974625\n",
      "Epoch 94, Batch 194 Loss:0.011068690568208694\n",
      "Epoch 94, Batch 195 Loss:0.005621868185698986\n",
      "Epoch 94, Batch 196 Loss:0.015981249511241913\n",
      "Epoch 94, Batch 197 Loss:0.02962624281644821\n",
      "Epoch 94, Batch 198 Loss:0.004003720823675394\n",
      "Epoch 94, Batch 199 Loss:0.012648815289139748\n",
      "Epoch 94, Batch 200 Loss:0.0033849403262138367\n",
      "Epoch 94, Batch 201 Loss:0.021381523460149765\n",
      "Epoch 94, Batch 202 Loss:0.0027061300352215767\n",
      "Epoch 94, Batch 203 Loss:0.007746141403913498\n",
      "Epoch 94, Batch 204 Loss:0.005052922759205103\n",
      "Epoch 94, Batch 205 Loss:0.017038172110915184\n",
      "Epoch 94, Batch 206 Loss:0.002967913867905736\n",
      "Epoch 94, Batch 207 Loss:0.014532657340168953\n",
      "Epoch 94, Batch 208 Loss:0.003965556621551514\n",
      "Epoch 94, Batch 209 Loss:0.004854037892073393\n",
      "Epoch 94, Batch 210 Loss:0.006986766122281551\n",
      "Epoch 94, Batch 211 Loss:0.006507625803351402\n",
      "Epoch 94, Batch 212 Loss:0.0030648657120764256\n",
      "Epoch 94, Batch 213 Loss:0.006906690075993538\n",
      "Epoch 94, Batch 214 Loss:0.002318461425602436\n",
      "Epoch 94, Batch 215 Loss:0.001185485627502203\n",
      "Epoch 94, Batch 216 Loss:0.004690933041274548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94, Batch 217 Loss:0.018584465608000755\n",
      "Epoch 94, Batch 218 Loss:0.0031590904109179974\n",
      "Epoch 94, Batch 219 Loss:0.008525808341801167\n",
      "Epoch 94, Batch 220 Loss:0.026868455111980438\n",
      "Epoch 94, Batch 221 Loss:0.006934278644621372\n",
      "Epoch 94, Batch 222 Loss:0.006801830139011145\n",
      "Epoch 94, Batch 223 Loss:0.01807524636387825\n",
      "Epoch 94, Batch 224 Loss:0.0033252346329391003\n",
      "Epoch 94, Batch 225 Loss:0.00990480650216341\n",
      "Epoch 94, Batch 226 Loss:0.0038686359766870737\n",
      "Epoch 94, Batch 227 Loss:0.0027951046358793974\n",
      "Epoch 94, Batch 228 Loss:0.004186861217021942\n",
      "Epoch 94, Batch 229 Loss:0.0017610141076147556\n",
      "Epoch 94, Batch 230 Loss:0.004592761863023043\n",
      "Epoch 94, Batch 231 Loss:0.004849391058087349\n",
      "Epoch 94, Batch 232 Loss:0.006388185545802116\n",
      "Epoch 94, Batch 233 Loss:0.0017160457791760564\n",
      "Loss in this Epoch is: 0.171604577918 %\n",
      "Accuracy in this Epoch is: 88.5900020599 %\n",
      "Epoch 95, Batch 0 Loss:0.0007995245978236198\n",
      "Epoch 95, Batch 1 Loss:0.0019065981032326818\n",
      "Epoch 95, Batch 2 Loss:0.016692880541086197\n",
      "Epoch 95, Batch 3 Loss:0.0033085409086197615\n",
      "Epoch 95, Batch 4 Loss:0.001557568204589188\n",
      "Epoch 95, Batch 5 Loss:0.0012481686426326632\n",
      "Epoch 95, Batch 6 Loss:0.03347206115722656\n",
      "Epoch 95, Batch 7 Loss:0.0025963978841900826\n",
      "Epoch 95, Batch 8 Loss:0.022174594923853874\n",
      "Epoch 95, Batch 9 Loss:0.0008608747739344835\n",
      "Epoch 95, Batch 10 Loss:0.0005938754184171557\n",
      "Epoch 95, Batch 11 Loss:0.0005504448781721294\n",
      "Epoch 95, Batch 12 Loss:0.0007451475248672068\n",
      "Epoch 95, Batch 13 Loss:0.004976659547537565\n",
      "Epoch 95, Batch 14 Loss:0.0004418334865476936\n",
      "Epoch 95, Batch 15 Loss:0.0008978820405900478\n",
      "Epoch 95, Batch 16 Loss:0.006159147713333368\n",
      "Epoch 95, Batch 17 Loss:0.0015578867169097066\n",
      "Epoch 95, Batch 18 Loss:0.0007094917236827314\n",
      "Epoch 95, Batch 19 Loss:0.0018612388521432877\n",
      "Epoch 95, Batch 20 Loss:0.0012880301801487803\n",
      "Epoch 95, Batch 21 Loss:0.0026800688356161118\n",
      "Epoch 95, Batch 22 Loss:0.0017095577204599977\n",
      "Epoch 95, Batch 23 Loss:0.0012485957704484463\n",
      "Epoch 95, Batch 24 Loss:0.00971174892038107\n",
      "Epoch 95, Batch 25 Loss:0.023832065984606743\n",
      "Epoch 95, Batch 26 Loss:0.004494325257837772\n",
      "Epoch 95, Batch 27 Loss:0.0017187204211950302\n",
      "Epoch 95, Batch 28 Loss:0.007476739585399628\n",
      "Epoch 95, Batch 29 Loss:0.002451907843351364\n",
      "Epoch 95, Batch 30 Loss:0.0049788327887654305\n",
      "Epoch 95, Batch 31 Loss:0.0006931849056854844\n",
      "Epoch 95, Batch 32 Loss:0.0023508728481829166\n",
      "Epoch 95, Batch 33 Loss:0.023189010098576546\n",
      "Epoch 95, Batch 34 Loss:0.003925207071006298\n",
      "Epoch 95, Batch 35 Loss:0.002457029651850462\n",
      "Epoch 95, Batch 36 Loss:0.009264056570827961\n",
      "Epoch 95, Batch 37 Loss:0.0019190795719623566\n",
      "Epoch 95, Batch 38 Loss:0.005707814823836088\n",
      "Epoch 95, Batch 39 Loss:0.0038424532394856215\n",
      "Epoch 95, Batch 40 Loss:0.0008681628969497979\n",
      "Epoch 95, Batch 41 Loss:0.004156333394348621\n",
      "Epoch 95, Batch 42 Loss:0.014212691225111485\n",
      "Epoch 95, Batch 43 Loss:0.008664574474096298\n",
      "Epoch 95, Batch 44 Loss:0.006634061690419912\n",
      "Epoch 95, Batch 45 Loss:0.005789401941001415\n",
      "Epoch 95, Batch 46 Loss:0.013836059719324112\n",
      "Epoch 95, Batch 47 Loss:0.003263514954596758\n",
      "Epoch 95, Batch 48 Loss:0.011018593795597553\n",
      "Epoch 95, Batch 49 Loss:0.011827131733298302\n",
      "Epoch 95, Batch 50 Loss:0.039806291460990906\n",
      "Epoch 95, Batch 51 Loss:0.062306925654411316\n",
      "Epoch 95, Batch 52 Loss:0.0067672510631382465\n",
      "Epoch 95, Batch 53 Loss:0.0024875812232494354\n",
      "Epoch 95, Batch 54 Loss:0.006753517780452967\n",
      "Epoch 95, Batch 55 Loss:0.002461975673213601\n",
      "Epoch 95, Batch 56 Loss:0.0021730477456003428\n",
      "Epoch 95, Batch 57 Loss:0.02407253533601761\n",
      "Epoch 95, Batch 58 Loss:0.006477842107415199\n",
      "Epoch 95, Batch 59 Loss:0.011056063696742058\n",
      "Epoch 95, Batch 60 Loss:0.007415843661874533\n",
      "Epoch 95, Batch 61 Loss:0.04351816698908806\n",
      "Epoch 95, Batch 62 Loss:0.018892303109169006\n",
      "Epoch 95, Batch 63 Loss:0.016624340787529945\n",
      "Epoch 95, Batch 64 Loss:0.033485881984233856\n",
      "Epoch 95, Batch 65 Loss:0.026825781911611557\n",
      "Epoch 95, Batch 66 Loss:0.011388342827558517\n",
      "Epoch 95, Batch 67 Loss:0.0029958304949104786\n",
      "Epoch 95, Batch 68 Loss:0.0013830276438966393\n",
      "Epoch 95, Batch 69 Loss:0.04069371521472931\n",
      "Epoch 95, Batch 70 Loss:0.007983594201505184\n",
      "Epoch 95, Batch 71 Loss:0.0019487612880766392\n",
      "Epoch 95, Batch 72 Loss:0.010842599906027317\n",
      "Epoch 95, Batch 73 Loss:0.009353210218250751\n",
      "Epoch 95, Batch 74 Loss:0.0063818818889558315\n",
      "Epoch 95, Batch 75 Loss:0.0070784203708171844\n",
      "Epoch 95, Batch 76 Loss:0.007217096164822578\n",
      "Epoch 95, Batch 77 Loss:0.028275661170482635\n",
      "Epoch 95, Batch 78 Loss:0.005131471902132034\n",
      "Epoch 95, Batch 79 Loss:0.014741785824298859\n",
      "Epoch 95, Batch 80 Loss:0.010495473630726337\n",
      "Epoch 95, Batch 81 Loss:0.00949109997600317\n",
      "Epoch 95, Batch 82 Loss:0.007043763995170593\n",
      "Epoch 95, Batch 83 Loss:0.0075551955960690975\n",
      "Epoch 95, Batch 84 Loss:0.0036979722790420055\n",
      "Epoch 95, Batch 85 Loss:0.004280950408428907\n",
      "Epoch 95, Batch 86 Loss:0.003971535712480545\n",
      "Epoch 95, Batch 87 Loss:0.013526782393455505\n",
      "Epoch 95, Batch 88 Loss:0.03304817155003548\n",
      "Epoch 95, Batch 89 Loss:0.040637772530317307\n",
      "Epoch 95, Batch 90 Loss:0.027325907722115517\n",
      "Epoch 95, Batch 91 Loss:0.07053809612989426\n",
      "Epoch 95, Batch 92 Loss:0.008609123528003693\n",
      "Epoch 95, Batch 93 Loss:0.0171704962849617\n",
      "Epoch 95, Batch 94 Loss:0.05217493325471878\n",
      "Epoch 95, Batch 95 Loss:0.08774138242006302\n",
      "Epoch 95, Batch 96 Loss:0.012596428394317627\n",
      "Epoch 95, Batch 97 Loss:0.0226370170712471\n",
      "Epoch 95, Batch 98 Loss:0.015749746933579445\n",
      "Epoch 95, Batch 99 Loss:0.013352693989872932\n",
      "Epoch 95, Batch 100 Loss:0.02273385599255562\n",
      "Epoch 95, Batch 101 Loss:0.013323748484253883\n",
      "Epoch 95, Batch 102 Loss:0.01431223563849926\n",
      "Epoch 95, Batch 103 Loss:0.021505357697606087\n",
      "Epoch 95, Batch 104 Loss:0.022203074768185616\n",
      "Epoch 95, Batch 105 Loss:0.023181449621915817\n",
      "Epoch 95, Batch 106 Loss:0.02597951516509056\n",
      "Epoch 95, Batch 107 Loss:0.022584982216358185\n",
      "Epoch 95, Batch 108 Loss:0.0071166763082146645\n",
      "Epoch 95, Batch 109 Loss:0.039865221828222275\n",
      "Epoch 95, Batch 110 Loss:0.029988044872879982\n",
      "Epoch 95, Batch 111 Loss:0.022683830931782722\n",
      "Epoch 95, Batch 112 Loss:0.01463347114622593\n",
      "Epoch 95, Batch 113 Loss:0.03006104938685894\n",
      "Epoch 95, Batch 114 Loss:0.04703427106142044\n",
      "Epoch 95, Batch 115 Loss:0.02101825177669525\n",
      "Epoch 95, Batch 116 Loss:0.06403349339962006\n",
      "Epoch 95, Batch 117 Loss:0.01225871592760086\n",
      "Epoch 95, Batch 118 Loss:0.01070784218609333\n",
      "Epoch 95, Batch 119 Loss:0.03110659122467041\n",
      "Epoch 95, Batch 120 Loss:0.007269904017448425\n",
      "Epoch 95, Batch 121 Loss:0.035939425230026245\n",
      "Epoch 95, Batch 122 Loss:0.021843180060386658\n",
      "Epoch 95, Batch 123 Loss:0.01969178020954132\n",
      "Epoch 95, Batch 124 Loss:0.020009057596325874\n",
      "Epoch 95, Batch 125 Loss:0.01040690764784813\n",
      "Epoch 95, Batch 126 Loss:0.020184287801384926\n",
      "Epoch 95, Batch 127 Loss:0.01264602318406105\n",
      "Epoch 95, Batch 128 Loss:0.06849262118339539\n",
      "Epoch 95, Batch 129 Loss:0.01937086693942547\n",
      "Epoch 95, Batch 130 Loss:0.01880575716495514\n",
      "Epoch 95, Batch 131 Loss:0.006853215396404266\n",
      "Epoch 95, Batch 132 Loss:0.009530611336231232\n",
      "Epoch 95, Batch 133 Loss:0.023127034306526184\n",
      "Epoch 95, Batch 134 Loss:0.008103318512439728\n",
      "Epoch 95, Batch 135 Loss:0.009050087071955204\n",
      "Epoch 95, Batch 136 Loss:0.017069727182388306\n",
      "Epoch 95, Batch 137 Loss:0.04264834150671959\n",
      "Epoch 95, Batch 138 Loss:0.014473864808678627\n",
      "Epoch 95, Batch 139 Loss:0.02195102535188198\n",
      "Epoch 95, Batch 140 Loss:0.013758575543761253\n",
      "Epoch 95, Batch 141 Loss:0.01497130747884512\n",
      "Epoch 95, Batch 142 Loss:0.038558702915906906\n",
      "Epoch 95, Batch 143 Loss:0.014979385770857334\n",
      "Epoch 95, Batch 144 Loss:0.033696625381708145\n",
      "Epoch 95, Batch 145 Loss:0.014003146439790726\n",
      "Epoch 95, Batch 146 Loss:0.010762106627225876\n",
      "Epoch 95, Batch 147 Loss:0.019640296697616577\n",
      "Epoch 95, Batch 148 Loss:0.023220842704176903\n",
      "Epoch 95, Batch 149 Loss:0.015795866027474403\n",
      "Epoch 95, Batch 150 Loss:0.0034809578210115433\n",
      "Epoch 95, Batch 151 Loss:0.02125602774322033\n",
      "Epoch 95, Batch 152 Loss:0.012089976109564304\n",
      "Epoch 95, Batch 153 Loss:0.024881727993488312\n",
      "Epoch 95, Batch 154 Loss:0.030206460505723953\n",
      "Epoch 95, Batch 155 Loss:0.015100004151463509\n",
      "Epoch 95, Batch 156 Loss:0.01723763346672058\n",
      "Epoch 95, Batch 157 Loss:0.016647731885313988\n",
      "Epoch 95, Batch 158 Loss:0.0179732833057642\n",
      "Epoch 95, Batch 159 Loss:0.0086127407848835\n",
      "Epoch 95, Batch 160 Loss:0.03052307851612568\n",
      "Epoch 95, Batch 161 Loss:0.007490111980587244\n",
      "Epoch 95, Batch 162 Loss:0.011772658675909042\n",
      "Epoch 95, Batch 163 Loss:0.009672767482697964\n",
      "Epoch 95, Batch 164 Loss:0.032971929758787155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95, Batch 165 Loss:0.04071661829948425\n",
      "Epoch 95, Batch 166 Loss:0.0033842495176941156\n",
      "Epoch 95, Batch 167 Loss:0.0024902387522161007\n",
      "Epoch 95, Batch 168 Loss:0.0222746841609478\n",
      "Epoch 95, Batch 169 Loss:0.01443987712264061\n",
      "Epoch 95, Batch 170 Loss:0.0053498754277825356\n",
      "Epoch 95, Batch 171 Loss:0.044923681765794754\n",
      "Epoch 95, Batch 172 Loss:0.041658513247966766\n",
      "Epoch 95, Batch 173 Loss:0.013090130873024464\n",
      "Epoch 95, Batch 174 Loss:0.0585407093167305\n",
      "Epoch 95, Batch 175 Loss:0.0147002087906003\n",
      "Epoch 95, Batch 176 Loss:0.030231330543756485\n",
      "Epoch 95, Batch 177 Loss:0.03917400911450386\n",
      "Epoch 95, Batch 178 Loss:0.010883364826440811\n",
      "Epoch 95, Batch 179 Loss:0.034567929804325104\n",
      "Epoch 95, Batch 180 Loss:0.02844284102320671\n",
      "Epoch 95, Batch 181 Loss:0.026524262502789497\n",
      "Epoch 95, Batch 182 Loss:0.023788122460246086\n",
      "Epoch 95, Batch 183 Loss:0.02184910885989666\n",
      "Epoch 95, Batch 184 Loss:0.026137053966522217\n",
      "Epoch 95, Batch 185 Loss:0.014474848285317421\n",
      "Epoch 95, Batch 186 Loss:0.023719586431980133\n",
      "Epoch 95, Batch 187 Loss:0.08085599541664124\n",
      "Epoch 95, Batch 188 Loss:0.004234147258102894\n",
      "Epoch 95, Batch 189 Loss:0.014502806589007378\n",
      "Epoch 95, Batch 190 Loss:0.01687898114323616\n",
      "Epoch 95, Batch 191 Loss:0.04275626689195633\n",
      "Epoch 95, Batch 192 Loss:0.015510371886193752\n",
      "Epoch 95, Batch 193 Loss:0.051758475601673126\n",
      "Epoch 95, Batch 194 Loss:0.07614351063966751\n",
      "Epoch 95, Batch 195 Loss:0.013372551649808884\n",
      "Epoch 95, Batch 196 Loss:0.010969799011945724\n",
      "Epoch 95, Batch 197 Loss:0.05558842048048973\n",
      "Epoch 95, Batch 198 Loss:0.03422132879495621\n",
      "Epoch 95, Batch 199 Loss:0.028369620442390442\n",
      "Epoch 95, Batch 200 Loss:0.021516604349017143\n",
      "Epoch 95, Batch 201 Loss:0.007824109867215157\n",
      "Epoch 95, Batch 202 Loss:0.02579931914806366\n",
      "Epoch 95, Batch 203 Loss:0.01653166301548481\n",
      "Epoch 95, Batch 204 Loss:0.04132445901632309\n",
      "Epoch 95, Batch 205 Loss:0.018746934831142426\n",
      "Epoch 95, Batch 206 Loss:0.029518987983465195\n",
      "Epoch 95, Batch 207 Loss:0.16659478843212128\n",
      "Epoch 95, Batch 208 Loss:0.032839931547641754\n",
      "Epoch 95, Batch 209 Loss:0.019719218835234642\n",
      "Epoch 95, Batch 210 Loss:0.044039636850357056\n",
      "Epoch 95, Batch 211 Loss:0.026898540556430817\n",
      "Epoch 95, Batch 212 Loss:0.023140480741858482\n",
      "Epoch 95, Batch 213 Loss:0.01942439377307892\n",
      "Epoch 95, Batch 214 Loss:0.033202752470970154\n",
      "Epoch 95, Batch 215 Loss:0.060584716498851776\n",
      "Epoch 95, Batch 216 Loss:0.08347828686237335\n",
      "Epoch 95, Batch 217 Loss:0.03532711789011955\n",
      "Epoch 95, Batch 218 Loss:0.044046610593795776\n",
      "Epoch 95, Batch 219 Loss:0.04277053475379944\n",
      "Epoch 95, Batch 220 Loss:0.009655186906456947\n",
      "Epoch 95, Batch 221 Loss:0.03462338447570801\n",
      "Epoch 95, Batch 222 Loss:0.05247010290622711\n",
      "Epoch 95, Batch 223 Loss:0.014273757115006447\n",
      "Epoch 95, Batch 224 Loss:0.03153211250901222\n",
      "Epoch 95, Batch 225 Loss:0.005550500005483627\n",
      "Epoch 95, Batch 226 Loss:0.02529124543070793\n",
      "Epoch 95, Batch 227 Loss:0.02387944981455803\n",
      "Epoch 95, Batch 228 Loss:0.025536444038152695\n",
      "Epoch 95, Batch 229 Loss:0.01181112788617611\n",
      "Epoch 95, Batch 230 Loss:0.044388096779584885\n",
      "Epoch 95, Batch 231 Loss:0.021089298650622368\n",
      "Epoch 95, Batch 232 Loss:0.018093599006533623\n",
      "Epoch 95, Batch 233 Loss:0.07217651605606079\n",
      "Loss in this Epoch is: 7.21765160561 %\n",
      "Accuracy in this Epoch is: 88.1299972534 %\n",
      "Epoch 96, Batch 0 Loss:0.043441757559776306\n",
      "Epoch 96, Batch 1 Loss:0.01977154053747654\n",
      "Epoch 96, Batch 2 Loss:0.04805181547999382\n",
      "Epoch 96, Batch 3 Loss:0.019707074388861656\n",
      "Epoch 96, Batch 4 Loss:0.027525411918759346\n",
      "Epoch 96, Batch 5 Loss:0.06284981966018677\n",
      "Epoch 96, Batch 6 Loss:0.03462031111121178\n",
      "Epoch 96, Batch 7 Loss:0.034320492297410965\n",
      "Epoch 96, Batch 8 Loss:0.051564622670412064\n",
      "Epoch 96, Batch 9 Loss:0.014808667823672295\n",
      "Epoch 96, Batch 10 Loss:0.02985389158129692\n",
      "Epoch 96, Batch 11 Loss:0.03233844041824341\n",
      "Epoch 96, Batch 12 Loss:0.02183646522462368\n",
      "Epoch 96, Batch 13 Loss:0.02663174271583557\n",
      "Epoch 96, Batch 14 Loss:0.03462451323866844\n",
      "Epoch 96, Batch 15 Loss:0.023997673764824867\n",
      "Epoch 96, Batch 16 Loss:0.03276369348168373\n",
      "Epoch 96, Batch 17 Loss:0.007431569509208202\n",
      "Epoch 96, Batch 18 Loss:0.0343659482896328\n",
      "Epoch 96, Batch 19 Loss:0.052165597677230835\n",
      "Epoch 96, Batch 20 Loss:0.025424035266041756\n",
      "Epoch 96, Batch 21 Loss:0.013909936882555485\n",
      "Epoch 96, Batch 22 Loss:0.01763310842216015\n",
      "Epoch 96, Batch 23 Loss:0.03697062283754349\n",
      "Epoch 96, Batch 24 Loss:0.013368526473641396\n",
      "Epoch 96, Batch 25 Loss:0.01748710870742798\n",
      "Epoch 96, Batch 26 Loss:0.05746181681752205\n",
      "Epoch 96, Batch 27 Loss:0.01818087510764599\n",
      "Epoch 96, Batch 28 Loss:0.01283286139369011\n",
      "Epoch 96, Batch 29 Loss:0.009334148839116096\n",
      "Epoch 96, Batch 30 Loss:0.01651422679424286\n",
      "Epoch 96, Batch 31 Loss:0.02356862835586071\n",
      "Epoch 96, Batch 32 Loss:0.012031287886202335\n",
      "Epoch 96, Batch 33 Loss:0.002254309132695198\n",
      "Epoch 96, Batch 34 Loss:0.029009336605668068\n",
      "Epoch 96, Batch 35 Loss:0.002021957654505968\n",
      "Epoch 96, Batch 36 Loss:0.027970630675554276\n",
      "Epoch 96, Batch 37 Loss:0.02666228637099266\n",
      "Epoch 96, Batch 38 Loss:0.008270645514130592\n",
      "Epoch 96, Batch 39 Loss:0.055698610842227936\n",
      "Epoch 96, Batch 40 Loss:0.005436481907963753\n",
      "Epoch 96, Batch 41 Loss:0.0031046175863593817\n",
      "Epoch 96, Batch 42 Loss:0.059986576437950134\n",
      "Epoch 96, Batch 43 Loss:0.010830528102815151\n",
      "Epoch 96, Batch 44 Loss:0.016162417829036713\n",
      "Epoch 96, Batch 45 Loss:0.016057832166552544\n",
      "Epoch 96, Batch 46 Loss:0.023940563201904297\n",
      "Epoch 96, Batch 47 Loss:0.007002544589340687\n",
      "Epoch 96, Batch 48 Loss:0.014884674921631813\n",
      "Epoch 96, Batch 49 Loss:0.026306098327040672\n",
      "Epoch 96, Batch 50 Loss:0.016871172934770584\n",
      "Epoch 96, Batch 51 Loss:0.10108703374862671\n",
      "Epoch 96, Batch 52 Loss:0.016468483954668045\n",
      "Epoch 96, Batch 53 Loss:0.0344654880464077\n",
      "Epoch 96, Batch 54 Loss:0.02110944129526615\n",
      "Epoch 96, Batch 55 Loss:0.012707957066595554\n",
      "Epoch 96, Batch 56 Loss:0.015983199700713158\n",
      "Epoch 96, Batch 57 Loss:0.007410872261971235\n",
      "Epoch 96, Batch 58 Loss:0.0510597825050354\n",
      "Epoch 96, Batch 59 Loss:0.015674157068133354\n",
      "Epoch 96, Batch 60 Loss:0.044111475348472595\n",
      "Epoch 96, Batch 61 Loss:0.013754523359239101\n",
      "Epoch 96, Batch 62 Loss:0.0393657311797142\n",
      "Epoch 96, Batch 63 Loss:0.013887330889701843\n",
      "Epoch 96, Batch 64 Loss:0.015131518244743347\n",
      "Epoch 96, Batch 65 Loss:0.028831256553530693\n",
      "Epoch 96, Batch 66 Loss:0.009052492678165436\n",
      "Epoch 96, Batch 67 Loss:0.05043485760688782\n",
      "Epoch 96, Batch 68 Loss:0.020937124267220497\n",
      "Epoch 96, Batch 69 Loss:0.050157852470874786\n",
      "Epoch 96, Batch 70 Loss:0.010200459510087967\n",
      "Epoch 96, Batch 71 Loss:0.005464555695652962\n",
      "Epoch 96, Batch 72 Loss:0.010864900425076485\n",
      "Epoch 96, Batch 73 Loss:0.03428148850798607\n",
      "Epoch 96, Batch 74 Loss:0.017866427078843117\n",
      "Epoch 96, Batch 75 Loss:0.018895847722887993\n",
      "Epoch 96, Batch 76 Loss:0.04984339699149132\n",
      "Epoch 96, Batch 77 Loss:0.02988358959555626\n",
      "Epoch 96, Batch 78 Loss:0.0186659786850214\n",
      "Epoch 96, Batch 79 Loss:0.014655707404017448\n",
      "Epoch 96, Batch 80 Loss:0.0022340656723827124\n",
      "Epoch 96, Batch 81 Loss:0.05177305266261101\n",
      "Epoch 96, Batch 82 Loss:0.05779002234339714\n",
      "Epoch 96, Batch 83 Loss:0.01017454732209444\n",
      "Epoch 96, Batch 84 Loss:0.037167083472013474\n",
      "Epoch 96, Batch 85 Loss:0.004676684271544218\n",
      "Epoch 96, Batch 86 Loss:0.03117402084171772\n",
      "Epoch 96, Batch 87 Loss:0.020451782271265984\n",
      "Epoch 96, Batch 88 Loss:0.01121793407946825\n",
      "Epoch 96, Batch 89 Loss:0.03356633707880974\n",
      "Epoch 96, Batch 90 Loss:0.003950669430196285\n",
      "Epoch 96, Batch 91 Loss:0.004352353047579527\n",
      "Epoch 96, Batch 92 Loss:0.00947392638772726\n",
      "Epoch 96, Batch 93 Loss:0.007292104419320822\n",
      "Epoch 96, Batch 94 Loss:0.016517238691449165\n",
      "Epoch 96, Batch 95 Loss:0.018296275287866592\n",
      "Epoch 96, Batch 96 Loss:0.012623130343854427\n",
      "Epoch 96, Batch 97 Loss:0.06741973012685776\n",
      "Epoch 96, Batch 98 Loss:0.01925559714436531\n",
      "Epoch 96, Batch 99 Loss:0.048405617475509644\n",
      "Epoch 96, Batch 100 Loss:0.029215345159173012\n",
      "Epoch 96, Batch 101 Loss:0.04776071757078171\n",
      "Epoch 96, Batch 102 Loss:0.05339033901691437\n",
      "Epoch 96, Batch 103 Loss:0.0072516570799052715\n",
      "Epoch 96, Batch 104 Loss:0.012985887937247753\n",
      "Epoch 96, Batch 105 Loss:0.02067483216524124\n",
      "Epoch 96, Batch 106 Loss:0.030163675546646118\n",
      "Epoch 96, Batch 107 Loss:0.01263241283595562\n",
      "Epoch 96, Batch 108 Loss:0.023804061114788055\n",
      "Epoch 96, Batch 109 Loss:0.008126087486743927\n",
      "Epoch 96, Batch 110 Loss:0.027539392933249474\n",
      "Epoch 96, Batch 111 Loss:0.01548232976347208\n",
      "Epoch 96, Batch 112 Loss:0.038440387696027756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96, Batch 113 Loss:0.021703897044062614\n",
      "Epoch 96, Batch 114 Loss:0.05185367166996002\n",
      "Epoch 96, Batch 115 Loss:0.07709059119224548\n",
      "Epoch 96, Batch 116 Loss:0.03280029073357582\n",
      "Epoch 96, Batch 117 Loss:0.03288625180721283\n",
      "Epoch 96, Batch 118 Loss:0.01849781535565853\n",
      "Epoch 96, Batch 119 Loss:0.011140462942421436\n",
      "Epoch 96, Batch 120 Loss:0.02572275884449482\n",
      "Epoch 96, Batch 121 Loss:0.021219896152615547\n",
      "Epoch 96, Batch 122 Loss:0.006066349800676107\n",
      "Epoch 96, Batch 123 Loss:0.025337230414152145\n",
      "Epoch 96, Batch 124 Loss:0.007189788855612278\n",
      "Epoch 96, Batch 125 Loss:0.014119033701717854\n",
      "Epoch 96, Batch 126 Loss:0.02016918919980526\n",
      "Epoch 96, Batch 127 Loss:0.014017721638083458\n",
      "Epoch 96, Batch 128 Loss:0.0047307126224040985\n",
      "Epoch 96, Batch 129 Loss:0.07505454123020172\n",
      "Epoch 96, Batch 130 Loss:0.020204607397317886\n",
      "Epoch 96, Batch 131 Loss:0.04230376332998276\n",
      "Epoch 96, Batch 132 Loss:0.025457575917243958\n",
      "Epoch 96, Batch 133 Loss:0.0221155546605587\n",
      "Epoch 96, Batch 134 Loss:0.039680324494838715\n",
      "Epoch 96, Batch 135 Loss:0.035873379558324814\n",
      "Epoch 96, Batch 136 Loss:0.012513224966824055\n",
      "Epoch 96, Batch 137 Loss:0.011350185610353947\n",
      "Epoch 96, Batch 138 Loss:0.021785927936434746\n",
      "Epoch 96, Batch 139 Loss:0.01153603009879589\n",
      "Epoch 96, Batch 140 Loss:0.019470490515232086\n",
      "Epoch 96, Batch 141 Loss:0.017853915691375732\n",
      "Epoch 96, Batch 142 Loss:0.02642185240983963\n",
      "Epoch 96, Batch 143 Loss:0.002669532783329487\n",
      "Epoch 96, Batch 144 Loss:0.03221161291003227\n",
      "Epoch 96, Batch 145 Loss:0.035581327974796295\n",
      "Epoch 96, Batch 146 Loss:0.010033986531198025\n",
      "Epoch 96, Batch 147 Loss:0.006125575862824917\n",
      "Epoch 96, Batch 148 Loss:0.02730293571949005\n",
      "Epoch 96, Batch 149 Loss:0.00864703580737114\n",
      "Epoch 96, Batch 150 Loss:0.0066324258223176\n",
      "Epoch 96, Batch 151 Loss:0.01830204576253891\n",
      "Epoch 96, Batch 152 Loss:0.017140932381153107\n",
      "Epoch 96, Batch 153 Loss:0.01995798386633396\n",
      "Epoch 96, Batch 154 Loss:0.003776673460379243\n",
      "Epoch 96, Batch 155 Loss:0.009882518090307713\n",
      "Epoch 96, Batch 156 Loss:0.010300015099346638\n",
      "Epoch 96, Batch 157 Loss:0.012860093265771866\n",
      "Epoch 96, Batch 158 Loss:0.007511847652494907\n",
      "Epoch 96, Batch 159 Loss:0.009654974564909935\n",
      "Epoch 96, Batch 160 Loss:0.06044760346412659\n",
      "Epoch 96, Batch 161 Loss:0.018724119290709496\n",
      "Epoch 96, Batch 162 Loss:0.011715859174728394\n",
      "Epoch 96, Batch 163 Loss:0.02088410221040249\n",
      "Epoch 96, Batch 164 Loss:0.0030455077067017555\n",
      "Epoch 96, Batch 165 Loss:0.0031867302022874355\n",
      "Epoch 96, Batch 166 Loss:0.013044273480772972\n",
      "Epoch 96, Batch 167 Loss:0.012818156741559505\n",
      "Epoch 96, Batch 168 Loss:0.00974730309098959\n",
      "Epoch 96, Batch 169 Loss:0.01517678052186966\n",
      "Epoch 96, Batch 170 Loss:0.012644365429878235\n",
      "Epoch 96, Batch 171 Loss:0.01303152646869421\n",
      "Epoch 96, Batch 172 Loss:0.006960509344935417\n",
      "Epoch 96, Batch 173 Loss:0.0015759228263050318\n",
      "Epoch 96, Batch 174 Loss:0.0071930307894945145\n",
      "Epoch 96, Batch 175 Loss:0.002807623241096735\n",
      "Epoch 96, Batch 176 Loss:0.0040777092799544334\n",
      "Epoch 96, Batch 177 Loss:0.025797057896852493\n",
      "Epoch 96, Batch 178 Loss:0.010153700597584248\n",
      "Epoch 96, Batch 179 Loss:0.0068358611315488815\n",
      "Epoch 96, Batch 180 Loss:0.002738925628364086\n",
      "Epoch 96, Batch 181 Loss:0.006327624432742596\n",
      "Epoch 96, Batch 182 Loss:0.004800225142389536\n",
      "Epoch 96, Batch 183 Loss:0.0018046136246994138\n",
      "Epoch 96, Batch 184 Loss:0.006429620087146759\n",
      "Epoch 96, Batch 185 Loss:0.002656199038028717\n",
      "Epoch 96, Batch 186 Loss:0.004949960857629776\n",
      "Epoch 96, Batch 187 Loss:0.004189128056168556\n",
      "Epoch 96, Batch 188 Loss:0.013312943279743195\n",
      "Epoch 96, Batch 189 Loss:0.003751297714188695\n",
      "Epoch 96, Batch 190 Loss:0.002594012999907136\n",
      "Epoch 96, Batch 191 Loss:0.025425542145967484\n",
      "Epoch 96, Batch 192 Loss:0.03017069399356842\n",
      "Epoch 96, Batch 193 Loss:0.07775556296110153\n",
      "Epoch 96, Batch 194 Loss:0.08257665485143661\n",
      "Epoch 96, Batch 195 Loss:0.017671452835202217\n",
      "Epoch 96, Batch 196 Loss:0.018632633611559868\n",
      "Epoch 96, Batch 197 Loss:0.09005627036094666\n",
      "Epoch 96, Batch 198 Loss:0.07295683026313782\n",
      "Epoch 96, Batch 199 Loss:0.051075778901576996\n",
      "Epoch 96, Batch 200 Loss:0.10234212875366211\n",
      "Epoch 96, Batch 201 Loss:0.029747337102890015\n",
      "Epoch 96, Batch 202 Loss:0.07741934806108475\n",
      "Epoch 96, Batch 203 Loss:0.06656306982040405\n",
      "Epoch 96, Batch 204 Loss:0.042927950620651245\n",
      "Epoch 96, Batch 205 Loss:0.03779672086238861\n",
      "Epoch 96, Batch 206 Loss:0.06448650360107422\n",
      "Epoch 96, Batch 207 Loss:0.02698379196226597\n",
      "Epoch 96, Batch 208 Loss:0.07314203679561615\n",
      "Epoch 96, Batch 209 Loss:0.08515114337205887\n",
      "Epoch 96, Batch 210 Loss:0.027398455888032913\n",
      "Epoch 96, Batch 211 Loss:0.10923377424478531\n",
      "Epoch 96, Batch 212 Loss:0.0842350572347641\n",
      "Epoch 96, Batch 213 Loss:0.05037336423993111\n",
      "Epoch 96, Batch 214 Loss:0.08511091768741608\n",
      "Epoch 96, Batch 215 Loss:0.04389302060008049\n",
      "Epoch 96, Batch 216 Loss:0.06540390849113464\n",
      "Epoch 96, Batch 217 Loss:0.05859284847974777\n",
      "Epoch 96, Batch 218 Loss:0.08834942430257797\n",
      "Epoch 96, Batch 219 Loss:0.060358498245477676\n",
      "Epoch 96, Batch 220 Loss:0.05133680999279022\n",
      "Epoch 96, Batch 221 Loss:0.03714872896671295\n",
      "Epoch 96, Batch 222 Loss:0.07102704048156738\n",
      "Epoch 96, Batch 223 Loss:0.050123997032642365\n",
      "Epoch 96, Batch 224 Loss:0.032673995941877365\n",
      "Epoch 96, Batch 225 Loss:0.06656980514526367\n",
      "Epoch 96, Batch 226 Loss:0.023209216073155403\n",
      "Epoch 96, Batch 227 Loss:0.02680046111345291\n",
      "Epoch 96, Batch 228 Loss:0.040818147361278534\n",
      "Epoch 96, Batch 229 Loss:0.019217142835259438\n",
      "Epoch 96, Batch 230 Loss:0.06008892506361008\n",
      "Epoch 96, Batch 231 Loss:0.0216489527374506\n",
      "Epoch 96, Batch 232 Loss:0.0426458977162838\n",
      "Epoch 96, Batch 233 Loss:0.05873185396194458\n",
      "Loss in this Epoch is: 5.87318539619 %\n",
      "Accuracy in this Epoch is: 88.5100007057 %\n",
      "Epoch 97, Batch 0 Loss:0.03431972116231918\n",
      "Epoch 97, Batch 1 Loss:0.012618999928236008\n",
      "Epoch 97, Batch 2 Loss:0.03336208313703537\n",
      "Epoch 97, Batch 3 Loss:0.008945392444729805\n",
      "Epoch 97, Batch 4 Loss:0.027714714407920837\n",
      "Epoch 97, Batch 5 Loss:0.015620220452547073\n",
      "Epoch 97, Batch 6 Loss:0.026660194620490074\n",
      "Epoch 97, Batch 7 Loss:0.008312268182635307\n",
      "Epoch 97, Batch 8 Loss:0.01756102219223976\n",
      "Epoch 97, Batch 9 Loss:0.006514345295727253\n",
      "Epoch 97, Batch 10 Loss:0.00632118433713913\n",
      "Epoch 97, Batch 11 Loss:0.011030210182070732\n",
      "Epoch 97, Batch 12 Loss:0.008078359067440033\n",
      "Epoch 97, Batch 13 Loss:0.005842016544193029\n",
      "Epoch 97, Batch 14 Loss:0.03088265098631382\n",
      "Epoch 97, Batch 15 Loss:0.016278831288218498\n",
      "Epoch 97, Batch 16 Loss:0.011238914914429188\n",
      "Epoch 97, Batch 17 Loss:0.047604210674762726\n",
      "Epoch 97, Batch 18 Loss:0.02178344875574112\n",
      "Epoch 97, Batch 19 Loss:0.00856850016862154\n",
      "Epoch 97, Batch 20 Loss:0.02974601276218891\n",
      "Epoch 97, Batch 21 Loss:0.01004649605602026\n",
      "Epoch 97, Batch 22 Loss:0.034444741904735565\n",
      "Epoch 97, Batch 23 Loss:0.013525632210075855\n",
      "Epoch 97, Batch 24 Loss:0.020079346373677254\n",
      "Epoch 97, Batch 25 Loss:0.005940271075814962\n",
      "Epoch 97, Batch 26 Loss:0.03513157740235329\n",
      "Epoch 97, Batch 27 Loss:0.014753498136997223\n",
      "Epoch 97, Batch 28 Loss:0.0033921317663043737\n",
      "Epoch 97, Batch 29 Loss:0.01958952099084854\n",
      "Epoch 97, Batch 30 Loss:0.019547238945961\n",
      "Epoch 97, Batch 31 Loss:0.018866676837205887\n",
      "Epoch 97, Batch 32 Loss:0.005670447368174791\n",
      "Epoch 97, Batch 33 Loss:0.005055008921772242\n",
      "Epoch 97, Batch 34 Loss:0.015346250496804714\n",
      "Epoch 97, Batch 35 Loss:0.009096340276300907\n",
      "Epoch 97, Batch 36 Loss:0.013171990402042866\n",
      "Epoch 97, Batch 37 Loss:0.014310342259705067\n",
      "Epoch 97, Batch 38 Loss:0.0036858869716525078\n",
      "Epoch 97, Batch 39 Loss:0.016686877235770226\n",
      "Epoch 97, Batch 40 Loss:0.07285266369581223\n",
      "Epoch 97, Batch 41 Loss:0.02604779228568077\n",
      "Epoch 97, Batch 42 Loss:0.01632733829319477\n",
      "Epoch 97, Batch 43 Loss:0.010662521235644817\n",
      "Epoch 97, Batch 44 Loss:0.01215321198105812\n",
      "Epoch 97, Batch 45 Loss:0.015870142728090286\n",
      "Epoch 97, Batch 46 Loss:0.01492723636329174\n",
      "Epoch 97, Batch 47 Loss:0.005996915511786938\n",
      "Epoch 97, Batch 48 Loss:0.01813957281410694\n",
      "Epoch 97, Batch 49 Loss:0.021452393382787704\n",
      "Epoch 97, Batch 50 Loss:0.005554245784878731\n",
      "Epoch 97, Batch 51 Loss:0.02373276837170124\n",
      "Epoch 97, Batch 52 Loss:0.010547579266130924\n",
      "Epoch 97, Batch 53 Loss:0.018980829045176506\n",
      "Epoch 97, Batch 54 Loss:0.019670670852065086\n",
      "Epoch 97, Batch 55 Loss:0.015775339677929878\n",
      "Epoch 97, Batch 56 Loss:0.023860281333327293\n",
      "Epoch 97, Batch 57 Loss:0.02275252528488636\n",
      "Epoch 97, Batch 58 Loss:0.021318992599844933\n",
      "Epoch 97, Batch 59 Loss:0.0034799817949533463\n",
      "Epoch 97, Batch 60 Loss:0.013544630259275436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97, Batch 61 Loss:0.016349120065569878\n",
      "Epoch 97, Batch 62 Loss:0.01354068424552679\n",
      "Epoch 97, Batch 63 Loss:0.0346156507730484\n",
      "Epoch 97, Batch 64 Loss:0.015120677649974823\n",
      "Epoch 97, Batch 65 Loss:0.0145295774564147\n",
      "Epoch 97, Batch 66 Loss:0.01858513429760933\n",
      "Epoch 97, Batch 67 Loss:0.006519983988255262\n",
      "Epoch 97, Batch 68 Loss:0.01515684649348259\n",
      "Epoch 97, Batch 69 Loss:0.022944413125514984\n",
      "Epoch 97, Batch 70 Loss:0.01956513524055481\n",
      "Epoch 97, Batch 71 Loss:0.008967779576778412\n",
      "Epoch 97, Batch 72 Loss:0.021033141762018204\n",
      "Epoch 97, Batch 73 Loss:0.012586012482643127\n",
      "Epoch 97, Batch 74 Loss:0.01594064198434353\n",
      "Epoch 97, Batch 75 Loss:0.0318361334502697\n",
      "Epoch 97, Batch 76 Loss:0.03708173334598541\n",
      "Epoch 97, Batch 77 Loss:0.010033505037426949\n",
      "Epoch 97, Batch 78 Loss:0.008090177550911903\n",
      "Epoch 97, Batch 79 Loss:0.006045965943485498\n",
      "Epoch 97, Batch 80 Loss:0.01887226477265358\n",
      "Epoch 97, Batch 81 Loss:0.012981376610696316\n",
      "Epoch 97, Batch 82 Loss:0.03571309149265289\n",
      "Epoch 97, Batch 83 Loss:0.04694083705544472\n",
      "Epoch 97, Batch 84 Loss:0.010471047833561897\n",
      "Epoch 97, Batch 85 Loss:0.02906419336795807\n",
      "Epoch 97, Batch 86 Loss:0.01611269824206829\n",
      "Epoch 97, Batch 87 Loss:0.04095712676644325\n",
      "Epoch 97, Batch 88 Loss:0.011288965120911598\n",
      "Epoch 97, Batch 89 Loss:0.011171088553965092\n",
      "Epoch 97, Batch 90 Loss:0.009184611029922962\n",
      "Epoch 97, Batch 91 Loss:0.010651461780071259\n",
      "Epoch 97, Batch 92 Loss:0.030618466436862946\n",
      "Epoch 97, Batch 93 Loss:0.005398413632065058\n",
      "Epoch 97, Batch 94 Loss:0.007737959269434214\n",
      "Epoch 97, Batch 95 Loss:0.013580049388110638\n",
      "Epoch 97, Batch 96 Loss:0.021583756431937218\n",
      "Epoch 97, Batch 97 Loss:0.009100401774048805\n",
      "Epoch 97, Batch 98 Loss:0.018447259441018105\n",
      "Epoch 97, Batch 99 Loss:0.016223374754190445\n",
      "Epoch 97, Batch 100 Loss:0.03085341677069664\n",
      "Epoch 97, Batch 101 Loss:0.012327314354479313\n",
      "Epoch 97, Batch 102 Loss:0.010280498303472996\n",
      "Epoch 97, Batch 103 Loss:0.01897921785712242\n",
      "Epoch 97, Batch 104 Loss:0.018542617559432983\n",
      "Epoch 97, Batch 105 Loss:0.016525419428944588\n",
      "Epoch 97, Batch 106 Loss:0.03822698816657066\n",
      "Epoch 97, Batch 107 Loss:0.004371295217424631\n",
      "Epoch 97, Batch 108 Loss:0.018231887370347977\n",
      "Epoch 97, Batch 109 Loss:0.009840043261647224\n",
      "Epoch 97, Batch 110 Loss:0.00956004485487938\n",
      "Epoch 97, Batch 111 Loss:0.017442695796489716\n",
      "Epoch 97, Batch 112 Loss:0.020278379321098328\n",
      "Epoch 97, Batch 113 Loss:0.020977884531021118\n",
      "Epoch 97, Batch 114 Loss:0.006958067417144775\n",
      "Epoch 97, Batch 115 Loss:0.010935759171843529\n",
      "Epoch 97, Batch 116 Loss:0.009278373792767525\n",
      "Epoch 97, Batch 117 Loss:0.05661047250032425\n",
      "Epoch 97, Batch 118 Loss:0.003443164750933647\n",
      "Epoch 97, Batch 119 Loss:0.009365927428007126\n",
      "Epoch 97, Batch 120 Loss:0.002781397895887494\n",
      "Epoch 97, Batch 121 Loss:0.024158034473657608\n",
      "Epoch 97, Batch 122 Loss:0.0029716147109866142\n",
      "Epoch 97, Batch 123 Loss:0.05829280614852905\n",
      "Epoch 97, Batch 124 Loss:0.0030579441227018833\n",
      "Epoch 97, Batch 125 Loss:0.010343817062675953\n",
      "Epoch 97, Batch 126 Loss:0.025247007608413696\n",
      "Epoch 97, Batch 127 Loss:0.009324122220277786\n",
      "Epoch 97, Batch 128 Loss:0.006319316569715738\n",
      "Epoch 97, Batch 129 Loss:0.011951006017625332\n",
      "Epoch 97, Batch 130 Loss:0.0021651475690305233\n",
      "Epoch 97, Batch 131 Loss:0.02102774940431118\n",
      "Epoch 97, Batch 132 Loss:0.013398532755672932\n",
      "Epoch 97, Batch 133 Loss:0.005524159874767065\n",
      "Epoch 97, Batch 134 Loss:0.017111562192440033\n",
      "Epoch 97, Batch 135 Loss:0.008470666594803333\n",
      "Epoch 97, Batch 136 Loss:0.043906982988119125\n",
      "Epoch 97, Batch 137 Loss:0.003378945402801037\n",
      "Epoch 97, Batch 138 Loss:0.005189254879951477\n",
      "Epoch 97, Batch 139 Loss:0.016377227380871773\n",
      "Epoch 97, Batch 140 Loss:0.03601769357919693\n",
      "Epoch 97, Batch 141 Loss:0.005966192111372948\n",
      "Epoch 97, Batch 142 Loss:0.012616261839866638\n",
      "Epoch 97, Batch 143 Loss:0.026893354952335358\n",
      "Epoch 97, Batch 144 Loss:0.00507002230733633\n",
      "Epoch 97, Batch 145 Loss:0.004850796423852444\n",
      "Epoch 97, Batch 146 Loss:0.010679293423891068\n",
      "Epoch 97, Batch 147 Loss:0.008719390258193016\n",
      "Epoch 97, Batch 148 Loss:0.025344552472233772\n",
      "Epoch 97, Batch 149 Loss:0.009592602960765362\n",
      "Epoch 97, Batch 150 Loss:0.009879793971776962\n",
      "Epoch 97, Batch 151 Loss:0.03311161696910858\n",
      "Epoch 97, Batch 152 Loss:0.03335968777537346\n",
      "Epoch 97, Batch 153 Loss:0.016551993787288666\n",
      "Epoch 97, Batch 154 Loss:0.013708770275115967\n",
      "Epoch 97, Batch 155 Loss:0.00328744831494987\n",
      "Epoch 97, Batch 156 Loss:0.025775576010346413\n",
      "Epoch 97, Batch 157 Loss:0.043815359473228455\n",
      "Epoch 97, Batch 158 Loss:0.02014472708106041\n",
      "Epoch 97, Batch 159 Loss:0.005316771101206541\n",
      "Epoch 97, Batch 160 Loss:0.013989514671266079\n",
      "Epoch 97, Batch 161 Loss:0.016507823020219803\n",
      "Epoch 97, Batch 162 Loss:0.03843412920832634\n",
      "Epoch 97, Batch 163 Loss:0.004930803552269936\n",
      "Epoch 97, Batch 164 Loss:0.02266760915517807\n",
      "Epoch 97, Batch 165 Loss:0.007274008356034756\n",
      "Epoch 97, Batch 166 Loss:0.01428991463035345\n",
      "Epoch 97, Batch 167 Loss:0.00573513749986887\n",
      "Epoch 97, Batch 168 Loss:0.05458178371191025\n",
      "Epoch 97, Batch 169 Loss:0.00474627735093236\n",
      "Epoch 97, Batch 170 Loss:0.001293546287342906\n",
      "Epoch 97, Batch 171 Loss:0.013009021058678627\n",
      "Epoch 97, Batch 172 Loss:0.00939775351434946\n",
      "Epoch 97, Batch 173 Loss:0.006118018180131912\n",
      "Epoch 97, Batch 174 Loss:0.010641703382134438\n",
      "Epoch 97, Batch 175 Loss:0.021675463765859604\n",
      "Epoch 97, Batch 176 Loss:0.01667146570980549\n",
      "Epoch 97, Batch 177 Loss:0.015676209703087807\n",
      "Epoch 97, Batch 178 Loss:0.0454852394759655\n",
      "Epoch 97, Batch 179 Loss:0.008882924914360046\n",
      "Epoch 97, Batch 180 Loss:0.009270431473851204\n",
      "Epoch 97, Batch 181 Loss:0.015060918405652046\n",
      "Epoch 97, Batch 182 Loss:0.005388708785176277\n",
      "Epoch 97, Batch 183 Loss:0.08651464432477951\n",
      "Epoch 97, Batch 184 Loss:0.015380468219518661\n",
      "Epoch 97, Batch 185 Loss:0.011342129670083523\n",
      "Epoch 97, Batch 186 Loss:0.046455301344394684\n",
      "Epoch 97, Batch 187 Loss:0.0070287203416228294\n",
      "Epoch 97, Batch 188 Loss:0.009158719331026077\n",
      "Epoch 97, Batch 189 Loss:0.019665226340293884\n",
      "Epoch 97, Batch 190 Loss:0.016885120421648026\n",
      "Epoch 97, Batch 191 Loss:0.00902292039245367\n",
      "Epoch 97, Batch 192 Loss:0.038246039301157\n",
      "Epoch 97, Batch 193 Loss:0.010018352419137955\n",
      "Epoch 97, Batch 194 Loss:0.012677391991019249\n",
      "Epoch 97, Batch 195 Loss:0.01400886569172144\n",
      "Epoch 97, Batch 196 Loss:0.014193834736943245\n",
      "Epoch 97, Batch 197 Loss:0.00667413929477334\n",
      "Epoch 97, Batch 198 Loss:0.0192684605717659\n",
      "Epoch 97, Batch 199 Loss:0.009717416018247604\n",
      "Epoch 97, Batch 200 Loss:0.0572068952023983\n",
      "Epoch 97, Batch 201 Loss:0.009416315704584122\n",
      "Epoch 97, Batch 202 Loss:0.015575679019093513\n",
      "Epoch 97, Batch 203 Loss:0.006319839507341385\n",
      "Epoch 97, Batch 204 Loss:0.00747948233038187\n",
      "Epoch 97, Batch 205 Loss:0.002353824209421873\n",
      "Epoch 97, Batch 206 Loss:0.006561860907822847\n",
      "Epoch 97, Batch 207 Loss:0.0018984030466526747\n",
      "Epoch 97, Batch 208 Loss:0.018104512244462967\n",
      "Epoch 97, Batch 209 Loss:0.002196670975536108\n",
      "Epoch 97, Batch 210 Loss:0.019353188574314117\n",
      "Epoch 97, Batch 211 Loss:0.012996460311114788\n",
      "Epoch 97, Batch 212 Loss:0.0488000325858593\n",
      "Epoch 97, Batch 213 Loss:0.04698357731103897\n",
      "Epoch 97, Batch 214 Loss:0.006953614763915539\n",
      "Epoch 97, Batch 215 Loss:0.021785367280244827\n",
      "Epoch 97, Batch 216 Loss:0.0030421896371990442\n",
      "Epoch 97, Batch 217 Loss:0.0035643051378428936\n",
      "Epoch 97, Batch 218 Loss:0.05629858374595642\n",
      "Epoch 97, Batch 219 Loss:0.002705010585486889\n",
      "Epoch 97, Batch 220 Loss:0.024946527555584908\n",
      "Epoch 97, Batch 221 Loss:0.009080272167921066\n",
      "Epoch 97, Batch 222 Loss:0.010984349995851517\n",
      "Epoch 97, Batch 223 Loss:0.01611793413758278\n",
      "Epoch 97, Batch 224 Loss:0.04512915015220642\n",
      "Epoch 97, Batch 225 Loss:0.05332692340016365\n",
      "Epoch 97, Batch 226 Loss:0.04367358982563019\n",
      "Epoch 97, Batch 227 Loss:0.10592541843652725\n",
      "Epoch 97, Batch 228 Loss:0.026964306831359863\n",
      "Epoch 97, Batch 229 Loss:0.008434951305389404\n",
      "Epoch 97, Batch 230 Loss:0.05969206243753433\n",
      "Epoch 97, Batch 231 Loss:0.011529999785125256\n",
      "Epoch 97, Batch 232 Loss:0.02343272790312767\n",
      "Epoch 97, Batch 233 Loss:0.06028538942337036\n",
      "Loss in this Epoch is: 6.02853894234 %\n",
      "Accuracy in this Epoch is: 88.2499992847 %\n",
      "Epoch 98, Batch 0 Loss:0.03172307834029198\n",
      "Epoch 98, Batch 1 Loss:0.015130961313843727\n",
      "Epoch 98, Batch 2 Loss:0.03362109139561653\n",
      "Epoch 98, Batch 3 Loss:0.02239789254963398\n",
      "Epoch 98, Batch 4 Loss:0.01925712451338768\n",
      "Epoch 98, Batch 5 Loss:0.04879621043801308\n",
      "Epoch 98, Batch 6 Loss:0.02993539534509182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Batch 7 Loss:0.04123450815677643\n",
      "Epoch 98, Batch 8 Loss:0.07346785068511963\n",
      "Epoch 98, Batch 9 Loss:0.049410320818424225\n",
      "Epoch 98, Batch 10 Loss:0.040833670645952225\n",
      "Epoch 98, Batch 11 Loss:0.052921272814273834\n",
      "Epoch 98, Batch 12 Loss:0.0404324047267437\n",
      "Epoch 98, Batch 13 Loss:0.024084055796265602\n",
      "Epoch 98, Batch 14 Loss:0.044944167137145996\n",
      "Epoch 98, Batch 15 Loss:0.08589203655719757\n",
      "Epoch 98, Batch 16 Loss:0.08452869206666946\n",
      "Epoch 98, Batch 17 Loss:0.020665176212787628\n",
      "Epoch 98, Batch 18 Loss:0.023327870294451714\n",
      "Epoch 98, Batch 19 Loss:0.009714705869555473\n",
      "Epoch 98, Batch 20 Loss:0.04569908231496811\n",
      "Epoch 98, Batch 21 Loss:0.04369616135954857\n",
      "Epoch 98, Batch 22 Loss:0.016667582094669342\n",
      "Epoch 98, Batch 23 Loss:0.04263715818524361\n",
      "Epoch 98, Batch 24 Loss:0.14877308905124664\n",
      "Epoch 98, Batch 25 Loss:0.0562005452811718\n",
      "Epoch 98, Batch 26 Loss:0.04834013804793358\n",
      "Epoch 98, Batch 27 Loss:0.010547742247581482\n",
      "Epoch 98, Batch 28 Loss:0.020315075293183327\n",
      "Epoch 98, Batch 29 Loss:0.0460488423705101\n",
      "Epoch 98, Batch 30 Loss:0.019173815846443176\n",
      "Epoch 98, Batch 31 Loss:0.04622672498226166\n",
      "Epoch 98, Batch 32 Loss:0.04312475770711899\n",
      "Epoch 98, Batch 33 Loss:0.041488178074359894\n",
      "Epoch 98, Batch 34 Loss:0.018984120339155197\n",
      "Epoch 98, Batch 35 Loss:0.040567610412836075\n",
      "Epoch 98, Batch 36 Loss:0.019285662099719048\n",
      "Epoch 98, Batch 37 Loss:0.02899177372455597\n",
      "Epoch 98, Batch 38 Loss:0.12516333162784576\n",
      "Epoch 98, Batch 39 Loss:0.04406237602233887\n",
      "Epoch 98, Batch 40 Loss:0.041294172406196594\n",
      "Epoch 98, Batch 41 Loss:0.07204106450080872\n",
      "Epoch 98, Batch 42 Loss:0.04896450787782669\n",
      "Epoch 98, Batch 43 Loss:0.03162625432014465\n",
      "Epoch 98, Batch 44 Loss:0.06633608043193817\n",
      "Epoch 98, Batch 45 Loss:0.08734426647424698\n",
      "Epoch 98, Batch 46 Loss:0.07133572548627853\n",
      "Epoch 98, Batch 47 Loss:0.02431047521531582\n",
      "Epoch 98, Batch 48 Loss:0.041472744196653366\n",
      "Epoch 98, Batch 49 Loss:0.016248617321252823\n",
      "Epoch 98, Batch 50 Loss:0.057877250015735626\n",
      "Epoch 98, Batch 51 Loss:0.05345620587468147\n",
      "Epoch 98, Batch 52 Loss:0.04532070457935333\n",
      "Epoch 98, Batch 53 Loss:0.069723941385746\n",
      "Epoch 98, Batch 54 Loss:0.023237153887748718\n",
      "Epoch 98, Batch 55 Loss:0.01885867491364479\n",
      "Epoch 98, Batch 56 Loss:0.038594942539930344\n",
      "Epoch 98, Batch 57 Loss:0.09286405891180038\n",
      "Epoch 98, Batch 58 Loss:0.043973322957754135\n",
      "Epoch 98, Batch 59 Loss:0.05335414037108421\n",
      "Epoch 98, Batch 60 Loss:0.03605291619896889\n",
      "Epoch 98, Batch 61 Loss:0.03295578435063362\n",
      "Epoch 98, Batch 62 Loss:0.07870425283908844\n",
      "Epoch 98, Batch 63 Loss:0.04316747561097145\n",
      "Epoch 98, Batch 64 Loss:0.06886456161737442\n",
      "Epoch 98, Batch 65 Loss:0.022295812144875526\n",
      "Epoch 98, Batch 66 Loss:0.012717757374048233\n",
      "Epoch 98, Batch 67 Loss:0.034988973289728165\n",
      "Epoch 98, Batch 68 Loss:0.0695810317993164\n",
      "Epoch 98, Batch 69 Loss:0.0234859399497509\n",
      "Epoch 98, Batch 70 Loss:0.019659796729683876\n",
      "Epoch 98, Batch 71 Loss:0.03926997259259224\n",
      "Epoch 98, Batch 72 Loss:0.04349673166871071\n",
      "Epoch 98, Batch 73 Loss:0.054018620401620865\n",
      "Epoch 98, Batch 74 Loss:0.03136562183499336\n",
      "Epoch 98, Batch 75 Loss:0.024900712072849274\n",
      "Epoch 98, Batch 76 Loss:0.0894165113568306\n",
      "Epoch 98, Batch 77 Loss:0.051496993750333786\n",
      "Epoch 98, Batch 78 Loss:0.07804135978221893\n",
      "Epoch 98, Batch 79 Loss:0.03868885338306427\n",
      "Epoch 98, Batch 80 Loss:0.01103261113166809\n",
      "Epoch 98, Batch 81 Loss:0.02355525642633438\n",
      "Epoch 98, Batch 82 Loss:0.025431890040636063\n",
      "Epoch 98, Batch 83 Loss:0.15811187028884888\n",
      "Epoch 98, Batch 84 Loss:0.042173948138952255\n",
      "Epoch 98, Batch 85 Loss:0.02109231799840927\n",
      "Epoch 98, Batch 86 Loss:0.009928609244525433\n",
      "Epoch 98, Batch 87 Loss:0.021274682134389877\n",
      "Epoch 98, Batch 88 Loss:0.023041335865855217\n",
      "Epoch 98, Batch 89 Loss:0.042453717440366745\n",
      "Epoch 98, Batch 90 Loss:0.018941642716526985\n",
      "Epoch 98, Batch 91 Loss:0.05033286660909653\n",
      "Epoch 98, Batch 92 Loss:0.05527973920106888\n",
      "Epoch 98, Batch 93 Loss:0.041766200214624405\n",
      "Epoch 98, Batch 94 Loss:0.012965518049895763\n",
      "Epoch 98, Batch 95 Loss:0.029539721086621284\n",
      "Epoch 98, Batch 96 Loss:0.025474390015006065\n",
      "Epoch 98, Batch 97 Loss:0.006085037719458342\n",
      "Epoch 98, Batch 98 Loss:0.021274384111166\n",
      "Epoch 98, Batch 99 Loss:0.014102568849921227\n",
      "Epoch 98, Batch 100 Loss:0.03417991101741791\n",
      "Epoch 98, Batch 101 Loss:0.018750682473182678\n",
      "Epoch 98, Batch 102 Loss:0.02803964912891388\n",
      "Epoch 98, Batch 103 Loss:0.020807988941669464\n",
      "Epoch 98, Batch 104 Loss:0.011011296883225441\n",
      "Epoch 98, Batch 105 Loss:0.0165309626609087\n",
      "Epoch 98, Batch 106 Loss:0.021375739946961403\n",
      "Epoch 98, Batch 107 Loss:0.004777856171131134\n",
      "Epoch 98, Batch 108 Loss:0.013501714915037155\n",
      "Epoch 98, Batch 109 Loss:0.021803148090839386\n",
      "Epoch 98, Batch 110 Loss:0.011264756321907043\n",
      "Epoch 98, Batch 111 Loss:0.017382895573973656\n",
      "Epoch 98, Batch 112 Loss:0.013099230825901031\n",
      "Epoch 98, Batch 113 Loss:0.028647862374782562\n",
      "Epoch 98, Batch 114 Loss:0.018424158915877342\n",
      "Epoch 98, Batch 115 Loss:0.04169715195894241\n",
      "Epoch 98, Batch 116 Loss:0.046583641320466995\n",
      "Epoch 98, Batch 117 Loss:0.0242448840290308\n",
      "Epoch 98, Batch 118 Loss:0.010642988607287407\n",
      "Epoch 98, Batch 119 Loss:0.02342626079916954\n",
      "Epoch 98, Batch 120 Loss:0.00746565219014883\n",
      "Epoch 98, Batch 121 Loss:0.016285592690110207\n",
      "Epoch 98, Batch 122 Loss:0.007959404960274696\n",
      "Epoch 98, Batch 123 Loss:0.005196233280003071\n",
      "Epoch 98, Batch 124 Loss:0.01086430810391903\n",
      "Epoch 98, Batch 125 Loss:0.01677863672375679\n",
      "Epoch 98, Batch 126 Loss:0.003519113641232252\n",
      "Epoch 98, Batch 127 Loss:0.02159685641527176\n",
      "Epoch 98, Batch 128 Loss:0.010129418224096298\n",
      "Epoch 98, Batch 129 Loss:0.004682411439716816\n",
      "Epoch 98, Batch 130 Loss:0.029889486730098724\n",
      "Epoch 98, Batch 131 Loss:0.01659056916832924\n",
      "Epoch 98, Batch 132 Loss:0.0163519736379385\n",
      "Epoch 98, Batch 133 Loss:0.014693893492221832\n",
      "Epoch 98, Batch 134 Loss:0.01995166763663292\n",
      "Epoch 98, Batch 135 Loss:0.0432540625333786\n",
      "Epoch 98, Batch 136 Loss:0.017635289579629898\n",
      "Epoch 98, Batch 137 Loss:0.033224500715732574\n",
      "Epoch 98, Batch 138 Loss:0.012136779725551605\n",
      "Epoch 98, Batch 139 Loss:0.01294127106666565\n",
      "Epoch 98, Batch 140 Loss:0.017152735963463783\n",
      "Epoch 98, Batch 141 Loss:0.023369871079921722\n",
      "Epoch 98, Batch 142 Loss:0.02865968644618988\n",
      "Epoch 98, Batch 143 Loss:0.02475462667644024\n",
      "Epoch 98, Batch 144 Loss:0.050677746534347534\n",
      "Epoch 98, Batch 145 Loss:0.020974591374397278\n",
      "Epoch 98, Batch 146 Loss:0.04824104160070419\n",
      "Epoch 98, Batch 147 Loss:0.04567569121718407\n",
      "Epoch 98, Batch 148 Loss:0.02434748038649559\n",
      "Epoch 98, Batch 149 Loss:0.00961314607411623\n",
      "Epoch 98, Batch 150 Loss:0.011199251748621464\n",
      "Epoch 98, Batch 151 Loss:0.0259232334792614\n",
      "Epoch 98, Batch 152 Loss:0.01264986116439104\n",
      "Epoch 98, Batch 153 Loss:0.012623948976397514\n",
      "Epoch 98, Batch 154 Loss:0.02530696429312229\n",
      "Epoch 98, Batch 155 Loss:0.005280902609229088\n",
      "Epoch 98, Batch 156 Loss:0.012762927450239658\n",
      "Epoch 98, Batch 157 Loss:0.02317618578672409\n",
      "Epoch 98, Batch 158 Loss:0.03822687268257141\n",
      "Epoch 98, Batch 159 Loss:0.046732716262340546\n",
      "Epoch 98, Batch 160 Loss:0.03225670009851456\n",
      "Epoch 98, Batch 161 Loss:0.023182354867458344\n",
      "Epoch 98, Batch 162 Loss:0.03203720599412918\n",
      "Epoch 98, Batch 163 Loss:0.05807027593255043\n",
      "Epoch 98, Batch 164 Loss:0.027485815808176994\n",
      "Epoch 98, Batch 165 Loss:0.025672396644949913\n",
      "Epoch 98, Batch 166 Loss:0.019080214202404022\n",
      "Epoch 98, Batch 167 Loss:0.013973800465464592\n",
      "Epoch 98, Batch 168 Loss:0.06986702233552933\n",
      "Epoch 98, Batch 169 Loss:0.03639621660113335\n",
      "Epoch 98, Batch 170 Loss:0.044428810477256775\n",
      "Epoch 98, Batch 171 Loss:0.006277895532548428\n",
      "Epoch 98, Batch 172 Loss:0.08626338094472885\n",
      "Epoch 98, Batch 173 Loss:0.03870946541428566\n",
      "Epoch 98, Batch 174 Loss:0.009777799248695374\n",
      "Epoch 98, Batch 175 Loss:0.013787545263767242\n",
      "Epoch 98, Batch 176 Loss:0.017905451357364655\n",
      "Epoch 98, Batch 177 Loss:0.03722912818193436\n",
      "Epoch 98, Batch 178 Loss:0.048095181584358215\n",
      "Epoch 98, Batch 179 Loss:0.0067594535648822784\n",
      "Epoch 98, Batch 180 Loss:0.02420787885785103\n",
      "Epoch 98, Batch 181 Loss:0.026046626269817352\n",
      "Epoch 98, Batch 182 Loss:0.025672482326626778\n",
      "Epoch 98, Batch 183 Loss:0.005966216791421175\n",
      "Epoch 98, Batch 184 Loss:0.024916797876358032\n",
      "Epoch 98, Batch 185 Loss:0.05061734840273857\n",
      "Epoch 98, Batch 186 Loss:0.023950938135385513\n",
      "Epoch 98, Batch 187 Loss:0.024556713178753853\n",
      "Epoch 98, Batch 188 Loss:0.014694673009216785\n",
      "Epoch 98, Batch 189 Loss:0.03265465050935745\n",
      "Epoch 98, Batch 190 Loss:0.019978895783424377\n",
      "Epoch 98, Batch 191 Loss:0.00862784031778574\n",
      "Epoch 98, Batch 192 Loss:0.0631801187992096\n",
      "Epoch 98, Batch 193 Loss:0.02467096969485283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98, Batch 194 Loss:0.05000246316194534\n",
      "Epoch 98, Batch 195 Loss:0.039258427917957306\n",
      "Epoch 98, Batch 196 Loss:0.02187282405793667\n",
      "Epoch 98, Batch 197 Loss:0.06092430651187897\n",
      "Epoch 98, Batch 198 Loss:0.04490232467651367\n",
      "Epoch 98, Batch 199 Loss:0.0655958503484726\n",
      "Epoch 98, Batch 200 Loss:0.1221102848649025\n",
      "Epoch 98, Batch 201 Loss:0.02822774648666382\n",
      "Epoch 98, Batch 202 Loss:0.10975553095340729\n",
      "Epoch 98, Batch 203 Loss:0.09036479890346527\n",
      "Epoch 98, Batch 204 Loss:0.044204749166965485\n",
      "Epoch 98, Batch 205 Loss:0.0590234249830246\n",
      "Epoch 98, Batch 206 Loss:0.02786988392472267\n",
      "Epoch 98, Batch 207 Loss:0.06067679077386856\n",
      "Epoch 98, Batch 208 Loss:0.0745585709810257\n",
      "Epoch 98, Batch 209 Loss:0.06198585778474808\n",
      "Epoch 98, Batch 210 Loss:0.0660022497177124\n",
      "Epoch 98, Batch 211 Loss:0.029813161119818687\n",
      "Epoch 98, Batch 212 Loss:0.02191803604364395\n",
      "Epoch 98, Batch 213 Loss:0.020611267536878586\n",
      "Epoch 98, Batch 214 Loss:0.07649741321802139\n",
      "Epoch 98, Batch 215 Loss:0.027632497251033783\n",
      "Epoch 98, Batch 216 Loss:0.03296264261007309\n",
      "Epoch 98, Batch 217 Loss:0.029847456142306328\n",
      "Epoch 98, Batch 218 Loss:0.03596797585487366\n",
      "Epoch 98, Batch 219 Loss:0.050284795463085175\n",
      "Epoch 98, Batch 220 Loss:0.042893148958683014\n",
      "Epoch 98, Batch 221 Loss:0.021503571420907974\n",
      "Epoch 98, Batch 222 Loss:0.11046876013278961\n",
      "Epoch 98, Batch 223 Loss:0.048717740923166275\n",
      "Epoch 98, Batch 224 Loss:0.02823246642947197\n",
      "Epoch 98, Batch 225 Loss:0.015280874446034431\n",
      "Epoch 98, Batch 226 Loss:0.02784552425146103\n",
      "Epoch 98, Batch 227 Loss:0.014239108189940453\n",
      "Epoch 98, Batch 228 Loss:0.015302108600735664\n",
      "Epoch 98, Batch 229 Loss:0.009700818918645382\n",
      "Epoch 98, Batch 230 Loss:0.024854030460119247\n",
      "Epoch 98, Batch 231 Loss:0.021769611164927483\n",
      "Epoch 98, Batch 232 Loss:0.01489205751568079\n",
      "Epoch 98, Batch 233 Loss:0.016576822847127914\n",
      "Loss in this Epoch is: 1.65768228471 %\n",
      "Accuracy in this Epoch is: 88.3499979973 %\n",
      "Epoch 99, Batch 0 Loss:0.012208111584186554\n",
      "Epoch 99, Batch 1 Loss:0.045432716608047485\n",
      "Epoch 99, Batch 2 Loss:0.0038921991363167763\n",
      "Epoch 99, Batch 3 Loss:0.010540216229856014\n",
      "Epoch 99, Batch 4 Loss:0.004694504197686911\n",
      "Epoch 99, Batch 5 Loss:0.012637557461857796\n",
      "Epoch 99, Batch 6 Loss:0.015878362581133842\n",
      "Epoch 99, Batch 7 Loss:0.014646192081272602\n",
      "Epoch 99, Batch 8 Loss:0.012832112610340118\n",
      "Epoch 99, Batch 9 Loss:0.02313435636460781\n",
      "Epoch 99, Batch 10 Loss:0.06653716415166855\n",
      "Epoch 99, Batch 11 Loss:0.006612556055188179\n",
      "Epoch 99, Batch 12 Loss:0.03824663162231445\n",
      "Epoch 99, Batch 13 Loss:0.018484512344002724\n",
      "Epoch 99, Batch 14 Loss:0.017214076593518257\n",
      "Epoch 99, Batch 15 Loss:0.011031115427613258\n",
      "Epoch 99, Batch 16 Loss:0.010529863648116589\n",
      "Epoch 99, Batch 17 Loss:0.0212384145706892\n",
      "Epoch 99, Batch 18 Loss:0.009240712970495224\n",
      "Epoch 99, Batch 19 Loss:0.021365245804190636\n",
      "Epoch 99, Batch 20 Loss:0.03954749554395676\n",
      "Epoch 99, Batch 21 Loss:0.01650025136768818\n",
      "Epoch 99, Batch 22 Loss:0.012418667785823345\n",
      "Epoch 99, Batch 23 Loss:0.007508368231356144\n",
      "Epoch 99, Batch 24 Loss:0.02029755897819996\n",
      "Epoch 99, Batch 25 Loss:0.03391864150762558\n",
      "Epoch 99, Batch 26 Loss:0.03216950595378876\n",
      "Epoch 99, Batch 27 Loss:0.011696560308337212\n",
      "Epoch 99, Batch 28 Loss:0.012921384535729885\n",
      "Epoch 99, Batch 29 Loss:0.011632638052105904\n",
      "Epoch 99, Batch 30 Loss:0.01304045133292675\n",
      "Epoch 99, Batch 31 Loss:0.03307563439011574\n",
      "Epoch 99, Batch 32 Loss:0.005776960402727127\n",
      "Epoch 99, Batch 33 Loss:0.009255212731659412\n",
      "Epoch 99, Batch 34 Loss:0.001000689808279276\n",
      "Epoch 99, Batch 35 Loss:0.012812517583370209\n",
      "Epoch 99, Batch 36 Loss:0.05058113485574722\n",
      "Epoch 99, Batch 37 Loss:0.011264204047620296\n",
      "Epoch 99, Batch 38 Loss:0.011969516985118389\n",
      "Epoch 99, Batch 39 Loss:0.020525595173239708\n",
      "Epoch 99, Batch 40 Loss:0.0064595360308885574\n",
      "Epoch 99, Batch 41 Loss:0.03887521103024483\n",
      "Epoch 99, Batch 42 Loss:0.007482852321118116\n",
      "Epoch 99, Batch 43 Loss:0.011799797415733337\n",
      "Epoch 99, Batch 44 Loss:0.04313162341713905\n",
      "Epoch 99, Batch 45 Loss:0.008727507665753365\n",
      "Epoch 99, Batch 46 Loss:0.013005892746150494\n",
      "Epoch 99, Batch 47 Loss:0.018634479492902756\n",
      "Epoch 99, Batch 48 Loss:0.01614658534526825\n",
      "Epoch 99, Batch 49 Loss:0.0024815243668854237\n",
      "Epoch 99, Batch 50 Loss:0.0029067217838019133\n",
      "Epoch 99, Batch 51 Loss:0.0036748703569173813\n",
      "Epoch 99, Batch 52 Loss:0.009453424252569675\n",
      "Epoch 99, Batch 53 Loss:0.0224369578063488\n",
      "Epoch 99, Batch 54 Loss:0.02838023751974106\n",
      "Epoch 99, Batch 55 Loss:0.017060412093997\n",
      "Epoch 99, Batch 56 Loss:0.0070920223370194435\n",
      "Epoch 99, Batch 57 Loss:0.019126733765006065\n",
      "Epoch 99, Batch 58 Loss:0.011059361509978771\n",
      "Epoch 99, Batch 59 Loss:0.037987034767866135\n",
      "Epoch 99, Batch 60 Loss:0.05322226136922836\n",
      "Epoch 99, Batch 61 Loss:0.03330373764038086\n",
      "Epoch 99, Batch 62 Loss:0.017278121784329414\n",
      "Epoch 99, Batch 63 Loss:0.016720620915293694\n",
      "Epoch 99, Batch 64 Loss:0.012528948485851288\n",
      "Epoch 99, Batch 65 Loss:0.015324834734201431\n",
      "Epoch 99, Batch 66 Loss:0.015367185696959496\n",
      "Epoch 99, Batch 67 Loss:0.02784387767314911\n",
      "Epoch 99, Batch 68 Loss:0.015074975788593292\n",
      "Epoch 99, Batch 69 Loss:0.003378068096935749\n",
      "Epoch 99, Batch 70 Loss:0.03133063763380051\n",
      "Epoch 99, Batch 71 Loss:0.006394799333065748\n",
      "Epoch 99, Batch 72 Loss:0.02527138590812683\n",
      "Epoch 99, Batch 73 Loss:0.006870882119983435\n",
      "Epoch 99, Batch 74 Loss:0.004313520155847073\n",
      "Epoch 99, Batch 75 Loss:0.023457162082195282\n",
      "Epoch 99, Batch 76 Loss:0.007148616015911102\n",
      "Epoch 99, Batch 77 Loss:0.007664369884878397\n",
      "Epoch 99, Batch 78 Loss:0.08142606168985367\n",
      "Epoch 99, Batch 79 Loss:0.012096952646970749\n",
      "Epoch 99, Batch 80 Loss:0.03096780553460121\n",
      "Epoch 99, Batch 81 Loss:0.035444777458906174\n",
      "Epoch 99, Batch 82 Loss:0.04128554090857506\n",
      "Epoch 99, Batch 83 Loss:0.041661571711301804\n",
      "Epoch 99, Batch 84 Loss:0.041275136172771454\n",
      "Epoch 99, Batch 85 Loss:0.03405698761343956\n",
      "Epoch 99, Batch 86 Loss:0.03451255336403847\n",
      "Epoch 99, Batch 87 Loss:0.014419913291931152\n",
      "Epoch 99, Batch 88 Loss:0.09694700688123703\n",
      "Epoch 99, Batch 89 Loss:0.11924052983522415\n",
      "Epoch 99, Batch 90 Loss:0.07783181220293045\n",
      "Epoch 99, Batch 91 Loss:0.023801183328032494\n",
      "Epoch 99, Batch 92 Loss:0.09417232871055603\n",
      "Epoch 99, Batch 93 Loss:0.06598741561174393\n",
      "Epoch 99, Batch 94 Loss:0.04246033728122711\n",
      "Epoch 99, Batch 95 Loss:0.0691961944103241\n",
      "Epoch 99, Batch 96 Loss:0.07160567492246628\n",
      "Epoch 99, Batch 97 Loss:0.10784734785556793\n",
      "Epoch 99, Batch 98 Loss:0.04865552484989166\n",
      "Epoch 99, Batch 99 Loss:0.07381457090377808\n",
      "Epoch 99, Batch 100 Loss:0.08377567678689957\n",
      "Epoch 99, Batch 101 Loss:0.058100633323192596\n",
      "Epoch 99, Batch 102 Loss:0.07991297543048859\n",
      "Epoch 99, Batch 103 Loss:0.10463858395814896\n",
      "Epoch 99, Batch 104 Loss:0.039786022156476974\n",
      "Epoch 99, Batch 105 Loss:0.07511234283447266\n",
      "Epoch 99, Batch 106 Loss:0.033786263316869736\n",
      "Epoch 99, Batch 107 Loss:0.05442594736814499\n",
      "Epoch 99, Batch 108 Loss:0.03547053411602974\n",
      "Epoch 99, Batch 109 Loss:0.04411594197154045\n",
      "Epoch 99, Batch 110 Loss:0.130327045917511\n",
      "Epoch 99, Batch 111 Loss:0.05572764202952385\n",
      "Epoch 99, Batch 112 Loss:0.049419011920690536\n",
      "Epoch 99, Batch 113 Loss:0.05543387308716774\n",
      "Epoch 99, Batch 114 Loss:0.08830513060092926\n",
      "Epoch 99, Batch 115 Loss:0.05316898226737976\n",
      "Epoch 99, Batch 116 Loss:0.0662917047739029\n",
      "Epoch 99, Batch 117 Loss:0.13004222512245178\n",
      "Epoch 99, Batch 118 Loss:0.10445930063724518\n",
      "Epoch 99, Batch 119 Loss:0.04981696978211403\n",
      "Epoch 99, Batch 120 Loss:0.08099395036697388\n",
      "Epoch 99, Batch 121 Loss:0.0811990424990654\n",
      "Epoch 99, Batch 122 Loss:0.08897413313388824\n",
      "Epoch 99, Batch 123 Loss:0.04743314906954765\n",
      "Epoch 99, Batch 124 Loss:0.033052828162908554\n",
      "Epoch 99, Batch 125 Loss:0.11474376171827316\n",
      "Epoch 99, Batch 126 Loss:0.09670253098011017\n",
      "Epoch 99, Batch 127 Loss:0.025548510253429413\n",
      "Epoch 99, Batch 128 Loss:0.09011563658714294\n",
      "Epoch 99, Batch 129 Loss:0.013200612738728523\n",
      "Epoch 99, Batch 130 Loss:0.039863284677267075\n",
      "Epoch 99, Batch 131 Loss:0.0307746771723032\n",
      "Epoch 99, Batch 132 Loss:0.03670874238014221\n",
      "Epoch 99, Batch 133 Loss:0.05353870242834091\n",
      "Epoch 99, Batch 134 Loss:0.0317857600748539\n",
      "Epoch 99, Batch 135 Loss:0.007134676910936832\n",
      "Epoch 99, Batch 136 Loss:0.020492956042289734\n",
      "Epoch 99, Batch 137 Loss:0.0740114226937294\n",
      "Epoch 99, Batch 138 Loss:0.013550855219364166\n",
      "Epoch 99, Batch 139 Loss:0.024714447557926178\n",
      "Epoch 99, Batch 140 Loss:0.01443241722881794\n",
      "Epoch 99, Batch 141 Loss:0.01425506453961134\n",
      "Epoch 99, Batch 142 Loss:0.02396240271627903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99, Batch 143 Loss:0.019928479567170143\n",
      "Epoch 99, Batch 144 Loss:0.03890557959675789\n",
      "Epoch 99, Batch 145 Loss:0.01596098020672798\n",
      "Epoch 99, Batch 146 Loss:0.016186902299523354\n",
      "Epoch 99, Batch 147 Loss:0.02604343742132187\n",
      "Epoch 99, Batch 148 Loss:0.014410343021154404\n",
      "Epoch 99, Batch 149 Loss:0.04021139070391655\n",
      "Epoch 99, Batch 150 Loss:0.03954881802201271\n",
      "Epoch 99, Batch 151 Loss:0.13175833225250244\n",
      "Epoch 99, Batch 152 Loss:0.05433870851993561\n",
      "Epoch 99, Batch 153 Loss:0.06959591060876846\n",
      "Epoch 99, Batch 154 Loss:0.056315042078495026\n",
      "Epoch 99, Batch 155 Loss:0.035051051527261734\n",
      "Epoch 99, Batch 156 Loss:0.0236476119607687\n",
      "Epoch 99, Batch 157 Loss:0.06848263740539551\n",
      "Epoch 99, Batch 158 Loss:0.2973177433013916\n",
      "Epoch 99, Batch 159 Loss:0.056204378604888916\n",
      "Epoch 99, Batch 160 Loss:0.02813529223203659\n",
      "Epoch 99, Batch 161 Loss:0.08236411213874817\n",
      "Epoch 99, Batch 162 Loss:0.08383077383041382\n",
      "Epoch 99, Batch 163 Loss:0.009446490556001663\n",
      "Epoch 99, Batch 164 Loss:0.060283683240413666\n",
      "Epoch 99, Batch 165 Loss:0.07814532518386841\n",
      "Epoch 99, Batch 166 Loss:0.026508314535021782\n",
      "Epoch 99, Batch 167 Loss:0.030838733538985252\n",
      "Epoch 99, Batch 168 Loss:0.06400902569293976\n",
      "Epoch 99, Batch 169 Loss:0.035236530005931854\n",
      "Epoch 99, Batch 170 Loss:0.08944928646087646\n",
      "Epoch 99, Batch 171 Loss:0.041186679154634476\n",
      "Epoch 99, Batch 172 Loss:0.06859666109085083\n",
      "Epoch 99, Batch 173 Loss:0.09192684292793274\n",
      "Epoch 99, Batch 174 Loss:0.05730362609028816\n",
      "Epoch 99, Batch 175 Loss:0.07992326468229294\n",
      "Epoch 99, Batch 176 Loss:0.03847125917673111\n",
      "Epoch 99, Batch 177 Loss:0.07184305042028427\n",
      "Epoch 99, Batch 178 Loss:0.07517772167921066\n",
      "Epoch 99, Batch 179 Loss:0.03612697869539261\n",
      "Epoch 99, Batch 180 Loss:0.06818839907646179\n",
      "Epoch 99, Batch 181 Loss:0.05568787083029747\n",
      "Epoch 99, Batch 182 Loss:0.06269174814224243\n",
      "Epoch 99, Batch 183 Loss:0.019421841949224472\n",
      "Epoch 99, Batch 184 Loss:0.030367333441972733\n",
      "Epoch 99, Batch 185 Loss:0.02272380143404007\n",
      "Epoch 99, Batch 186 Loss:0.04826062172651291\n",
      "Epoch 99, Batch 187 Loss:0.04083262383937836\n",
      "Epoch 99, Batch 188 Loss:0.03564184159040451\n",
      "Epoch 99, Batch 189 Loss:0.06892696022987366\n",
      "Epoch 99, Batch 190 Loss:0.00974474847316742\n",
      "Epoch 99, Batch 191 Loss:0.023534927517175674\n",
      "Epoch 99, Batch 192 Loss:0.056082017719745636\n",
      "Epoch 99, Batch 193 Loss:0.09316772222518921\n",
      "Epoch 99, Batch 194 Loss:0.04376491904258728\n",
      "Epoch 99, Batch 195 Loss:0.03778446093201637\n",
      "Epoch 99, Batch 196 Loss:0.03288041055202484\n",
      "Epoch 99, Batch 197 Loss:0.030381716787815094\n",
      "Epoch 99, Batch 198 Loss:0.0380040742456913\n",
      "Epoch 99, Batch 199 Loss:0.027608247473835945\n",
      "Epoch 99, Batch 200 Loss:0.02070361189544201\n",
      "Epoch 99, Batch 201 Loss:0.05256623402237892\n",
      "Epoch 99, Batch 202 Loss:0.01323324628174305\n",
      "Epoch 99, Batch 203 Loss:0.0191379152238369\n",
      "Epoch 99, Batch 204 Loss:0.03473687544465065\n",
      "Epoch 99, Batch 205 Loss:0.04884020611643791\n",
      "Epoch 99, Batch 206 Loss:0.02965189330279827\n",
      "Epoch 99, Batch 207 Loss:0.051348574459552765\n",
      "Epoch 99, Batch 208 Loss:0.03856920823454857\n",
      "Epoch 99, Batch 209 Loss:0.04278978705406189\n",
      "Epoch 99, Batch 210 Loss:0.012114707380533218\n",
      "Epoch 99, Batch 211 Loss:0.022352328523993492\n",
      "Epoch 99, Batch 212 Loss:0.017221106216311455\n",
      "Epoch 99, Batch 213 Loss:0.014893976971507072\n",
      "Epoch 99, Batch 214 Loss:0.03992556780576706\n",
      "Epoch 99, Batch 215 Loss:0.018507685512304306\n",
      "Epoch 99, Batch 216 Loss:0.021100178360939026\n",
      "Epoch 99, Batch 217 Loss:0.00599373783916235\n",
      "Epoch 99, Batch 218 Loss:0.013976071961224079\n",
      "Epoch 99, Batch 219 Loss:0.050633400678634644\n",
      "Epoch 99, Batch 220 Loss:0.03771675005555153\n",
      "Epoch 99, Batch 221 Loss:0.0336969755589962\n",
      "Epoch 99, Batch 222 Loss:0.021176915615797043\n",
      "Epoch 99, Batch 223 Loss:0.019193723797798157\n",
      "Epoch 99, Batch 224 Loss:0.022228844463825226\n",
      "Epoch 99, Batch 225 Loss:0.023895785212516785\n",
      "Epoch 99, Batch 226 Loss:0.034056052565574646\n",
      "Epoch 99, Batch 227 Loss:0.023083942010998726\n",
      "Epoch 99, Batch 228 Loss:0.020274953916668892\n",
      "Epoch 99, Batch 229 Loss:0.0077196937054395676\n",
      "Epoch 99, Batch 230 Loss:0.02002512663602829\n",
      "Epoch 99, Batch 231 Loss:0.020710330456495285\n",
      "Epoch 99, Batch 232 Loss:0.031669981777668\n",
      "Epoch 99, Batch 233 Loss:0.014763293787837029\n",
      "Loss in this Epoch is: 1.47632937878 %\n",
      "Accuracy in this Epoch is: 88.4400010109 %\n",
      "Epoch 100, Batch 0 Loss:0.011644336394965649\n",
      "Epoch 100, Batch 1 Loss:0.008176050148904324\n",
      "Epoch 100, Batch 2 Loss:0.019425498321652412\n",
      "Epoch 100, Batch 3 Loss:0.043772295117378235\n",
      "Epoch 100, Batch 4 Loss:0.016899796202778816\n",
      "Epoch 100, Batch 5 Loss:0.058289241045713425\n",
      "Epoch 100, Batch 6 Loss:0.011449110694229603\n",
      "Epoch 100, Batch 7 Loss:0.007781438063830137\n",
      "Epoch 100, Batch 8 Loss:0.01110374927520752\n",
      "Epoch 100, Batch 9 Loss:0.03734893724322319\n",
      "Epoch 100, Batch 10 Loss:0.04252853989601135\n",
      "Epoch 100, Batch 11 Loss:0.023760220035910606\n",
      "Epoch 100, Batch 12 Loss:0.046411652117967606\n",
      "Epoch 100, Batch 13 Loss:0.010396132245659828\n",
      "Epoch 100, Batch 14 Loss:0.044031985104084015\n",
      "Epoch 100, Batch 15 Loss:0.016346409916877747\n",
      "Epoch 100, Batch 16 Loss:0.07474058866500854\n",
      "Epoch 100, Batch 17 Loss:0.04465748742222786\n",
      "Epoch 100, Batch 18 Loss:0.049297649413347244\n",
      "Epoch 100, Batch 19 Loss:0.08227528631687164\n",
      "Epoch 100, Batch 20 Loss:0.03803243860602379\n",
      "Epoch 100, Batch 21 Loss:0.012859159149229527\n",
      "Epoch 100, Batch 22 Loss:0.012637379579246044\n",
      "Epoch 100, Batch 23 Loss:0.023491429165005684\n",
      "Epoch 100, Batch 24 Loss:0.010932557284832\n",
      "Epoch 100, Batch 25 Loss:0.013308309949934483\n",
      "Epoch 100, Batch 26 Loss:0.01231027115136385\n",
      "Epoch 100, Batch 27 Loss:0.018442703410983086\n",
      "Epoch 100, Batch 28 Loss:0.014715264551341534\n",
      "Epoch 100, Batch 29 Loss:0.01855967566370964\n",
      "Epoch 100, Batch 30 Loss:0.02195712737739086\n",
      "Epoch 100, Batch 31 Loss:0.033371198922395706\n",
      "Epoch 100, Batch 32 Loss:0.011152677237987518\n",
      "Epoch 100, Batch 33 Loss:0.022788405418395996\n",
      "Epoch 100, Batch 34 Loss:0.004554437939077616\n",
      "Epoch 100, Batch 35 Loss:0.009368719533085823\n",
      "Epoch 100, Batch 36 Loss:0.010015669278800488\n",
      "Epoch 100, Batch 37 Loss:0.004251376260071993\n",
      "Epoch 100, Batch 38 Loss:0.06454866379499435\n",
      "Epoch 100, Batch 39 Loss:0.011080755852162838\n",
      "Epoch 100, Batch 40 Loss:0.0077973403967916965\n",
      "Epoch 100, Batch 41 Loss:0.04799788445234299\n",
      "Epoch 100, Batch 42 Loss:0.008454018272459507\n",
      "Epoch 100, Batch 43 Loss:0.014704904519021511\n",
      "Epoch 100, Batch 44 Loss:0.03255779668688774\n",
      "Epoch 100, Batch 45 Loss:0.00742693617939949\n",
      "Epoch 100, Batch 46 Loss:0.004811571910977364\n",
      "Epoch 100, Batch 47 Loss:0.022613367065787315\n",
      "Epoch 100, Batch 48 Loss:0.01989147625863552\n",
      "Epoch 100, Batch 49 Loss:0.009241728112101555\n",
      "Epoch 100, Batch 50 Loss:0.009034928865730762\n",
      "Epoch 100, Batch 51 Loss:0.023049039766192436\n",
      "Epoch 100, Batch 52 Loss:0.03665018081665039\n",
      "Epoch 100, Batch 53 Loss:0.01436238456517458\n",
      "Epoch 100, Batch 54 Loss:0.012340446934103966\n",
      "Epoch 100, Batch 55 Loss:0.006349221803247929\n",
      "Epoch 100, Batch 56 Loss:0.02204432711005211\n",
      "Epoch 100, Batch 57 Loss:0.01802716590464115\n",
      "Epoch 100, Batch 58 Loss:0.007578691467642784\n",
      "Epoch 100, Batch 59 Loss:0.004302439745515585\n",
      "Epoch 100, Batch 60 Loss:0.08658072352409363\n",
      "Epoch 100, Batch 61 Loss:0.033984120935201645\n",
      "Epoch 100, Batch 62 Loss:0.01865617372095585\n",
      "Epoch 100, Batch 63 Loss:0.026407210156321526\n",
      "Epoch 100, Batch 64 Loss:0.009060702286660671\n",
      "Epoch 100, Batch 65 Loss:0.01394364982843399\n",
      "Epoch 100, Batch 66 Loss:0.014443893916904926\n",
      "Epoch 100, Batch 67 Loss:0.03129623830318451\n",
      "Epoch 100, Batch 68 Loss:0.007347897160798311\n",
      "Epoch 100, Batch 69 Loss:0.006687541957944632\n",
      "Epoch 100, Batch 70 Loss:0.05286339297890663\n",
      "Epoch 100, Batch 71 Loss:0.03565719351172447\n",
      "Epoch 100, Batch 72 Loss:0.05383595451712608\n",
      "Epoch 100, Batch 73 Loss:0.013147389516234398\n",
      "Epoch 100, Batch 74 Loss:0.027257023379206657\n",
      "Epoch 100, Batch 75 Loss:0.012164938263595104\n",
      "Epoch 100, Batch 76 Loss:0.007026633247733116\n",
      "Epoch 100, Batch 77 Loss:0.0026462909299880266\n",
      "Epoch 100, Batch 78 Loss:0.011546456255018711\n",
      "Epoch 100, Batch 79 Loss:0.033981047570705414\n",
      "Epoch 100, Batch 80 Loss:0.019092265516519547\n",
      "Epoch 100, Batch 81 Loss:0.030967794358730316\n",
      "Epoch 100, Batch 82 Loss:0.02035386860370636\n",
      "Epoch 100, Batch 83 Loss:0.020097939297556877\n",
      "Epoch 100, Batch 84 Loss:0.05614473670721054\n",
      "Epoch 100, Batch 85 Loss:0.017741000279784203\n",
      "Epoch 100, Batch 86 Loss:0.02799678035080433\n",
      "Epoch 100, Batch 87 Loss:0.012544731609523296\n",
      "Epoch 100, Batch 88 Loss:0.006045210175216198\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100, Batch 89 Loss:0.015491750091314316\n",
      "Epoch 100, Batch 90 Loss:0.033545561134815216\n",
      "Epoch 100, Batch 91 Loss:0.060186903923749924\n",
      "Epoch 100, Batch 92 Loss:0.05031886696815491\n",
      "Epoch 100, Batch 93 Loss:0.007457227446138859\n",
      "Epoch 100, Batch 94 Loss:0.007810401730239391\n",
      "Epoch 100, Batch 95 Loss:0.010221888311207294\n",
      "Epoch 100, Batch 96 Loss:0.03193165734410286\n",
      "Epoch 100, Batch 97 Loss:0.01852875016629696\n",
      "Epoch 100, Batch 98 Loss:0.06010545790195465\n",
      "Epoch 100, Batch 99 Loss:0.03635776787996292\n",
      "Epoch 100, Batch 100 Loss:0.006794814486056566\n",
      "Epoch 100, Batch 101 Loss:0.01610552705824375\n",
      "Epoch 100, Batch 102 Loss:0.009335264563560486\n",
      "Epoch 100, Batch 103 Loss:0.009476172737777233\n",
      "Epoch 100, Batch 104 Loss:0.014506308361887932\n",
      "Epoch 100, Batch 105 Loss:0.026582572609186172\n",
      "Epoch 100, Batch 106 Loss:0.013223160989582539\n",
      "Epoch 100, Batch 107 Loss:0.013235904276371002\n",
      "Epoch 100, Batch 108 Loss:0.016758045181632042\n",
      "Epoch 100, Batch 109 Loss:0.02522658370435238\n",
      "Epoch 100, Batch 110 Loss:0.06213730573654175\n",
      "Epoch 100, Batch 111 Loss:0.014116160571575165\n",
      "Epoch 100, Batch 112 Loss:0.01021951250731945\n",
      "Epoch 100, Batch 113 Loss:0.013585185632109642\n",
      "Epoch 100, Batch 114 Loss:0.010832032188773155\n",
      "Epoch 100, Batch 115 Loss:0.009839853271842003\n",
      "Epoch 100, Batch 116 Loss:0.02143104001879692\n",
      "Epoch 100, Batch 117 Loss:0.01391634251922369\n",
      "Epoch 100, Batch 118 Loss:0.01689019799232483\n",
      "Epoch 100, Batch 119 Loss:0.06149205192923546\n",
      "Epoch 100, Batch 120 Loss:0.005211235024034977\n",
      "Epoch 100, Batch 121 Loss:0.018500808626413345\n",
      "Epoch 100, Batch 122 Loss:0.04280374199151993\n",
      "Epoch 100, Batch 123 Loss:0.02511795423924923\n",
      "Epoch 100, Batch 124 Loss:0.04753265529870987\n",
      "Epoch 100, Batch 125 Loss:0.01386556588113308\n",
      "Epoch 100, Batch 126 Loss:0.015924615785479546\n",
      "Epoch 100, Batch 127 Loss:0.015251068398356438\n",
      "Epoch 100, Batch 128 Loss:0.014324162155389786\n",
      "Epoch 100, Batch 129 Loss:0.024863947182893753\n",
      "Epoch 100, Batch 130 Loss:0.005791197530925274\n",
      "Epoch 100, Batch 131 Loss:0.04005121439695358\n",
      "Epoch 100, Batch 132 Loss:0.01904154196381569\n",
      "Epoch 100, Batch 133 Loss:0.017877597361803055\n",
      "Epoch 100, Batch 134 Loss:0.029083337634801865\n",
      "Epoch 100, Batch 135 Loss:0.013027332723140717\n",
      "Epoch 100, Batch 136 Loss:0.014750372618436813\n",
      "Epoch 100, Batch 137 Loss:0.009973464533686638\n",
      "Epoch 100, Batch 138 Loss:0.02695908397436142\n",
      "Epoch 100, Batch 139 Loss:0.00744968093931675\n",
      "Epoch 100, Batch 140 Loss:0.042285289615392685\n",
      "Epoch 100, Batch 141 Loss:0.004298036452382803\n",
      "Epoch 100, Batch 142 Loss:0.028395287692546844\n",
      "Epoch 100, Batch 143 Loss:0.014335231855511665\n",
      "Epoch 100, Batch 144 Loss:0.006799563765525818\n",
      "Epoch 100, Batch 145 Loss:0.039087824523448944\n",
      "Epoch 100, Batch 146 Loss:0.07067281007766724\n",
      "Epoch 100, Batch 147 Loss:0.018504180014133453\n",
      "Epoch 100, Batch 148 Loss:0.03432835638523102\n",
      "Epoch 100, Batch 149 Loss:0.010492471046745777\n",
      "Epoch 100, Batch 150 Loss:0.008759785443544388\n",
      "Epoch 100, Batch 151 Loss:0.04535546898841858\n",
      "Epoch 100, Batch 152 Loss:0.011915050446987152\n",
      "Epoch 100, Batch 153 Loss:0.02796069346368313\n",
      "Epoch 100, Batch 154 Loss:0.005521170329302549\n",
      "Epoch 100, Batch 155 Loss:0.0115274703130126\n",
      "Epoch 100, Batch 156 Loss:0.013905586674809456\n",
      "Epoch 100, Batch 157 Loss:0.06204592436552048\n",
      "Epoch 100, Batch 158 Loss:0.006295246072113514\n",
      "Epoch 100, Batch 159 Loss:0.01677747443318367\n",
      "Epoch 100, Batch 160 Loss:0.00980538222938776\n",
      "Epoch 100, Batch 161 Loss:0.04010564088821411\n",
      "Epoch 100, Batch 162 Loss:0.010259801521897316\n",
      "Epoch 100, Batch 163 Loss:0.007544639520347118\n",
      "Epoch 100, Batch 164 Loss:0.010020153596997261\n",
      "Epoch 100, Batch 165 Loss:0.034760501235723495\n",
      "Epoch 100, Batch 166 Loss:0.009168310090899467\n",
      "Epoch 100, Batch 167 Loss:0.0086615439504385\n",
      "Epoch 100, Batch 168 Loss:0.005381611175835133\n",
      "Epoch 100, Batch 169 Loss:0.024188674986362457\n",
      "Epoch 100, Batch 170 Loss:0.011180269531905651\n",
      "Epoch 100, Batch 171 Loss:0.03424676135182381\n",
      "Epoch 100, Batch 172 Loss:0.006248916033655405\n",
      "Epoch 100, Batch 173 Loss:0.031462740153074265\n",
      "Epoch 100, Batch 174 Loss:0.004845723509788513\n",
      "Epoch 100, Batch 175 Loss:0.010141311213374138\n",
      "Epoch 100, Batch 176 Loss:0.012122056446969509\n",
      "Epoch 100, Batch 177 Loss:0.006711472291499376\n",
      "Epoch 100, Batch 178 Loss:0.03614052012562752\n",
      "Epoch 100, Batch 179 Loss:0.015454933047294617\n",
      "Epoch 100, Batch 180 Loss:0.01674834080040455\n",
      "Epoch 100, Batch 181 Loss:0.01108628697693348\n",
      "Epoch 100, Batch 182 Loss:0.03488234058022499\n",
      "Epoch 100, Batch 183 Loss:0.010676528327167034\n",
      "Epoch 100, Batch 184 Loss:0.03288896009325981\n",
      "Epoch 100, Batch 185 Loss:0.012190337292850018\n",
      "Epoch 100, Batch 186 Loss:0.060619283467531204\n",
      "Epoch 100, Batch 187 Loss:0.03358083963394165\n",
      "Epoch 100, Batch 188 Loss:0.07869621366262436\n",
      "Epoch 100, Batch 189 Loss:0.0340539924800396\n",
      "Epoch 100, Batch 190 Loss:0.01961837150156498\n",
      "Epoch 100, Batch 191 Loss:0.03220564126968384\n",
      "Epoch 100, Batch 192 Loss:0.007421459536999464\n",
      "Epoch 100, Batch 193 Loss:0.016028527170419693\n",
      "Epoch 100, Batch 194 Loss:0.009881301783025265\n",
      "Epoch 100, Batch 195 Loss:0.024778300896286964\n",
      "Epoch 100, Batch 196 Loss:0.03646330535411835\n",
      "Epoch 100, Batch 197 Loss:0.02051876112818718\n",
      "Epoch 100, Batch 198 Loss:0.021877501159906387\n",
      "Epoch 100, Batch 199 Loss:0.006283930502831936\n",
      "Epoch 100, Batch 200 Loss:0.014118732884526253\n",
      "Epoch 100, Batch 201 Loss:0.012785210274159908\n",
      "Epoch 100, Batch 202 Loss:0.015986111015081406\n",
      "Epoch 100, Batch 203 Loss:0.012267912738025188\n",
      "Epoch 100, Batch 204 Loss:0.2389015555381775\n",
      "Epoch 100, Batch 205 Loss:0.007560386322438717\n",
      "Epoch 100, Batch 206 Loss:0.011544205248355865\n",
      "Epoch 100, Batch 207 Loss:0.003265705658122897\n",
      "Epoch 100, Batch 208 Loss:0.014291391707956791\n",
      "Epoch 100, Batch 209 Loss:0.010911171324551105\n",
      "Epoch 100, Batch 210 Loss:0.015419586561620235\n",
      "Epoch 100, Batch 211 Loss:0.011788183823227882\n",
      "Epoch 100, Batch 212 Loss:0.008636967279016972\n",
      "Epoch 100, Batch 213 Loss:0.008734917268157005\n",
      "Epoch 100, Batch 214 Loss:0.016531482338905334\n",
      "Epoch 100, Batch 215 Loss:0.03200257942080498\n",
      "Epoch 100, Batch 216 Loss:0.011247232556343079\n",
      "Epoch 100, Batch 217 Loss:0.031732093542814255\n",
      "Epoch 100, Batch 218 Loss:0.009641412645578384\n",
      "Epoch 100, Batch 219 Loss:0.01835769973695278\n",
      "Epoch 100, Batch 220 Loss:0.01774556189775467\n",
      "Epoch 100, Batch 221 Loss:0.011570576578378677\n",
      "Epoch 100, Batch 222 Loss:0.006914276629686356\n",
      "Epoch 100, Batch 223 Loss:0.030528638511896133\n",
      "Epoch 100, Batch 224 Loss:0.027631118893623352\n",
      "Epoch 100, Batch 225 Loss:0.09651799499988556\n",
      "Epoch 100, Batch 226 Loss:0.015292502008378506\n",
      "Epoch 100, Batch 227 Loss:0.008593645878136158\n",
      "Epoch 100, Batch 228 Loss:0.002036821795627475\n",
      "Epoch 100, Batch 229 Loss:0.009715763852000237\n",
      "Epoch 100, Batch 230 Loss:0.03364627808332443\n",
      "Epoch 100, Batch 231 Loss:0.019303448498249054\n",
      "Epoch 100, Batch 232 Loss:0.03805604577064514\n",
      "Epoch 100, Batch 233 Loss:0.03469596803188324\n",
      "Loss in this Epoch is: 3.46959680319 %\n",
      "Accuracy in this Epoch is: 88.5900020599 %\n"
     ]
    }
   ],
   "source": [
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    x_batches, y_batches = get_data_batches(batch_size)\n",
    "    train_neural_network(sess, optimizer, keep_probability, x_batches[0], y_batches[0])\n",
    "    print_stats(sess, x_batches[0], y_batches[0], cost, accuracy)\n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_features, batch_labels = get_data_batches(batch_size)\n",
    "        for batch_num in range(len(batch_features)):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features[batch_num], batch_labels[batch_num])\n",
    "            loss = sess.run(cost, feed_dict={\n",
    "                x: batch_features[batch_num],\n",
    "                y: batch_labels[batch_num],\n",
    "                keep_prob: 1.})\n",
    "            print('Epoch {:>2}, Batch {} Loss:{}'.format(epoch + 1, batch_num, loss))\n",
    "        print_stats(sess, batch_features[-1], batch_labels[-1], cost, accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### TEST THE MODEL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
