{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Goal of this model is taking the reviews of different amazon food reviews and being able to predict the reviews of each item \n",
    "from scale of 1-5. \n",
    "The Steps we going to take are:\n",
    "    \n",
    "1. Take each and every review and convert review into batch set using tokenizing\n",
    "2. Load in the glove vector embeddings\n",
    "3. Replace each word with its associated glove embedding\n",
    "4. Make Training and Testing Set Division\n",
    "5. Using Keras build model in Keras (Embedding Layer - Convolution Network - LSTM Network - Dense Layer - (1-5))\n",
    "6. Train the model and Test the model for accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take each and every review and convert review into batch set using tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "import nltk\n",
    "from tqdm import tqdm\n",
    "from keras.layers import *\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from statistics import mean,mode, median, median_high\n",
    "try:\n",
    "    import h5py\n",
    "except ImportError:\n",
    "    h5py = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>B000UA0QIQ</td>\n",
       "      <td>A395BORC6FGVXV</td>\n",
       "      <td>Karl</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1307923200</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1UQRSCLF8GW1T</td>\n",
       "      <td>Michael D. Bigham \"M. Wassir\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1350777600</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>ADT0SRK1MGOEU</td>\n",
       "      <td>Twoapennything</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1342051200</td>\n",
       "      <td>Nice Taffy</td>\n",
       "      <td>I got a wild hair for taffy and ordered this f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A1SP2KVKFXXRU1</td>\n",
       "      <td>David C. Sullivan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1340150400</td>\n",
       "      <td>Great!  Just as good as the expensive brands!</td>\n",
       "      <td>This saltwater taffy had great flavors and was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>B006K2ZZ7K</td>\n",
       "      <td>A3JRGQVEQN31IQ</td>\n",
       "      <td>Pamela G. Williams</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336003200</td>\n",
       "      <td>Wonderful, tasty taffy</td>\n",
       "      <td>This taffy is so good.  It is very soft and ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>B000E7L2R4</td>\n",
       "      <td>A1MZYO9TZK0BBI</td>\n",
       "      <td>R. James</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1322006400</td>\n",
       "      <td>Yay Barley</td>\n",
       "      <td>Right now I'm mostly just sprouting this so my...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>B00171APVA</td>\n",
       "      <td>A21BT40VZCCYT4</td>\n",
       "      <td>Carol A. Reed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1351209600</td>\n",
       "      <td>Healthy Dog Food</td>\n",
       "      <td>This is a very healthy dog food. Good for thei...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
       "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
       "5   6  B006K2ZZ7K   ADT0SRK1MGOEU                   Twoapennything   \n",
       "6   7  B006K2ZZ7K  A1SP2KVKFXXRU1                David C. Sullivan   \n",
       "7   8  B006K2ZZ7K  A3JRGQVEQN31IQ               Pamela G. Williams   \n",
       "8   9  B000E7L2R4  A1MZYO9TZK0BBI                         R. James   \n",
       "9  10  B00171APVA  A21BT40VZCCYT4                    Carol A. Reed   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "3                     3                       3      2  1307923200   \n",
       "4                     0                       0      5  1350777600   \n",
       "5                     0                       0      4  1342051200   \n",
       "6                     0                       0      5  1340150400   \n",
       "7                     0                       0      5  1336003200   \n",
       "8                     1                       1      5  1322006400   \n",
       "9                     0                       0      5  1351209600   \n",
       "\n",
       "                                         Summary  \\\n",
       "0                          Good Quality Dog Food   \n",
       "1                              Not as Advertised   \n",
       "2                          \"Delight\" says it all   \n",
       "3                                 Cough Medicine   \n",
       "4                                    Great taffy   \n",
       "5                                     Nice Taffy   \n",
       "6  Great!  Just as good as the expensive brands!   \n",
       "7                         Wonderful, tasty taffy   \n",
       "8                                     Yay Barley   \n",
       "9                               Healthy Dog Food   \n",
       "\n",
       "                                                Text  \n",
       "0  I have bought several of the Vitality canned d...  \n",
       "1  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  This is a confection that has been around a fe...  \n",
       "3  If you are looking for the secret ingredient i...  \n",
       "4  Great taffy at a great price.  There was a wid...  \n",
       "5  I got a wild hair for taffy and ordered this f...  \n",
       "6  This saltwater taffy had great flavors and was...  \n",
       "7  This taffy is so good.  It is very soft and ch...  \n",
       "8  Right now I'm mostly just sprouting this so my...  \n",
       "9  This is a very healthy dog food. Good for thei...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load in the reviews file and inspect the reviews\n",
    "\n",
    "df = pd.read_csv('data/Reviews.csv')\n",
    "\n",
    "X = df['Text'].values\n",
    "y = df['Score'].values\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing Every Review and Preprocessing the text\n",
    "MAX_SENTENCE_SIZE = 200\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X)\n",
    "sequences = tokenizer.texts_to_sequences(X)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Adding in the padding\n",
    "\n",
    "padded_sequences = pad_sequences(sequences, maxlen=MAX_SENTENCE_SIZE)\n",
    "\n",
    "# X = [text_to_word_sequence(sentence,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "#                                                lower=True,\n",
    "#                                                split=\" \") for sentence in tqdm(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Reviews: 568454\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Number of Reviews:\",len(sequences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the glove vector embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#Loading up the embedding matrix from GLove DB\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('data/embedding/glove.6B.100d.txt',\"r\",encoding='utf8')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace each word with its associated glove embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the Embedding matrix\n",
    "#Embedding Matrix is combination of word list (index-word) and word emebeddings to give (index-embeddings)\n",
    "\n",
    "EMBEDDING_DIM=100\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Hot Encode the Label Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conver the Labels into one hot encoding output\n",
    "LABEL_CATEGORIES = 6\n",
    "y = np.eye(LABEL_CATEGORIES)[y.reshape(-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Training and Testing Set Division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_limit = int(len(padded_sequences) * 0.85)\n",
    "X_train = padded_sequences[:train_limit]\n",
    "X_test = padded_sequences[train_limit:]\n",
    "y_train = y[:train_limit]\n",
    "y_test = y[train_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unison_shuffled_copies(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    p = np.random.permutation(len(a))\n",
    "    return a[p], b[p]\n",
    "def get_training_sets():\n",
    "    return X_train,y_train #unison_shuffled_copies(X_train, y_train)\n",
    "def get_testing_sets():\n",
    "    return X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Keras build model in Keras (Embedding Layer - Convolution Network - LSTM Network - Dense Layer - (1-5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_neural_network():\n",
    "\n",
    "    input_layer = Input(shape=(MAX_SENTENCE_SIZE,), dtype='int32')\n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                               input_length=MAX_SENTENCE_SIZE, trainable=False)\n",
    "    embedding_layer = embedding_layer(input_layer)\n",
    "     \n",
    "    conv_layer = Conv1D(256, 8, activation='relu')(embedding_layer)\n",
    "    conv_layer = MaxPooling1D(8)(conv_layer)\n",
    "    \n",
    "    lstm_layer = LSTM(256)(conv_layer)\n",
    "    \n",
    "    #flatten_layer = Flatten()(lstm_layer)\n",
    "    dense_layer = Dense(256, activation='relu')(lstm_layer)\n",
    "    \n",
    "    prediction = Dense(LABEL_CATEGORIES, activation='softmax')(dense_layer)\n",
    "    \n",
    "    model = Model(inputs=input_layer,outputs=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_neural_network()\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model and Test the model for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 483185 samples, validate on 85269 samples\n",
      "Epoch 1/25\n",
      " - 55s - loss: 0.9289 - acc: 0.6701 - val_loss: 0.7617 - val_acc: 0.7161\n",
      "Epoch 2/25\n",
      " - 55s - loss: 0.7396 - acc: 0.7244 - val_loss: 0.6819 - val_acc: 0.7453\n",
      "Epoch 3/25\n",
      " - 55s - loss: 0.6487 - acc: 0.7581 - val_loss: 0.6504 - val_acc: 0.7568\n",
      "Epoch 4/25\n",
      " - 55s - loss: 0.5717 - acc: 0.7896 - val_loss: 0.6120 - val_acc: 0.7800\n",
      "Epoch 5/25\n",
      " - 55s - loss: 0.5034 - acc: 0.8171 - val_loss: 0.5998 - val_acc: 0.7895\n",
      "Epoch 6/25\n",
      " - 55s - loss: 0.4428 - acc: 0.8407 - val_loss: 0.6141 - val_acc: 0.8018\n",
      "Epoch 7/25\n",
      " - 55s - loss: 0.3875 - acc: 0.8620 - val_loss: 0.6297 - val_acc: 0.8037\n",
      "Epoch 8/25\n",
      " - 55s - loss: 0.3366 - acc: 0.8806 - val_loss: 0.6411 - val_acc: 0.8030\n",
      "Epoch 9/25\n",
      " - 55s - loss: 0.2914 - acc: 0.8967 - val_loss: 0.6666 - val_acc: 0.8078\n",
      "Epoch 10/25\n",
      " - 54s - loss: 0.2512 - acc: 0.9114 - val_loss: 0.7298 - val_acc: 0.8093\n",
      "Epoch 11/25\n",
      " - 55s - loss: 0.2168 - acc: 0.9233 - val_loss: 0.8227 - val_acc: 0.7870\n",
      "Epoch 12/25\n",
      " - 54s - loss: 0.1872 - acc: 0.9337 - val_loss: 0.8460 - val_acc: 0.7991\n",
      "Epoch 13/25\n",
      " - 55s - loss: 0.1632 - acc: 0.9420 - val_loss: 0.9245 - val_acc: 0.8080\n",
      "Epoch 14/25\n",
      " - 55s - loss: 0.1438 - acc: 0.9485 - val_loss: 0.9519 - val_acc: 0.8031\n",
      "Epoch 15/25\n",
      " - 55s - loss: 0.1284 - acc: 0.9541 - val_loss: 1.0337 - val_acc: 0.7979\n",
      "Epoch 16/25\n",
      " - 54s - loss: 0.1164 - acc: 0.9582 - val_loss: 1.0741 - val_acc: 0.8041\n",
      "Epoch 17/25\n",
      " - 55s - loss: 0.1069 - acc: 0.9618 - val_loss: 1.1817 - val_acc: 0.8069\n",
      "Epoch 18/25\n",
      " - 55s - loss: 0.0993 - acc: 0.9645 - val_loss: 1.1759 - val_acc: 0.7992\n",
      "Epoch 19/25\n",
      " - 55s - loss: 0.0929 - acc: 0.9667 - val_loss: 1.1876 - val_acc: 0.8015\n",
      "Epoch 20/25\n",
      " - 55s - loss: 0.0878 - acc: 0.9686 - val_loss: 1.2098 - val_acc: 0.8035\n",
      "Epoch 21/25\n",
      " - 55s - loss: 0.0837 - acc: 0.9702 - val_loss: 1.2804 - val_acc: 0.8032\n",
      "Epoch 22/25\n",
      " - 55s - loss: 0.0797 - acc: 0.9717 - val_loss: 1.2876 - val_acc: 0.8073\n",
      "Epoch 23/25\n",
      " - 55s - loss: 0.0775 - acc: 0.9722 - val_loss: 1.2775 - val_acc: 0.7952\n",
      "Epoch 24/25\n",
      " - 55s - loss: 0.0745 - acc: 0.9735 - val_loss: 1.3086 - val_acc: 0.8054\n",
      "Epoch 25/25\n",
      " - 55s - loss: 0.0724 - acc: 0.9743 - val_loss: 1.3736 - val_acc: 0.7945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14b10d27cf8>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train,y_train = get_training_sets()\n",
    "x_test,y_test = get_testing_sets()\n",
    "model.fit(x_train, y_train, validation_data=(x_test,y_test), epochs=25, verbose=2, batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the Model\n",
    "model.save('sentiment_analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
