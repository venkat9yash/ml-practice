{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THIS MODEL CREATES A CHATBOT FOR MOVIE CONVERSATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### DOWNLOAD AND CLEAN THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The logic behind this execution is that we will convert all the movie lines by thier id into a cosumable pandas df\n",
    "#The second part is enriching the conversation with details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('L1045', ['m0', 'BIANCA', 'They do not!']), ('L1044', ['m0', 'CAMERON', 'They do to!']), ('L985', ['m0', 'BIANCA', 'I hope so.']), ('L984', ['m0', 'CAMERON', 'She okay?']), ('L925', ['m0', 'BIANCA', \"Let's go.\"])]\n",
      "    0   1   2                              3\n",
      "0  u0  u2  m0  ['L194' 'L195' 'L196' 'L197']\n",
      "1  u0  u2  m0                ['L198' 'L199']\n",
      "2  u0  u2  m0  ['L200' 'L201' 'L202' 'L203']\n",
      "3  u0  u2  m0         ['L204' 'L205' 'L206']\n",
      "4  u0  u2  m0                ['L207' 'L208']\n",
      "    0         1   2                           3  4  5\n",
      "0  u0    BIANCA  m0  10 things i hate about you  f  4\n",
      "1  u1     BRUCE  m0  10 things i hate about you  ?  ?\n",
      "2  u2   CAMERON  m0  10 things i hate about you  m  3\n",
      "3  u3  CHASTITY  m0  10 things i hate about you  ?  ?\n",
      "4  u4      JOEY  m0  10 things i hate about you  m  6\n",
      "[('m0', ['10 things i hate about you']), ('m1', ['1492: conquest of paradise']), ('m2', ['15 minutes']), ('m3', ['2001: a space odyssey']), ('m4', ['48 hrs.'])]\n"
     ]
    }
   ],
   "source": [
    "#Function to convert a file into \n",
    "def read_csv_into_ds(file_name):\n",
    "    result = []\n",
    "    with open(file_name,encoding=\"utf8\") as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter='\\t')\n",
    "        for line in readCSV:\n",
    "            result.append(line)\n",
    "    return result\n",
    "    \n",
    "movie_lines = 'dialog-data/movie_lines.tsv'\n",
    "movie_conversations = 'dialog-data/movie_conversations.tsv'\n",
    "movie_characters = 'dialog-data/movie_characters_metadata.tsv'\n",
    "movie_titles = 'dialog-data/movie_titles_metadata.tsv'\n",
    "\n",
    "lines_ds = (read_csv_into_ds(movie_lines))\n",
    "lines_dict = {}\n",
    "for line in lines_ds:\n",
    "    if(len(line)==5):\n",
    "        lines_dict[line[0]] = [line[2],line[3],line[4]]\n",
    "\n",
    "conv_ds = (read_csv_into_ds(movie_conversations))\n",
    "conv_df = pd.DataFrame(conv_ds)\n",
    "\n",
    "chars_ds = (read_csv_into_ds(movie_characters))\n",
    "chars_df = pd.DataFrame(chars_ds).iloc[:, : 6]\n",
    "\n",
    "titles_ds = (read_csv_into_ds(movie_titles))\n",
    "titles_dict = {}\n",
    "for title in titles_ds:\n",
    "    if len(title) > 1:\n",
    "        titles_dict[title[0]] = [title[1]]\n",
    "    \n",
    "print(list(lines_dict.items())[:5])\n",
    "print(conv_df.head(5))\n",
    "print(chars_df.head(5))\n",
    "print(list(titles_dict.items())[:5])\n",
    "#Loading the movie lines data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Conversations: 79808\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('10 things i hate about you',\n",
       "   'BIANCA',\n",
       "   'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'),\n",
       "  ('10 things i hate about you',\n",
       "   'CAMERON',\n",
       "   \"Well I thought we'd start with pronunciation if that's okay with you.\"),\n",
       "  ('10 things i hate about you',\n",
       "   'BIANCA',\n",
       "   'Not the hacking and gagging and spitting part.  Please.'),\n",
       "  ('10 things i hate about you',\n",
       "   'CAMERON',\n",
       "   \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\")]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enrich the Conversation data and pull in list of conversations\n",
    "dialog_records = []\n",
    "for record in conv_df.values:\n",
    "    dialog_record = []        \n",
    "    dialoges = [x[1:-1] for x in record[3][1:-1].split(' ')]\n",
    "    for dialog in dialoges:\n",
    "        if dialog in lines_dict.keys() and record[2] in titles_dict.keys():\n",
    "            dia_details = lines_dict[dialog]\n",
    "            dialog_record.append((titles_dict[record[2]][0], dia_details[1], dia_details[2]))\n",
    "    if(len(dialog_record)!=0):        \n",
    "        dialog_records.append(dialog_record)\n",
    "print(\"Total Conversations:\" ,len(dialog_records))\n",
    "dialog_records[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['can we make this quick  roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad  again',\n",
       "  'well i thought wed start with pronunciation if thats okay with you',\n",
       "  'not the hacking and gagging and spitting part  please',\n",
       "  'okay then how bout we try out some french cuisine  saturday  night'],\n",
       " ['youre asking me out  thats so cute whats your name again', 'forget it'],\n",
       " ['no no its my fault  we didnt have a proper introduction ',\n",
       "  '<UNK>',\n",
       "  'the thing is cameron  im at the mercy of a particularly hideous breed of loser  my sister  i cant date until she does',\n",
       "  'seems like she could get a date easy enough'],\n",
       " ['why',\n",
       "  'unsolved mystery  she used to be really popular when she started high school then it was just like she got sick of it or something',\n",
       "  'thats a shame'],\n",
       " ['gosh if only we could find kat a boyfriend', 'let me see what i can do'],\n",
       " ['cesc ma tete this is my head',\n",
       "  'right  see  youre ready for the quiz',\n",
       "  'i dont want to know how to say that though  i want to know useful things like where the good stores are  how much does champagne cost  stuff like chat  i have never in my life had to point out my head to someone',\n",
       "  'thats because its such a nice one',\n",
       "  'forget french'],\n",
       " ['how is our little find the wench a date plan progressing',\n",
       "  'well theres someone i think might be '],\n",
       " ['there', 'where'],\n",
       " ['you got something on your mind',\n",
       "  'i counted on you to help my cause you and that thug are obviously failing arent we ever going on our date'],\n",
       " ['you have my word  as a gentleman', 'youre sweet']]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pick only the conversation data to make sequences\n",
    "\n",
    "dialog_sequences = list(map(lambda x:[re.sub('[^A-Za-z0-9\\s]+', '', y[2]).lower().replace(y[1].lower(), '<UNK>') for y in x] ,dialog_records))\n",
    "dialog_sequences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLEAN THE DATA AND TOKENIZE EVERY WORD IN THE DIAGLOUGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of Words: 62389\n",
      "Top 20 Frequent Words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('you', 119431),\n",
       " ('', 117300),\n",
       " ('i', 95026),\n",
       " ('the', 90589),\n",
       " ('to', 74725),\n",
       " ('a', 65163),\n",
       " ('it', 43736),\n",
       " ('and', 41320),\n",
       " ('of', 35982),\n",
       " ('that', 32368),\n",
       " ('in', 31250),\n",
       " ('what', 29757),\n",
       " ('me', 29550),\n",
       " ('is', 26814),\n",
       " ('dont', 22898),\n",
       " ('this', 22556),\n",
       " ('for', 21568),\n",
       " ('do', 21266),\n",
       " ('im', 20949),\n",
       " ('know', 20104)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counter = {}\n",
    "\n",
    "for conv in dialog_sequences:\n",
    "    for seq in conv:\n",
    "        for word in seq.lower().split(' '):#word_tokenize(seq.lower()):\n",
    "            if word in word_counter.keys():\n",
    "                word_counter[word] = word_counter[word] + 1\n",
    "            else:\n",
    "                word_counter[word] = 1\n",
    "print(\"Total Number of Words:\",len(word_counter.keys()))\n",
    "print(\"Top 20 Frequent Words\")\n",
    "#The first 3 are assigned to 1.START 2. END 3. PAD \n",
    "word_int = dict(list(zip(word_counter.keys(),range(4,len(word_counter.keys())+3))))\n",
    "sorted(word_counter.items(), key=lambda x: x[1],reverse=True)[:20]\n",
    "\n",
    "#TODO might have to remove the '' character sometimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[4,\n",
       "   5,\n",
       "   6,\n",
       "   7,\n",
       "   8,\n",
       "   9,\n",
       "   10,\n",
       "   11,\n",
       "   12,\n",
       "   13,\n",
       "   14,\n",
       "   15,\n",
       "   16,\n",
       "   17,\n",
       "   18,\n",
       "   19,\n",
       "   20,\n",
       "   21,\n",
       "   22,\n",
       "   23,\n",
       "   24,\n",
       "   25,\n",
       "   9,\n",
       "   26],\n",
       "  [27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 32, 37],\n",
       "  [38, 24, 39, 12, 40, 12, 41, 42, 9, 43],\n",
       "  [36, 44, 45, 46, 5, 47, 48, 49, 50, 51, 9, 52, 9, 53]]]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Figure out why there are NULL values\n",
    "def check_word(word):\n",
    "    if word in word_int.keys():\n",
    "        return word_int[word]\n",
    "    \n",
    "words_in_seq = list(map(lambda x: list(map(lambda y: [check_word(word) for word in y.lower().split(' ')], x)),dialog_sequences))\n",
    "words_in_seq[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### PREPARE TRAINING AND TESTING SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 3d data into 2d\n",
    "encoder_inputs = []\n",
    "decoder_inputs = []\n",
    "decoder_outputs = []\n",
    "\n",
    "for conversation in words_in_seq:\n",
    "    for id in range(0,len(conversation)-2):\n",
    "        # Adding START for decoder inputs and Appending END for decoder outputs\n",
    "        # Pad every sentence to 5 or 10 words\n",
    "        seq_len = 10\n",
    "        def pad(input_list, pad_len):\n",
    "            return input_list + [3 for x in range(0,pad_len-len(input_list))]\n",
    "        e_input = list(filter(None.__ne__, conversation[id]))\n",
    "        e_input = pad(e_input,seq_len)\n",
    "        d_input = list(filter(None.__ne__, conversation[id+1]))\n",
    "        d_input = [1] + pad(d_input,seq_len-1)\n",
    "        d_output = list(filter(None.__ne__, conversation[id+1]))#.append(2)\n",
    "        d_output = pad(d_output,seq_len-1)[:seq_len-1] + [2]\n",
    "        encoder_inputs.append(e_input[:seq_len])\n",
    "        decoder_inputs.append(d_input[:seq_len])\n",
    "        decoder_outputs.append(d_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 5, 6, 7, 8, 9, 10, 11, 12, 13], [27, 28, 29, 30, 31, 32, 33, 34, 35, 36], [64, 64, 65, 66, 67, 9, 5, 68, 69, 70], [73, 3, 3, 3, 3, 3, 3, 3, 3, 3], [97, 3, 3, 3, 3, 3, 3, 3, 3, 3]]\n",
      "[[1, 27, 28, 29, 30, 31, 32, 33, 34, 35], [1, 38, 24, 39, 12, 40, 12, 41, 42, 9], [1, 73, 3, 3, 3, 3, 3, 3, 3, 3], [1, 24, 74, 75, 76, 9, 77, 78, 24, 79], [1, 98, 99, 9, 89, 100, 101, 102, 103, 104]]\n",
      "[[27, 28, 29, 30, 31, 32, 33, 34, 35, 2], [38, 24, 39, 12, 40, 12, 41, 42, 9, 2], [73, 3, 3, 3, 3, 3, 3, 3, 3, 2], [24, 74, 75, 76, 9, 77, 78, 24, 79, 2], [98, 99, 9, 89, 100, 101, 102, 103, 104, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(encoder_inputs[:5])\n",
    "print(decoder_inputs[:5])\n",
    "print(decoder_outputs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### BUILD THE NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    inputs = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None])\n",
    "    learning_rate = tf.placeholder(tf.float32)\n",
    "    keep_probability = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    target_sequence_length = tf.placeholder(tf.int32, [None], name='target_sequence_length')\n",
    "    max_target_len = tf.placeholder(tf.int32, [None], name='max_target_len')\n",
    "    max_target_len = tf.reduce_max(target_sequence_length)\n",
    "    source = tf.placeholder(tf.int32, [None], name='source_sequence_length')\n",
    "    return inputs, targets, learning_rate, keep_probability, target_sequence_length, max_target_len, source"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_inputs, rnn_size, num_layers, keep_prob, \n",
    "                   source_sequence_length, source_vocab_size, \n",
    "                   encoding_embedding_size):\n",
    "    # Encoder embedding\n",
    "    enc_embed_input = tf.contrib.layers.embed_sequence(rnn_inputs, source_vocab_size,  )\n",
    "    \n",
    "        # RNN cell\n",
    "    def make_cell(rnn_size):\n",
    "        lstm_cell = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                           initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "        return tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob)\n",
    "\n",
    "    lstm_cell = tf.contrib.rnn.MultiRNNCell([make_cell(rnn_size) for _ in range(num_layers)])\n",
    "    \n",
    "    return tf.nn.dynamic_rnn(lstm_cell, enc_embed_input, sequence_length=source_sequence_length, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### NETWORK LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, batch_size,\n",
    "                  source_sequence_length, target_sequence_length,\n",
    "                  max_target_sentence_length,\n",
    "                  source_vocab_size, target_vocab_size,\n",
    "                  enc_embedding_size, dec_embedding_size,\n",
    "                  rnn_size, num_layers, target_vocab_to_int):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
